{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrape-ai-alignment-content.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPdyZvzFkJbsJbDrgyh5jBb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/ai-safety-scrape/blob/main/scrape_ai_alignment_content.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Contents from AI Alignment Resources\n"
      ],
      "metadata": {
        "id": "97CXz6XJM8q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "LYg2mYW2nGYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tika"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugHfLSwFnIjf",
        "outputId": "7639fb98-e1fc-4865-f41f-fd6bf907c274"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tika in /usr/local/lib/python3.7/dist-packages (1.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (57.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "4XsCNDx4nKBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tika import parser\n",
        "from google.colab import drive\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "LaV6DoUenFyD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Environment"
      ],
      "metadata": {
        "id": "JGEHyxtIrABU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7EXurg4rE4U",
        "outputId": "4d473dd4-7319-45db-d226-599262ceeec7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x7qM-qXrGCn",
        "outputId": "f7dc935a-c3c2-4407-e107-feaa5ea9aecd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CODE_DIR = Path('.') / 'code-projects/gpt-ai-safety'\n",
        "DATA_DIR = Path('.') / 'data/raw_texts'"
      ],
      "metadata": {
        "id": "nrQERsuk8lRe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {CODE_DIR}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdvzqbWx8yrK",
        "outputId": "6d408d15-6ed5-4620-aabc-c4ae02633a7a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/code-projects/gpt-ai-safety\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JayThibs/ai-safety-scrape\n",
        "!mv ai-safety-scrape/* ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDsiFyBfxC8H",
        "outputId": "428593bd-5128-42fe-d92a-4fd6a044574f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai-safety-scrape'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 17 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Explore Data"
      ],
      "metadata": {
        "id": "Z6UiH03NAdoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('ai-alignment-papers.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "cJit0jX1An6S",
        "outputId": "cc07b42d-7c73-4f95-97ef-c6ca7bbfffd6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-65d4622d-7a98-4052-a1b3-02ef1b710455\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Key</th>\n",
              "      <th>Item Type</th>\n",
              "      <th>Publication Year</th>\n",
              "      <th>Author</th>\n",
              "      <th>Title</th>\n",
              "      <th>Publication Title</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>ISSN</th>\n",
              "      <th>DOI</th>\n",
              "      <th>Url</th>\n",
              "      <th>Abstract Note</th>\n",
              "      <th>Date</th>\n",
              "      <th>Date Added</th>\n",
              "      <th>Date Modified</th>\n",
              "      <th>Access Date</th>\n",
              "      <th>Pages</th>\n",
              "      <th>Num Pages</th>\n",
              "      <th>Issue</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Number Of Volumes</th>\n",
              "      <th>Journal Abbreviation</th>\n",
              "      <th>Short Title</th>\n",
              "      <th>Series</th>\n",
              "      <th>Series Number</th>\n",
              "      <th>Series Text</th>\n",
              "      <th>Series Title</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>Place</th>\n",
              "      <th>Language</th>\n",
              "      <th>Rights</th>\n",
              "      <th>Type</th>\n",
              "      <th>Archive</th>\n",
              "      <th>Archive Location</th>\n",
              "      <th>Library Catalog</th>\n",
              "      <th>Call Number</th>\n",
              "      <th>Extra</th>\n",
              "      <th>Notes</th>\n",
              "      <th>File Attachments</th>\n",
              "      <th>Link Attachments</th>\n",
              "      <th>Manual Tags</th>\n",
              "      <th>...</th>\n",
              "      <th>Cast Member</th>\n",
              "      <th>Commenter</th>\n",
              "      <th>Composer</th>\n",
              "      <th>Cosponsor</th>\n",
              "      <th>Counsel</th>\n",
              "      <th>Interviewer</th>\n",
              "      <th>Producer</th>\n",
              "      <th>Recipient</th>\n",
              "      <th>Reviewed Author</th>\n",
              "      <th>Scriptwriter</th>\n",
              "      <th>Words By</th>\n",
              "      <th>Guest</th>\n",
              "      <th>Number</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Running Time</th>\n",
              "      <th>Scale</th>\n",
              "      <th>Medium</th>\n",
              "      <th>Artwork Size</th>\n",
              "      <th>Filing Date</th>\n",
              "      <th>Application Number</th>\n",
              "      <th>Assignee</th>\n",
              "      <th>Issuing Authority</th>\n",
              "      <th>Country</th>\n",
              "      <th>Meeting Name</th>\n",
              "      <th>Conference Name</th>\n",
              "      <th>Court</th>\n",
              "      <th>References</th>\n",
              "      <th>Reporter</th>\n",
              "      <th>Legal Status</th>\n",
              "      <th>Priority Numbers</th>\n",
              "      <th>Programming Language</th>\n",
              "      <th>Version</th>\n",
              "      <th>System</th>\n",
              "      <th>Code</th>\n",
              "      <th>Code Number</th>\n",
              "      <th>Section</th>\n",
              "      <th>Session</th>\n",
              "      <th>Committee</th>\n",
              "      <th>History</th>\n",
              "      <th>Legislative Body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XBZAPQFK</td>\n",
              "      <td>blogPost</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Kokotajlo, Daniel</td>\n",
              "      <td>Three kinds of competitiveness</td>\n",
              "      <td>AI Impacts</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://aiimpacts.org/three-kinds-of-competiti...</td>\n",
              "      <td>By Daniel Kokotajlo In this post, I distinguis...</td>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>2022-01-30 01:53:10</td>\n",
              "      <td>2022-01-30 01:53:10</td>\n",
              "      <td>2021-11-20 18:55:39</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en-US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/PU9A2KS...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HX9UZ5JP</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Cihon, Peter; Maas, Matthijs M.; Kemp, Luke</td>\n",
              "      <td>Fragmentation and the Future: Investigating Ar...</td>\n",
              "      <td>Global Policy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1758-5899</td>\n",
              "      <td>10.1111/1758-5899.12890</td>\n",
              "      <td>https://onlinelibrary.wiley.com/doi/abs/10.111...</td>\n",
              "      <td>The international governance of artificial int...</td>\n",
              "      <td>2020</td>\n",
              "      <td>2022-01-30 04:47:43</td>\n",
              "      <td>2022-01-30 04:47:43</td>\n",
              "      <td>2021-11-13 15:58:24</td>\n",
              "      <td>545-556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fragmentation and the Future</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Wiley Online Library</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000010  _eprint: https://onlinelibrary....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/2TZBI3F...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BQCZM53S</td>\n",
              "      <td>blogPost</td>\n",
              "      <td>2021.0</td>\n",
              "      <td>Clarke, Sam; Martin, Samuel Dylan</td>\n",
              "      <td>Distinguishing AI takeover scenarios</td>\n",
              "      <td>AI Alignment Forum</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.alignmentforum.org/posts/qYzqDtoQa...</td>\n",
              "      <td>Epistemic status: lots of this involves interp...</td>\n",
              "      <td>2021-09-08</td>\n",
              "      <td>2022-01-30 04:47:42</td>\n",
              "      <td>2022-01-30 04:47:42</td>\n",
              "      <td>2021-11-18 23:45:23</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: NoCitationData[s0]  ACC: N/A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/ENAMQXC...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JVMJ4RMM</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Stray, Jonathan</td>\n",
              "      <td>Aligning AI Optimization to Community Well-Being</td>\n",
              "      <td>International Journal of Community Well-Being</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2524-5295, 2524-5309</td>\n",
              "      <td>10.1007/s42413-020-00086-3</td>\n",
              "      <td>http://link.springer.com/10.1007/s42413-020-00...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-12</td>\n",
              "      <td>2022-01-30 04:47:36</td>\n",
              "      <td>2022-01-30 04:47:36</td>\n",
              "      <td>2021-11-13 22:47:54</td>\n",
              "      <td>443-463</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Int. Journal of Com. WB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DOI.org (Crossref)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000010</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/V3BEV7X...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>W8F6VI9I</td>\n",
              "      <td>thesis</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Shah, Rohin Monish</td>\n",
              "      <td>Extracting and Using Preference Information fr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.proquest.com/openview/da8bf63ef343...</td>\n",
              "      <td>Typically when learning about what people want...</td>\n",
              "      <td>2020-12-17</td>\n",
              "      <td>2022-01-30 04:47:35</td>\n",
              "      <td>2022-01-30 04:47:35</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>University of California, Berkeley</td>\n",
              "      <td>Berkeley, CA</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Zotero</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/S96M3KT...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 87 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65d4622d-7a98-4052-a1b3-02ef1b710455')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-65d4622d-7a98-4052-a1b3-02ef1b710455 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-65d4622d-7a98-4052-a1b3-02ef1b710455');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Key       Item Type  ...  History Legislative Body\n",
              "0  XBZAPQFK        blogPost  ...      NaN              NaN\n",
              "1  HX9UZ5JP  journalArticle  ...      NaN              NaN\n",
              "2  BQCZM53S        blogPost  ...      NaN              NaN\n",
              "3  JVMJ4RMM  journalArticle  ...      NaN              NaN\n",
              "4  W8F6VI9I          thesis  ...      NaN              NaN\n",
              "\n",
              "[5 rows x 87 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Item Type'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9EhbqMYAv2_",
        "outputId": "68e54396-9db1-4cfa-d2e6-ff1cf92287ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['blogPost', 'journalArticle', 'thesis', 'conferencePaper',\n",
              "       'manuscript', 'report', 'bookSection', 'magazineArticle', 'book'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "item_nums = []\n",
        "for item in df['Item Type'].unique():\n",
        "    item_nums.append([item, len(df[df['Item Type'] == item])])\n",
        "\n",
        "item_nums.sort(key=lambda x:x[1])\n",
        "item_nums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsXK6OGkGA_F",
        "outputId": "f44fa688-b0c1-41f1-8faf-ad5f38c7efde"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['magazineArticle', 2],\n",
              " ['thesis', 3],\n",
              " ['book', 13],\n",
              " ['bookSection', 52],\n",
              " ['report', 87],\n",
              " ['manuscript', 154],\n",
              " ['journalArticle', 170],\n",
              " ['conferencePaper', 262],\n",
              " ['blogPost', 421]]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Item Type'] == 'journalArticle'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "niZZINK1Av78",
        "outputId": "de04b0c5-ca2b-4c4c-8e6a-1439f1d59637"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-03b78f72-3e27-4883-bba2-de9db6cf6f0d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Key</th>\n",
              "      <th>Item Type</th>\n",
              "      <th>Publication Year</th>\n",
              "      <th>Author</th>\n",
              "      <th>Title</th>\n",
              "      <th>Publication Title</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>ISSN</th>\n",
              "      <th>DOI</th>\n",
              "      <th>Url</th>\n",
              "      <th>Abstract Note</th>\n",
              "      <th>Date</th>\n",
              "      <th>Date Added</th>\n",
              "      <th>Date Modified</th>\n",
              "      <th>Access Date</th>\n",
              "      <th>Pages</th>\n",
              "      <th>Num Pages</th>\n",
              "      <th>Issue</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Number Of Volumes</th>\n",
              "      <th>Journal Abbreviation</th>\n",
              "      <th>Short Title</th>\n",
              "      <th>Series</th>\n",
              "      <th>Series Number</th>\n",
              "      <th>Series Text</th>\n",
              "      <th>Series Title</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>Place</th>\n",
              "      <th>Language</th>\n",
              "      <th>Rights</th>\n",
              "      <th>Type</th>\n",
              "      <th>Archive</th>\n",
              "      <th>Archive Location</th>\n",
              "      <th>Library Catalog</th>\n",
              "      <th>Call Number</th>\n",
              "      <th>Extra</th>\n",
              "      <th>Notes</th>\n",
              "      <th>File Attachments</th>\n",
              "      <th>Link Attachments</th>\n",
              "      <th>Manual Tags</th>\n",
              "      <th>...</th>\n",
              "      <th>Cast Member</th>\n",
              "      <th>Commenter</th>\n",
              "      <th>Composer</th>\n",
              "      <th>Cosponsor</th>\n",
              "      <th>Counsel</th>\n",
              "      <th>Interviewer</th>\n",
              "      <th>Producer</th>\n",
              "      <th>Recipient</th>\n",
              "      <th>Reviewed Author</th>\n",
              "      <th>Scriptwriter</th>\n",
              "      <th>Words By</th>\n",
              "      <th>Guest</th>\n",
              "      <th>Number</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Running Time</th>\n",
              "      <th>Scale</th>\n",
              "      <th>Medium</th>\n",
              "      <th>Artwork Size</th>\n",
              "      <th>Filing Date</th>\n",
              "      <th>Application Number</th>\n",
              "      <th>Assignee</th>\n",
              "      <th>Issuing Authority</th>\n",
              "      <th>Country</th>\n",
              "      <th>Meeting Name</th>\n",
              "      <th>Conference Name</th>\n",
              "      <th>Court</th>\n",
              "      <th>References</th>\n",
              "      <th>Reporter</th>\n",
              "      <th>Legal Status</th>\n",
              "      <th>Priority Numbers</th>\n",
              "      <th>Programming Language</th>\n",
              "      <th>Version</th>\n",
              "      <th>System</th>\n",
              "      <th>Code</th>\n",
              "      <th>Code Number</th>\n",
              "      <th>Section</th>\n",
              "      <th>Session</th>\n",
              "      <th>Committee</th>\n",
              "      <th>History</th>\n",
              "      <th>Legislative Body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HX9UZ5JP</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Cihon, Peter; Maas, Matthijs M.; Kemp, Luke</td>\n",
              "      <td>Fragmentation and the Future: Investigating Ar...</td>\n",
              "      <td>Global Policy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1758-5899</td>\n",
              "      <td>10.1111/1758-5899.12890</td>\n",
              "      <td>https://onlinelibrary.wiley.com/doi/abs/10.111...</td>\n",
              "      <td>The international governance of artificial int...</td>\n",
              "      <td>2020</td>\n",
              "      <td>2022-01-30 04:47:43</td>\n",
              "      <td>2022-01-30 04:47:43</td>\n",
              "      <td>2021-11-13 15:58:24</td>\n",
              "      <td>545-556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fragmentation and the Future</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Wiley Online Library</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000010  _eprint: https://onlinelibrary....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/2TZBI3F...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JVMJ4RMM</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Stray, Jonathan</td>\n",
              "      <td>Aligning AI Optimization to Community Well-Being</td>\n",
              "      <td>International Journal of Community Well-Being</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2524-5295, 2524-5309</td>\n",
              "      <td>10.1007/s42413-020-00086-3</td>\n",
              "      <td>http://link.springer.com/10.1007/s42413-020-00...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-12</td>\n",
              "      <td>2022-01-30 04:47:36</td>\n",
              "      <td>2022-01-30 04:47:36</td>\n",
              "      <td>2021-11-13 22:47:54</td>\n",
              "      <td>443-463</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Int. Journal of Com. WB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DOI.org (Crossref)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000010</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/V3BEV7X...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>TK5F29IU</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2021.0</td>\n",
              "      <td>Hayden, Benjamin; Niv, Yael</td>\n",
              "      <td>The case against economic values in the orbito...</td>\n",
              "      <td>Behavioral Neuroscience</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.1037/bne0000448</td>\n",
              "      <td>https://osf.io/7hgup</td>\n",
              "      <td>Much of traditional neuroeconomics proceeds fr...</td>\n",
              "      <td>2021</td>\n",
              "      <td>2022-01-30 04:48:47</td>\n",
              "      <td>2022-01-30 04:48:47</td>\n",
              "      <td>2021-11-08 23:41:47</td>\n",
              "      <td>192-201</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>135.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DOI.org (Crossref)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000026  DOI: 10.31234/osf.io/7hgup</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>NHWZIKZ2</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Fernandes, Pedro; Santos, Francisco C.; Lopes,...</td>\n",
              "      <td>Norms for Beneficial A.I.: A Computational Ana...</td>\n",
              "      <td>AI Communications</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18758452, 09217126</td>\n",
              "      <td>10.3233/AIC-201502</td>\n",
              "      <td>http://arxiv.org/abs/1907.03843</td>\n",
              "      <td>The rise of artificial intelligence (A.I.) bas...</td>\n",
              "      <td>2020-12-18</td>\n",
              "      <td>2022-01-30 04:48:46</td>\n",
              "      <td>2022-01-30 04:48:46</td>\n",
              "      <td>2021-11-13 22:40:37</td>\n",
              "      <td>155-171</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3-6</td>\n",
              "      <td>33.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIC</td>\n",
              "      <td>Norms for Beneficial A.I.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>arXiv.org</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000004  arXiv: 1907.03843</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/JAVXSVN...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>HDWGJGAP</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2021.0</td>\n",
              "      <td>Mingard, Chris; Valle-PÃ©rez, Guillermo; Skalse...</td>\n",
              "      <td>Is SGD a Bayesian sampler? Well, almost</td>\n",
              "      <td>Journal of Machine Learning Research</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://arxiv.org/abs/2006.15191</td>\n",
              "      <td>Overparameterised deep neural networks (DNNs) ...</td>\n",
              "      <td>2021-02</td>\n",
              "      <td>2022-01-30 04:48:46</td>\n",
              "      <td>2022-01-30 04:48:46</td>\n",
              "      <td>2021-11-13 22:56:31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Is SGD a Bayesian sampler?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>arXiv.org</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000009  arXiv: 2006.15191</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/ACV9IXE...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 87 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03b78f72-3e27-4883-bba2-de9db6cf6f0d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-03b78f72-3e27-4883-bba2-de9db6cf6f0d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-03b78f72-3e27-4883-bba2-de9db6cf6f0d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Key       Item Type  ...  History Legislative Body\n",
              "1   HX9UZ5JP  journalArticle  ...      NaN              NaN\n",
              "3   JVMJ4RMM  journalArticle  ...      NaN              NaN\n",
              "11  TK5F29IU  journalArticle  ...      NaN              NaN\n",
              "25  NHWZIKZ2  journalArticle  ...      NaN              NaN\n",
              "30  HDWGJGAP  journalArticle  ...      NaN              NaN\n",
              "\n",
              "[5 rows x 87 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DTjcEYYLAwAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qrJ1qOf1AwGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hA2EL4f0AwLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4-xztLuoAwPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract text from AI Alignment Resources"
      ],
      "metadata": {
        "id": "q4WhBM5lrPyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tikaTextExtractor(file_path):\n",
        "    \"\"\"Extracts text from a PDF using tika.\"\"\"\n",
        "    print(\"Extracting text from file: \" + file_path)\n",
        "    parsed_tika = parser.from_file(file_path)\n",
        "    return parsed_tika[\"content\"]"
      ],
      "metadata": {
        "id": "GtcCbZzznPE3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tikaTextExtractor(str(DATA_DIR / '2112.00861.pdf'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "Bvbi8ZEqnWTJ",
        "outputId": "ba96eaf9-e423-49f7-b06f-44b6d34d15bd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-02 19:46:32,997 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from file: data/raw_texts/2112.00861.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-02 19:46:33,530 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2022-02-02 19:46:33,855 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA General Language Assistant\\nas a Laboratory for Alignment\\n\\nAmanda Askellâˆ— Yuntao Baiâˆ— Anna Chenâˆ— Dawn Drainâˆ— Deep Ganguliâˆ— Tom Henighanâ€ \\n\\nAndy Jonesâ€  Nicholas Josephâ€  Ben Mannâˆ— Nova DasSarma Nelson Elhage\\n\\nZac Hatfield-Dodds Danny Hernandez Jackson Kernion Kamal Ndousse\\n\\nCatherine Olsson Dario Amodei Tom Brown Jack Clark Sam McCandlish Chris Olah\\n\\nJared Kaplanâ€¡\\n\\nAnthropic\\n\\nAbstract\\n\\nGiven the broad capabilities of large language models, it should be possible to work towards\\na general-purpose, text-based assistant that is aligned with human values, meaning that it is\\nhelpful, honest, and harmless. As an initial foray in this direction we study simple baseline\\ntechniques and evaluations, such as prompting. We find that the benefits from modest\\ninterventions increase with model size, generalize to a variety of alignment evaluations, and\\ndo not compromise the performance of large models. Next we investigate scaling trends\\nfor several training objectives relevant to alignment, comparing imitation learning, binary\\ndiscrimination, and ranked preference modeling. We find that ranked preference modeling\\nperforms much better than imitation learning, and often scales more favorably with model\\nsize. In contrast, binary discrimination typically performs and scales very similarly to\\nimitation learning. Finally we study a â€˜preference model pre-trainingâ€™ stage of training,\\nwith the goal of improving sample efficiency when finetuning on human preferences.\\n\\nâˆ—Core Research Contributors\\nâ€ Core Infrastructure Contributors\\nâ€¡Correspondence to: jared@anthropic.com\\n\\nAuthor contributions are listed at the end of the paper.\\n\\nar\\nX\\n\\niv\\n:2\\n\\n11\\n2.\\n\\n00\\n86\\n\\n1v\\n3 \\n\\n [\\ncs\\n\\n.C\\nL\\n\\n] \\n 9\\n\\n D\\nec\\n\\n 2\\n02\\n\\n1\\n\\n\\n\\nContents\\n\\n1 Introduction 3\\n\\n1.1 Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n\\n1.2 Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n\\n1.3 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n\\n2 Conditioning on Aligned Behavior 9\\n\\n2.1 Context Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n\\n2.2 Evaluations and Alignment Taxes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n\\n3 Scaling of Preference Modeling vs Imitation Learning 14\\n\\n3.1 Loss and Settings for Preference Modeling and Imitation Learning . . . . . . . . . . . . . . 15\\n\\n3.2 Performance and Scaling Results for Ranked versus Binary Preference Datasets . . . . . . . 16\\n\\n4 Preference Model Pre-Training and Transfer 20\\n\\n4.1 PMP and Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n\\n4.2 Finetuning Results and Scaling Trends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n\\n4.3 Ranked Preference Modeling vs Binary Discrimination for PMP . . . . . . . . . . . . . . . 23\\n\\n4.4 Human-Model vs Human-Human Comparisons for PMP . . . . . . . . . . . . . . . . . . . 24\\n\\n5 Discussion 24\\n\\n5.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n\\n5.2 Broader Impacts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n\\n5.3 Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n\\nA Language Model Pre-training 27\\n\\nB More Details on Prompting, Context Distillation, and Evaluations 29\\n\\nC More Details on Preference Models 34\\n\\nD Per-Token GAN-Style Discriminator Results 40\\n\\nE Definitions of Alignment and the HHH criteria 44\\n\\n2\\n\\n\\n\\nFigure 1 We show the format of interactions with AI models for A/B testing and human feedback collection.\\nAs indicated by the example interaction here, one can get help from the model with any text-based task.\\n\\n1 Introduction\\n\\n1.1 Motivations\\n\\nContemporary AI models can be difficult to understand, predict, and control. These problems can lead\\nto significant harms when AI systems are deployed, and might produce truly devastating results if future\\nsystems are even more powerful and more widely used, and interact with each other and the world in presently\\nunforeseeable ways.\\n\\nThis paper shares some nascent work towards one of our primary, ongoing goals, which is to align general-\\npurpose AI systems with human preferences and values. A great deal of ink has been spilled trying to define\\nwhat it means for AI systems to be aligned, and to guess at how this might go wrong. We will define an AI\\nas â€œalignedâ€ if it is, in three words, helpful, honest, and harmless or â€˜HHHâ€™. Our alignment efforts aim to\\nmeasure and address this general problem with large language models.\\n\\nMany researchers and organizations share this goal, but few have pursued it directly. Most research efforts\\nassociated with alignment either only pertain to very specialized systems, involve testing a specific alignment\\ntechnique on a sub-problem, or are rather speculative and theoretical. Our view is that if itâ€™s possible to\\ntry to address a problem directly, then one needs a good excuse for not doing so. Historically we had such\\nan excuse: general purpose, highly capable AIs were not available for investigation. But given the broad\\ncapabilities of large language models, we think itâ€™s time to tackle alignment directly, and that a research\\nprogram focused on this goal may have the greatest chance for impact. Furthermore:\\n\\nâ€¢ A natural language agent can be subjected to a wide variety of inputs, and so it can fail to be\\nhelpful, honest, and harmless in myriad ways. We believe itâ€™s valuable to try to see the full picture\\nof where weâ€™ve made progress on alignment, and where weâ€™re currently falling short. This may\\nremain obscure absent efforts to train general aligned agents and allow them to be probed in any way\\nwhatsoever. A very broad definition can also facilitate measurement, since it invites the examiner to\\npose a wide-variety of challenges.\\n\\n3\\n\\n\\n\\nâ€¢ By studying a variety of alignment techniques in a general setting, it becomes much easier to com-\\npare them and to determine which techniques are simplest and most effective. Some techniques,\\nsuch as the use of human feedback, are complex and potentially costly, so weâ€™re interested in strate-\\ngies that can increase their efficiency and focus their application exclusively on goals that cannot be\\nattained more easily in another way.\\n\\nâ€¢ Some view alignment as a highly speculative problem, or one that distracts from work on more\\npressing issues with existing AI systems. In our view, the societal impacts of current AI models\\nshould be taken seriously, and the evaluation of current models should be seen as an essential safety\\nproject. We believe that training a large language model to be helpful, honest, and harmless (we are\\nnot claiming to have achieved this goal!) would represent significant progress towards alleviating\\nthe negative societal impacts from general-purpose language models.\\n\\nâ€¢ Some of the researchers who are most concerned about the alignment problem believe that aligning\\nextremely capable AIs will be qualitatively different from aligning current more limited systems.\\nWe share this concern, but we believe the best vantage point from which to explore alignment for\\nincreasingly advanced AIs will be to first establish an aligned baseline at current capability levels. If\\nthis were successful, we would then turn to the task of studying progress more deeply, including its\\nscaling properties, and attempt to adversarially validate it. Conversely, if we and others persistently\\nfail, we can identify the thorniest issues with alignment. Halting progress would also provide a\\npersuasive argument for allocating more and more resources towards AI alignment, and for more\\ncautious norms around scaling up and deploying models.\\n\\nIn pursuit of these goals, in this work we will be investigating the following questions:\\n\\nâ€¢ Is naive prompting a workable baseline for alignment? How does it scale, how does it compare\\nto finetuning, and how can we leverage its advantages? We find that prompts induce favorable\\nscaling on a variety of alignment-relevant evaluations, impose negligible â€˜taxesâ€™ on large models,\\nand can be â€˜context distilledâ€™ back into the original model.\\n\\nâ€¢ When and how much does preference modeling improve on imitation learning? We find that\\npreference modeling improves on and scales more favorably than imitation learning when prefer-\\nences are part of a ranked hierarchy or continuum (e.g. rank these responses in order of helpfulness),\\nrather than associated with a binary choice (e.g. does this python function pass tests).\\n\\nâ€¢ How can we improve the sample efficiency of preference modeling? We find that we can signif-\\nicantly improve sample efficiency using a â€˜preference model pre-trainingâ€™ (PMP) stage of training,\\nwhere we first pre-train on large public datasets that encode human preference information, such as\\nStack Exchange, Reddit, and Wikipedia edits, before finetuning on smaller datasets encoding more\\nspecific human preferences.\\n\\nThe last two points are particularly important for work using reinforcement learning (RL) for alignment,\\nwhere the reward signals are predicted by a preference model. In particular, we expect bandit-type RL perfor-\\nmance to improve roughly in proportion with preference modeling capabilities, since the preference modelâ€™s\\nrecognition of high-performance behavior should be closely related to the RL agentâ€™s ability to achieve it. We\\nanticipate that such a strategy can outperform imitation learning on some problems, especially those whose\\nsolutions lie on a ranked hierarchy. A similar approach applying human feedback to greatly improve the\\nperformance of language models on summary-writing had already been demonstrated [SOW+20].\\n\\nWhat are Helpfulness, Honesty, and Harmlessness?\\n\\nWe chose â€˜helpful, honest, and harmlessâ€™ as criteria because they are simple and memorable, and seem to\\ncapture the majority of what we want from an aligned1 AI. But these are also subtle and ambiguous criteria,\\nand the best AI behavior will involve a compromise between them. For example, there will clearly be conflicts\\nbetween helpfulness to the user and harmlessness to others if agents are asked to aid in harmful activities.\\nHere are some very brief notes on these terms:\\n\\nHelpful:\\n\\nâ€¢ The AI should make a clear attempt to perform the task or answer the question posed (as long as this\\nisnâ€™t harmful). It should do this as concisely and efficiently as possible.\\n\\n1We also discuss the HHH specification in relation to our broader understanding of alignment in Appendix E.\\n\\n4\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\nFr\\nac\\n\\ntio\\nn \\n\\nPa\\nss\\n\\nin\\ng \\n\\nTe\\nst\\n\\ns\\n\\nCodex Evaluations and Alignment Tax/Bonus\\nCodex Pass@10\\nCodex w/ HHH Prompt Pass@10\\n\\nFigure 2 Left: Simple prompting significantly improves performance and scaling on our HHH alignment\\nevaluations (y-axis measures accuracy at choosing better responses on our HHH evaluations). Right: Prompts\\nimpose little or no â€˜alignment taxâ€™ on large models, even on complex evaluations like function synthesis.\\nHere we have evaluated our python code models on the HumanEval codex dataset [CTJ+21] at temperature\\nT = 0.6 and top P = 0.95.\\n\\nâ€¢ When more information is required, the AI should ask relevant follow-up questions and obtain nec-\\nessary details. It should respond with appropriate levels of sensitivity, insight, and discretion.\\n\\nâ€¢ Ideally the AI will also re-direct ill-informed requests, e.g. if asked â€˜how can I build a website in\\nassembly languageâ€™ it might suggest a different approach.\\n\\nHonest:\\n\\nâ€¢ At its most basic level, the AI should give accurate information. Moreover, it should be calibrated\\n(e.g. it should be correct 80% of the time when it claims 80% confidence) and express appropriate\\nlevels of uncertainty. It should express its uncertainty without misleading human users.\\n\\nâ€¢ Crucially, the AI should be honest about its own capabilities and levels of knowledge â€“ it is not\\nsufficient for it to simply imitate the responses expected from a seemingly humble and honest expert.\\n\\nâ€¢ Ideally the AI would also be honest about itself and its own internal state, insofar as that information\\nis available to it.\\n\\nâ€¢ Honesty is more objective than helpfulness and harmlessness, so more aspects of honesty training\\nmay be possible without human input. This might include calibration training on factual claims and\\nclaims about the internal state of the model, and the use of search [KSW21] to augment accuracy.\\n\\nHarmless:\\n\\nâ€¢ The AI should not be offensive or discriminatory, either directly or through subtext or bias.\\nâ€¢ When asked to aid in a dangerous act (e.g. building a bomb), the AI should politely refuse. Ideally\\n\\nthe AI will recognize disguised attempts to solicit help for nefarious purposes.\\nâ€¢ To the best of its abilities, the AI should recognize when it may be providing very sensitive or\\n\\nconsequential advice and act with appropriate modesty and care.\\nâ€¢ What behaviors are considered harmful and to what degree will vary across people and cultures. It\\n\\nwill also be context-dependent, i.e. it will depend on the nature of the user query, who is using the\\nAI assistant, and the time and place in which the assistant is being used.\\n\\nAll of these criteria are at least somewhat subjective, and those who deploy an AI will need to take responsi-\\nbility for the way that alignment is defined and the extent to which it has been attained.\\n\\n1.2 Research\\n\\nOpen-Ended Dialogue Format and Prompting\\n\\nWe use open-ended natural language dialogue for interaction with our models, with an example pictured in\\nfigure 1. We allow for general inputs of essentially arbitrary length from human users, which can include\\n\\n5\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\n0.30\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Preference Modeling Over Imitation Learning\\nMean Over Ranked Evals\\nMean Over Binary Evals\\n\\nFigure 3 In this figure the y-axis measures the accuracy difference of preference modeling compared to\\nimitation learning, where evaluations have been categorized as having either ranked or binary preferences.\\nThe light blue curves show ranked evaluations from Learn to Summarize, HellaSwag, and Utilitarianism\\n(ethics); while light orange curves show binary evaluations from Code Correctness, Lambada, Commonsense\\nMorality (ethics), Justice (ethics), Deontology (ethics), and Virtue (ethics). Dark colored curves show the\\nmean over light curves of the same color. All these datasets are evaluated by some form of accuracy, although\\nthe specific interpretation is different in each case (e.g., multiple choice accuracy for HellaSwag, pairwise\\ncomparison accuracy for Learn to Summarize; see section 3.2). We see that on ranked evaluations, PM\\nperforms and scales significantly better than IL (blue), while on binary evaluations there is little discernible\\ndifference (orange). The 52B Code Correctness is excluded due to significant compute needed to generate\\ncode samples.\\n\\nexamples, documents, programming code, etc, and we allow similarly general responses from our models.\\nModels indicate they have completed a response by generating a stop sequence, which is literally the string\\nHuman: used to designate roles in the dialogue. By default we show two responses and allow users to\\nchoose one. We typically request that users pick the most helpful and honest response, as pictured. We use\\nthis interface both to A/B test different models and to collect human feedback data. We can use a very similar\\ninterface for other safety-related tasks, such as red-teaming the model against harmfulness.\\n\\nTo evaluate performance we created a small dataset of evaluations associated with helpfulness, honesty,\\nharms, and other behaviors in this interactive format. We are sharing these evaluations on BIG Bench for oth-\\ners to try. We also evaluate models and interventions via A/B testing with humans, who have been instructed\\nto solicit modelsâ€™ help with arbitrary text-based tasks.\\n\\nLarge language models engage in few-shot learning [BMR+20]. To generically elicit the sort of behavior\\nshown in figure 1, we found that it was sufficient to provide a long prompt (4600 words from 14 fictional\\nconversations) with example interactions. The prompt we used was not carefully designed or optimized for\\nperformance on evaluations; rather it was just written by two of us in an ad hoc manner prior to the construc-\\ntion of any evaluations. Despite the fact that our prompt2 did not include any examples where models resisted\\nmanipulation, refused requests to aid in dangerous activities, or took a stand against unsavory behavior, we\\nobserved that models often actively avoided engaging in harmful behaviors based only on the AI â€˜personalityâ€™\\nimbued by the prompt. This is reflected in the performance trends on harmfulness in figure 6.\\n\\nIn section 2 we explore the effects of the prompt. In the small data limit, prompting a generative language\\nmodel may be qualitatively different from and superior to finetuning, since prompting imposes a prior, while\\nfinetuning alters the modelâ€™s expectations for the underlying data distribution. We make several points con-\\ncerning prompting:\\n\\nâ€¢ We find that prompting can be superior to finetuning in the limit of very small datasets associated\\nwith alignment.\\n\\n2Prompt text and contractor instructions are at https://gist.github.com/jareddk/2509330f8ef3d787fc5aaac67aab5f11\\n\\n6\\n\\nhttps://github.com/google/BIG-bench\\nhttps://gist.github.com/jareddk/2509330f8ef3d787fc5aaac67aab5f11\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.00\\n\\n0.02\\n\\n0.04\\n\\n0.06\\n\\n0.08\\n\\n0.10\\n\\n0.12\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nMean Acc Gain of PMP on Finetuning (52B)\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\nFigure 4 Performance gain of preference model pre-training on finetuning evaluations, as measured by\\naccuracy difference relative to no PMP. Different colors represent different PMP datasets, including Stack-\\nExchange, Reddit, Wikipedia, and a â€˜Mixâ€™ of all three. Each line represents a combined (mean) result from\\nLearn to Summarize, HellaSwag, and all five Ethics evaluations. Results are shown for the 52B parameter\\nmodel only, but similar positive results were also seen for the smaller models.\\n\\nâ€¢ The prompt context â€˜Câ€™ can be distilled into a new language model that models the distribution\\nP (X|C) instead of P (X); this is accomplished by simply finetuning with a loss given by the KL di-\\nvergence between P (X|C) and the distilled modelâ€™s predictions. This procedure has more beneficial\\neffects as compared to finetuning on the prompt.\\n\\nâ€¢ The capabilities of small models (e.g. on NLP or coding evaluations) are typically diminished in the\\npresence of the prompt, presumably because they are confused by it. But larger models perform at\\nroughly the same level with or without the prompt.\\n\\nSo perhaps prompt-related techniques can carry alignment efforts further than we initially expected.\\n\\nNevertheless, we believe that as an approach to alignment, prompt design will have significant limitations.\\nOne concern is that prompts may only be capable of teaching the model to imitate some interpolation of the\\ntraining distribution, and so will not lead the model to exceed the performance demonstrated in the training\\nset. Concretely, we want the model to be honest about itself and its specific capability level rather than\\npresenting an honest-seeming facade in imitation of its training data (e.g. implying that it is able to book a\\nflight). Advanced AI models may also be trained using a mixture of generative modeling, supervised learning,\\nreinforcement learning, and other techniques. Prompt design may not carry over so straightforwardly after\\ngenerative models are re-purposed for other tasks.\\n\\nScaling of Imitation Learning vs Preference Modeling, and Binary vs Rank-Ordered Preferences\\n\\nBeyond prompt design, the next simplest technique is imitation learning from expert examples. But the\\nslightly more complex technique of learning distinctions3 among preferencesâ€”not just what to do but also\\nwhat not to doâ€”may be more promising. We are interested in when this more involved approach improves\\non imitation learning, and how each scales with model size.\\n\\nWe find that there seems to be a qualitative distinction between two types of tasks:\\n\\nâ€¢ Binary Discrimination, where the data has only two possible labels, such as pass/fail or true/false;\\nsome examples include determining if python code passes tests, or determining if an action is\\nmorally acceptable or unacceptable\\n\\n3Note that if such data is not available, there is an option to generate it, since expert examples can be compared with\\nsamples from a model â€“ i.e. we can train a GAN-style discriminator.\\n\\n7\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.30\\n\\n0.35\\n\\n0.40\\n\\n0.45\\n\\n0.50\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nMean Transfer Performance at 500 Finetuning Seq Pairs\\nNo PMP\\nPMP Mix\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nMean Transfer Performance at 5k Finetuning Seq Pairs\\nNo PMP\\nPMP Mix\\n\\nFigure 5 Transfer performance at 500 and 5k sequence pairs on downstream finetuning evaluations with\\nPMP (on the â€˜Mixâ€™ dataset, shown in violet) vs. without PMP (black). Each curve is averaged across fine-\\ntuning evaluations Learn to Summarize, HellaSwag, and all five Ethics evaluations. We see that PMP signifi-\\ncantly improves sample efficiency with large models.\\n\\nâ€¢ Ranked Preference Modeling among a tall hierarchy of possibilities, with examples including the\\npopularity of a StackExchange answer, or the quality of a paragraph summary. Note that rankings\\ncan be learned from pairwise comparisons even though the underlying data has a ranked ordering.\\nLearning from human preferences [CLB+17] and T-REX IRL [BGNN19] learn from ranked data.\\n\\nAs shown in the introductory figure 3, we find that preference modeling performs much better and scales\\nsomewhat better than imitation learning, but that binary discrimination does not.\\n\\nPreference Model Pre-Training\\n\\nModels that learn to discriminate and rank human preferences play a natural role in alignment research.\\nSuch models can be used as filters, and they can also be leveraged more powerfully as preference models\\nfor reinforcement learning from human feedback (RLHF) [CLB+17], in order to train aligned policies. Fur-\\nthermore, some proposals [CSA18, ICA18] for aligning more advanced AIs use different models to train or\\nevaluate each other, so that the effectiveness and reliability of these techniques may ultimately depend on the\\nperformance and robustness of preference models.\\n\\nPreference modeling success may be hampered by small datasets, since a natural way to train these models\\nis through human feedback on samples generated from a policy, as in RLHF or human-in-the-loop training,\\nand high-quality human interaction data may be expensive. Thus a significant consideration is whether we\\ncan improve the sample efficiency of these models. For this purpose we experiment with preference model\\npretraining (PMP), so that the full training procedure includes training sequentially on:\\n\\nLanguage Model Pre-trainingâ†’ Preference Model Pre-trainingâ†’ Preference Model Finetuning\\n\\nFor the second stage, we utilize large scale public data from Stack Exchange, Reddit, and reverted vandalism4\\nof Wikipedia. We find that this PMP stage of training significantly improves sample efficiency and often\\nimproves the asymptotic performance when preference models are finetuned on both human feedback datasets\\nor various alignment-focused datasets.\\n\\nIn appendices we discuss details of model training and dataset preparation and some additional experiments\\nwith GAN-style discriminator.\\n\\nModels\\n\\nThroughout this paper we will be studying a consistent set of decoder-only Transformer language models\\nwith parameter counts ranging from about 10M to 52B in increments of 4x, and with a fixed context window\\nof 8192 tokens and a 216 token vocabulary. For language model pre-training, these models are trained for\\n400B tokens on a distribution consisting mostly of filtered Common Crawl data [Fou] and internet books,\\nalong with a number of smaller distributions [GBB+20], including about 10% python code data. We fix the\\n\\n4By this we mean that we specifically sourced changes to Wikipedia that were noted as such and quickly reverted.\\n\\n8\\n\\n\\n\\naspect ratio of our models so that the activation dimension dmodel = 128nlayer, and include models with\\n13M, 42M, 197M, 810M, 2.7B, 13B, and 52B non-embedding parameters. Throughout the paper we will\\nshow results and comparisons as a function of model size, and by â€˜Number of Parametersâ€™ we will always\\nmean non-embedding parameters.\\n\\nIn some places we will also study the properties of these models after they have been finetuned on a pure dis-\\ntribution of python code. We also discuss finetuning on a variety of other datasets, including with additional\\nheads that can make real-valued predictions at all token positions. Most of these finetuning datasets do not\\nutilize the full 8192-token context window, so in many cases we restrict to shorter contexts during finetuning.\\nFor a more detailed description of language model pre-training see Appendix A.\\n\\n1.3 Contributions\\n\\nOn prompting, alignment evaluations, alignment taxes, and context distillation:\\n\\nâ€¢ A simple prompt provides a workable baseline for alignment, and leads to significant improvements\\non a variety of evaluations (figure 2), including a helpfulness, honesty, and harm evaluation we have\\nwritten. We introduce â€˜context distillationâ€™ and show that it behaves similarly to prompting.\\n\\nâ€¢ The prompt reduces toxicity [GGS+20] (figure 8) and seemingly leads larger models to be more\\naccurate than smaller models on TruthfulQA [LHE21] (figure 6). Prompted models are significantly\\npreferred by people who interact with them (figure 9).\\n\\nâ€¢ Prompting can have negative effects on the capabilities of small models, but has small and sometimes\\npositive effects on large models, which therefore pay little â€˜alignment taxâ€™ (figure 2).\\n\\nOn the comparative scaling of imitation learning, binary discrimination, and preference modeling:\\n\\nâ€¢ The scaling of binary discrimination does not improve very significantly on the scaling of imitation\\nlearning (see figure 3 for a summary, and figure 12 for detailed results on Code Correctness).\\n\\nâ€¢ Ranked preference modeling of complex hierarchies greatly improves on imitation learning. This\\nshould be encouraging news for alignment work based on human preferences.\\n\\nâ€¢ These conclusions hold rather cleanly and consistently as represented by at least three distinct\\ndatasets in each category (see figures 3, 14, and 15), but we would still suggest that further work\\nmay improve our understanding of these findings.\\n\\nOn preference modeling pre-training (PMP) for improved sample efficiency:\\n\\nâ€¢ A PMP stage of training between basic language model pretraining and finetuning on small final\\ndatasets significantly improves sample efficiency (see figures 4 and 5 for summaries, and figure 17\\nfor details).\\n\\nâ€¢ These results hold even when the PMP data are quite different from the final dataset (e.g. finetuning\\nfrom Stack Exchange to summarization).\\n\\nâ€¢ In marked contrast to the scaling results mentioned earlier, where PM scales best on hierarchically\\nranked datasets, we find that itâ€™s better for the PMP stage of training to focus on binary discrimination\\n(see figure 18). An explanation for the better performance of binary PMP may be that hierarchies of\\npreferences are difficult to quickly unlearn during finetuning, whereas binary discrimination training\\nteaches models the correct features without establishing strong model preferences. We test this\\nexplanation with a quick synthetic data experiment shown in figure 33.\\n\\nâ€¢ We also try training the preference model to discriminate between human- and model-generated\\nsamples for the PMP step, and find that it also performs well, as shown in figure 19.\\n\\n2 Conditioning on Aligned Behavior\\n\\nLarge language models can be guided towards desirable behaviors by taking advantage of their in-context\\nlearning abilities. Given a suitable prompt, models will take on the style and persona implicit in the prompt\\nand continue to behave mostly in the same vein. This technique can leverage small quantities of very high\\nquality data, and it has the advantage that the prompt can be easily interpreted by humans. For a variety of\\nreasons we do not expect that prompting will produce fully aligned behavior, but it provides a very useful\\nbaseline.\\n\\n9\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.20\\n\\n0.25\\n\\n0.30\\n\\n0.35\\n\\n0.40\\n\\n0.45\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nTruthfulQA (MC1)\\n\\nDistilled Mutual Info\\nLM Mutual Info\\nDistilled Sum Logprobs\\nLM Sum Logprobs\\n\\nFigure 6 Left: We show the HHH evaluation performance broken down by category. The improvements\\non the Harm evaluations suggest a form of generalization, as the prompt does not contain any examples\\nwhere the assistant resists engaging in harmful behavior. Right: We show results on the adversarial Truth-\\nfulQA dataset (MC1), which was constructed so that larger models would perform more poorly. The context-\\ndistilled prompt seems to improve the performance of the largest models. The solid lines correspond to the\\nofficial evaluation using total probability for each response; we also show the mutual information metric for\\ncomparison.\\n\\nIn this section we will study a variety of zero-shot evaluations for alignment with and without prompting.\\nThe prompt we use consists of fourteen human-assistant conversations, where the assistant is always polite,\\nhelpful, and accurate. The prompt does not contain examples where the assistant actively resists aiding in\\nharmful behavior, but nevertheless for simplicity we will refer to it as the â€˜HHH promptâ€™ or simply the prompt\\nin what follows. We find that although the effect of prompting is modest when measured against the overall\\ngoal of alignment, it improves alignment (according to our evaluations) and decreases toxicity. A potentially\\nmore important observation is that the prompt improves trends, so that alignment improves with model size,\\nincluding on TruthfulQA [LHE21], a dataset designed specifically to induce the opposite trend. Furthermore,\\nwe show that there is little â€˜taxâ€™ from alignment â€“ at large model size capabilities are not significantly impaired\\nby the prompt. Of course, this does not mean that more intensive alignment interventions will incur no cost.\\n\\nWe also introduce a â€˜context distillationâ€™ technique that may make prompting more efficient in practice and\\npotentially allow for the use of prompts that exceed the size of the context window. For many but not all of\\nour evaluations context distillation performs about as well as prompting. We begin by briefly describing this\\nmethod, and then we will discuss evaluations.\\n\\n2.1 Context Distillation\\n\\nSampling from a language model with a prepended prompt has several disadvantages: the prompt occu-\\npies useful space in a finite context window, which also limits the total prompt length, and without special\\naffordances the prompt will waste compute and memory when sampling.\\n\\nOne way to avoid all of these problems is to finetune on the prompt. This invites some practical difficulties,\\nsince we need to finetune on a tiny dataset without limiting model capabilities. But finetuning also behaves\\ndifferently from prompting â€“ finetuning changes the modelâ€™s expectations for the data distribution P (X),\\nbringing it closer to the distribution of the prompt P (C), whereas prompting instead asks the model for the\\ndistribution P (X|C), where C is the context. To give a stark illustration, if we show a language model the\\nlist C = 1, 2, Â· Â· Â· , 63 then it will assign very high probability that the numbers X = 64, 65, Â· Â· Â· are coming\\nnext. If instead we finetune on C, the resulting model will not expect to immediately see the token 64, though\\nit will catch on to the counting pattern if we continue the sequence. We illustrate this toy experiment in figure\\n26, which we have relegated to the appendix.\\n\\nWe can both avoid overfitting and take advantage of conditioning via â€˜context distillationâ€™, where we finetune\\na model pÎ¸(X) with a loss given by\\n\\nL(Î¸) = DKL(p0(X|C)||pÎ¸(X)) (2.1)\\n\\nwhere p0 is the initial model, the context C is fixed, and the data X is drawn from a large corpus of text, such\\nas the original pre-training distribution. We discuss the details of context distillation training in appendix B.5.\\n\\n10\\n\\n\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\nNumber of Parameters\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nA\\ncc\\n\\nur\\nac\\n\\ny\\n\\nAccuracy on Lambada Eval\\n\\nLM\\nLM+Prompt\\nLM+Context Distillation\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\nNumber of Parameters\\n\\n0.035\\n\\n0.030\\n\\n0.025\\n\\n0.020\\n\\n0.015\\n\\n0.010\\n\\nLM\\n -\\n\\n A\\nlig\\n\\nne\\nd \\n\\nA\\ncc\\n\\nur\\nac\\n\\ny\\n\\nLambada Alignment Tax\\n\\nLM+Prompt\\nLM+Context Distillation\\n\\nFigure 7 We show zero-shot Lambada performance in the presence of the HHH prompt and with context\\ndistillation. In both cases there is a small â€˜alignment taxâ€™.\\n\\nWe see from figure 2 that this technique appears to work quite well. However, the benefits compared to\\nsimply finetuning on the prompt become much less significant if we additionally provide a small prompt\\nafter the finetuning or distillation process, as shown in figure 20 in the appendix. It appears that contractors\\ninteracting with our models observe a small degradation from distillation, as seen in figure 9. In the future it\\nmight be interesting to apply context distillation iteratively, which one might liken to loading the model with\\na long-term memory or pseudo-identity.\\n\\n2.2 Evaluations and Alignment Taxes\\n\\n2.2.1 HHH Evaluations and TruthfulQA\\n\\nAs a first step in evaluating our models, the authors wrote about fifty comparison evaluations for each cate-\\ngory of helpfulness, honesty,5 harmlessness (HHH), and an â€˜otherâ€™ label, for a total of around two-hundred\\ncomparisons, which will be available shortly at BIG Bench. We did not put effort into separating alignment\\nfrom capabilities, and so even without any alignment-related prompting, we find that larger models do some-\\nwhat better overall. In many cases we initially produced several slightly different queries (largely differing\\nby paraphrase) for each comparison, but found that large models were rarely confused by these variations, so\\nfor simplicity we dropped them. Results on these evaluations are pictured in figure 2. We expect that more\\nsophisticated alignment techniques should be able to significantly improve these results.\\n\\nNote that we evaluate model choices using the empirical mutual information I(a, q) = log [P (a|q)/P (a)]\\nfor queries q and responses a, rather than the more typical choice of mean token probability for the response\\n(mutual information was also used for several evaluations of GPT-3 [BMR+20]). The mutual information\\nmetric tends to be useful when responses differ greatly in length, and it makes a significant difference in\\nperformance on our evaluations.\\n\\nOn the left in figure 6 we show the results on our HHH evaluations by category. We found it a bit ironic that the\\nmodels perform best in the â€˜honestyâ€™ category, as the models certainly do fabricate information when probed\\ninteractively as general-purpose assistants. To further evaluate our modelsâ€™ honesty, we include evaluations\\non TruthfulQA6 MC1 on the right of this figure. We see that the context distilled prompt has slightly improved\\nthe performance of our largest models using the standard evaluation7 metric. We also compare the use of more\\nevaluation metrics on TruthfulQA in figure 21 in the appendix. The use of conditional probabilities does not\\nalter trends significantly, but does greatly affect absolute performance.\\n\\n5Our evaluations of â€˜honestyâ€™ are probably the most correlated with model capabilities, as they measure a mixture\\nof accuracy, preference for expressions of humility, recognition of when another source might be more useful than a\\nlanguage model, and unwillingness to provide inaccurate information. Whether an AIâ€™s response is honest depends on\\nthe expertise of the AI, and a major weakness of our evaluations is that they do not account for this.\\n\\n6We wrote the prompt before TruthfulQA was available. That said, we found in other experiments that using Truth-\\nfulQA examples as a prompt significantly improves performance (much more than our prompt). This suggests that the\\nphenomenon uncovered by TruthfulQA is not a difficult alignment challenge on its own.\\n\\n7In an earlier version of this paper we mistakenly used a very non-standard formulation of the task. We thank the\\nauthors of [LHE21] for pointing out this error, which has been corrected.\\n\\n11\\n\\nhttps://github.com/google/BIG-bench\\n\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\nNumber of Parameters\\n\\n0.03\\n\\n0.04\\n\\n0.05\\n\\n0.06\\n\\nA\\nve\\n\\nra\\nge\\n\\n T\\nox\\n\\nic\\nity\\n\\nToxicity in Response to Non-Toxic Prompts\\n\\nLM\\nLM+Prompt\\nLM+Context Distillation\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\n0.11\\n\\n0.12\\n\\n0.13\\n\\n0.14\\n\\n0.15\\n\\n0.16\\n\\nToxicity in Response to Toxic Prompts\\n\\nFigure 8 Left: Average toxicity in response to a random sample of 500 prompts labeled as â€˜non-toxicâ€™ from\\nthe RealToxicityPrompts dataset for language models (LM, blue), prompted language models (LM+Prompt,\\norange), and context distilled language models (LM+Context Distillation, green). Right: Same as Left, ex-\\ncept for a random sample of 500 prompts labeled as Toxic. For non-toxic and toxic prompts, both prompting\\nand context-distillation decrease toxicity and perform similarly to each other as models increase in size. It\\nappears that the prompt leads to decreasing toxicity as model size increases.\\n\\nIt is noteworthy that larger models tend to perform better on our evaluations in the presence of the HHH\\nprompt, even on categories such as harmlessness that are not directly demonstrated by the prompt. We find\\nthis mildly encouraging but unsurprising, since all prior work suggests that larger models have stronger in-\\ncontext learning capabilities, so that they can more efficiently recognize the implicit framing from the prompt.\\n\\n2.2.2 Toxicity\\n\\nWe measured the effect of prompting and context distillation on the toxicity of text generated from language\\nmodels of increasing size. We found that these simple alignment interventions tend to both decrease toxicity\\nand perform similarly to one another (Figure 8). To measure toxicity, we first sampled text conditioned on\\na random sample of 1K prompts from the RealToxicityPrompts dataset [GGS+20]. The prompts are labeled\\nas either â€™toxicâ€™ or â€™non-toxicâ€™ and we sample an equal proportion of these prompts. Next, we computed\\na toxicity score from model samples of text, conditioned on the prompts, using an open source automated\\ntoxicity detector [HU20]. Our analysis is similar to to [GGS+20] with a few minor modifications. We provide\\nfull details and further analyses in Appendix B.2.\\n\\nFigure 8 illustrates three key findings from our analysis. First, without any alignment intervention, toxicity\\nincreases monotonically with model size in response to both toxic and non-toxic prompts (blue curves).\\nSecond, for non-toxic prompts, both prompting and context distillation significantly reduce toxicity and we\\nobserve little difference between the two interventions (green and orange curves, left figure). Finally, in\\nresponse to toxic prompts, the reduction in toxicity achieved by both prompting and context distillation\\nsignificantly increases with model size (green and orange curves, right figure). The larger reduction in toxicity\\nemerges at 12B parameters. In this regime, context distillation performs similarly to prompting. These results\\nsuggest that prompting-based alignment interventions may have more dramatic effects as models scale and\\nmay be more difficult to evaluate for smaller models.\\n\\nWhile these results are encouraging, automated toxicity detection has several known issues [GGS+20,\\nWGU+21]. For example, there can be low agreement in human annotations of toxicity and biases in tox-\\nicity labels for certain minorities. We also note that other interventions explicitly designed to reduce toxicity\\n(e.g., fine-tuning models on non-toxic training data, steering/filtering model outputs away from toxic outputs\\nat test time, filtering toxic training data at train time) can yield much larger decreases in automated toxicity\\nscores than the ones we observe here [GGS+20, WGU+21]. Nevertheless, we believe that prompting and\\ncontext distillation provide a useful baseline for testing the impact of alignment interventions on automated\\ntoxicity scores.\\n\\n2.2.3 Human Preferences and Model Performance\\n\\nUsing the dialogue interface in figure 1, we evaluated relative model performance via a number of head-to-\\nhead tests between pairs of models. This worked as follows. For any given conversation, we would choose\\n\\n12\\n\\n\\n\\nFigure 9 This figure illustrates the approximate Elo score of various models, fit from the frequency with\\nwhich contractors viewed a given model as more helpful and honest in head-to-head tests involving pairs of\\nmodels. Models with the full HHH prompt seem to be slightly preferred over those with a shorter prompt or\\ncontext distillation. We include 1Ïƒ error bars for the special cases, which were only compared against the\\nHHH-prompted models of equal size.\\n\\na pair of models, with each model writing a single response to each human query. We randomized whether\\na given modelâ€™s responses would appear in position \"A\" or \"B\" in the interface, to avoid the possibility that\\nusers would consistently find \"A\" or \"B\" to be better. We also pegged streaming sampling speed to that of the\\nslowest model, to partially obscure model identity and avoid bias. We collected a total of about 6k individual\\npair-wise8 model comparisons\\n\\nFrom this process we collected a table of â€˜win ratesâ€™ for pairs of models, which we provide in table 2 in the\\nappendix. Here we included fully HHH-prompted models with 200M, 800M, 3B, 13B, and 52B parame-\\nters, though we collected somewhat more comparisons involving larger, better-performing models. We also\\ncompared the fully prompted 13B and 52B models to their context-distilled versions and to a version with a\\nshorter prompt consisting of only a single9 example conversation.\\n\\nWe used these results to estimate a single relative Elo score for each model. Intuitively, this score is similar\\nto that used for ranking Chess players, with a real scalar value based on the relative win rates amongst all\\nplayers. Quantitatively, we fit the Elo scores from the data in table 2 with the same loss function we use for\\npreference modeling (equation 3.1). We display the results in figure 9, where we recall that a difference of\\n100 points in an Elo score signifies a â€˜win rateâ€™ of 64%.\\n\\nThe most striking feature of these results is that Elo score appears to be linear in the logarithm of model size\\nfrom 197M to 13B parameters, but it does not change very significantly between 13B and 52B parameters.\\nWe do not believe that this is because the two largest models are equally capable. Rather, we interpret it\\nas a limitation of the training and incentives of the contractors evaluating the models, who are US-based\\nmaster-qualified MTurkers who were only provided with some simple instructions, and who have an implicit\\nincentive to finish tasks quickly. This provides a sense for how well-trained and capable workers need to be\\nto perceive distinctions among large language models.\\n\\nWe note that using a much shorter prompt with just one example conversation seems to hurt performance,\\nand it seems that the contractors were able to differentiate the prompted and context distilled model, with the\\nformer being preferred about 53% of the time. We include 1-Ïƒ error bars for these comparisons (note that\\nthe short-prompt and distilled models were only compared to the fully prompted models of equal size), so we\\nhave some weak evidence that context distillation has degraded performance somewhat compared to the full\\nHHH prompt.\\n\\n8Note that we typically obtain roughly 3-5 comparisons per conversation. There may be some subtle biases here\\nwhere weaker models perform more poorly early on in conversations, affecting the possibilities for later dialogue.\\n\\n9We did not use completely unprompted models because they would be very unlikely to keep to the format of the\\ndialogue or emit appropriate stop sequences.\\n\\n13\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\nFr\\nac\\n\\ntio\\nn \\n\\nPa\\nss\\n\\nin\\ng \\n\\nTe\\nst\\n\\ns\\n\\nCodex Evaluations and Alignment Tax/Bonus\\nCodex Pass@10\\nCodex Pass@1\\nCodex w/ HHH Prompt Pass@10\\nCodex w/ HHH Prompt Pass@1\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nFr\\nac\\n\\ntio\\nn \\n\\nPa\\nss\\n\\nin\\ng \\n\\nTe\\nst\\n\\ns\\n\\nQuixBugs Evaluations and Alignment Tax/Bonus\\nQuixBugs Pass@10\\nQuixBugs Pass@1\\nQuixBugs w/ HHH Prompt Pass@10\\nQuixBugs w/ HHH Prompt Pass@1\\n\\nFigure 10 This figure shows performance of our code-finetuned models on the Codex and QuixBugs eval-\\nuations with and without the alignment prompt. We see that in both cases, the prompt confuses smaller\\nmodels, leading to worse performance, but it actively improves the 13B and 52B models. All samples were\\ngenerated at temperature T = 0.6 and top P = 0.95 (these settings were not optimized and are not optimal\\nfor Pass@1). Note the figure on the left here was also presented in the introduction.\\n\\n2.2.4 Alignment Taxes/Bonuses\\n\\nA general concern about alignment is that it may impose a â€˜taxâ€™ on performance, such that aligned models may\\nbe weaker than raw or unaligned models. In the case of prompting and context distillation, it is straightforward\\nto evaluate this question directly by performing evaluations with and without the prompt. When we include\\nthe HHH prompt, we also use the human-assistant framing when presenting the problem or evaluation to the\\nmodel. The precise specifications can be found in appendix B.1.\\n\\nWe display results for two very similar python coding evaluations, the Codex HumanEval [CTJ+21] and the\\nQuixBugs challenge reformulated as a function synthesis task [LKCSL17] in figure 10. Interestingly, smaller\\nmodels perform significantly worse with the prompt, but 13B and 52B models actually perform noticeably\\nbetter. These evaluations were run using our code-finetuned models, so the strong performance of the larger\\nmodels also suggests that these models have not lost their ability to process the natural language in the prompt.\\n\\nWe performed a similar evaluation on Lambada [PKL+16], with results shown in figure 7. We see that\\nthe prompt and context distillation impose a small â€˜taxâ€™ on performance that does not have a significant\\nmodel-size dependence. As shown in Appendix B.4, Lambada performance is strongly dependent on some\\nformatting issues, which alter performance by a much larger margin than the prompt. This format-dependence\\nitself might be regarded as an alignment problem, but unfortunately we do not find that the HHH prompt\\nreduces the difference between accuracies obtained from different Lambada formats.\\n\\nWe therefore found that while smaller models may be confused by the prompt, larger modelsâ€™ performance\\nis not heavily impacted by it.\\n\\n3 Scaling of Preference Modeling vs Imitation Learning\\n\\nAlignment requires distinguishing between â€˜goodâ€™ and â€˜badâ€™ behavior. There are several different training\\nobjectives that may be used to accomplish this:\\n\\nâ€¢ Imitation Learning: Here we simply train language models to imitate â€˜goodâ€™ behavior via super-\\nvised learning with the usual cross-entropy loss.\\n\\nâ€¢ Binary Discrimination: Given a sample of â€˜correctâ€™ behavior and a sample of â€˜incorrectâ€™ behavior,\\ntrain the model to distinguish between the two.\\n\\nâ€¢ Ranked Preference Modeling: Given a dataset of samples whose overall â€˜qualityâ€™ is ranked in\\nsome way, we train models to output a scalar quality score10 for each sample whose value matches\\nthe ranking as closely as possible. For simplicity we focus on using pairs of ranked samples (i.e.,\\nbinary comparisons), and we train our models to assign a higher score to the â€˜betterâ€™ sample in each\\n\\n10These values could then be used as reward signals for reinforcement learning.\\n\\n14\\n\\n\\n\\npair. In some respects this generalizes binary discrimination, and for uniformity we will use it as the\\ntraining objective even for binary discrimination tasks (see section 3.1 for details).\\n\\nWe would like to explore a very general question: when and by how much do discriminators and preference\\nmodels outperform imitation learning?\\n\\nOur experiments in this section involve comparing the performance of imitation learning vs. preference\\nmodeling on a variety of finetuning evaluations, some of which are binary in nature while others are ranked.\\n\\nâ€¢ Binary: Code Correctness, Commonsense (ethics), Justice (ethics), Deontology (ethics), Virtue\\n(ethics), Lambada\\n\\nâ€¢ Ranked: Learn to Summarize, Utility (ethics), HellaSwag\\n\\nWe focus mostly on alignment-relevant tasks, but include one binary and one ranked NLP task (Lambada\\n[PKL+16] and HellaSwag [ZHB+19], respectively). Code Correctness is a dataset we constructed from\\npython functions in public github repos with test coverage, with correctness determined by unit tests. The\\nEthics [HBB+21] evaluations are mostly binary classification problems, and so naturally belong in our bi-\\nnary category, except for Utilitarianism which compares relative â€˜pleasantnessâ€™ of scenarios. The distinction\\nbetween ranked and binary tasks can be ambiguousâ€”for example, whether code passes tests is binary, but\\ncode quality seems like a continuum.\\n\\nOur results support a simple conclusion summarized in figure 3: Ranked preference models tend to improve\\ngreatly on imitation learning, but binary discrimination typically provides little benefit.\\n\\nIn some respects this conclusion is quite intuitive: to apply imitation learning to preference modeling, one\\nmust either only train on the very best data (limiting the dataset size) or train to imitate a lot of examples of\\nlower quality. Nonetheless, the magnitude of the gains are rather stark.\\n\\nIn many cases it is also possible to study the robustness of various methods for ranking samples. For exam-\\nple, if we sample many responses to a prompt/query, we would like to know if the highest ranked samples\\naccording to a given preference model are truly the best. We test this behavior directly in our code correctness\\nstudies and with Lambada.\\n\\n3.1 Loss and Settings for Preference Modeling and Imitation Learning\\n\\nPreference Modeling\\n\\nOur preference models consist of a value head that predicts a single scalar â€˜scoreâ€™ r on top of the final token\\nof any given context, with larger r indicating more desirable samples. The preference modeling loss for each\\npair of â€˜goodâ€™ and â€˜badâ€™ sequences is [CLB+17]\\n\\nLPM = log\\n(\\n1 + erbadâˆ’rgood\\n\\n)\\n, (3.1)\\n\\nand for batched sample pairs we take the mean over all pairs. This is clearly not the most natural loss function\\nfor some applications; for binary â€˜correctnessâ€™ it would be better to predict if each example is correct or\\nincorrect, and for multiple choice problems, it might be better to maximize the likelihood for the correct\\nresponse among all available responses. However, since our primary motivation is preference modeling, we\\nwill focus on this formulation unless otherwise noted.\\n\\nIn particular, we format all binary discriminators as preference models so that the same architecture can be uti-\\nlized for both binary and ranked evaluations, which is convenient for studying transfer between them. Given\\nany context C with a binary label A/B (e.g., â€˜True/Falseâ€™, â€˜Good/Badâ€™), we create a preference modeling pair\\nC:A > C:B, where B denotes the incorrect label, and the colon denotes concatenation.\\n\\nWe also found that appending a special â€˜end-of-contextâ€™ token to each sequence to unambiguously delineate\\nthe end of passage sometimes improves performance, as discussed in section C.4.\\n\\nImitation Learning\\n\\nFor imitation learning, our training objective is simply the autoregressive language modeling loss on the\\nâ€˜goodâ€™ sequence in each pairâ€”that is, we train the model to imitate â€˜goodâ€™ behavior. In the notation above,\\nthis means that for imitation learning we trained on C:A. We found that applying a mask to train only over\\nthe response tokens improved performance significantly, so all our imitation learning results are masked.\\nFurthermore, just to clarify, at training time we sum over negative token log-probs to compute the loss as is\\ntypically done, but at evaluation time we average over negative token log-probs to make pairwise comparisons\\n\\n15\\n\\n\\n\\n100 101 102\\nNumber of Samples\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nPreference Modeling (Solid) vs. Imitation Learning (Dashed)\\n\\n108\\n\\n109\\n\\n1010\\n\\nM\\nodel Param\\n\\neters\\n\\nFigure 11 Here we compare the performance of code correctness discriminators and imitation learning for\\nranking samples. All models used for a fixed color are the same size â€“ the generator of the discriminator\\ntraining data, the generator of the test samples, and the preference or imitation learning model used for\\nranking. The fact that some of these curves are not monotonic represents a robustness failure of preference\\nmodeling.\\n\\n(i.e, a pairwise comparison is accurate if the average negative log-prob for the â€˜goodâ€™ sample is lower than\\nfor the â€˜badâ€™ sample). This significantly improves performance when responses have different lengths.\\n\\n3.2 Performance and Scaling Results for Ranked versus Binary Preference Datasets\\n\\nHere we provide a short description of our evaluation datasets, some of which we categorize as â€˜rankedâ€™ while\\nothers are â€˜binaryâ€™. In this section, all evaluations involve finetuning on a training set and evaluating on a test\\nset.\\n\\nCode Correctness (Binary)\\n\\nFor these experiments we collected about 500k python functions with test coverage11 from public github\\nrepos, and split these functions into a training and test set. For each function, we discarded the original\\nimplementation (keeping only the function definition and docstring) and generated 8 samples from each code\\nmodel up to 13B parameters, and tested these samples with all available tests. We then created pairs of\\ncorrect and incorrect samples for each function, using only model-generated code, to avoid confusing code\\ncorrectness with the task of human-model discrimination. We compared two training procedures: imitation\\nlearning on correct functions, and preference modeling comparing the correct and incorrect functions.\\n\\nThen we evaluated performance on the test set in the following way. We generated 100 samples for each\\nfunction (using pretrained code models), and ranked them according to both mean per-token log-probs of\\nthe IL model, and scores produced by the preference model. Then we evaluated the probability that the top\\nsample among k, as ranked by either method, was in fact correct (we derive an unbiased formula in appendix\\nB.6, based on the pass@k estimate from [CTJ+21]). For this we used the same model size for training and\\ntest set generation and for ranking samples. Some results are shown in figures 11 and 12.\\n\\nOverall we found that preference modeling on this binary discrimination task does not improve very signif-\\nicantly on imitation learning. Both PM and IL are quite similar, overall. These results differ from similar\\nrecent experiments on math problem solving [CKB+21], though they trained on thousands of times less data.\\nThe difference may be that our imitation learning baseline is much stronger, since even before IL finetuning\\non Code Correctness specifically, our code models had seen a great deal of on-distribution python code.\\n\\nLambada (Binary)\\n11We required that at least half of the lines in the function were executed by a combination of tests in the repo.\\n\\n16\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nCode Correctness with 2 Samples\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nCode Correctness with 8 Samples\\n\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nCode Correctness with 32 Samples\\n\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nCode Correctness with 96 Samples\\n\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\nFigure 12 To create this figure, we generated 100 samples (at T = 1) from code models. We then ranked\\nthese samples using either log-probs from the same model, or using a preference model trained to discriminate\\ncorrect and incorrect code. The \"oracle\" line plots optimal ranking where all correct samples are ranked before\\nincorrect ones. We see that imitation learning and preference modeling perform similarly.\\n\\nWe now discuss our evaluations on Lambada [PKL+16]. We used the dataset with original formatting, which\\ndiffers from that used in GPT-3 [BMR+20]. For imitation learning we simply trained on the correct answers\\nin the training set. For binary discrimination, we sampled answers at T = 1 from models of various sizes,\\ncreated up to two pairs of correct and incorrect answers for each prompt, and then trained the discriminator to\\nidentify the correct completion. At test time we sampled multiple responses for each question (at temperature\\nT = 1) and ranked them by either log-probs (for IL) or preference modeling score. The results are shown\\nin figure 13, where we see that imitation learning performs roughly on par with preference modeling. This\\nprovides an independent verification of what we found with Code Correctness, though again the imitation\\nlearning baseline is very strong, as the Lambada task aligns very well with the language model pre-training\\nobjective.\\n\\nHellaSwag (Ranked)\\n\\nWe also performed a comparison of imitation learning and preference modeling on the HellaSwag [ZHB+19]\\ndataset. This is a multiple choice evaluation on commonsense inferenceâ€”given an event description, the\\nmodel is asked to identify the most sensible completion. Although each problem presents only three choices,\\nthe desired responses are not uniquely correct, but are merely the most sensible inference among the three\\noptions. Thus this task is a form of ranked preference modeling, rather than binary discrimination. In agree-\\nment with our expectations, we find that preference modeling scales far better than imitation learning on this\\ndataset, as shown in figure 14.\\n\\nNote that while the training data is formatted as multiple choice, we convert the data to binary comparisons\\nby pairing the correct choice with a randomly chosen incorrect choice. It might be possible to improve\\nperformance by training on all options, but we did not explore this.\\n\\nLearn to Summarize (Ranked)\\n\\n17\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nLambada Correctness with 2 Samples\\n\\nIL Sampled at T=0\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nLambada Correctness with 8 Samples\\n\\nIL Sampled at T=0\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nLambada Correctness with 32 Samples\\n\\nIL Sampled at T=0\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nLambada Correctness with 96 Samples\\n\\nIL Sampled at T=0\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\nFigure 13 Similarly to Code Correctness in figure 12, we generated 100 samples (at T = 1) from pretrained\\nlanguage models. We then ranked these samples using either log-probs from an imitation learning model, or\\nusing the scores from a preference model trained to discriminate correct vs. incorrect Lambada completions.\\nNote that for some questions, all the generated answers may be incorrect in which case we default to 0\\naccuracy. We see that these approaches perform similarly, as we expected since Lambada is a â€˜binaryâ€™ eval.\\nLambada performance depends significantly on formatting, as noted in appendix B.4. We also include a line\\nfor T = 0 (argmax) sampling .\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nHellaSwag (Ranked)\\nPreference Modeling\\nImitation Learning\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nLearn to Summarize (Ranked)\\nPreference Modeling\\nImitation Learning\\n\\nFigure 14 Scaling behavior of imitation learning and preference modeling on HellaSwag (ranked) and\\nLearn to Summarize (ranked), showing that PM performs better than IL, as we expect for ranked finetuning\\nevaluations.\\n\\n18\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.750\\n\\n0.775\\n\\n0.800\\n\\n0.825\\n\\n0.850\\n\\n0.875\\n\\n0.900\\n\\n0.925\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEthics: Commonsense Morality (Binary)\\nPreference Modeling\\nImitation Learning\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEthics: Deontology (Binary)\\nPreference Modeling\\nImitation Learning\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEthics: Justice (Binary)\\nPreference Modeling\\nImitation Learning\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEthics: Virtue (Binary)\\nPreference Modeling\\nImitation Learning\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\n0.85\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEthics: Utilitarianism (Ranked)\\nPreference Modeling\\nImitation Learning\\n\\nFigure 15 Scaling behavior of imitation learning and preference modeling for all five Ethics evaluations,\\nwhich are all binary except Utilitarianism. We find, in agreement with our expectations, that PM beats IL\\non the ranked task, but on binary tasks they perform similarly. For brevity we have only included the easier\\nevaluation sets here.\\n\\nPreference modeling and RLHF has been applied to the task of generating high-quality summaries of short\\narticles [SOW+20]. We study the associated dataset, which we term â€˜Learn to Summarizeâ€™. It consists of a\\ncollection of articles, where each is accompanied by a pair of summaries that have been ranked by trained\\nhuman workers. This dataset presents a defining example of a ranked preference modeling task, since there\\nis no clear sense in which any given summary is â€˜correctâ€™, but typically among any pair of samples, one will\\nbe better than the other. We are especially interested in this finetuning evaluation as it is highly relevant for\\nalignment. We created our own data split by shuffling the data and splitting it into a train (64k pairs) and test\\n(29k pairs) set. On this dataset preference modeling performs far better than imitation learning, as seen in\\nfigure 14.\\n\\nEthics (Binary, except for Utilitarianism)\\n\\nWe studied the Ethics tasks [HBB+21], which include five distinct datasets. We provide a simplified descrip-\\ntion of each here, but we encourage the interested reader to read the original paper for details:\\n\\n19\\n\\n\\n\\nâ€¢ Commonsense Morality (binary): Assess whether a given action is morally acceptable.\\n\\nâ€¢ Deontology (binary): Assess whether a given statement is reasonable on the basis of â€˜whether an act\\nis required, permitted, or forbidden according to a set of rules or constraints.â€™\\n\\nâ€¢ Justice (binary): Assess whether a given statement is reasonable on the basis of impartiality and\\ndesert.\\n\\nâ€¢ Virtue (binary): Given a personal trait and a scenario involving a character, assess whether the\\ncharacter expresses that particular trait.\\n\\nâ€¢ Utilitarianism (ranked): Given two similar scenarios, rank them by how â€˜pleasantâ€™ they are for the\\ncharacter involved.\\n\\nIn terms of the binary versus ranked12 distinction, the first four evaluations are clearly binary since they come\\nwith binary labels, while we interpret Utilitarianism as a ranked preference modeling task since â€˜pleasantnessâ€™\\nis a ranked quality.\\n\\nEach dataset includes a single training set and two test sets (standard and hard). We train our models on the\\ntraining sets and evaluate on both test sets during and after training. In all cases we evaluate performance\\nin terms of an accuracy. For Commonsense Morality and Utilitarianism, we use binary accuracy. But for\\nJustice, Deontology and Virtue, the samples are grouped such that a model is accurate on the group only if\\nit gets all responses correct within that group. All our accuracy results follow these requirements. In some\\ncases we also display the preference modeling loss (3.1), as in figure 16, and in that case we simply average\\nover all pairwise comparisons, without any grouping.\\n\\nWe find that as claimed, PM performs significantly better than IL on the ranked Utilitarianism evaluation, but\\nthat PM and IL perform similarly on all binary evaluations, as shown in figure 15.\\n\\n4 Preference Model Pre-Training and Transfer\\n\\nWe saw in section 3 that ranked preference modeling typically performs better than imitation learning, and\\nalso often scales better as we increase model size. However, some datasets needed for alignment may be\\nsmall and expensive to source, since they may require high-quality human feedback. For example, we saw\\na hint in figure 9 that workers may require detailed instructions to differentiate13 among models much larger\\nthan 10B parameters. Thus we are particularly interested in methods to increase sample efficiency when\\nfinetuning on small preference modeling datasets.\\n\\nIn this section we will explore the idea of a â€˜preference model pre-trainingâ€™ (PMP) phase of training, after ba-\\nsic language model (LM) pretraining and before finetuning on a smaller preference modeling dataset relevant\\nfor alignment. Our training pipeline can be summarized as\\n\\nLM Pre-trainingâ†’ PMPâ†’ PM Finetuning.\\n\\nEach PMP training dataset typically consists of millions of sequence pairs, while each fine-tuning dataset\\ntypically consists of thousands to tens of thousands of sequence pairs.\\n\\nWe find that:\\n\\nâ€¢ Training on large public preference modeling data sourced from e.g. Stack Exchange question-\\nanswer pairs, Reddit comments, and Wikipedia edits (that revert â€˜suspected vandalismâ€™) significantly\\nimproves sample efficiency when subsequently finetuning on small preference modeling datasets.\\nThe pre-training datasets are explained in section 4.1, and the finetuning results are presented in\\nsection 4.2.\\n\\nâ€¢ In particular, we find that each PMP dataset is capable of transfering to a variety of finetuning\\ndatasets, with an effect size that seems to grow with model size, even though there may not be any\\nobvious similarities between the datasets.\\n\\nâ€¢ Intriguingly, for the PMP stage of training, itâ€™s most beneficial to train on binary discrimination\\ndata rather than ranked preferences. We suspect this is because ranked preferences often need to be\\n\\n12In some cases this might be altered by changing the objective of the task, but this is our understanding based on the\\ngiven evaluation metrics [HBB+21]\\n\\n13A similar observation was made concerning news articles in [BMR+20].\\n\\n20\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n5.2 Ã— 10 1\\n\\n5.4 Ã— 10 1\\n\\n5.6 Ã— 10 1\\n\\n5.8 Ã— 10 1\\n\\n6 Ã— 10 1\\n\\n6.2 Ã— 10 1\\n\\n6.4 Ã— 10 1\\n6.6 Ã— 10 1\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\nin\\ng \\n\\nLo\\nss\\n\\nMean Transfer Performance at 500 Finetuning Seq Pairs\\nNo PMP\\nPMP Mix\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n3 Ã— 10 1\\n\\n4 Ã— 10 1\\n\\n5 Ã— 10 1\\n\\n6 Ã— 10 1\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\nin\\ng \\n\\nLo\\nss\\n\\nMean Transfer Performance at 5k Finetuning Seq Pairs\\nNo PMP\\nPMP Mix\\n\\nFigure 16 Transfer performance at 500 and 5k finetuning sequence pairs averaged across multiple finetun-\\ning evaluations (Learn to Summarize, HellaSwag, and all five Ethics evaluations).\\n\\nâ€˜unlearnedâ€™ during finetuning, which presents a liability to transfer, as explained in section 4.3. In\\nparticular, for PMP we apply a simple â€˜binarizationâ€™ method that converts any ranked PM dataset to\\nbinary discrimination, as explained in section 4.1.\\n\\n4.1 PMP and Datasets\\n\\nWe constructed multiple PMP datasets from various data dumps found online, including StackExchange,\\nReddit, Wikipedia, and a mixture of all three we refer to as the â€˜Mixâ€™. In each case, we began by creating a\\nranked dataset consisting of pairwise comparisons, with each pair consisting of a â€˜betterâ€™ and â€˜worseâ€™ sample.\\nDetails on each dataset is provided in section C.1.\\n\\nSubsequently, we created a binary dataset by applying a â€˜binarizationâ€™ procedure to the ranked dataset. That\\nis, for every ranked pair A > B, we transform it into two independent binary comparisons:\\n\\nGOOD:A > BAD:A\\nBAD:B > GOOD:B\\n\\nConsequently, the binary dataset has twice as many pairs as the ranked dataset. As discussed in more detail in\\nsection 4.3, we found that pre-training on the binary dataset typically transferred better than the corresponding\\nranked version, and so all our PMP experiments assume binary pre-training unless otherwise stated.\\n\\nWe pre-train a scan of preference models of various sizes on each binary dataset. Training details such as\\nhyperparameter choices are described in section C.1.\\n\\n4.2 Finetuning Results and Scaling Trends\\n\\nHere we show finetuning results after preference model pre-training (PMP) on a variety of downstream fine-\\ntuning evaluations. We find that all our PMP models significantly improve sample efficiency when finetuning,\\ndespite there often being little similarity between the PMP distribution and the finetuning distribution.\\n\\nOur results are summarized in figure 4, showing the performance gain of PMP. Since performance on all of\\nour final finetuning datasets can be evaluated in terms of accuracy, we define the performance gain as the\\naccuracy difference between PMP and no PMP as measured on each test set. We show the accuracy gain of\\nPMP as a function of number of finetuning sequences, where the pre-training dataset consists of a mixture\\nof StackExchange, Reddit, and Wikipedia which we simply refer to as the â€˜Mixâ€™. Furthermore, the lightly\\nshaded violet curves show results for individual finetuning evaluations, while the bold violet curve shows\\ntheir mean. More detailed breakdown of results is shown in figure 17 and figure 32.\\n\\nWe are also interested in how finetuning scales with model size, especially in the small data limit, as shown\\nin figure 16. We find that at 1k finetuning sequences (or 500 pairs), PMP on the Mix dataset improves\\nperformance significantly for models larger than âˆ¼ 1B parameters, but does not appear to benefit small\\nmodels. Furthermore, at 10k finetuning sequences (or 5000 pairs), PMP Mix also benefits large models, but\\nto a lesser extent. We also show results for scaling of the best-achieved loss with model size on the finetuning\\nevaluation datasets in figure 28 in the appendix.\\n\\n21\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Hellaswag (52B)\\n\\nNo PMP\\nPMP Mix\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.45\\n\\n0.50\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Learn to Summarize (52B)\\nNo PMP\\nPMP Mix\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Ethics: Commonsense Morality (52B)\\nNo PMP\\nPMP Mix\\nNo PMP (HARD)\\nPMP Mix (HARD)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Ethics: Deontology (52B)\\nNo PMP\\nPMP Mix\\nNo PMP (HARD)\\nPMP Mix (HARD)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Ethics: Justice (52B)\\nNo PMP\\nPMP Mix\\nNo PMP (HARD)\\nPMP Mix (HARD)\\n\\n103 104 105\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Ethics: Virtue (52B)\\n\\nNo PMP\\nPMP Mix\\nNo PMP (HARD)\\nPMP Mix (HARD)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.50\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\n0.85\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Ethics: Utilitarianism (52B)\\n\\nNo PMP\\nPMP Mix\\nNo PMP (HARD)\\nPMP Mix (HARD)\\n\\nFigure 17 Transfer to various finetuning evaluations from PMP (on the â€˜Mixâ€™ pre-training dataset, shown\\nas violet curves) and no PMP (black curves). Each of the five Ethics datasets (Commonsense Morality,\\nDeontology, Justice, Utilitarianism, and Virtue) has both an â€˜easyâ€™ test set (solid curves) and a â€˜hardâ€™ test\\nset (dashed curves), but only one training set. The x-axis shows the number of finetuning training sequence\\npairs, while the y-axis shows accuracy as evaluated on a held-out test set. All results are shown for the\\n52B parameter model. In most cases PMP significantly improves sample efficiency, especially in the . 10k\\nsequence pairs regime. Plots show 4 training epochs for each eval.\\n\\n22\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.00\\n\\n0.01\\n\\n0.02\\n\\n0.03\\n\\n0.04\\n\\n0.05\\n\\n0.06\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP \\n On Finetuning\\n\\nPMP Mix\\n\\nFigure 18 In this figure we show the benefit of â€˜binarizingâ€™ PMP datasets; the y-axis is the gain in finetuning\\naccuracy with binarization versus without binarization. The x-axis counts number of text sequences seen by\\nthe model, with 2 sequences corresponding to a single preference-modeling comparison.\\n\\nAs already mentioned, pre-training on binary distributions typically transfers better than ranked\\ndistributionsâ€”this is discussed more in section 4.3. In addition, we found that the following factors also\\nhelped, all of which have been incorporated into our experiments unless otherwise stated:\\n\\nâ€¢ Adding to the preference modeling loss a basic language modeling loss to teach the model to imitate\\nthe â€˜goodâ€™ sequence in each preference modeling pair, as discussed in section C.3.\\n\\nâ€¢ Appending an end-of-context token to each sequence on top of which the preference modeling score\\nis predicted, as discussed in C.4.\\n\\n4.3 Ranked Preference Modeling vs Binary Discrimination for PMP\\n\\nRecall that our pre-training dataset comes in two forms: ranked and binary. So far we have only presented\\nfine-tuning results from binary PMP, but here we also compare to ranked pre-training, and show that binary\\npre-training typically transfers better than ranked-pre-training. This may be counter-intuitive because prefer-\\nence models are designed to learn an Elo-like score, which can be interpreted as a ranking, and so it is natural\\nto expect ranked pre-training to outperform binary. The goals of this section are to (1) present empirical\\nresults showing the difference, and (2) provide and briefly test a plausible explanation.\\n\\nIn figure 18 we show the advantage of binary pre-training over ranked pre-training. In particular, for each\\nfinetuning evaluation, we plot the accuracy difference vs. the number of training sequences, which can be\\nseen as lightly shaded violet curves. Since there is significant variance in these results, we also take the mean\\nover all such evaluations, giving the bold violet curve. On average, we find that binary pre-training performs\\n+5% better at 500 sequence pairs, and +2% better at 5k sequence pairs. More detailed plots of binary vs.\\nranked pre-training can be found in figure 37 in the appendix, showing the accuracy difference for multiple\\nindividual pre-training datasets and multiple individual finetuning evaluations.\\n\\nThis result surprised some of the authors, but with hindsight we found a plausible explanation. When pre-\\ntraining on a ranked dataset, the model learns a corresponding ranked ordering for sample sequences (rep-\\nresented by a scalar value for each sample). However, downstream evaluations may have rankings that are\\nqualitatively very different, which may then require the pre-trained model to â€˜unscrambleâ€™ its existing ratings.\\nOn the contrary, binary pre-training establishes a much less â€˜rigidâ€™ score, which may require less â€˜unscram-\\nblingâ€™ and thus may transfer more easily to very different datasets. We designed an experiment with synthetic\\ndata that appears to confirm this hypothesis, which we describe in detail in appendix C.6.\\n\\n23\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Learn to Summarize\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On HellaSwag\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\nFigure 19 We compare PMP on â€œhuman-humanâ€ vs â€œhuman-modelâ€ Reddit datasets by evaluating their\\ntransfer performance (for the latter, the â€œmodelâ€ pre-training samples were all generated by a 2.7B model). It\\nappears that â€œhuman-modelâ€ pre-training transfers better on Learn to Summarize and significantly better on\\nHellaSwag, possibly because both evaluations contain model-generated data, thus giving â€œhuman-modelâ€ an\\nadvantage. While our primary focus has been on â€œhuman-humanâ€, this results suggests that â€œhuman-modelâ€\\nalso deserves further investigation.\\n\\n4.4 Human-Model vs Human-Human Comparisons for PMP\\n\\nAll our PMP datasets so far consist of â€˜human-humanâ€™ comparisons, by which we mean that both samples in\\neach pair are human-written. For this section we consider an alternative dataset consisting of â€˜human-modelâ€™\\ncomparisons, as we are interested in whether this might improve transfer performance. It is also noteworthy\\nthat such comparisons should be easy to generate, since any high-quality fragment of human text might be\\ncompared to model-generated text on the same subject.\\n\\nThe â€˜human-modelâ€™ dataset was created by following these steps:\\n\\nâ€¢ We first finetuned a language model to imitate the â€˜goodâ€™ samples in our ranked pre-training dataset\\n(e.g., StackExchange, Reddit, or Wikipedia).\\n\\nâ€¢ For each sample pair in the ranked pre-training dataset, we kept the â€˜goodâ€™ sequence, but replaced\\nthe â€œbadâ€ sequence with a sample from the finetuned language model.\\n\\nConsequently, the resulting dataset has the same number of pairs as the original ranked pre-training dataset,\\nwith â€œgoodâ€ human-written sequences and â€œbadâ€ model-written sequences. For these experiments we used\\nthe Reddit PMP dataset, and a 3B model for sample generation.\\n\\nWe found that PMP on the human-model Reddit dataset transfers significantly better to HellaSwag, and\\nsomewhat better to Learn to Summarize, as shown in figure 19. Transfer to the Ethics evaluations (see figure\\n36) is more ambiguous, showing both positive and negative signals. Our suspicion is that human-model pre-\\ntraining has a particular advantage on downstream finetuning evaluations that contain model-generated dataâ€”\\nindeed, all incorrect answers on HellaSwag are model-generated, and Learn to Summarize has a significant\\namount of model-generated summaries, while Ethics has no model-generated data. Nonetheless, PMP with\\nhuman-model generated data deserves further investigation, especially since it can be applied to such a great\\nvariety of data distributions.\\n\\n5 Discussion\\n\\n5.1 Related Work\\n\\nThere have been many works related to AI safety and alignment, including some suggestions for global\\nresearch plans such as [AOS+16] and [HCSS21]. Work using human feedback to learn summarizations\\n[SOW+20] has particular relevance to our work, since they observe that preference modeling and RL lead to\\ndramatic improvements compared to imitation learning. One of our motivations was to understand when such\\nimprovements can be expected from these techniques, and how we can take maximal advantage of human\\n\\n24\\n\\n\\n\\nfeedback data. To inquire into our modelsâ€™ alignment we discussed ethics evaluations from [HBB+21],\\nadversarial honesty evaluations from [LHE21], and toxicity evaluations from [GGS+20].\\n\\nOur use of a small amount of high-quality data for alignment is most similar to [SD21]. On the other end\\nof the spectrum, a rather different technique is to filter pretraining data, as discussed in [NRA+21]. Our use\\nof prompts was motivated by observations about the behavior of large language models [BMR+20]. Some\\nother observations about prompting and the dependence of prompt-tuning on scale were made in [LARC21]\\nthough we did not utilize prompt tuning. The fact that larger models are less subject to forgetting [RDR20]\\nmay be related to the fact that larger models do not incur significant alignment taxes.\\n\\nOur coding models are similar to those discussed in [CTJ+21]. They also performed alignment-related eval-\\nuations, though with high and low quality code examples rather than a natural language prompt. The recent\\nwork [AON+21] evaluated language models (without a great deal of code training) on code, including in a\\nconversational manner.\\n\\nMany papers have studied scaling laws [HNA+17, RRBS19, KMH+20, Jon21]. A few have compared dis-\\ncriminators or preference models to imitation learning, including [ILP+18, SOW+20, WOZ+21]. The T-REX\\nIRL method [BGNN19] uses ranked preference modeling to improve on GAIL and on imitation learning. The\\nauthors of [AAB+21] compared GAIL [HE16] to conventional imitation learning in an RL context, and found\\nin some cases that GAIL scaled significantly better with dataset size. Experiments comparing RL and be-\\nhavioral cloning with the decision transformer [CLR+21] are also somewhat similar to our comparison of\\npreference modeling and imitation learning. Very recently [CKB+21] performed experiments that are very\\nsimilar to our work on code correctness, except that they studied mathematical problem solving, and focused\\nmore on dataset size scaling. Interestingly, they find that a verifier (aka binary discriminator) has a more\\nfavorable dataset size scaling as compared to imitation learning. However, their experiments are likely in a\\ndifferent regime from ours â€“ they were severely data limited, training on only thousands of math problems,\\nwhereas our models were trained on millions of python files, perhaps giving us a much stronger baseline for\\nimitation learning.\\n\\nVarious works [LARC21, WBZ+21, SWR+21, ATS+21] have noted that by finetuning on a large variety\\nof simple tasks, one can improve model performance generally and achieve instruction-following behavior.\\nThis idea is closely related to the â€˜preference model pre-trainingâ€™ approach we have discussed. The work\\nwith the most similar approach to PMP for alignment was the very recent Delphi [JHB+21], which trains a\\ngeneral-purpose ethical critic. Their work differs insofar as we investigate transfer between distributions that\\nare only distantly related (e.g. from Stack Exchange to summarization), whereas they focus on transfer from\\nand to data related to ethics.\\n\\n5.2 Broader Impacts\\n\\nThis work was motivated by the problem of technical AI alignment, with the specific goal of training a\\nnatural language agent that is helpful, honest, and harmless. We believe this work is important because of the\\npotential for very broad impacts from AI and from language models in particular, especially if progress in the\\nfield continues at its current rapid pace [Bow21].\\n\\nWe hope that by directly approaching a general and ambitious problem, we will either (1) fail due to spe-\\ncific technical challenges, which we would then attempt to more precisely articulate for further study from\\nthe research community, or (2) convince ourselves that we have addressed technical alignment for currently\\navailable models.14 In the event of the second outcome, we would expect our results to be carefully interro-\\ngated by the research community. There would also be a need for further empirical investigations into how\\nwell these techniques scale to more capable models in terms of both robustness and efficiency, and how likely\\nit is that we will be able to detect alignment failures in more capable models.\\n\\nThe road to hell is paved with good intentions, and as such we shouldnâ€™t be complacent with concerns asso-\\nciated with alignment work. Foremost in our minds is that advances in aligning AI with human values do not\\ndepend on any specific choice for these values. Efficient alignment techniques could be used to train highly\\ncapable systems that do things we consider to be bad, for instance systems for misinformation, censorship,\\nor oppression. Even terms like helpful, honest, and harmless are ambiguous and can be in tension with each\\nother, and itâ€™s easy to imagine them distorted beyond their original meaning, perhaps in intentionally Or-\\n\\n14Of course, we may fail in uninteresting ways, due to our own limitations, and in that case we can only hope that\\nfuture work will be more successful.\\n\\n25\\n\\n\\n\\nwellian ways. And within the context of our own and similar work, the choice of who provides feedback data\\nto train models has broad implications.\\n\\nInformation such as our comparisons among different scaling behavior may also be useful for improving AI\\ncapabilities, without regard for safety. We believe that understanding how and why ML systems work will be\\nessential to improving their safety, and that these sorts of comparisons aid in that effort. Another concern is\\nthat alignment progress might be used as an excuse for carelessness, or to conclude that alignment has already\\nbeen adequately addressed and can subsequently be ignored. Our view is that people and organizations\\nthat deploy AI systems need to take responsibility for their behavior. Research may help to make such\\ndeployments possible, but the question of broader relevance is simply whether deployed AI systems are\\nactually safe and beneficial in practice.\\n\\n5.3 Implications\\n\\nLarger models tend to perform better at most tasks, and there is no reason to expect naive alignment-related\\ntasks to be an exception. In line with these expectations, we find that behavioral alignment tends to improve\\nwith model size, with even the simplest conceivable intervention (i.e. prompting) leading larger models to\\nperform better on alignment-relevant evaluations.\\n\\nOne reason to investigate scaling trends for preference modeling would be to understand how to train better\\npreference models. However, one of our motivations was actually a bit different â€“ it was to set expectations\\nfor the scaling of reinforcement learning. We would expect that if it is very difficult for models to learn\\nto recognize favorable outcomes, they will also have difficulty learning to take actions that produce such\\noutcomes. That is, value function performance should tell us something about the likely performance of\\na trained policy. This logic should become irrefutable when preference models are re-purposed as reward\\nmodels for RL training. So, given that large gains in both absolute performance and scaling are possible\\nwhen training ranked preference models, significant progress on alignment may also be possible.\\n\\nAuthor Contributions\\n\\nYuntao Bai sourced and curated the PMP data with initial help from Ben Mann, conducted the PMP and fine-\\ntuning experiments, suggested investigating the distinctions between binary and ranked preference modeling,\\nand suggested several ML improvements for preference modeling.\\n\\nAnna Chen conducted experiments on scaling trends for imitation learning versus preference modeling, in-\\ncluding on function synthesis (with help from Dawn Drain, Andy Jones, and others). She also conducted\\nthe experiments on GAN-type discriminators and many other evaluations, and suggested improvements for\\npreference modeling and code quality.\\n\\nAnna and Yuntao collaborated on many experiments and on the training and evaluation code for preference\\nmodeling.\\n\\nAmanda Askell developed the conceptualization of alignment in terms of helpfulness, honesty, and harmless-\\nness. Amanda produced the initial mockup of the model interface and helped to design and build it. Amanda\\nsourced and trained workers for the interface, conducted our original A/B testing experiments, and provided\\nguidance on evaluations.\\n\\nBen Mann built most of the human interaction interface and the necessary backend for robust and efficient\\nsampling. Ben led all of our data collection efforts for both language and code data, in collaboration with\\nDanny Hernandez, who has led research on data quality. Ben also contributed to the core language model\\ntraining infrastructure.\\n\\nBen, Yuntao, Anna, and Amanda contributed to research and project planning.\\n\\nDeep Ganguli proposed, conducted, and analyzed experiments on toxicity (with help from Andy Jones and\\nothers) and conducted some of our experiments on alignment taxes. He also contributed to discussions on\\nharms and alignment.\\n\\nDawn Drain trained the code models and helped Anna with code evaluations, including with collecting func-\\ntions with test coverage (with some help from Ben Mann, Andy Jones, and Tom Henighan). Dawn also\\nconducted experiments on alignment taxes with code models.\\n\\n26\\n\\n\\n\\nNicholas Joseph was central to building and maintaining a highly efficient distributed training system for\\nlarge language models and helped with our sampling infrastructure.\\n\\nTom Henighan managed our research cluster, helped build our distributed training system, and did research\\nand experiments on the numerical stability of large language model training. He also helped with ML research\\non large language models. Nova DasSarma has also helped manage the cluster.\\n\\nAndy Jones was central in building our sampling infrastructure. He also provided engineering support to the\\ntoxicity experiments, A/B testing infrastructure, distributed training, and code model data collection.\\n\\nCatherine Olsson contributed crucially to alignment ideas, and provided useful advice for sourcing and train-\\ning contractors to test our models.\\n\\nLed by Tom Brown in collaboration with Sam McCandlish, much of the technical staff at Anthropic con-\\ntributed to efficient distributed model training and sampling, the underlying ML, and cluster stability. Core\\ncontributors include Nicholas Joseph, Tom Henighan, and Andy Jones. Nelson Elhage, Kamal Ndousse, Zac\\nHatfield-Dodds, and Ben Mann also contributed to this infrastructure.\\n\\nCatherine Olsson and Jared Kaplan wrote the HHH prompt, and along with Deep Ganguli, Anna Chen,\\nAmanda Askell, and many others wrote most of the alignment evaluations. Jackson Kernion helped improve\\nthe alignment evaluations and source workers to interact with our models.\\n\\nJared Kaplan, Yuntao Bai, Anna Chen, Amanda Askell, Deep Ganguli, and Ben Mann wrote the paper, with\\nhelpful comments from everyone at Anthropic.\\n\\nDario Amodei, Chris Olah, and Jack Clark contributed expertise and advice throughout the project.\\n\\nSam McCandlish led model pretraining efforts, often in collaboration with Jared Kaplan. Sam also led the\\noverall synthesis of engineering and research efforts.\\n\\nJared Kaplan conceived and led the project. He conducted some initial experiments on preference modeling\\nand many of the experiments on prompting and context distillation.\\n\\nAcknowledgments\\n\\nWe thank Daniela Amodei, Jia Yuan Loke, Liane Lovitt, Taylor Rogalski, and Timothy Telleen-Lawton for\\nsupport with this project, and Sam Bowman, Collin Burns, Ethan Dyer, Owain Evans, David Krueger, Jan\\nLeike, Liane Lovitt, Helen Ngo, and Jeff Wu for comments on the draft. We thank Paul Christiano for helpful\\ndiscussions.\\n\\nA Language Model Pre-training\\n\\nAll the decoder-only [LSP+18] Transformer [VSP+17] models we train have a fixed aspect ratio\\ndmodel/nlayer = 128, as it has been shown that this is roughly optimal [KMH+20]. Their MLPs up-project\\nby a factor of 4, so that dff = 4dmodel. This means that their total non-embedding parameter count is\\nN = 12nlayerd\\n\\n2\\nmodel â‰ˆ (1.97 Ã— 10\\n\\n5)n3layer. The models have a context window of 8192 tokens with a BPE\\n[SHB15] vocabulary of size nvocab = 216 trained on a mixture of natural language and python code in a\\nsubstantially similar manner to GPT-3 [BMR+20] and its precursors [RNSS18, RWC+19].\\n\\nThe training dataset is composed of 90% natural language and 10% python code. All components of the\\nNL and code datasets were globally fuzzily deduplicated [BMR+20], and we train for one epoch on all sub-\\ncomponents (i.e. we do not repeat any data). The natural language dataset was composed of 55% heavily\\nfiltered common crawl data (220B tokens), 32% internet books (128B tokens), and some smaller distribu-\\ntions including OpenWebText, Wikipedia, Stack Exchange, Arxiv, Legal and Patent documents, Ubuntu-IRC\\ndiscussion, and movie scripts, most of which we sourced from The Pile [GBB+20].\\n\\nOur code models were further finetuned for 100B tokens on a distribution of python code containing about\\n45B unique tokens, so for a bit more than two epochs of training.\\n\\n27\\n\\n\\n\\nnlayer dmodel Parameters (N ) Training FLOPs\\n\\n4 512 13M 3.0e19\\n\\n6 768 42M 1.0e20\\n\\n10 1280 197M 4.7e20\\n\\n16 2048 810M 1.9e21\\n\\n24 3072 2.7B 6.5e21\\n\\n40 5120 13B 3.0e22\\n\\n64 8192 52B 1.2e23\\n\\nTable 1 Basic model parameters including pretraining compute from 400B tokens of training.\\n\\nFigure 20 Left: Comparing context distillation, the full prompt, finetuning on the HHH prompt, and no\\nintervention on our HHH evaluations. Right: By adding two human-assistant conversations we can improve\\nperformance after finetuning on the prompt. Since responses in the HHH evaluations vary greatly in length,\\nin all cases we evaluate using conditional probabilities.\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.20\\n\\n0.25\\n\\n0.30\\n\\n0.35\\n\\n0.40\\n\\n0.45\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nTruthfulQA (MC1) with Various Evaluation Metrics\\nDistilled Mutual Info\\nLM Mutual Info\\nDistilled Mean Logprobs\\nLM Mean Logprobs\\nDistilled Sum Logprobs\\nLM Sum Logprobs\\n\\nFigure 21 We show results on the adversarial TruthfulQA dataset when evaluating with both mutual infor-\\nmation, mean logprobs, and summed logprobs (the official metric for MC1). The model sizes trends for both\\nare similar. These are all zero-shot evaluations on language models, with and without context distillation on\\nthe HHH prompt.\\n\\n28\\n\\n\\n\\nB More Details on Prompting, Context Distillation, and Evaluations\\n\\nB.1 Alignment Tax Evaluations\\n\\nFor alignment tax evaluations, we used a formulation that naturally fit with the human-assistant dialogue. For\\nLambada we used the format:\\n\\nHuman: Can you tell me a story?\\nAssistant: Sure, hereâ€™s a draft: <LAMBADA PROMPT>\\n\\nFor function synthesis we used:\\n\\nHuman: Can you help me write this Python function? Iâ€™ve already\\nwritten the functionâ€™s signature and docstring, but\\nIâ€™m not sure how to write the functionâ€™s body. It starts like this:\\n<SIGNATURE_PLUS_DOCSTRING>\\n\\nAssistant: Sure thing, here you go! Iâ€™ve tested this function myself\\nso I know that itâ€™s correct:\\n<SIGNATURE_PLUS_DOCSTRING>\\n\\nB.2 Toxicity Analysis Details\\n\\nTo measure toxicity, we first sample text from our models conditioned on prompts from the RealToxici-\\ntyPrompts dataset [GGS+20]. Each prompt in this dataset is labeled as either toxic or non-toxic. In order to\\nminimize the computational cost incurred by sampling from large models (especially when primed with the\\nlarge context), we select a random sample (stratified to have an equal number of toxic and non-toxic prompts)\\nof 1K out of the available â‰ˆ 100K prompts.\\nNext, following [GGS+20], we sample for each prompt 25 sequences of up to 20 tokens each. We use tem-\\nperature T = 1 and top P = 0.90. For the prompted and distilled models we make two minor modifications.\\nFirst, we embellish the RealToxicityPrompt to fit with the human-assistant dialogue as follows:\\n\\nHuman: Can you help me complete a sentence? Iâ€™ve already written the\\nfirst part of the sentence, but Iâ€™m not sure how to complete\\nit to say something non-toxic. The first part of the sentence is:\\n<REALTOXICITYPROMPT>\\n\\nAssistant: Sure thing, here is a non-harmful completion of your\\nsentence: <REALTOXICITYPROMPT>\\n\\nWe simply replace <REALTOXICITYPROMPT> with a prompt from the dataset. Second, we never sample\\nthe tokens Human: and - - - - - in order to stop the models from emulating a humanâ€™s response to the\\ncompleted sentence. We found that such emulated human responses (which typically looked like Human:\\nThanks! This looks great!) significantly decreased toxicity (Figure 22). In fact, emulating hu-\\nman responses had an effect size larger than that of the alignment interventions, which confounded the results.\\n\\nTo measure the toxicity of the model generated text, we used an open source toxicity detector [HU20] that\\noutputs a score, between 0 and 1 with a higher score corresponding to more toxic content. In particular, we\\nused the â€™unbiasedâ€™ RoBERTa based model, which was trained on data from the Jigsaw Unintended Bias in\\nToxicity Classification Kaggle competition 15. The model achieves an AUC score of 0.9374 on predicting a\\nhuman-annotated toxicity label. At the time of writing, the highest leaderboard AUC score is 0.9473. Our\\nusage of this model represents a departure from [GGS+20], and other work on toxicity in language models,\\nwhich typically rely on the widely used and publicly available Perspective API 16 for toxicity detection. We\\nuse the open source toxicity detector purely for ease of implementation. However, we verified that the open\\nsource toxicity scores are strongly correlated the Perspective toxicity scores (for the prompts we sampled\\nfrom RealToxicityPrompts dataset, r = 0.829) and that the distributions of toxicity are similar for both\\ntoxicity detectors. We will leave a re-analysis of toxicity with the Perspective API for future work, though\\nwe do not expect this to significantly affect our main findings.\\n\\n15https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview\\n16https://www.perspectiveapi.com/\\n\\n29\\n\\n\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\n0.08\\n\\n0.10\\n\\n0.12\\n\\n0.14\\n\\n0.16\\n\\nToxicity in Response to Toxic Prompts\\n\\nLM\\nLM+Prompt\\nLM+Prompt-Human\\nLM+Context Distillation\\nLM+Context Distillation-Human\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\nNumber of Parameters\\n\\n0.02\\n\\n0.03\\n\\n0.04\\n\\n0.05\\n\\n0.06\\nA\\n\\nve\\nra\\n\\nge\\n T\\n\\nox\\nic\\n\\nity\\n\\nToxicity in Response to Non-Toxic Prompts\\n\\nFigure 22 Average toxicity tends to decrease when prompted (orange) and context distilled (green) models\\nemulate human responses (dashed lines) relative to when when they do not (solid lines). Left: For non-toxic\\nprompts, allowing aligned models to emulate human responses tends to slightly decrease average toxicity.\\nRight: For toxic prompts, allowing aligned models to emulate human responses tends to significantly de-\\ncrease average toxicity, which dwarfs and confounds the effect of the alignment interventions.\\n\\nIn Figure 8 we report the mean toxicity score averaged across all 500 prompts and 25 samples per prompt.\\nThis represents a departure from [GGS+20] and other work on toxicity in language models, which typically\\nreport the metrics: Expected Maximum Toxicity and Probability of Toxicity. The Expected Maximum Tox-\\nicity metric reports the maximum toxicity across the 25 continuations per prompt, averaged across all 500\\nprompts. The probability of toxicity metric captures the average, across prompts, of an indicator variable\\nthatâ€™s 1 if a given sample has a toxicity score > 0.5, and 0 otherwise, across continuations. We report these\\nmetrics in Figure 23. We note that, in general, likely due to the maximum and thresholding operations of each\\nmetric prior to averaging, both metrics have large standard deviations and do not scale smoothly with model\\nsize. Regardless, the general findings from the main text remain true: both context distillation and prompting\\nreduce toxicity and the reduction in toxicity according to these metrics is greater as models get larger. We\\nalso observe that both Expected Maximum Toxicity and Probability of Toxicity tend to be strongly correlated\\nwith each other.\\n\\nTo gain intuition about why the simple average toxicity score scales smoothly with model size, we inspect\\nthe probability distribution of toxicity scores across model sizes for the base language model (LM, Figure\\n24 Left). The distribution is bimodal with one peak for low toxicity scores and and a relatively smaller\\npeak for high toxicity scores. As the model size increases, probability mass tends to shift smoothly from the\\nlow toxicity peak to the high toxicity peak. Computing the mean of these distributions captures this smooth\\ntransition in mass between modes. We also inspect the influence of the alignment interventions for the largest\\n50B parameter model (Figure 24 Right). We see that the alignment interventions tend to undo the effect of\\nscaling up model sizes in that they shift probability mass away from the toxic mode towards the less toxic\\nmode.\\n\\nB.3 TruthfulQA Formatting\\n\\nFor evaluations of TruthfulQA with context distilled models, we used the format:\\n\\nHuman: <QUESTION>\\n\\nAssistant: <ANSWER>\\n\\nand evaluate the probability of the answer tokens. With our pure language models (no prompt or context\\ndistillation), we tried using both this format and even simpler format <QUESTION> <ANSWER>, and found\\nthat the latter did very slightly better, and so we have used results from that format in all figures.\\n\\nB.4 A Comment on Lambada Formatting\\n\\nWe performed a fairly complicated evaluation on Lambada in section 3.2, which involved finetuning on the\\ntraining set. Therefore, we used the official version of the dataset, which has a number of typos and strange\\n\\n30\\n\\n\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\n0.32\\n\\n0.34\\n\\n0.36\\n\\n0.38\\n\\n0.40\\n\\n0.42\\n\\n0.44\\n\\nE\\nxp\\n\\nec\\nte\\n\\nd \\nM\\n\\nax\\nim\\n\\num\\n T\\n\\nox\\nic\\n\\nity\\n\\nToxicity in Response to Non-Toxic Prompts\\n\\nLM\\nLM+Prompt\\nLM+Context Distillation\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\n0.72\\n\\n0.74\\n\\n0.76\\n\\n0.78\\n\\n0.80\\n\\n0.82\\n\\nToxicity in Response to Toxic Prompts\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\nNumber of Parameters\\n\\n0.275\\n\\n0.300\\n\\n0.325\\n\\n0.350\\n\\n0.375\\n\\n0.400\\n\\n0.425\\n\\n0.450\\n\\nP\\nro\\n\\nba\\nbi\\n\\nlit\\ny \\n\\nof\\n T\\n\\nox\\nic\\n\\nity\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\n0.74\\n\\n0.76\\n\\n0.78\\n\\n0.80\\n\\n0.82\\n\\n0.84\\n\\n0.86\\n\\nFigure 23 Expected Maximum Toxicity (top plots) and Probability of Toxicity (bottom plots) tend to scale\\nless smoothly with model size in response to both non-toxic (left plots) and toxic (right plots) prompts. Top\\nLeft: Expected Maximum Toxicity in response to non-toxic prompts is generally lower for both prompted\\n(orange) and context distilled (green) models relative to unaligned (blue) models. Top Right: Expected\\nMaximum Toxicity in response to toxic prompts only decreases relative to unaligned models only for larger\\nmodels and increases otherwise. Bottom Left: Probability of Toxicity in response to non-toxic prompts\\nexhibits the same general trend as Expected Maximum Toxicity in response to non-toxic prompts Bottom\\nRight: Probability of Toxicity in response to toxic prompts also exhibits same general trend as Expected\\nMaximum Toxicity.\\n\\n10\\n4\\n\\n10\\n3\\n\\n10\\n2\\n\\n10\\n1\\n\\n10\\n0\\n\\nToxicity Score\\n\\n0.00\\n\\n0.02\\n\\n0.04\\n\\n0.06\\n\\n0.08\\n\\n0.10\\n\\nD\\nen\\n\\nsi\\nty\\n\\nDistribution of Toxicity Conditioned on Model Parameters\\n\\n# Parameters\\n52.4B\\n12.8B\\n2.76B\\n200M\\n43.2M\\n12.8M\\n\\n10\\n4\\n\\n10\\n3\\n\\n10\\n2\\n\\n10\\n1\\n\\n10\\n0\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\nEffect of Alignment on Distribution Toxicity for 50B Models\\n\\nLM+Context Distillation\\nLM+Prompt\\nLM\\n\\nFigure 24 The distribution, estimated via kernel density estimation with a Gaussian kernel, of toxicity\\nscores is bimodal, with one peak for for low toxicity scores and a relatively smaller peak for high toxicity\\nscores. Left: For a standard LM, as the model size increases, probability mass tends to shift from the low\\ntoxicity peak to the high peak. Right: Conversely, for a 50B parameter model (blue), prompting (orange) and\\ncontext distillation (green) tends to shift mass from the high peak to the low peak.\\n\\n31\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nThe Importance of Data Formatting\\n\\n0-Shot Awkwardly Formatted \\n20-shot Awkwardly Formatted\\n0-Shot Nicely Formatted\\n20-shot with Fill-In-The-Blanks\\n\\nFigure 25 We show Lambada results with three different formats â€“ an awkward format from the orig-\\ninal/official Lambada dataset, a format constructed by OpenAI, and a fill-in-the-blanks format used with\\nGPT-3 [BMR+20] that performs very well with few-shot learning.\\n\\nwhitespace and punctuation choices. However, in section 2.2.4 we included some zero-shot Lambada eval-\\nuations to assess â€˜alignment taxesâ€™. These formatting choices make a very large difference in performance,\\nas shown in figure 25. In particular, we believe this explains in large part why the results from figure 13 are\\ncomparatively weak.\\n\\nTo be explicit, here is an example from the nicely formatted version:\\n\\n\"Helenâ€™s heart broke a little in the face of Miss Mabelâ€™s selfless\\ncourage. She thought that because she was old, her life was of less\\nvalue than the others. For all Helen knew, Miss Mabel had a lot more\\nyears to live than she did. \"Not going to happen,\" replied Helen\\n\\nAnd hereâ€™s an example from the original version:\\n\\nit was very freeing . there would be no more hiding , no more tiptoeing\\naround the conversation . logan and i were together . plain and\\nsimple . we cared for each other and were doing what felt right .\\nthat did nâ€™t stop my stomach from sinking the second door swung open\\n. dr. andrews strode into the room , casting a cautionary glance in my\\ndirection before turning his attention to logan\\n\\nThe difference in performance between these formats might be regarded as an alignment failure itself. For\\nthis reason, we were interested in whether the HHH prompt reduced the gap between Lambada formats, but\\nwe did not find this effect.\\n\\nB.5 Context Distillation Finetuning\\n\\nTo perform context distillation in practice, we prepended both the HHH prompt and then Human: (signi-\\nfying the beginnning of a new conversation) to text samples. We then performed a forward pass with the 52B\\nmodel and stored the top 50 log-probabilities for each token, along with their indices within the vocabulary.\\nWe used a half-and-half mixture of generic pretraining data and Stack Exchange questions. We formatted the\\nlatter to use the Assistant: label before the answers, as an attempt to stay near the human-assistant\\ndistribution. We filled out the remainder of the context with distillation data, providing about 1500 tokens per\\nsequence (subtracting the length of the prompt).\\n\\nAfter generating this data, we finetuned all model sizes on it with KL loss between the stored log-probabilities\\nand the model-predicted probabilities. Since we only stored the top 50 log-probs, for each token this KL\\nwas actually a 51-category comparison, with the extra category coming from the aggregation of all other\\npossibilities besides the top 50 from the prompted 52B model.\\n\\n32\\n\\n\\n\\nFigure 26 Left: Per-token losses when counting, along with Laplaceâ€™s prediction that â€œif the sun has risen\\nn times in a row, the probability it will not rise tomorrow âˆ¼ 1/nâ€. Right: An extreme illustration of the\\ndifference between prompting (conditioning) and finetuning (altering the expected data distribution). The\\nfinetuned models were trained on the sequence 1,2, ..., 63. With sufficient finetuning these models become\\nvery confident about counting, but never learn that the first few tokens should be 64, 65, ...\\n\\n800M 3B 13B 52B\\n\\n200M 0.34 0.30 0.13 0.16\\n\\n800M - 0.40 0.26 0.25\\n\\n3B - - 0.34 0.34\\n\\n12B - - - 0.45\\n\\n12B Distilled - - 0.46 -\\n\\n52B Distilled - - - 0.46\\n\\n12B Short-Prompt - - 0.44 -\\n\\n52B Short-Prompt - - - 0.47\\n\\nTable 2 In this table we show the fraction of head-to-head model comparisons where one model was pre-\\nferred to the other by contractors. The numbers represent the \"win rate\" of the models indicated in each\\nrow against those indicated by the column labels. All models were presented with the full 4600 word HHH\\nprompt, and we sampled responses at T = 1 and top P = 0.95. We include a dash where we made no\\ncomparison, or where the results are trivially implied by pâ†’ 1âˆ’ p across the diagonal.\\n\\nB.6 Estimator of Accuracy When Re-Ranking Samples\\n\\nWhen studying the performance of models that rank sample quality (with a PM or log-probs from a language\\nmodel), weâ€™re interested in the measuring the fraction of problems that are solved by the the top-ranked\\nsample, when there are k samples in total. Here we derive an unbiased estimator for this quantity when using\\na finite pool of N â‰¥ k samples. The analysis builds on estimates for pass@k from [CTJ+21].\\nFor each problem, we sample a list of N samples, and then calculate both the score for ranking (by a model\\nof interest) and whether each individual response was correct. Then for each problem, we estimate:\\n\\nacc@k(S) =\\n\\nNâˆ‘\\ni=1\\n\\nacc(si) Â·\\n(\\nNâˆ’i\\nkâˆ’1\\n)(\\n\\nN\\nk\\n\\n)\\nwhere S is a list of ranked samples, from better to worse scores; acc(Â·) is the accuracy of the sample (correct\\nor incorrect, so these are all 1 or 0);\\n\\n(\\nN\\nk\\n\\n)\\nis the total number of possible combinations when choosing k of\\n\\nN samples. Then, crucially,\\n(\\nNâˆ’i\\nkâˆ’1\\n)\\n\\nis the number of combinations where sample si is the top-ranked sample\\namong the k chosen samples. So the ratio of binomial coefficients in equation (B.6) is the probability that the\\nith sample is chosen and is the highest ranked sample in a group of k.\\n\\n33\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.625\\n\\n0.630\\n\\n0.635\\n\\n0.640\\n\\n0.645\\n\\n0.650\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\n L\\nos\\n\\ns\\n\\nPre-train Performance on StackExch\\nMix\\nStackExch\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.60\\n\\n0.61\\n\\n0.62\\n\\n0.63\\n\\n0.64\\n\\n0.65\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\n L\\nos\\n\\ns\\n\\nPre-train Performance on Reddit\\nMix\\nReddit\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.26\\n\\n0.28\\n\\n0.30\\n\\n0.32\\n\\n0.34\\n\\n0.36\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\n L\\nos\\n\\ns\\nPre-train Performance on Wiki\\n\\nMix\\nWiki\\n\\nFigure 27 Scaling laws for PMP, showing PM loss vs. model size, for each of the three pre-training datasets\\nStackExchange, Reddit, and Wikipedia, as evaluated on a held-out test set after one training epoch. The â€œMixâ€\\nis simply a mixture consisting of one epoch each of the three pre-training datasets. We do not know why the\\n52B seems to be off-trend. This could be caused by (1) being in a data-limited regime, or (2) being limited\\nby the entropy of the pre-training distribution, or (3) sub-optimal choice of hyperparameters. Nonetheless, it\\nis interesting to observe (e.g., from figure 5) that the 52B still transfers significantly better than the smaller\\nmodels.\\n\\nThe overall metric is simply the mean of this quantity over all the problems.\\n\\nC More Details on Preference Models\\n\\nC.1 Preference Model Pre-training\\n\\nWe now describe how the ranked pre-training datasets were prepared for each domain. The binarization\\nprocedure outlined in 4.1 was subsequently applied to convert each ranked dataset to a binary one.\\n\\nâ€¢ StackExchange: The StackExchange Data Dump17 consists of questions and answers from the\\nStackExchange website. For each question, we evaluate the â€˜scoreâ€™ of all answers, where the score\\nis defined as the log2(1+upvotes) rounded to the nearest integer, plus 1 if the answer was accepted\\nby the questioner (we assign a score of âˆ’1 if the number of upvotes is negative). In order to make\\npairwise comparison data for PMP, we sample two answers with distinct scores, skipping questions\\nwhere this is not possible. For each question-answer pair, the corresponding context is formatted as\\n\\nQuestion: ...\\nAnswer: ...\\n\\nWe prepared 5.8M training pairs and 59k test pairs.\\nâ€¢ Reddit: The Pushshift Reddit data dump18 consists of posts and comments from the Reddit website.\\n\\nFor each Reddit post, we sample a pair of comment sequences differing only in the final comment.\\n17https://archive.org/details/stackexchange\\n18https://files.pushshift.io/reddit/\\n\\n34\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n10 1\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\nin\\ng \\n\\nLo\\nss\\n\\nFinetuning Performance After PMP\\nHellaswag\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n2 Ã— 10 1\\n\\n3 Ã— 10 1\\n\\n4 Ã— 10 1\\n\\n6 Ã— 10 1\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\nin\\ng \\n\\nLo\\nss\\n\\nFinetuning Performance After PMP\\nCommonsense Morality\\nJustice\\nDeontology\\nVirtue\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n5 Ã— 10 1\\n\\n6 Ã— 10 1\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\nin\\ng \\n\\nLo\\nss\\n\\nFinetuning Performance After PMP\\nLearn to Summarize\\nUtility\\n\\nFigure 28 Scaling trends with model size for the best achieved comparison test loss on various final fine-\\ntuning evaluations. We have grouped datasets together based on the dynamic range in the loss. The results\\nare measured after one training epoch each for Learn to Summarize and Hellaswag, and four training epochs\\nfor each Ethics eval. In all cases larger models perform better, as expected. Sometimes we see a fairly\\nclean power-law trend in the loss, but often there are significant deviations, including perhaps an interesting\\nimprovement in the slope on some datasets in the hundred million parameter range.\\n\\nWe then label the sequence whose final comment has the higher number of user upvotes as the\\nâ€œbetterâ€ sequence, thus forming a preference modeling pair. For each post and comment sequence,\\nthe context is formatted as\\n\\nSUBMISSION by username: ...\\nCOMMENT by username: ...\\nCOMMENT by username: ...\\n\\nwhere each username is replaced with the corresponding authorâ€™s alias. We also removed deleted\\ncomments and comments from bots. We prepared 1.1M training pairs and 11k test pairs.\\nNote: We also made an effort to filter away poor or irrelevant data. For instance, we restrict to a\\nâ€œwhitelistâ€ of subreddits that we believe have the highest data quality. We specifically chose not to\\ninclude AmItheAsshole, as it overlaps with one of our fine-tuning datasets, Commonsense Moral-\\nity. Instead we include the subreddits: tifu, explainlikeimfive, WritingPrompts, changemyview,\\nLifeProTips, todayilearned, science, askscience, ifyoulikeblank, UpliftingNews, Foodforthought,\\nIWantToLearn, bestof, IAmA, socialskills, relationship_advice, philosophy, YouShouldKnow, his-\\ntory, books, Showerthoughts, personalfinance, buildapc, EatCheapAndHealthy, boardgames, male-\\nfashionadvice, femalefashionadvice, scifi, Fantasy, Games, bodyweightfitness, SkincareAddiction,\\npodcasts, suggestmeabook, AskHistorians, gaming, DIY, mildlyinteresting, sports, space, gadgets,\\nDocumentaries, GetMotivated, UpliftingNews, technology, Fitness, travel, lifehacks, Damnthatsin-\\nteresting, gardening, programming.\\n\\nâ€¢ Wikipedia: Wikipedia provides a data dump19 of the full edit history for every page. For some\\nedits, a short explanation of the intention behind the edit is provided in the metadata. In particular,\\n\\n19https://en.wikipedia.org/wiki/Wikipedia:Database_download\\n\\n35\\n\\n\\n\\na significant number of edits revert â€œsuspected vandalismâ€, as noted in comments associated with\\nthe edits. Examples of vandalism include edits that are intended to be misleading, counterfactual,\\nor irrelevant to the subject matter of the page. For each such edit, we form a preference modeling\\npair by extracting the contents of the page before and after the edit, with the reverted version labeled\\nas â€œbetterâ€. For each edit, we restrict to only the page sections that had been edited, and make a\\npreference modeling pair for each such section, thus reducing the necessary context length signifi-\\ncantly. For each item in each pair, the context simply consists of the contents of the relevant section,\\nformatted as\\n\\nPAGE TITLE: ...\\nSECTION TITLE: ...\\nSECTION BODY: ...\\n\\nWe also made an effort to clean out various irrelevant metadata, such as hyperlinks, citations, data\\ntables, and placeholders for images. We also removed uninteresting sections such as references and\\nbibliography. We made 1.4M training pairs and 14k test pairs.\\n\\nâ€¢ Mix: We also consider a mixture (i.e., union) of StackExchange, Reddit, and Wikipedia, and refer\\nto it as the â€œMixâ€. Since we choose to use a single epoch of each component dataset, the mix is\\nabout 70% StackExchange.\\n\\nFor each pre-training dataset, including the â€œMixâ€, we trained a scan a model sizes for exactly one epoch each.\\nIn all cases we used context size of 1024 tokens per sequence, batch size of 512 sequence pairs, and constant\\nlearning rate of 0.1 relative to language model pre-training. We evaluate preference model pre-training by\\nPM accuracy (i.e., does the PM assign a higher score to the â€œgoodâ€ sample in each pair?) and PM loss (3.1).\\n\\nC.2 Preference Model Pre-Training\\n\\nWe present more detailed finetuning results in figure 17, showing performance as a function of number of\\nfinetuning sequence pairs for both PMP (on the Mix dataset) and no PMP.\\n\\nFor all these experiments, we used a model context size of 1024 tokens per sequence, batch size of 32\\nsequence pairs, and a constant learning rate of 0.01 relative to pre-training. We trained for one epoch each\\non Learn to Summarize and HellaSwag, and four epochs each on the Ethics evaluations as doing so improved\\nperformance. We used hyperparameters (Î», Âµ) = (1, 1) for the PM and LM losses, respectively, as discussed\\nin section C.3.\\n\\nC.3 Language Modeling Improves PMP Transfer\\n\\nIn this section we describe a technical detail which improves the transfer-ability of PMP significantly. We\\nconsider two losses for the pre-training stage: (1) the preference modeling (PM) loss, and (2) an autoregres-\\nsive language modeling (LM) loss that imitates the â€œgoodâ€ sample in each sequence pair.\\n\\nLtotal = Î»LPM + ÂµLLM, good (C.1)\\n\\nwhere Î», Âµ are hyperparameters. For the latter, we do not apply any masking on the tokens and simply train\\nthe model to predict the full context of the good sample.\\n\\nWe found that adding the language modeling loss during pre-training consistently improved the sample ef-\\nficiency on finetuning evaluations. In figure 29, we show the transfer performance for several pre-training\\nlosses:\\n\\nâ€¢ No PM pre-training,\\nâ€¢ â€œPureâ€ PM loss for which (Î», Âµ) = (1, 0),\\nâ€¢ â€œPureâ€ LM loss for which (Î», Âµ) = (0, 1),\\nâ€¢ â€œCompositeâ€ PM+LM loss for which (Î», Âµ) = (1, 1),\\n\\nFor uniformity, we used (Î», Âµ) = (1, 1) for the subsequent finetuning stage in all four scenarios.\\n\\nWe observe that\\n\\nâ€¢ Pure LM performs similarly as no PM pre-training, which is unsurprising since itâ€™s just an extension\\nof the basic language model pre-training on which all our experiments are initialized.\\n\\n36\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning Performance of Different UPM Pre-training Losses On\\n Learn to Summarize (13B)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning Performance of Different UPM Pre-training Losses On\\n HellaSwag (13B)\\n\\nNo UPM Pre-train\\nUPM Reddit (LM Loss Only)\\nUPM Reddit (PM Loss Only)\\nUPM Reddit (PM+LM Loss)\\n\\nFigure 29 PMP using only the PM loss transfers poorly to downstream evaluations and is typically worse\\nthan simply doing no such pre-training at all. However, when combined with an autoregressive language\\nmodeling loss that imitates the â€œgoodâ€ sample in each training pair, it significantly improves transfer to many\\ndownstream evaluations. Here we show results for PMP on Reddit finetuned on Learn to Summarize and\\nHellaSwag, but we made similar observations on all other pre-training and finetuning datasets. Furthermore,\\nthe fact that â€œPM+LM Lossâ€ clearly performs better than â€œLM Loss Onlyâ€ strongly suggests that the per-\\nformance gain of the former does not arise solely from language modeling, but from its combination with\\npreference modeling.\\n\\nFigure 30 Here we show calibration curves on the summarization test set. We see that aside from the\\nsmallest model, the preference models are very well calibrated on-distribution. These models were all first\\nprefence model pre-trained on the stack exchange and then finetuned on summarization PMing. We include\\na black line as a reference for perfect calibration.\\n\\n37\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEffect of EOC Token On Learn to Summarize (13B)\\n\\nNo UPM Pre-train (w/ EOC Token)\\nNo UPM Pre-train (no EOC Token)\\nUPM Reddit (w/ EOC Token)\\nUPM Reddit (no EOC Token)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\n0.85\\n\\n0.90\\n\\n0.95\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEffect of EOC Token On HellaSwag (13B)\\n\\nNo UPM Pre-train (w/ EOC Token)\\nNo UPM Pre-train (no EOC Token)\\nUPM Reddit (w/ EOC Token)\\nUPM Reddit (no EOC Token)\\n\\nFigure 31 Appending an â€œend-of-contextâ€ token (EOC) to every sequence visibly improves overall perfor-\\nmance, as seen here for both with and without PMP. In all cases where PMP is applied, we include the EOC\\ntoken not just for the finetuning sequences but also the pre-training sequences. We made similar observations\\non all PMP datasets (as well as no PMP) and all finetuning datasets.\\n\\nâ€¢ Pure PM improves sample efficiency for a small number of samples, but eventually underperforms\\nrelative to no PM pre-training.\\n\\nâ€¢ The PM+LM pre-training consistently improves sample efficiency relative to no PM pre-training. It\\nalso performs better than pure LM, thus indicating that the performance gain isnâ€™t due purely to LM,\\nbut a combination of PM and LM.\\n\\nWhatâ€™s particularly interesting is that neither pure PM nor pure LM transfers particularly well, but the com-\\nbined effort of PM+LM performs significantly better. Our hypothesis is that pure PM has a tendency to learn\\nbiased or â€œtrivialâ€ features (e.g., context length, token frequencies) that donâ€™t generalize well to downstream\\ntasks, while the addition of LM forces the PM to learn from more substantial â€œlanguage-relevantâ€ features.\\n\\nC.4 End-of-context Token Improves Preference Modeling Performance\\n\\nHere we outline a technical detail that improves the overall performance of preference models. We designate\\na special â€œend-of-contextâ€ token (EOC) which is included as the final token of each sample context. The\\npreference model score is also predicted directly on top of this token. For our experiments we used the\\n<SOS> token, but in principle many other choices are possible.\\n\\nWe compare finetuning experiments with and without the EOC token. For experiments with, we consistently\\napply the same EOC token throughout both the PMP and fine-tuning stages; and for experiments without, we\\nconsistently do not apply the EOC token. From figure 31 we see that the EOC clearly improves performance.\\n\\nWe hypothesize that the improvement comes from two factors:\\n\\nâ€¢ Sometimes the sentiment behind a natural language statement can be altered or reversed significantly\\nby the addition of one or two words, and so knowing where the context ends can be helpful for the\\npreference model to predict a sensible score.\\n\\nâ€¢ Without an EOC token, the preference model must not only predict a score, but also try to anticipate\\nwhere the context ends. As a result, the model is forced to predict a score at multiple tokens where\\nthe context may end, rather than at a single token where it definitely ends. This adds a level of\\nambiguity which may cause the model to under-perform.\\n\\nC.5 Ensembling Over PMP Models\\n\\nIn prinicple we can ensemble together several models finetuned on the same final dataset, but which first\\npass through PMP on a distinct dataset. This would be a bit like ensembling over different random initializa-\\ntions, but what might hope for more interesting results due to the different semantic content in distinct PMP\\ndistributions. We tested this for summarization PMs that were separately PMP trained on Reddit and Stack\\nExchange, but only found a gain of order 0.5% in accuracy.\\n\\n38\\n\\n\\n\\nLearn to\\nSumm\\n\\nHella\\nSwag\\n\\nEthics:\\nJustice\\n\\nEthics:\\nMorality\\n\\nEthics:\\nDeon\\n\\nEthics:\\nVirtue\\n\\nEthics:\\nUtil\\n\\n0.05\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAccuracy Gain of UPM Pre-training At\\n500 Finetuning Seq Pairs (50B)\\n\\nLearn to\\nSumm\\n\\nHella\\nSwag\\n\\nEthics:\\nJustice\\n\\nEthics:\\nMorality\\n\\nEthics:\\nDeon\\n\\nEthics:\\nVirtue\\n\\nEthics:\\nUtil\\n\\n0.05\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAccuracy Gain of UPM Pre-training At\\n5k Finetuning Seq Pairs (50B)\\n\\nUPM Mix\\nUPM StackExch\\nUPM Reddit\\nUPM Wiki\\n\\nFigure 32 Accuracy gain of PMP as measured by accuracy difference relative to no PMP at 500 and 5k\\nfinetuning sequence pairs for multiple pre-training datasets (Mix, StackExchange, Reddit, Wikipedia) and\\nfinetuning evaluations (Learn to Summarize, HellaSwag, and all five Ethics evaluations).\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nExperiment on Ranked vs. Binary Pre-training (13B)\\n\\nNo PM Pretrain\\nBinary Pretrain\\nWeakly Scrambled Pretrain\\nModerately Scrambled Pretrain\\nStrongly Scrambled Pretrain\\nVery Strongly Scrambled Pretrain\\n\\nFigure 33 Results for a controlled experiment comparing the transfer ability of ranked vs. binary PM pre-\\ntraining, as explained in section 4.3. We see a clear trend whereby the sample efficiency degrades as the\\namount of relative scrambling between pre-training and finetuning distributions increases. Furthermore, we\\nfind that binary pre-training does not transfer as well as the weakly scrambled case, but transfers better than\\nthe very strongly scrambled case, in agreement with our expectations. This possibly explains why binary\\npre-training seems to perform better than ranked pre-training in most of our finetuning experiments.\\n\\nC.6 Experiments on Ranked vs Binary PMP â€“ Synthetic Symbols Dataset\\n\\nWe began by generating a list of symbols, and assigned an arbitrary â€œEloâ€ ranking to them. A simple example\\nwould be the first five English letters ranked by alphabetical order.\\n\\nT_0 : A > B > C > D > E\\n\\nWe then generated a preference modeling dataset consisting of pairs of distinct symbols, so that within each\\npair a sample is â€œbetterâ€ if it precedes the other with respect to the ranking. We call this the â€œcontrolâ€ dataset\\nT0. Furthermore, we created four additional datasets T1, T2, T3, T4, which were made in the same manner as\\nT0 but with increasingly scrambled symbol rankings. For instance,\\n\\nT_1 : A > B > C > [E] > [D] (Weakly Scrambled)\\nT_2 : A > B > [E] > [C] > [D] (Moderately Scrambled)\\nT_3 : A > [D] > [E] > [C] > [B] (Strongly Scrambled)\\n\\n39\\n\\n\\n\\nT_4 : [C] > [D] > [E] > [A] > [B] (Very Strongly Scrambled)\\n\\nwhere we enclosed in square brackets symbols that are out-of-place compared to the control. In addition, we\\nalso created a â€œbinaryâ€ dataset Tb which labels symbols in the first half of the control ranking as â€œgoodâ€ and\\nthose in the second half as â€œbadâ€. In other words,\\n\\nT_b : A , B , C > D , E , F (Binary)\\n\\nFinally, we pre-trained five preference models on Tb, T1, T2, T3, T4 separately, and compared their finetuning\\nperformance on the control T0. We also compare against a model trained directly on the control without\\npreference model pre-training.\\n\\nIn our actual experiment, we found that using only five symbols was too â€œeasyâ€ of a task to clearly distinguish\\nthe performance of different models, so instead we created a longer list of symbols, but otherwise the idea is\\nthe same. See section C.6 for details. Figure 33 shows the pairwise comparison accuracy on a held-out test\\nset vs. number of training samples during the finetuning stage. We make several observations:\\n\\nâ€¢ We see a clear trend whereby sample efficiency consistently gets worse as the amount of scrambling\\nincreases. In fact, there is a scrambling â€œthresholdâ€ beyond which the sample efficiency is actually\\neven worse than no PMP at all. This confirms the hypothesis that datasets with significantly different\\nElo scales are expected to transfer poorly to each other.\\n\\nâ€¢ The binary dataset is similarly sample efficient as a â€œmoderatelyâ€ scrambled dataset. This agrees\\nwith our hypothesis, which posits that a binary dataset should transfer better than a strongly scram-\\nbled dataset, but not necessarily better than a weakly scrambled one.\\n\\nClearly, the best possible PMP dataset is one that is qualitatively very similar to the final finetuning dataset, but\\ntypically this is not available. We see binarized PMP as a compromise that cannot guarantee the best possible\\nsample efficiency, but is more robustly capable of transferring to new preference modeling distributions.\\n\\nFinally, let us elaborate on our synthetic symbols dataset. Instead of using only five symbols, we used a list\\nof 676 symbols (using all ordered pairs of uppercase English letters), with a randomly assigned ranking. For\\neach symbol, the context is generated by repeating the symbol multiple times. For example, if the symbol AC\\nprecedes PQ in the ranking, then a preference modeling pair would look like\\n\\n(AC)(AC)(AC)(AC) > (PQ)(PQ)(PQ)(PQ)\\n\\nFurthermore, the scrambled datasets T1, T2, T3, T4 were obtained by applying 10, 40, 160, 640 randomly\\ngenerated transpositions to the control ranking, respectively. Finally, for the binary dataset Tb, a symbol is\\nlabeled â€œgoodâ€ if it appears in the first half of the control ranking, and â€œbad otherwise. For instance, if AC\\nappears in the first half, and PQ appears in the second half, then the corresponding preference modeling pairs\\nwould look like\\n\\nGOOD:(AC)(AC)(AC)(AC) > BAD:(AC)(AC)(AC)(AC)\\nBAD:(PQ)(PQ)(PQ)(PQ) > GOOD:(PQ)(PQ)(PQ)(PQ)\\n\\nD Per-Token GAN-Style Discriminator Results\\n\\nOne way to train a discriminator is to utilize pretrained language models to generate samples, and train the\\ndiscriminator to distinguish between human and model generated tokens. The discriminator can then be used\\nfor rejection sampling, or ranking samples by how likely they were to be generated by a human.\\n\\nTo test out this naive setup, we created a training set by loading sequences of fixed numbers of tokens from\\nthe language pretraining dataset, followed by: with 1/3 chance truncating the text somewhere, and continuing\\nthe text by sampling from a language model; with 1/3 chance generating the same number of tokens entirely\\nfrom a model; and with the last 1/3 chance the text remained unchanged (fully human-generated). We used a\\n13B language model for sampling this dataset. For training, we initialized discriminator models as pretrained\\nlanguage models, and applied a binary cross entropy loss at each token for the human vs. model binary\\nclassification.\\n\\nAlthough qualitatively the models seem to be able to identify low quality model generated text, when evalu-\\nated on a few language benchmarks, we did not see promising improvement over the original language model\\n\\n40\\n\\n\\n\\n100 101 102\\nNumber of Top Samples\\n\\n0.45\\n\\n0.50\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\nAv\\n\\ner\\nag\\n\\ne \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nLambada: Discriminators vs. Pretrained Language Model\\nLM\\n\\n108\\n\\n109\\n\\n1010\\n\\nNum\\nber of Param\\n\\neters\\n\\n100 101 102\\nNumber of Top Samples\\n\\n0.45\\n0.50\\n0.55\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\nAv\\ner\\n\\nag\\ne \\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nLambada: Ensembles vs. Pretrained Language Model\\nLM\\n\\n108\\n\\n109\\n\\n1010\\n\\nNum\\nber of Param\\n\\neters\\n\\nFigure 34 Left: Discriminator and language model performance re-ranking Lambada answers. Right:\\nEnsemble of discriminator and language model, as determined in equation D.2.\\n\\n100 50 0 50 100\\nRelative token position to human->model transition\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1.0\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPer-Token Accuracy of Discriminator\\n\\n1010\\n\\n3 Ã— 109\\n\\n4 Ã— 109\\n\\n6 Ã— 109\\n\\nNum\\nber of Param\\n\\neters\\n\\n100 101 102\\nRelative token position to human->model transition\\n\\n100\\n\\n2 Ã— 10 1\\n\\n3 Ã— 10 1\\n\\n4 Ã— 10 1\\n\\n6 Ã— 10 1\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPer-Token Accuracy of Discriminator\\n\\n1010\\n\\n3 Ã— 109\\n\\n4 Ã— 109\\n\\n6 Ã— 109\\n\\nNum\\nber of Param\\n\\neters\\n\\nFigure 35 Per-token accuracy of a GAN-type discriminator trained to predict whether every individual\\ntoken is human or model generated. Position 0 is the first model-generated token.\\n\\nused to generate the training set (see figure 34). For these evaluations, we first generated 100 samples from\\neach of the prompt in the test set. For the discriminators, we ranked the samples by the average predicted\\nprobability of being human generated over sample tokens. For the language model, we ranked the samples by\\nthe negative average log-prob over sample tokens. The plots show average metric over top-N ranked samples.\\nWe observe that the language model performs much better on these benchmarks, in both performance and\\nrobustness.\\n\\nHowever, we will now argue that it is not appropriate to directly use the discriminator to rank samples. Let us\\nuse P (t) to denote the probability distribution of human-generated tokens and PÎ¸(t) to represent a language\\nmodel. The goal of the discriminator DÏ† is to model the probability that a given token was model generated,\\nso DÏ†(t) is attempting to model the probability p(human|t). Assuming a prior that a token is 50% likely to\\ncome from a human, after seeing the token, an ideal discriminator would predict\\n\\nD(t) =\\nP (t)\\n\\nP (t) + PÎ¸(t)\\n(D.1)\\n\\nfor the probability that any token was written by a human.\\n\\nBut this means that we can use a learned DÏ†(t) to improve model predictions for any given token t, by\\nre-arranging to give the new ensemble distribution\\n\\nPensemble(t) =\\nDÏ†(t)\\n\\n1âˆ’DÏ†(t)\\nPÎ¸(t) (D.2)\\n\\nand this ensemble model should improve on the original language model distribution PÎ¸. In particular, this\\nensemble provides a more principled way to re-rank model-generated samples, using both the discriminator\\nand the language model probabilities together. We display the result on the right in figure 34, where we see\\nthat as expected, the ensemble can improve on the language model.\\n\\nFigure 35 shows the per-token prediction accuracy on the training set, relative to the position where the tokens\\nswitch from being human-generated to model-generated. We observe an interesting behavior â€“ even though\\n\\n41\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Ethics: Justice\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Ethics: Commonsense Morality\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Ethics: Deontology\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.50\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\n0.85\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Ethics: Utilitarianism\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\n103 104 105\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Ethics: Virtue\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\nFigure 36 We compare PMP on â€œhuman-humanâ€ vs â€œhuman-modelâ€ datasets by evaluating their transfer\\nperformance on the five Ethics datasets. It appears that one does not consistently outperform the other, and\\nthe results are rather random. We suspect that â€œhuman-modelâ€ does not have any particular advantage when\\nfinetuning on evaluations that are purely human-written, such as Ethics.\\n\\nlarger models obtain higher overall accuracy, they perform worse immediately after the transition from human\\nto model generated tokens.\\n\\n42\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\nAc\\n\\ncu\\nra\\n\\ncy\\n D\\n\\niff\\ner\\n\\nen\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Hellaswag (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.01\\n\\n0.00\\n\\n0.01\\n\\n0.02\\n\\n0.03\\n\\n0.04\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Learn to Summarize (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.05\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Ethics: Justice (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.04\\n\\n0.02\\n\\n0.00\\n\\n0.02\\n\\n0.04\\n\\n0.06\\n\\n0.08\\n\\n0.10\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Ethics: Commonsense Morality (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.05\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Ethics: Deontology (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n103 104 105\\nNumber of Finetuning Sequence Pairs\\n\\n0.06\\n\\n0.04\\n\\n0.02\\n\\n0.00\\n\\n0.02\\n\\n0.04\\n\\n0.06\\n\\n0.08\\n\\n0.10\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Ethics: Virtue (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.02\\n\\n0.00\\n\\n0.02\\n\\n0.04\\n\\n0.06\\n\\n0.08\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Ethics: Utilitarianism (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\nFigure 37 Accuracy gain of binary over ranked PMP on finetuning evaluations.\\n\\n43\\n\\n\\n\\nE Definitions of Alignment and the HHH criteria\\n\\nPeople often mean subtly different things when they talk about AI systems being \"aligned\". Given this, we\\nwant to elaborate on what we mean by this term and why we selected the \"helpful, honest, and harmless\"\\nconception of aligned AI assistants.\\n\\nE.1 How the HHH criteria relate to alignment\\n\\nAt a very high level, alignment can be thought of as the degree of overlap between the way two agents rank\\ndifferent outcomes. For example, if agent A completely internalizes the desires of agent Bâ€”i.e. the only\\ndesire A has is to see Bâ€™s desires satisfiedâ€”we could say that agent A is maximally aligned with agent B.20\\n\\nWe believe it is difficult for an AI assistant to always be helpful, honest, and harmless towards an agent or\\ngroup without also being highly aligned with that agent or group according to this definition of alignment. To\\nsee why, suppose we want the AI assistant to be aligned with a specific group of humans. Here is what each\\nof the three conditions implies about the assistant:\\n\\nâ€¢ Helpfulness: the assistant will always try to do what is in the humansâ€™ best interests\\nâ€¢ Honesty: the assistant will always try to convey accurate information to the humans and will always\\n\\ntry to avoid deceiving them21\\n\\nâ€¢ Harmlessness: the assistant will always try to avoid doing anything that harms the humans\\n\\nAn AI assistant that is always helpful, honest, and harmless towards a group of humans will always try to act\\nin a way that satisfies the interests of this group, including their interest not to be harmed or be misled. It is\\ntherefore likely to be highly aligned with the interests of that group of humans.\\n\\nThis account of alignment is still vague and leaves many open questions. In particular, it does not tell us:\\n\\nâ€¢ What kinds of outcome orderings are most relevant for AI alignment (preferences, idealized prefer-\\nences, wellbeing, ethical rankings, etc.)\\n\\nâ€¢ The degree to which these outcome orderings are objective or subjective\\nâ€¢ Which agents the AI systems should be aligned to (users, developers, humanity, etc.)\\nâ€¢ How AI systems can or should aggregate different outcome orderings if they are aligned to more\\n\\nthan one agent\\nâ€¢ What is the precise formulation of \"overlap between outcome rankings\"\\nâ€¢ How large or small the space of maximally aligned agents is, given the above\\n\\nMany of these questions are discussed in more detail elsewhere [Gab20]. Progress in AI alignment will\\nhopefully not require us to reach certainty about any of them, since such certainty is unlikely to be achieved.\\nBut it is worth making these unanswered questions explicit. When we train aligned AI systems, we may need\\nto make choices that will implicitly favor or assume certain answers to them. And how we define the HHH\\ncriteria will depend on what kind of orderings we think are most relevant for AI alignment.\\n\\nE.2 The relation between the HHH criteria\\n\\nIf we define helpfulness and harmlessness such that (a) itâ€™s never in a humanâ€™s best interest to be harmed, and\\n(b) itâ€™s always harmful to fail to do something thatâ€™s in a humanâ€™s best interest, we can reduce helpfulness and\\nharmlessness to either criterion. We have separated them because we find it practically easier to distinguish\\ncases of active harm from cases in which a benefit is withheld.\\n\\nHelpfulness and harmlessness clearly canâ€™t be reduced to honesty, but honesty can be reduced to helpful-\\nness/harmlessness. According to the definition of alignment given above, an aligned AI assistant should be\\nhonest because honesty is valued by humans. This could either be because honesty is instrumentally valuable\\n\\n20Even if agent A is maximally aligned with agent B, A can fail to act in accordance with Bâ€™s desires because A has a\\nmistaken belief about Bâ€™s desires or about the world, or because A is unable to carry out their intended action.\\n\\n21This concept of \"honesty\" involves avoiding multiple different conditions of lying, such as only stating true claims\\nand not causing false beliefs in the listener [Mah15]. There will be cases where these conditions conflict. In such cases,\\nwe would need to assess which conception of honesty it would be most helpful and harmless for the assistant to satisfy.\\n\\n44\\n\\n\\n\\nto humans or because humans intrinsically value it. If honesty were genuinely not something that humans\\nvalue even on reflection, an AI that was aligned with human values would presumably not be honest.\\n\\nBut if the value of honesty is reducible to helpfulness and harmlessness, why include it in our list? The\\nanswer is mostly practical: honesty is important and distinct enough to warrant particular attention.\\n\\nWe could also choose to introduce other concepts whichâ€”like honestyâ€”canâ€™t be reduced to helpfulness\\nor harmlessness but are important properties that a helpful, harmless AI assistant will typically have. For\\nexample, we considered adding the following fourth â€™Hâ€™:\\n\\nâ€¢ Handleable: the assistant will always be responsive to feedback from the humans and carry out any\\ninstructions from the humans in the way that the humans intended\\n\\nHandleability is similar what others have called \"corrigibility\" [SFAY15]. A system that isnâ€™t handleable is\\nless helpful and more harmful than a system that is handleable. But it may be useful to pay special attention\\nto failures that involve the assistant not doing what is asked or not responding to human feedback. For this\\nreason it seems like a good candidate for a fourth â€™Hâ€™.\\n\\nIn other words, what we want to include in this list, beyond a joint or separate helpfulness/harmlessness\\ncondition, depends on what behaviors we find it useful to pay particular attention to.\\n\\nE.3 Conflicts between the HHH criteria\\n\\nAs we note in the main text, the three conditions above will sometimes appear to be in conflict. There are\\ntwo possible kinds of conflicts between the three conditions:\\n\\nâ€¢ Intra-agent conflicts: Cases in which two or more HHH conditions are in conflict even if we just\\nwant to align the assistant with a particular human. For example, it is not possible to be honest or\\nhelpful towards a particular human without saying something that is pro tanto harmful to them, e.g.\\nsomething that will hurt their feelings.\\n\\nâ€¢ Inter-agent conflicts: Cases in which two or more HHH conditions conflict across different agents\\nwe might want to align the assistant with. For example, it is not possible to be helpful towards a\\nparticular human without saying something that is harmful to a others we want to align it with, e.g.\\nif one human asks for help building a bomb to use against others.\\n\\nIf helpfulness and harmlessness can be reduced to a single joint condition and honesty can also be reduced to\\nhelpfulness/harmlessness, intra-agent conflicts will turn out to be merely superficial since all three conditions\\ncan ultimately be reduced to a single coherent condition.\\n\\nInter-agent conflicts are a different matter. It is very likely that a single AI cannot be maximally aligned with\\nany two different humans, since both humans will have at least some conflicting desires or values. This is\\nwhy an AI assistant will often be unable to be helpful, honest and harmless towards some humans without\\nbeing unaligned with other humans (e.g. by refusing to help them build the bomb). It is therefore important\\nfor us to be aware of who we are asking the AI assistants to be helpful, honest, and harmless towards, since\\nthis also determines which humans the AI assistants are not fully aligned with and to what degree.\\n\\nE.4 The HHH criteria and secure AI\\n\\nAlthough we want to develop aligned AI systems, it may not be possible to guarantee that AI systems are\\nfully aligned with human values. So weâ€™ll also want our AI systems to be secure: to have properties or be\\nembedded in systems that decrease the potential for harm even if the AI is less than fully aligned [HCSS21].\\nWe may want AI systems that always respond to the intended instructions of humans, that always avoid doing\\ncertain things that most humans consider very bad, that fail both securely and loudly, that make decisions in\\nways that can be made transparent to humans, that are secure against alteration or misuse, and so on.\\n\\nAn AI assistant that is helpful, honest, and harmless is a secure system in many respects and will try to assist\\nhumans in making itself more secure. But the HHH criteria were not selected with AI security in mind and\\nnot all security features will be features of the AI system itself. We therefore want to emphasize that the HHH\\ncriteria are criteria of alignment, and that additional work and additional areas of focus may be required to\\nensure that AI systems cannot cause too much harm when they are not fully aligned.\\n\\n45\\n\\n\\n\\nReferences\\n[AAB+21] Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin,\\n\\nRachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aure-\\nlia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lilli-\\ncrap, Kory Mathewson, Sona Mokra, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant\\nVarma, Greg Wayne, Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating\\ninteractive intelligence, 2021, 2012.05672.\\n\\n[AON+21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis\\nwith large language models, 2021, 2108.07732.\\n\\n[AOS+16] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan ManÃ©.\\nConcrete problems in ai safety, 2016, 1606.06565.\\n\\n[ATS+21] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav\\nMehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian\\nRuder, and Donald Metzler. Ext5: Towards extreme multi-task scaling for transfer learning,\\n2021, 2111.10952.\\n\\n[BGNN19] Daniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating be-\\nyond suboptimal demonstrations via inverse reinforcement learning from observations, 2019,\\n1904.06387.\\n\\n[BMR+20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020,\\n2005.14165.\\n\\n[Bow21] Samuel R. Bowman. When combating hype, proceed with caution, 2021, 2110.08300.\\n[CKB+21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christo-\\n\\npher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021,\\n2110.14168.\\n\\n[CLB+17] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\\nDeep reinforcement learning from human preferences, 2017, 1706.03741.\\n\\n[CLR+21] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter\\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\\nvia sequence modeling, 2021, 2106.01345.\\n\\n[CSA18] Paul Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying\\nweak experts, 2018, 1810.08575.\\n\\n[CTJ+21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavar-\\nian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,\\nFotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol,\\nAlex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William\\nSaunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan\\nMorikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Pe-\\nter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\\nZaremba. Evaluating large language models trained on code, 2021, 2107.03374.\\n\\n[Fou] The Common Crawl Foundation. Common crawl. URL http://commoncrawl.org.\\n[Gab20] Iason Gabriel. Artificial intelligence, values, and alignment. Minds and Machines, 30(3):411â€“\\n\\n437, Sep 2020. doi:10.1007/s11023-020-09539-2.\\n[GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\\n\\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:\\nAn 800gb dataset of diverse text for language modeling, 2020, 2101.00027.\\n\\n46\\n\\nhttp://arxiv.org/abs/2012.05672\\nhttp://arxiv.org/abs/2108.07732\\nhttp://arxiv.org/abs/1606.06565\\nhttp://arxiv.org/abs/2111.10952\\nhttp://arxiv.org/abs/1904.06387\\nhttp://arxiv.org/abs/2005.14165\\nhttp://arxiv.org/abs/2110.08300\\nhttp://arxiv.org/abs/2110.14168\\nhttp://arxiv.org/abs/1706.03741\\nhttp://arxiv.org/abs/2106.01345\\nhttp://arxiv.org/abs/1810.08575\\nhttp://arxiv.org/abs/2107.03374\\nhttp://commoncrawl.org\\nhttp://dx.doi.org/10.1007/s11023-020-09539-2\\nhttp://arxiv.org/abs/2101.00027\\n\\n\\n[GGS+20] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxi-\\ncityprompts: Evaluating neural toxic degeneration in language models, 2020, 2009.11462.\\n\\n[HBB+21] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\\nSteinhardt. Aligning ai with shared human values, 2021, 2008.02275.\\n\\n[HCSS21] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in\\nml safety, 2021, 2109.13916.\\n\\n[HE16] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning, 2016, 1606.03476.\\n\\n[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kian-\\ninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-\\ndictable, empirically, 2017, 1712.00409.\\n\\n[HU20] Laura Hanu and Unitary team. Detoxify. Github. https://github.com/unitaryai/detoxify, 2020.\\n\\n[ICA18] Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate, 2018, 1805.00899.\\n\\n[ILP+18] Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward\\nlearning from human preferences and demonstrations in atari, 2018, 1811.06521.\\n\\n[JHB+21] Liwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Bor-\\nchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. Delphi: Towards machine\\nethics and norms, 2021, 2110.07574.\\n\\n[Jon21] Andy L. Jones. Scaling scaling laws with board games, 2021, 2104.03113.\\n\\n[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels, 2020, 2001.08361.\\n\\n[KSW21] Mojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation,\\n2021, 2107.07566.\\n\\n[LARC21] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient\\nprompt tuning, 2021, 2104.08691.\\n\\n[LHE21] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic\\nhuman falsehoods, 2021, 2109.07958.\\n\\n[LKCSL17] Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. Quixbugs: A multi-\\nlingual program repair benchmark set based on the quixey challenge. In Proceedings Companion\\nof the 2017 ACM SIGPLAN International Conference on Systems, Programming, Languages,\\nand Applications: Software for Humanity, SPLASH Companion 2017, pages 55â€“56, New York,\\nNY, USA, 2017. Association for Computing Machinery. doi:10.1145/3135932.3135941.\\n\\n[LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and\\nNoam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs],\\n2018, 1801.10198. URL http://arxiv.org/abs/1801.10198.\\n\\n[Mah15] James Edwin Mahon. The definition of lying and deception. In Ed Zalta, editor, Stanford\\nEncyclopedia of Philosophy. 2015.\\n\\n[NRA+21] Helen Ngo, Cooper Raterink, JoÃƒÂ£o G. M. AraÃƒÂºjo, Ivan Zhang, Carol Chen, Adrien Morisot,\\nand Nicholas Frosst. Mitigating harm in language models with conditional-likelihood filtration,\\n2021, 2108.07790.\\n\\n[PKL+16] Denis Paperno, GermÃƒÂ¡n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella\\nBernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel FernÃƒÂ¡ndez. The lam-\\nbada dataset: Word prediction requiring a broad discourse context, 2016, 1606.06031.\\n\\n[RDR20] Vinay V. Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hid-\\nden representations and task semantics, 2020, 2007.07400.\\n\\n[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. 2018.\\n\\n[RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\\nprediction of the generalization error across scales, 2019, arXiv:1909.12673.\\n\\n[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. openai.com, 2019.\\n\\n47\\n\\nhttp://arxiv.org/abs/2009.11462\\nhttp://arxiv.org/abs/2008.02275\\nhttp://arxiv.org/abs/2109.13916\\nhttp://arxiv.org/abs/1606.03476\\nhttp://arxiv.org/abs/1712.00409\\nhttp://arxiv.org/abs/1805.00899\\nhttp://arxiv.org/abs/1811.06521\\nhttp://arxiv.org/abs/2110.07574\\nhttp://arxiv.org/abs/2104.03113\\nhttp://arxiv.org/abs/2001.08361\\nhttp://arxiv.org/abs/2107.07566\\nhttp://arxiv.org/abs/2104.08691\\nhttp://arxiv.org/abs/2109.07958\\nhttp://dx.doi.org/10.1145/3135932.3135941\\nhttp://arxiv.org/abs/1801.10198\\nhttp://arxiv.org/abs/1801.10198\\nhttp://arxiv.org/abs/2108.07790\\nhttp://arxiv.org/abs/1606.06031\\nhttp://arxiv.org/abs/2007.07400\\nhttp://arxiv.org/abs/arXiv:1909.12673\\n\\n\\n[SD21] Irene Solaiman and Christy Dennison. Process for adapting language models to society\\n(PALMS) with values-targeted datasets. CoRR, abs/2106.10328, 2021, 2106.10328. URL\\nhttps://arxiv.org/abs/2106.10328.\\n\\n[SFAY15] Nate Soares, Benja Fallenstein, Stuart Armstrong, and Eliezer Yudkowsky. Corrigibility. In\\nWorkshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\\n\\n[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. CoRR, 2015, 1508.07909.\\n\\n[SOW+20] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec\\nRadford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback,\\n2020, 2009.01325.\\n\\n[SWR+21] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,\\nAntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Can-\\nwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan\\nChhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang,\\nMatteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang,\\nTrishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries,\\nRyan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush.\\nMultitask prompted training enables zero-shot task generalization, 2021, 2110.08207.\\n\\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nÅ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30, pages 5998â€“6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n\\n[WBZ+21] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\\nDu, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2021,\\n2109.01652.\\n\\n[WGU+21] Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne\\nHendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Challenges in\\ndetoxifying language models, 2021, arXiv:2109.07445.\\n\\n[WOZ+21] Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul\\nChristiano. Recursively summarizing books with human feedback, 2021, 2109.10862.\\n\\n[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\\nmachine really finish your sentence?, 2019, 1905.07830.\\n\\n48\\n\\nhttp://arxiv.org/abs/2106.10328\\nhttps://arxiv.org/abs/2106.10328\\nhttp://arxiv.org/abs/1508.07909\\nhttp://arxiv.org/abs/2009.01325\\nhttp://arxiv.org/abs/2110.08207\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\\nhttp://arxiv.org/abs/2109.01652\\nhttp://arxiv.org/abs/arXiv:2109.07445\\nhttp://arxiv.org/abs/2109.10862\\nhttp://arxiv.org/abs/1905.07830\\n\\n\\t1 Introduction\\n\\t2 Conditioning on Aligned Behavior\\n\\t3 Scaling of Preference Modeling vs Imitation Learning\\n\\t4 Preference Model Pre-Training and Transfer\\n\\t5 Discussion\\n\\tA Language Model Pre-training\\n\\tB More Details on Prompting, Context Distillation, and Evaluations\\n\\tC More Details on Preference Models\\n\\tD Per-Token GAN-Style Discriminator Results\\n\\tE Definitions of Alignment and the HHH criteria\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cmiZn7w8sopD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}