{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrape-ai-alignment-content.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyObJ3mmGK++Wq4CX2o3JSFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/ai-safety-scrape/blob/main/scrape_ai_alignment_content.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Contents from AI Alignment Resources\n"
      ],
      "metadata": {
        "id": "97CXz6XJM8q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "LYg2mYW2nGYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tika"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugHfLSwFnIjf",
        "outputId": "26b13d8d-8591-4482-8cd0-60ef8a82e97d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tika in /usr/local/lib/python3.7/dist-packages (1.24)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "4XsCNDx4nKBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tika import parser\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "from urllib import request\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "LaV6DoUenFyD"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Environment"
      ],
      "metadata": {
        "id": "JGEHyxtIrABU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7EXurg4rE4U",
        "outputId": "b43329dd-e71f-4f43-d544-4122be693048"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x7qM-qXrGCn",
        "outputId": "71ed5f99-a222-4bf0-e1b7-8d610da7ea80"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/'\n",
            "/content/drive/MyDrive/code-projects/gpt-ai-safety\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CODE_DIR = Path('.') / 'code-projects/gpt-ai-safety'\n",
        "RAW_DIR = Path('.') / 'data/raw'"
      ],
      "metadata": {
        "id": "nrQERsuk8lRe"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {CODE_DIR}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdvzqbWx8yrK",
        "outputId": "6d408d15-6ed5-4620-aabc-c4ae02633a7a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/code-projects/gpt-ai-safety\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/JayThibs/ai-safety-scrape\n",
        "# !mv ai-safety-scrape/* ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDsiFyBfxC8H",
        "outputId": "428593bd-5128-42fe-d92a-4fd6a044574f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai-safety-scrape'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 17 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Explore Data"
      ],
      "metadata": {
        "id": "Z6UiH03NAdoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('ai-alignment-papers.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "cJit0jX1An6S",
        "outputId": "e87d3973-7406-4fe1-b159-04cea8f82daa"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cc54ae5d-7063-45fb-ad18-543ec118021e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Key</th>\n",
              "      <th>Item Type</th>\n",
              "      <th>Publication Year</th>\n",
              "      <th>Author</th>\n",
              "      <th>Title</th>\n",
              "      <th>Publication Title</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>ISSN</th>\n",
              "      <th>DOI</th>\n",
              "      <th>Url</th>\n",
              "      <th>Abstract Note</th>\n",
              "      <th>Date</th>\n",
              "      <th>Date Added</th>\n",
              "      <th>Date Modified</th>\n",
              "      <th>Access Date</th>\n",
              "      <th>Pages</th>\n",
              "      <th>Num Pages</th>\n",
              "      <th>Issue</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Number Of Volumes</th>\n",
              "      <th>Journal Abbreviation</th>\n",
              "      <th>Short Title</th>\n",
              "      <th>Series</th>\n",
              "      <th>Series Number</th>\n",
              "      <th>Series Text</th>\n",
              "      <th>Series Title</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>Place</th>\n",
              "      <th>Language</th>\n",
              "      <th>Rights</th>\n",
              "      <th>Type</th>\n",
              "      <th>Archive</th>\n",
              "      <th>Archive Location</th>\n",
              "      <th>Library Catalog</th>\n",
              "      <th>Call Number</th>\n",
              "      <th>Extra</th>\n",
              "      <th>Notes</th>\n",
              "      <th>File Attachments</th>\n",
              "      <th>Link Attachments</th>\n",
              "      <th>Manual Tags</th>\n",
              "      <th>...</th>\n",
              "      <th>Cast Member</th>\n",
              "      <th>Commenter</th>\n",
              "      <th>Composer</th>\n",
              "      <th>Cosponsor</th>\n",
              "      <th>Counsel</th>\n",
              "      <th>Interviewer</th>\n",
              "      <th>Producer</th>\n",
              "      <th>Recipient</th>\n",
              "      <th>Reviewed Author</th>\n",
              "      <th>Scriptwriter</th>\n",
              "      <th>Words By</th>\n",
              "      <th>Guest</th>\n",
              "      <th>Number</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Running Time</th>\n",
              "      <th>Scale</th>\n",
              "      <th>Medium</th>\n",
              "      <th>Artwork Size</th>\n",
              "      <th>Filing Date</th>\n",
              "      <th>Application Number</th>\n",
              "      <th>Assignee</th>\n",
              "      <th>Issuing Authority</th>\n",
              "      <th>Country</th>\n",
              "      <th>Meeting Name</th>\n",
              "      <th>Conference Name</th>\n",
              "      <th>Court</th>\n",
              "      <th>References</th>\n",
              "      <th>Reporter</th>\n",
              "      <th>Legal Status</th>\n",
              "      <th>Priority Numbers</th>\n",
              "      <th>Programming Language</th>\n",
              "      <th>Version</th>\n",
              "      <th>System</th>\n",
              "      <th>Code</th>\n",
              "      <th>Code Number</th>\n",
              "      <th>Section</th>\n",
              "      <th>Session</th>\n",
              "      <th>Committee</th>\n",
              "      <th>History</th>\n",
              "      <th>Legislative Body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XBZAPQFK</td>\n",
              "      <td>blogPost</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Kokotajlo, Daniel</td>\n",
              "      <td>Three kinds of competitiveness</td>\n",
              "      <td>AI Impacts</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://aiimpacts.org/three-kinds-of-competiti...</td>\n",
              "      <td>By Daniel Kokotajlo In this post, I distinguis...</td>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>2022-01-30 01:53:10</td>\n",
              "      <td>2022-01-30 01:53:10</td>\n",
              "      <td>2021-11-20 18:55:39</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en-US</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/PU9A2KS...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HX9UZ5JP</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Cihon, Peter; Maas, Matthijs M.; Kemp, Luke</td>\n",
              "      <td>Fragmentation and the Future: Investigating Ar...</td>\n",
              "      <td>Global Policy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1758-5899</td>\n",
              "      <td>10.1111/1758-5899.12890</td>\n",
              "      <td>https://onlinelibrary.wiley.com/doi/abs/10.111...</td>\n",
              "      <td>The international governance of artificial int...</td>\n",
              "      <td>2020</td>\n",
              "      <td>2022-01-30 04:47:43</td>\n",
              "      <td>2022-01-30 04:47:43</td>\n",
              "      <td>2021-11-13 15:58:24</td>\n",
              "      <td>545-556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fragmentation and the Future</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Wiley Online Library</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000010  _eprint: https://onlinelibrary....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/2TZBI3F...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BQCZM53S</td>\n",
              "      <td>blogPost</td>\n",
              "      <td>2021.0</td>\n",
              "      <td>Clarke, Sam; Martin, Samuel Dylan</td>\n",
              "      <td>Distinguishing AI takeover scenarios</td>\n",
              "      <td>AI Alignment Forum</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.alignmentforum.org/posts/qYzqDtoQa...</td>\n",
              "      <td>Epistemic status: lots of this involves interp...</td>\n",
              "      <td>2021-09-08</td>\n",
              "      <td>2022-01-30 04:47:42</td>\n",
              "      <td>2022-01-30 04:47:42</td>\n",
              "      <td>2021-11-18 23:45:23</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: NoCitationData[s0]  ACC: N/A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/ENAMQXC...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JVMJ4RMM</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Stray, Jonathan</td>\n",
              "      <td>Aligning AI Optimization to Community Well-Being</td>\n",
              "      <td>International Journal of Community Well-Being</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2524-5295, 2524-5309</td>\n",
              "      <td>10.1007/s42413-020-00086-3</td>\n",
              "      <td>http://link.springer.com/10.1007/s42413-020-00...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-12</td>\n",
              "      <td>2022-01-30 04:47:36</td>\n",
              "      <td>2022-01-30 04:47:36</td>\n",
              "      <td>2021-11-13 22:47:54</td>\n",
              "      <td>443-463</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Int. Journal of Com. WB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DOI.org (Crossref)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000010</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/V3BEV7X...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>W8F6VI9I</td>\n",
              "      <td>thesis</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Shah, Rohin Monish</td>\n",
              "      <td>Extracting and Using Preference Information fr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.proquest.com/openview/da8bf63ef343...</td>\n",
              "      <td>Typically when learning about what people want...</td>\n",
              "      <td>2020-12-17</td>\n",
              "      <td>2022-01-30 04:47:35</td>\n",
              "      <td>2022-01-30 04:47:35</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>University of California, Berkeley</td>\n",
              "      <td>Berkeley, CA</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Zotero</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/S96M3KT...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 87 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc54ae5d-7063-45fb-ad18-543ec118021e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc54ae5d-7063-45fb-ad18-543ec118021e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc54ae5d-7063-45fb-ad18-543ec118021e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Key       Item Type  ...  History Legislative Body\n",
              "0  XBZAPQFK        blogPost  ...      NaN              NaN\n",
              "1  HX9UZ5JP  journalArticle  ...      NaN              NaN\n",
              "2  BQCZM53S        blogPost  ...      NaN              NaN\n",
              "3  JVMJ4RMM  journalArticle  ...      NaN              NaN\n",
              "4  W8F6VI9I          thesis  ...      NaN              NaN\n",
              "\n",
              "[5 rows x 87 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Item Type'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9EhbqMYAv2_",
        "outputId": "07401942-3de8-4fc8-af63-3895bd5a7527"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['blogPost', 'journalArticle', 'thesis', 'conferencePaper',\n",
              "       'manuscript', 'report', 'bookSection', 'magazineArticle', 'book'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "item_nums = []\n",
        "for item in df['Item Type'].unique():\n",
        "    item_nums.append([item, len(df[df['Item Type'] == item])])\n",
        "\n",
        "item_nums.sort(key=lambda x:x[1])\n",
        "item_nums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsXK6OGkGA_F",
        "outputId": "0fafef82-60f6-4b10-9d80-7cf9fc1c1473"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['magazineArticle', 2],\n",
              " ['thesis', 3],\n",
              " ['book', 13],\n",
              " ['bookSection', 52],\n",
              " ['report', 87],\n",
              " ['manuscript', 154],\n",
              " ['journalArticle', 170],\n",
              " ['conferencePaper', 262],\n",
              " ['blogPost', 421]]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Item Type'] == 'journalArticle'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "niZZINK1Av78",
        "outputId": "5fbcf3b8-95b9-4b00-f481-9ef9f6933deb"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ae03991c-4ba0-4012-95f5-806db4e3ea0f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Key</th>\n",
              "      <th>Item Type</th>\n",
              "      <th>Publication Year</th>\n",
              "      <th>Author</th>\n",
              "      <th>Title</th>\n",
              "      <th>Publication Title</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>ISSN</th>\n",
              "      <th>DOI</th>\n",
              "      <th>Url</th>\n",
              "      <th>Abstract Note</th>\n",
              "      <th>Date</th>\n",
              "      <th>Date Added</th>\n",
              "      <th>Date Modified</th>\n",
              "      <th>Access Date</th>\n",
              "      <th>Pages</th>\n",
              "      <th>Num Pages</th>\n",
              "      <th>Issue</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Number Of Volumes</th>\n",
              "      <th>Journal Abbreviation</th>\n",
              "      <th>Short Title</th>\n",
              "      <th>Series</th>\n",
              "      <th>Series Number</th>\n",
              "      <th>Series Text</th>\n",
              "      <th>Series Title</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>Place</th>\n",
              "      <th>Language</th>\n",
              "      <th>Rights</th>\n",
              "      <th>Type</th>\n",
              "      <th>Archive</th>\n",
              "      <th>Archive Location</th>\n",
              "      <th>Library Catalog</th>\n",
              "      <th>Call Number</th>\n",
              "      <th>Extra</th>\n",
              "      <th>Notes</th>\n",
              "      <th>File Attachments</th>\n",
              "      <th>Link Attachments</th>\n",
              "      <th>Manual Tags</th>\n",
              "      <th>...</th>\n",
              "      <th>Cast Member</th>\n",
              "      <th>Commenter</th>\n",
              "      <th>Composer</th>\n",
              "      <th>Cosponsor</th>\n",
              "      <th>Counsel</th>\n",
              "      <th>Interviewer</th>\n",
              "      <th>Producer</th>\n",
              "      <th>Recipient</th>\n",
              "      <th>Reviewed Author</th>\n",
              "      <th>Scriptwriter</th>\n",
              "      <th>Words By</th>\n",
              "      <th>Guest</th>\n",
              "      <th>Number</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Running Time</th>\n",
              "      <th>Scale</th>\n",
              "      <th>Medium</th>\n",
              "      <th>Artwork Size</th>\n",
              "      <th>Filing Date</th>\n",
              "      <th>Application Number</th>\n",
              "      <th>Assignee</th>\n",
              "      <th>Issuing Authority</th>\n",
              "      <th>Country</th>\n",
              "      <th>Meeting Name</th>\n",
              "      <th>Conference Name</th>\n",
              "      <th>Court</th>\n",
              "      <th>References</th>\n",
              "      <th>Reporter</th>\n",
              "      <th>Legal Status</th>\n",
              "      <th>Priority Numbers</th>\n",
              "      <th>Programming Language</th>\n",
              "      <th>Version</th>\n",
              "      <th>System</th>\n",
              "      <th>Code</th>\n",
              "      <th>Code Number</th>\n",
              "      <th>Section</th>\n",
              "      <th>Session</th>\n",
              "      <th>Committee</th>\n",
              "      <th>History</th>\n",
              "      <th>Legislative Body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HX9UZ5JP</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Cihon, Peter; Maas, Matthijs M.; Kemp, Luke</td>\n",
              "      <td>Fragmentation and the Future: Investigating Ar...</td>\n",
              "      <td>Global Policy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1758-5899</td>\n",
              "      <td>10.1111/1758-5899.12890</td>\n",
              "      <td>https://onlinelibrary.wiley.com/doi/abs/10.111...</td>\n",
              "      <td>The international governance of artificial int...</td>\n",
              "      <td>2020</td>\n",
              "      <td>2022-01-30 04:47:43</td>\n",
              "      <td>2022-01-30 04:47:43</td>\n",
              "      <td>2021-11-13 15:58:24</td>\n",
              "      <td>545-556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fragmentation and the Future</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Wiley Online Library</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000010  _eprint: https://onlinelibrary....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/2TZBI3F...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JVMJ4RMM</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Stray, Jonathan</td>\n",
              "      <td>Aligning AI Optimization to Community Well-Being</td>\n",
              "      <td>International Journal of Community Well-Being</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2524-5295, 2524-5309</td>\n",
              "      <td>10.1007/s42413-020-00086-3</td>\n",
              "      <td>http://link.springer.com/10.1007/s42413-020-00...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-12</td>\n",
              "      <td>2022-01-30 04:47:36</td>\n",
              "      <td>2022-01-30 04:47:36</td>\n",
              "      <td>2021-11-13 22:47:54</td>\n",
              "      <td>443-463</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Int. Journal of Com. WB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DOI.org (Crossref)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000010</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/V3BEV7X...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>TK5F29IU</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2021.0</td>\n",
              "      <td>Hayden, Benjamin; Niv, Yael</td>\n",
              "      <td>The case against economic values in the orbito...</td>\n",
              "      <td>Behavioral Neuroscience</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.1037/bne0000448</td>\n",
              "      <td>https://osf.io/7hgup</td>\n",
              "      <td>Much of traditional neuroeconomics proceeds fr...</td>\n",
              "      <td>2021</td>\n",
              "      <td>2022-01-30 04:48:47</td>\n",
              "      <td>2022-01-30 04:48:47</td>\n",
              "      <td>2021-11-08 23:41:47</td>\n",
              "      <td>192-201</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>135.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DOI.org (Crossref)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000026  DOI: 10.31234/osf.io/7hgup</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>NHWZIKZ2</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>Fernandes, Pedro; Santos, Francisco C.; Lopes,...</td>\n",
              "      <td>Norms for Beneficial A.I.: A Computational Ana...</td>\n",
              "      <td>AI Communications</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18758452, 09217126</td>\n",
              "      <td>10.3233/AIC-201502</td>\n",
              "      <td>http://arxiv.org/abs/1907.03843</td>\n",
              "      <td>The rise of artificial intelligence (A.I.) bas...</td>\n",
              "      <td>2020-12-18</td>\n",
              "      <td>2022-01-30 04:48:46</td>\n",
              "      <td>2022-01-30 04:48:46</td>\n",
              "      <td>2021-11-13 22:40:37</td>\n",
              "      <td>155-171</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3-6</td>\n",
              "      <td>33.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIC</td>\n",
              "      <td>Norms for Beneficial A.I.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>arXiv.org</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000004  arXiv: 1907.03843</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/JAVXSVN...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>HDWGJGAP</td>\n",
              "      <td>journalArticle</td>\n",
              "      <td>2021.0</td>\n",
              "      <td>Mingard, Chris; Valle-Pérez, Guillermo; Skalse...</td>\n",
              "      <td>Is SGD a Bayesian sampler? Well, almost</td>\n",
              "      <td>Journal of Machine Learning Research</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://arxiv.org/abs/2006.15191</td>\n",
              "      <td>Overparameterised deep neural networks (DNNs) ...</td>\n",
              "      <td>2021-02</td>\n",
              "      <td>2022-01-30 04:48:46</td>\n",
              "      <td>2022-01-30 04:48:46</td>\n",
              "      <td>2021-11-13 22:56:31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Is SGD a Bayesian sampler?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>arXiv.org</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZSCC: 0000009  arXiv: 2006.15191</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/Users/jacquesthibodeau/Zotero/storage/ACV9IXE...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UnsortedSafety</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 87 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae03991c-4ba0-4012-95f5-806db4e3ea0f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae03991c-4ba0-4012-95f5-806db4e3ea0f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae03991c-4ba0-4012-95f5-806db4e3ea0f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Key       Item Type  ...  History Legislative Body\n",
              "1   HX9UZ5JP  journalArticle  ...      NaN              NaN\n",
              "3   JVMJ4RMM  journalArticle  ...      NaN              NaN\n",
              "11  TK5F29IU  journalArticle  ...      NaN              NaN\n",
              "25  NHWZIKZ2  journalArticle  ...      NaN              NaN\n",
              "30  HDWGJGAP  journalArticle  ...      NaN              NaN\n",
              "\n",
              "[5 rows x 87 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_arxiv = df[df['Url'].str.contains('arxiv') == True]"
      ],
      "metadata": {
        "id": "DTjcEYYLAwAd"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_arxiv['Url'].str.replace('arxiv', 'ar5iv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrJ1qOf1AwGF",
        "outputId": "6cee4b7a-2fef-4384-c0ff-4ea4040d8006"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5       http://ar5iv.org/abs/2002.11328\n",
              "7       http://ar5iv.org/abs/2010.11645\n",
              "8       http://ar5iv.org/abs/2002.11708\n",
              "9       http://ar5iv.org/abs/2012.10800\n",
              "14      http://ar5iv.org/abs/2011.08512\n",
              "                     ...               \n",
              "1150    http://ar5iv.org/abs/1811.05590\n",
              "1154    http://ar5iv.org/abs/1807.08364\n",
              "1156    http://ar5iv.org/abs/1704.02882\n",
              "1159    http://ar5iv.org/abs/1808.04096\n",
              "1160    http://ar5iv.org/abs/1709.06166\n",
              "Name: Url, Length: 317, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grab_text_from_webpage(url):\n",
        "    with request.urlopen(url) as response:\n",
        "        html = response.read()\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    return soup.get_text()"
      ],
      "metadata": {
        "id": "hA2EL4f0AwLP"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_paper = grab_text_from_webpage('http://ar5iv.org/abs/2002.11328')"
      ],
      "metadata": {
        "id": "4-xztLuoAwPa"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_paper.split('\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BASwDT0CIfKQ",
        "outputId": "6aee8d45-9561-4f70-a69b-54ca0daf797f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '\\n[2002.11328] Rethinking Bias-Variance Trade-off for Generalization of Neural Networks',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\n  function detectColorScheme(){\\n    var theme=\"light\";\\n    var current_theme = localStorage.getItem(\"ar5iv_theme\");\\n    if(current_theme){\\n      if(current_theme == \"dark\"){\\n        theme = \"dark\";\\n      } }\\n    else if(!window.matchMedia) { return false; }\\n    else if(window.matchMedia(\"(prefers-color-scheme: dark)\").matches) {\\n      theme = \"dark\"; }\\n    if (theme==\"dark\") {\\n      document.documentElement.setAttribute(\"data-theme\", \"dark\");\\n    } else {\\n      document.documentElement.setAttribute(\"data-theme\", \"light\"); } }\\n  \\n  detectColorScheme();\\n  \\n  function toggleColorScheme(){\\n    var current_theme = localStorage.getItem(\"ar5iv_theme\");\\n    if (current_theme) {\\n      if (current_theme == \"light\") {\\n        localStorage.setItem(\"ar5iv_theme\", \"dark\"); }\\n      else {\\n        localStorage.setItem(\"ar5iv_theme\", \"light\"); } }\\n    else {\\n        localStorage.setItem(\"ar5iv_theme\", \"dark\"); }\\n    detectColorScheme(); }',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Rethinking Bias-Variance Trade-off for Generalization of Neural Networks',\n",
              " '\\nZitong Yang',\n",
              " '\\u2003\\u2003\\nYaodong Yu',\n",
              " '\\u2003\\u2003\\nChong You',\n",
              " '\\u2003\\u2003\\nJacob Steinhardt',\n",
              " '\\u2003\\u2003\\nYi Ma',\n",
              " '',\n",
              " 'Abstract\\nThe classical bias-variance trade-off predicts that bias decreases and variance increases with model complexity, leading to a U-shaped risk curve.\\nRecent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better.\\nWe provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network.\\nWe vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered.\\nThe risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case.\\nWe corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer.\\nFinally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount.\\nMoreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.',\n",
              " 'Machine Learning, ICML',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '1 Introduction',\n",
              " '',\n",
              " '\\n(a) Case 1',\n",
              " '',\n",
              " '\\n(b) Case 2',\n",
              " '',\n",
              " '\\n(c) Case 3',\n",
              " '',\n",
              " 'Figure 1: Typical cases of expected risk curve (in black) in neural networks. Blue: squared bias curve. Red: variance curve.',\n",
              " '\\nBias-variance tradeoff is a fundamental principle for understanding the generalization of predictive learning models (Hastie et\\xa0al., 2001).\\nThe bias is an error term that stems from a mismatch between the model class and the underlying data distribution, and is typically monotonically non-increasing as a function of the complexity of the model. The variance measures sensitivity to fluctuations in the training set and is often attributed to a large number of model parameters.\\nClassical wisdom predicts that model variance increases and bias decreases monotonically with model complexity (Geman et\\xa0al., 1992).\\nUnder this perspective, we should seek a model that has neither too little nor too much capacity and achieves the best trade-off between bias and variance.',\n",
              " '\\nIn contrast, modern practice for neural networks repeatedly demonstrates the benefit of increasing the number of neurons\\xa0(Krizhevsky et\\xa0al., 2012; Simonyan & Zisserman, 2015; Zhang et\\xa0al., 2017), even up to the point of saturating available memory.\\nThis gap between classical theory and modern practice motivates us to revisit the bias-variance trade-off theory in this new context.',\n",
              " '\\nRecently, a series of works offer an explanation for the generalization of deep learning methods by proposing a double descent risk curve (Belkin et\\xa0al., 2019a, 2018, b), which has also been analytically characterized for certain regression models (Mei & Montanari, 2019; Hastie et\\xa0al., 2019; Spigler et\\xa0al., 2019; Deng et\\xa0al., 2019; Advani & Saxe, 2017). However, there exists two mysteries around the double descent risk curve. First, the double descent phenomenon can not be robustly observed\\xa0(Ba et\\xa0al., 2020). To observe it in modern neural network architectures, we have to artificially inject label noise\\xa0(Nakkiran et\\xa0al., 2019). Second, there lacks an explanation for why the double descent risk curve should occur. In this work, we offer an simple explanation for these two mysteries by proposing an unexpected unimodal variance curve.',\n",
              " '\\nSpecifically, we measure the bias and variance of modern deep neural networks trained on commonly used computer vision datasets.\\nOur main finding is that while the bias is monotonically decreasing with network width as in the classical theory, the variance curve is unimodal or bell-shaped: it first increases and then decreases\\xa0(see Figure 2).\\nImportantly, this unimodal variance phenomenon occurs robustly for varying network architecture and dataset.\\nMoreover, by using a generalized bias-variance decomposition for Bregman divergences\\xa0(Pfau, 2013),\\nwe verify that it occurs for both squared loss and cross-entropy loss.',\n",
              " '\\nThis unimodal variance phenomenon initially appears to contradict recent theoretical work suggesting that both bias and variance are non-monotonic and exhibit a peak in some regimes\\xa0(Mei & Montanari, 2019; Hastie et\\xa0al., 2019) .\\nThe difference is that this previous work considered the fixed-design bias and variance, while we measure the random-design bias and variance (we describe the differences in detail in §2.1). Neal et\\xa0al. (2019) and Nakkiran (2019) also considered the random-design setting, but neither studied the shape of the variance curve as a function of model width and so had not investigated the unimodality phenomenon.',\n",
              " '\\nA key finding of our work is that the complex behavior of the risk curve arises due to the simple but non-classical variance unimodality phenomenon.\\nIndeed, since the expected risk (test loss) is the sum of bias and variance, monotonic bias and unimodal variance can lead to three characteristic behaviors, illustrated in Figure\\xa01, depending on the relative size of the bias and variance.\\nIf the bias completely dominates, we obtain monotonically decreasing risk curve\\xa0(see Figure\\xa01(a)).\\nMeanwhile, if the variance dominates, we obtain a bell-shaped risk curve that first increases then decreases (see Figure\\xa01(c)).\\nThe most complex behavior is if bias and variance dominate in different regimes, leading to the double-descent risk curve in Figure\\xa01(b).\\nAll three behaviors are well-aligned with the empirical observation in deep learning that larger models typically perform better. The most common behavior in our experiments is the first case (monotonically decreasing risk curve) as bias is typically larger than variance. We can observe the double-descent risk curve when label noise is added to the training set (see §3.3), and can observe the unimodal risk curve when we use the generalized bias-variance decomposition for cross-entropy loss (see §3.2).',\n",
              " '\\nFurther Implications.',\n",
              " 'The investigations described above characterize bias and variance as a function of network width, but we can explore the dependence on other quantities as well, such as model depth (§4.2). Indeed, we find that deeper models tend to have lower bias but higher variance.\\nSince bias is larger at current model sizes, this confirms the prevailing wisdom that we should generally use deeper models when possible.\\nOn the other hand, it suggests that this process may have a limit—eventually very deep models may have low bias but high variance such that increasing the depth further harms performance.',\n",
              " '\\nWe also investigate the commonly observed drop in accuracy for models evaluated on out-of-distribution data, and attribute it primarily to increased bias.\\nCombined with the previous observation, this suggests that increasing model depth may help combat the drop in out-of-distribution accuracy, which is supported by experimental findings in Hendrycks & Dietterich (2019).',\n",
              " '',\n",
              " 'Theoretical Analysis of A Two-Layer Neural Network.',\n",
              " 'Finally, we conduct a theoretical study of a two-layer linear network with a random Gaussian first layer. While this model is much simpler than those used in practice, we nevertheless observe the same characteristic behaviors for the bias and variance.\\nIn particular, by working in the asymptotic setting where the input data dimension, amount of training data, and network width go to infinity with fixed ratios, we show that the bias is monotonically decreasing while the variance curve is unimodal.\\nOur analysis also characterizes the location of the variance peak as the point where the number of hidden neurons is approximately half of the dimension of the input data.',\n",
              " '',\n",
              " '',\n",
              " '2 Preliminaries',\n",
              " 'In this section we present the bias-variance decomposition for squared loss. We also present a generalized bias-variance decomposition for cross-entropy loss in §2.2.\\nThe task is to learn a function f:ℝd→ℝc,:𝑓→superscriptℝ𝑑superscriptℝ𝑐f:\\\\mathbb{R}^{d}\\\\rightarrow\\\\mathbb{R}^{c},italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT , based on i.i.d. training samples 𝒯={(𝒙i,𝒚i)}i=1n𝒯superscriptsubscriptsubscript𝒙𝑖subscript𝒚𝑖𝑖1𝑛\\\\mathcal{T}=\\\\{(\\\\boldsymbol{x}_{i},\\\\boldsymbol{y}_{i})\\\\}_{i=1}^{n}caligraphic_T = { ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT drawn from a joint distribution P𝑃Pitalic_P on ℝd×ℝcsuperscriptℝ𝑑superscriptℝ𝑐\\\\mathbb{R}^{d}\\\\times\\\\mathbb{R}^{c}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT × blackboard_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT, such that the mean squared error 𝔼𝒙,𝒚\\u2062[∥𝒚-f\\u2062(𝒙,𝒯)∥22]subscript𝔼𝒙𝒚delimited-[]superscriptsubscriptnorm𝒚𝑓𝒙𝒯22\\\\mathbb{E}_{\\\\boldsymbol{x},\\\\boldsymbol{y}}\\\\left[\\\\|\\\\boldsymbol{y}-f(\\\\boldsymbol%\\n{x},\\\\mathcal{T})\\\\|_{2}^{2}\\\\right]blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_y end_POSTSUBSCRIPT [ ∥ bold_italic_y - italic_f ( bold_italic_x , caligraphic_T ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] is minimal, where (𝒙,𝒚)∼Psimilar-to𝒙𝒚𝑃(\\\\boldsymbol{x},\\\\boldsymbol{y})\\\\sim P( bold_italic_x , bold_italic_y ) ∼ italic_P.\\nHere we denote the learned function by f\\u2062(𝒙;𝒯)𝑓𝒙𝒯f(\\\\boldsymbol{x};\\\\mathcal{T})italic_f ( bold_italic_x ; caligraphic_T ) to make the dependence on the training samples clear.',\n",
              " '\\nNote that the learned predictor f\\u2062(𝒙;𝒯)𝑓𝒙𝒯f(\\\\boldsymbol{x};\\\\mathcal{T})italic_f ( bold_italic_x ; caligraphic_T ) is a random quantity depending on 𝒯𝒯\\\\mathcal{T}caligraphic_T.\\nWe can assess its performance in two different ways.\\nThe first way, random-design, takes the expectation over 𝒯𝒯\\\\mathcal{T}caligraphic_T such that we consider the expected error 𝔼𝒯\\u2062[∥𝒚-f\\u2062(𝒙,𝒯)∥22]subscript𝔼𝒯delimited-[]superscriptsubscriptnorm𝒚𝑓𝒙𝒯22\\\\mathbb{E}_{\\\\mathcal{T}}\\\\left[\\\\|\\\\boldsymbol{y}-f(\\\\boldsymbol{x},\\\\mathcal{T})\\\\|%\\n_{2}^{2}\\\\right]blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [ ∥ bold_italic_y - italic_f ( bold_italic_x , caligraphic_T ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ].\\nThe second way, fixed-design, holds the training covariates {𝒙i}i=1nsuperscriptsubscriptsubscript𝒙𝑖𝑖1𝑛\\\\{\\\\boldsymbol{x}_{i}\\\\}_{i=1}^{n}{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT fixed and only takes expectation over {𝒚i}i=1nsuperscriptsubscriptsubscript𝒚𝑖𝑖1𝑛\\\\{\\\\boldsymbol{y}_{i}\\\\}_{i=1}^{n}{ bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, i.e., 𝔼𝒯\\u2062[∥𝒚-f\\u2062(𝒙,𝒯)∥22∣{𝒙i}i=1n]subscript𝔼𝒯delimited-[]conditionalsuperscriptsubscriptnorm𝒚𝑓𝒙𝒯22superscriptsubscriptsubscript𝒙𝑖𝑖1𝑛\\\\mathbb{E}_{\\\\mathcal{T}}\\\\left[\\\\|\\\\boldsymbol{y}-f(\\\\boldsymbol{x},\\\\mathcal{T})\\\\|%\\n_{2}^{2}\\\\mid\\\\{\\\\boldsymbol{x}_{i}\\\\}_{i=1}^{n}\\\\right]blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [ ∥ bold_italic_y - italic_f ( bold_italic_x , caligraphic_T ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∣ { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ]. The choice of random/fixed-design leads to different bias-variance decompositions. Throughout the paper, we focus on random-design, as opposed to fixed-design studied in Mei & Montanari (2019); Hastie et\\xa0al. (2019); Ba et\\xa0al. (2020).',\n",
              " '',\n",
              " '2.1 Bias Variance Decomposition',\n",
              " 'Random Design.',\n",
              " 'In the random-design setting, decomposing the quantity 𝔼𝒯\\u2062[∥𝒚-f\\u2062(𝒙,𝒯)∥22]subscript𝔼𝒯delimited-[]superscriptsubscriptnorm𝒚𝑓𝒙𝒯22\\\\mathbb{E}_{\\\\mathcal{T}}\\\\left[\\\\|\\\\boldsymbol{y}-f(\\\\boldsymbol{x},\\\\mathcal{T})\\\\|%\\n_{2}^{2}\\\\right]blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [ ∥ bold_italic_y - italic_f ( bold_italic_x , caligraphic_T ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] gives the usual bias-variance trade-off from machine learning, e.g. Geman et\\xa0al. (1992); Hastie et\\xa0al. (2001).',\n",
              " '',\n",
              " '𝔼𝒙,𝒚\\u2062𝔼𝒯\\u2062[∥𝒚-f\\u2062(𝒙,𝒯)∥22]=subscript𝔼𝒙𝒚subscript𝔼𝒯delimited-[]superscriptsubscriptnorm𝒚𝑓𝒙𝒯22absent\\\\displaystyle\\\\mathbb{E}_{\\\\boldsymbol{x},\\\\boldsymbol{y}}\\\\mathbb{E}_{\\\\mathcal{T}%\\n}\\\\left[\\\\|\\\\boldsymbol{y}-f(\\\\boldsymbol{x},\\\\mathcal{T})\\\\|_{2}^{2}\\\\right]=blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_y end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [ ∥ bold_italic_y - italic_f ( bold_italic_x , caligraphic_T ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] =',\n",
              " '',\n",
              " '\\n𝔼𝒙,𝒚\\u2062[∥𝒚-f¯\\u2062(𝒙)∥22]⏟𝐁𝐢𝐚𝐬2+𝔼𝒙\\u2062𝔼𝒯\\u2062[∥f\\u2062(𝒙,𝒯)-f¯\\u2062(𝒙)∥22]⏟𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞,subscript⏟subscript𝔼𝒙𝒚delimited-[]superscriptsubscriptnorm𝒚¯𝑓𝒙22superscript𝐁𝐢𝐚𝐬2subscript⏟subscript𝔼𝒙subscript𝔼𝒯delimited-[]superscriptsubscriptnorm𝑓𝒙𝒯¯𝑓𝒙22𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞\\\\displaystyle\\\\underbrace{\\\\mathbb{E}_{\\\\boldsymbol{x},\\\\boldsymbol{y}}\\\\left[\\\\|%\\n\\\\boldsymbol{y}-\\\\bar{f}(\\\\boldsymbol{x})\\\\|_{2}^{2}\\\\right]}_{\\\\textbf{Bias}^{2}}+%\\n\\\\underbrace{\\\\mathbb{E}_{\\\\boldsymbol{x}}\\\\mathbb{E}_{\\\\mathcal{T}}\\\\left[\\\\|f(%\\n\\\\boldsymbol{x},\\\\mathcal{T})-\\\\bar{f}(\\\\boldsymbol{x})\\\\|_{2}^{2}\\\\right]}_{\\\\textbf%\\n{Variance}},under⏟ start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_y end_POSTSUBSCRIPT [ ∥ bold_italic_y - over¯ start_ARG italic_f end_ARG ( bold_italic_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] end_ARG start_POSTSUBSCRIPT Bias start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT + under⏟ start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [ ∥ italic_f ( bold_italic_x , caligraphic_T ) - over¯ start_ARG italic_f end_ARG ( bold_italic_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] end_ARG start_POSTSUBSCRIPT Variance end_POSTSUBSCRIPT ,',\n",
              " '',\n",
              " 'where f¯\\u2062(𝒙)=𝔼𝒯\\u2062f\\u2062(𝒙,𝒯)¯𝑓𝒙subscript𝔼𝒯𝑓𝒙𝒯\\\\bar{f}(\\\\boldsymbol{x})=\\\\mathbb{E}_{\\\\mathcal{T}}f(\\\\boldsymbol{x},\\\\mathcal{T})over¯ start_ARG italic_f end_ARG ( bold_italic_x ) = blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT italic_f ( bold_italic_x , caligraphic_T ). Here 𝔼𝒯[∥(𝒚-f(𝒙,𝒯)∥22]fragmentssubscript𝔼𝒯fragments[parallel-tofragments(yffragments(x,T)superscriptsubscriptparallel-to22]\\\\mathbb{E}_{\\\\mathcal{T}}\\\\left[\\\\|(\\\\boldsymbol{y}-f(\\\\boldsymbol{x},\\\\mathcal{T})%\\n\\\\|_{2}^{2}\\\\right]blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [ ∥ ( bold_italic_y - italic_f ( bold_italic_x , caligraphic_T ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] measures the average prediction error over different realizations of the training sample. In addition to take the expectation 𝔼𝒯subscript𝔼𝒯\\\\mathbb{E}_{\\\\mathcal{T}}blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT, we also average over 𝔼𝒙,𝒚subscript𝔼𝒙𝒚\\\\mathbb{E}_{\\\\boldsymbol{x},\\\\boldsymbol{y}}blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_y end_POSTSUBSCRIPT, as discussed in Bishop (2006, §3.2). For future reference, we define',\n",
              " '',\n",
              " '𝐁𝐢𝐚𝐬2superscript𝐁𝐢𝐚𝐬2\\\\displaystyle\\\\textbf{Bias}^{2}Bias start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\\n=𝔼𝒙,𝒚\\u2062[∥𝒚-f¯\\u2062(𝒙)∥22],absentsubscript𝔼𝒙𝒚delimited-[]superscriptsubscriptnorm𝒚¯𝑓𝒙22\\\\displaystyle=\\\\mathbb{E}_{\\\\boldsymbol{x},\\\\boldsymbol{y}}\\\\left[\\\\|\\\\boldsymbol{y}%\\n-\\\\bar{f}(\\\\boldsymbol{x})\\\\|_{2}^{2}\\\\right],= blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_y end_POSTSUBSCRIPT [ ∥ bold_italic_y - over¯ start_ARG italic_f end_ARG ( bold_italic_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ,',\n",
              " '(1)',\n",
              " '',\n",
              " 'Variance\\n=𝔼𝒙\\u2062𝔼𝒯\\u2062[∥f\\u2062(𝒙,𝒯)-f¯\\u2062(𝒙)∥22].absentsubscript𝔼𝒙subscript𝔼𝒯delimited-[]superscriptsubscriptnorm𝑓𝒙𝒯¯𝑓𝒙22\\\\displaystyle=\\\\mathbb{E}_{\\\\boldsymbol{x}}\\\\mathbb{E}_{\\\\mathcal{T}}\\\\left[\\\\|f(%\\n\\\\boldsymbol{x},\\\\mathcal{T})-\\\\bar{f}(\\\\boldsymbol{x})\\\\|_{2}^{2}\\\\right].= blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [ ∥ italic_f ( bold_italic_x , caligraphic_T ) - over¯ start_ARG italic_f end_ARG ( bold_italic_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .',\n",
              " '(2)',\n",
              " '\\nIn §2.2, we present our estimator for bias and variance in equation (1) and (2).',\n",
              " '',\n",
              " 'Fixed Design.',\n",
              " 'In fixed-design setting, the covariates {𝒙i}i=1nsuperscriptsubscriptsubscript𝒙𝑖𝑖1𝑛\\\\{\\\\boldsymbol{x}_{i}\\\\}_{i=1}^{n}{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT are held be fixed, and the only randomness in the training set 𝒯𝒯\\\\mathcal{T}caligraphic_T comes from 𝒚i∼P\\u2062(𝒀∣𝑿=𝒙i)similar-tosubscript𝒚𝑖𝑃conditional𝒀𝑿subscript𝒙𝑖\\\\boldsymbol{y}_{i}\\\\sim P(\\\\boldsymbol{Y}\\\\mid\\\\boldsymbol{X}=\\\\boldsymbol{x}_{i})bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ italic_P ( bold_italic_Y ∣ bold_italic_X = bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).\\nAs presented in Mei & Montanari (2019); Hastie et\\xa0al. (2019); Ba et\\xa0al. (2020), a more natural way to present the fixed-design assumption is to hold {𝒙i}i=1nsuperscriptsubscriptsubscript𝒙𝑖𝑖1𝑛\\\\{\\\\boldsymbol{x}_{i}\\\\}_{i=1}^{n}{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT to be fixed and let 𝒚i=f0\\u2062(𝒙)+ϵisubscript𝒚𝑖subscript𝑓0𝒙subscriptbold-italic-ϵ𝑖\\\\boldsymbol{y}_{i}=f_{0}(\\\\boldsymbol{x})+\\\\boldsymbol{\\\\epsilon}_{i}bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_x ) + bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for i=1,…,n𝑖1…𝑛i=1,\\\\dots,nitalic_i = 1 , … , italic_n, where f0\\u2062(𝒙)subscript𝑓0𝒙f_{0}(\\\\boldsymbol{x})italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_x ) is a ground-truth function and ϵisubscriptbold-italic-ϵ𝑖\\\\boldsymbol{\\\\epsilon}_{i}bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are random noises.\\nUnder this assumption, the randomness in 𝒯𝒯\\\\mathcal{T}caligraphic_T all comes from the random noise ϵisubscriptbold-italic-ϵ𝑖\\\\boldsymbol{\\\\epsilon}_{i}bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. To make this clear, we write 𝒯𝒯\\\\mathcal{T}caligraphic_T as 𝒯ϵisubscript𝒯subscriptbold-italic-ϵ𝑖\\\\mathcal{T}_{\\\\boldsymbol{\\\\epsilon}_{i}}caligraphic_T start_POSTSUBSCRIPT bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT.\\nThen, we obtain the fixed-design bias-variance decomposition',\n",
              " '',\n",
              " '𝔼ϵi[∥(𝒚-f(𝒙,𝒯ϵi)∥22]=fragmentssubscript𝔼subscriptbold-italic-ϵ𝑖fragments[parallel-tofragments(yffragments(x,subscript𝒯subscriptbold-italic-ϵ𝑖)superscriptsubscriptparallel-to22]\\\\displaystyle\\\\mathbb{E}_{\\\\boldsymbol{\\\\epsilon}_{i}}\\\\left[\\\\|(\\\\boldsymbol{y}-f(%\\n\\\\boldsymbol{x},\\\\mathcal{T}_{\\\\boldsymbol{\\\\epsilon}_{i}})\\\\|_{2}^{2}\\\\right]=blackboard_E start_POSTSUBSCRIPT bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ ∥ ( bold_italic_y - italic_f ( bold_italic_x , caligraphic_T start_POSTSUBSCRIPT bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] =',\n",
              " '',\n",
              " '\\n[∥(𝒚-f¯(𝒙)∥22]⏟𝐁𝐢𝐚𝐬2+𝔼ϵi[∥(f(𝒙,𝒯ϵi)-f¯(𝒙)∥22]⏟𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞,subscript⏟fragments[parallel-tofragments(y¯𝑓fragments(x)superscriptsubscriptparallel-to22]superscript𝐁𝐢𝐚𝐬2subscript⏟fragmentssubscript𝔼subscriptbold-italic-ϵ𝑖fragments[parallel-tofragments(ffragments(x,subscript𝒯subscriptbold-italic-ϵ𝑖)¯𝑓fragments(x)superscriptsubscriptparallel-to22]𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞\\\\displaystyle\\\\underbrace{\\\\left[\\\\|(\\\\boldsymbol{y}-\\\\bar{f}(\\\\boldsymbol{x})\\\\|_{2}%\\n^{2}\\\\right]}_{\\\\textbf{Bias}^{2}}+\\\\underbrace{\\\\mathbb{E}_{\\\\boldsymbol{\\\\epsilon}%\\n_{i}}\\\\left[\\\\|(f(\\\\boldsymbol{x},\\\\mathcal{T}_{\\\\boldsymbol{\\\\epsilon}_{i}})-\\\\bar{f%\\n}(\\\\boldsymbol{x})\\\\|_{2}^{2}\\\\right]}_{\\\\textbf{Variance}},under⏟ start_ARG [ ∥ ( bold_italic_y - over¯ start_ARG italic_f end_ARG ( bold_italic_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] end_ARG start_POSTSUBSCRIPT Bias start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT + under⏟ start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ ∥ ( italic_f ( bold_italic_x , caligraphic_T start_POSTSUBSCRIPT bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) - over¯ start_ARG italic_f end_ARG ( bold_italic_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] end_ARG start_POSTSUBSCRIPT Variance end_POSTSUBSCRIPT ,',\n",
              " '',\n",
              " 'where f¯\\u2062(𝒙)=𝔼ϵi\\u2062f\\u2062(𝒙,𝒯ϵi)¯𝑓𝒙subscript𝔼subscriptbold-italic-ϵ𝑖𝑓𝒙subscript𝒯subscriptbold-italic-ϵ𝑖\\\\bar{f}(\\\\boldsymbol{x})=\\\\mathbb{E}_{\\\\boldsymbol{\\\\epsilon}_{i}}f(\\\\boldsymbol{x}%\\n,\\\\mathcal{T}_{\\\\boldsymbol{\\\\epsilon}_{i}})over¯ start_ARG italic_f end_ARG ( bold_italic_x ) = blackboard_E start_POSTSUBSCRIPT bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_f ( bold_italic_x , caligraphic_T start_POSTSUBSCRIPT bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ). In most practical settings, the expectation 𝔼ϵi\\u2062f\\u2062(𝒙,𝒯ϵi)subscript𝔼subscriptbold-italic-ϵ𝑖𝑓𝒙subscript𝒯subscriptbold-italic-ϵ𝑖\\\\mathbb{E}_{\\\\boldsymbol{\\\\epsilon}_{i}}f(\\\\boldsymbol{x},\\\\mathcal{T}_{%\\n\\\\boldsymbol{\\\\epsilon}_{i}})blackboard_E start_POSTSUBSCRIPT bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_f ( bold_italic_x , caligraphic_T start_POSTSUBSCRIPT bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) cannot be estimated from training samples 𝒯={(𝒙i,𝒚i)}i=1n𝒯superscriptsubscriptsubscript𝒙𝑖subscript𝒚𝑖𝑖1𝑛\\\\mathcal{T}=\\\\{(\\\\boldsymbol{x}_{i},\\\\boldsymbol{y}_{i})\\\\}_{i=1}^{n}caligraphic_T = { ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, because we do not have access to independent copies of f\\u2062(𝒙i)+ϵi𝑓subscript𝒙𝑖subscriptbold-italic-ϵ𝑖f(\\\\boldsymbol{x}_{i})+\\\\boldsymbol{\\\\epsilon}_{i}italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + bold_italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\\nIn comparison to the random-design setting, the fixed-design setting tends to have larger bias and smaller variance, since more “randomness” is introduced into the variance term.',\n",
              " '',\n",
              " '',\n",
              " '2.2 Estimating Bias and Variance',\n",
              " 'In this section, we present the estimator we use to estimate the bias and variance as defined in equation (1) and (2).\\nThe high level idea is to approximate the expectation 𝔼𝒯subscript𝔼𝒯\\\\mathbb{E}_{\\\\mathcal{T}}blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT by computing the sample average using multiple training sets 𝒯1,…,𝒯Nsubscript𝒯1…subscript𝒯𝑁\\\\mathcal{T}_{1},\\\\dots,\\\\mathcal{T}_{N}caligraphic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , caligraphic_T start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT.\\nWhen evaluating the expectation 𝔼𝒯subscript𝔼𝒯\\\\mathbb{E}_{\\\\mathcal{T}}blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT, there is a trade-off between having larger training sets (n𝑛nitalic_n) within each training set and having larger number of splits (N𝑁Nitalic_N), since n×N=𝑛𝑁absentn\\\\times N=italic_n × italic_N = total number of training samples.',\n",
              " '\\nMean Squared Error (MSE).',\n",
              " 'To estimate bias and variance in equation (1) and (2), we introduce an unbiased estimator for variance, and obtain bias by subtracting the variance from the risk.\\nLet 𝒯=𝒯1∪⋯∪𝒯N𝒯subscript𝒯1⋯subscript𝒯𝑁\\\\mathcal{T}=\\\\mathcal{T}_{1}\\\\cup\\\\cdots\\\\cup\\\\mathcal{T}_{N}caligraphic_T = caligraphic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∪ ⋯ ∪ caligraphic_T start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT be a random disjoint split of training samples.\\nIn our experiment, we mainly take N=2𝑁2N=2italic_N = 2 (for CIFAR10 each 𝒯isubscript𝒯𝑖\\\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT has 25k samples).\\nTo estimate the variance, we use the unbiased estimator',\n",
              " '',\n",
              " '𝗏𝖺𝗋^\\u2062(𝒙,𝒯)=1N-1\\u2062∑j=1N∥f\\u2062(𝒙,𝒯j)-∑j=1N1N\\u2062f\\u2062(𝒙,𝒯j)∥22,^𝗏𝖺𝗋𝒙𝒯1𝑁1superscriptsubscript𝑗1𝑁superscriptsubscriptnorm𝑓𝒙subscript𝒯𝑗superscriptsubscript𝑗1𝑁1𝑁𝑓𝒙subscript𝒯𝑗22\\\\widehat{\\\\textsf{var}}(\\\\boldsymbol{x},\\\\mathcal{T})=\\\\frac{1}{N-1}\\\\sum_{j=1}^{N}%\\n\\\\Big{\\\\|}f(\\\\boldsymbol{x},\\\\mathcal{T}_{j})-{\\\\sum_{j=1}^{N}\\\\frac{1}{N}f(%\\n\\\\boldsymbol{x},\\\\mathcal{T}_{j})}\\\\Big{\\\\|}_{2}^{2},over^ start_ARG var end_ARG ( bold_italic_x , caligraphic_T ) = divide start_ARG 1 end_ARG start_ARG italic_N - 1 end_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ italic_f ( bold_italic_x , caligraphic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) - ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG italic_f ( bold_italic_x , caligraphic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,',\n",
              " '',\n",
              " 'where var depends on the test point 𝒙𝒙\\\\boldsymbol{x}bold_italic_x and on the random training set 𝒯𝒯\\\\mathcal{T}caligraphic_T.\\nWhile var is unbiased, its\\nvariance can be reduced by using multiple random splits to obtain estimators 𝗏𝖺𝗋^1,…,𝗏𝖺𝗋^ksubscript^𝗏𝖺𝗋1…subscript^𝗏𝖺𝗋𝑘\\\\widehat{\\\\textsf{var}}_{1},\\\\dots,\\\\widehat{\\\\textsf{var}}_{k}over^ start_ARG var end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , over^ start_ARG var end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and taking their average. This reduces the variance of the variance estimator since:',\n",
              " '',\n",
              " 'Var𝒯\\u2062(1k\\u2062∑i=1k𝗏𝖺𝗋^i)=∑i\\u2062jCov𝒯\\u2062(𝗏𝖺𝗋^i,𝗏𝖺𝗋^j)k2≤Var𝒯\\u2062(𝗏𝖺𝗋^1),subscriptVar𝒯1𝑘superscriptsubscript𝑖1𝑘subscript^𝗏𝖺𝗋𝑖subscript𝑖𝑗subscriptCov𝒯subscript^𝗏𝖺𝗋𝑖subscript^𝗏𝖺𝗋𝑗superscript𝑘2subscriptVar𝒯subscript^𝗏𝖺𝗋1\\\\displaystyle\\\\text{Var}_{\\\\mathcal{T}}\\\\Big{(}\\\\frac{1}{k}\\\\sum_{i=1}^{k}\\\\widehat{%\\n\\\\textsf{var}}_{i}\\\\Big{)}=\\\\frac{\\\\sum_{ij}\\\\text{Cov}_{\\\\mathcal{T}}(\\\\widehat{%\\n\\\\textsf{var}}_{i},\\\\widehat{\\\\textsf{var}}_{j})}{k^{2}}\\\\leq\\\\text{Var}_{\\\\mathcal{%\\nT}}(\\\\widehat{\\\\textsf{var}}_{1}),Var start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT ( divide start_ARG 1 end_ARG start_ARG italic_k end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT over^ start_ARG var end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT Cov start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT ( over^ start_ARG var end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG var end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ≤ Var start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT ( over^ start_ARG var end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ,',\n",
              " '',\n",
              " 'where the {𝗏𝖺𝗋^i}i=1ksuperscriptsubscriptsubscript^𝗏𝖺𝗋𝑖𝑖1𝑘\\\\{\\\\widehat{\\\\textsf{var}}_{i}\\\\}_{i=1}^{k}{ over^ start_ARG var end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT are identically distributed but not independent, and we used the Cauchy-Schwarz inequality.',\n",
              " '',\n",
              " 'Cross-Entropy Loss (CE).',\n",
              " 'In addition to the classical bias-variance decomposition for MSE loss, we also consider a generalized bias-variance decomposition for cross-entropy loss.\\nLet π\\u2062(𝒙,𝒯)∈ℝc𝜋𝒙𝒯superscriptℝ𝑐\\\\pi(\\\\boldsymbol{x},\\\\mathcal{T})\\\\in\\\\mathbb{R}^{c}italic_π ( bold_italic_x , caligraphic_T ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT be the output of the neural network (a probability distribution over the class labels).\\nπ\\u2062(𝒙,𝒯)𝜋𝒙𝒯\\\\pi(\\\\boldsymbol{x},\\\\mathcal{T})italic_π ( bold_italic_x , caligraphic_T ) is a random variable since the training set 𝒯𝒯\\\\mathcal{T}caligraphic_T is random.\\nLet π0\\u2062(𝒙)∈ℝcsubscript𝜋0𝒙superscriptℝ𝑐\\\\pi_{0}(\\\\boldsymbol{x})\\\\in\\\\mathbb{R}^{c}italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_x ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT be the one-hot encoding of the ground-truth label.\\nThen, omitting the dependence of π𝜋\\\\piitalic_π and π0subscript𝜋0\\\\pi_{0}italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT on 𝒙𝒙\\\\boldsymbol{x}bold_italic_x and 𝒯𝒯\\\\mathcal{T}caligraphic_T, the cross entropy loss',\n",
              " '',\n",
              " 'H\\u2062(π0,π)=∑l=1cπ0\\u2062[l]\\u2062log\\u2061(π\\u2062[l])𝐻subscript𝜋0𝜋superscriptsubscript𝑙1𝑐subscript𝜋0delimited-[]𝑙𝜋delimited-[]𝑙H(\\\\pi_{0},\\\\pi)=\\\\sum_{l=1}^{c}\\\\pi_{0}[l]\\\\log(\\\\pi[l])italic_H ( italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_π ) = ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT [ italic_l ] roman_log ( italic_π [ italic_l ] )',\n",
              " '',\n",
              " 'can be decomposed as',\n",
              " '\\nAlgorithm 1  Estimating Generalized Variance',\n",
              " '\\xa0\\xa0Input: Test point 𝒙𝒙\\\\boldsymbol{x}bold_italic_x, Training set 𝒯𝒯\\\\mathcal{T}caligraphic_T.',\n",
              " '\\n\\xa0\\xa0for\\xa0i=1𝑖1i=1italic_i = 1 to k𝑘kitalic_k\\xa0do',\n",
              " '\\xa0\\xa0\\xa0\\xa0\\xa0Split the 𝒯𝒯\\\\mathcal{T}caligraphic_T into 𝒯1(i),…,𝒯N(i)superscriptsubscript𝒯1𝑖…superscriptsubscript𝒯𝑁𝑖\\\\mathcal{T}_{1}^{(i)},\\\\dots,\\\\mathcal{T}_{N}^{(i)}caligraphic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , … , caligraphic_T start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT.',\n",
              " '\\n\\xa0\\xa0\\xa0\\xa0\\xa0for\\xa0j=1𝑗1j=1italic_j = 1 to N𝑁Nitalic_N\\xa0do',\n",
              " '\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Train the model using 𝒯j(i)superscriptsubscript𝒯𝑗𝑖\\\\mathcal{T}_{j}^{(i)}caligraphic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT;',\n",
              " '\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Evaluate the model at 𝒙𝒙\\\\boldsymbol{x}bold_italic_x; call the result πj(i)superscriptsubscript𝜋𝑗𝑖\\\\pi_{j}^{(i)}italic_π start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT;',\n",
              " '\\n\\xa0\\xa0\\xa0\\xa0\\xa0end\\xa0for',\n",
              " '\\xa0\\xa0end\\xa0for',\n",
              " '\\xa0\\xa0Compute π^=exp\\u2061{1N⋅k\\u2062∑i\\u2062jlog\\u2061(πj(i))}^𝜋1⋅𝑁𝑘subscript𝑖𝑗superscriptsubscript𝜋𝑗𝑖\\\\widehat{\\\\pi}=\\\\exp\\\\left\\\\{\\\\frac{1}{N\\\\cdot k}\\\\sum_{ij}\\\\log\\\\left(\\\\pi_{j}^{(i)}%\\n\\\\right)\\\\right\\\\}over^ start_ARG italic_π end_ARG = roman_exp { divide start_ARG 1 end_ARG start_ARG italic_N ⋅ italic_k end_ARG ∑ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT roman_log ( italic_π start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) }',\n",
              " '\\xa0\\xa0(using element-wise log and exp; π^normal-^𝜋\\\\widehat{\\\\pi}over^ start_ARG italic_π end_ARG estimates π¯normal-¯𝜋\\\\bar{\\\\pi}over¯ start_ARG italic_π end_ARG).',\n",
              " '\\n\\xa0\\xa0Normalize π^^𝜋\\\\widehat{\\\\pi}over^ start_ARG italic_π end_ARG to get a probability distribution.',\n",
              " '\\n\\xa0\\xa0Compute the variance 1N⋅k\\u2062∑i\\u2062jDKL\\u2062(π^∥πj(i))1⋅𝑁𝑘subscript𝑖𝑗subscript𝐷KLconditional^𝜋superscriptsubscript𝜋𝑗𝑖\\\\frac{1}{N\\\\cdot k}\\\\sum_{ij}D_{\\\\text{KL}}\\\\left(\\\\widehat{\\\\pi}\\\\|\\\\pi_{j}^{(i)}\\\\right)divide start_ARG 1 end_ARG start_ARG italic_N ⋅ italic_k end_ARG ∑ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( over^ start_ARG italic_π end_ARG ∥ italic_π start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ).',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\n𝔼𝒯\\u2062[H\\u2062(π0,π)]=DKL\\u2062(π0∥π¯)⏟𝐁𝐢𝐚𝐬2+𝔼𝒯\\u2062[DKL\\u2062(π¯∥π)]⏟𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞,subscript𝔼𝒯delimited-[]𝐻subscript𝜋0𝜋subscript⏟subscript𝐷KLconditionalsubscript𝜋0¯𝜋superscript𝐁𝐢𝐚𝐬2subscript⏟subscript𝔼𝒯delimited-[]subscript𝐷KLconditional¯𝜋𝜋𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞\\\\displaystyle\\\\mathbb{E}_{\\\\mathcal{T}}\\\\left[H(\\\\pi_{0},\\\\pi)\\\\right]=\\\\underbrace{D%\\n_{\\\\text{KL}}(\\\\pi_{0}\\\\|\\\\bar{\\\\pi})}_{\\\\textbf{Bias}^{2}}+\\\\underbrace{\\\\mathbb{E}_{%\\n\\\\mathcal{T}}\\\\left[D_{\\\\text{KL}}(\\\\bar{\\\\pi}\\\\|\\\\pi)\\\\right]}_{\\\\textbf{Variance}},blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [ italic_H ( italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_π ) ] = under⏟ start_ARG italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∥ over¯ start_ARG italic_π end_ARG ) end_ARG start_POSTSUBSCRIPT Bias start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT + under⏟ start_ARG blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [ italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( over¯ start_ARG italic_π end_ARG ∥ italic_π ) ] end_ARG start_POSTSUBSCRIPT Variance end_POSTSUBSCRIPT ,',\n",
              " '(3)',\n",
              " '\\nwhere π\\u2062[l]𝜋delimited-[]𝑙\\\\pi[l]italic_π [ italic_l ] is the l𝑙litalic_l-th element of π𝜋\\\\piitalic_π, and π¯¯𝜋\\\\bar{\\\\pi}over¯ start_ARG italic_π end_ARG is the average of log-probability after normalization, i.e.,',\n",
              " '',\n",
              " 'π¯\\u2062[l]∝exp\\u2061{𝔼𝒯\\u2062log\\u2061(π\\u2062[l])}\\u2062\\xa0for\\xa0\\u2062l=1,…,c.formulae-sequenceproportional-to¯𝜋delimited-[]𝑙subscript𝔼𝒯𝜋delimited-[]𝑙\\xa0for\\xa0𝑙1…𝑐\\\\bar{\\\\pi}[l]\\\\propto\\\\exp\\\\{\\\\mathbb{E}_{\\\\mathcal{T}}\\\\log(\\\\pi[l])\\\\}\\\\text{\\\\,\\\\, for %\\n\\\\,\\\\,}l=1,\\\\ldots,c.over¯ start_ARG italic_π end_ARG [ italic_l ] ∝ roman_exp { blackboard_E start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT roman_log ( italic_π [ italic_l ] ) } for italic_l = 1 , … , italic_c .',\n",
              " '',\n",
              " 'This decomposition is a special case of the general decomposition for Bregman divergence discussed in Pfau (2013).',\n",
              " '\\nWe apply Algorithm 1 to estimate the generalized variance in (3). Here we could not obtain an unbiased estimator, but the estimate is better if we take more random splits (larger k𝑘kitalic_k). In practice, we choose k𝑘kitalic_k to be large enough so that the estimated variance stabilizes when we further increase k𝑘kitalic_k (see §3.4).\\nSimilar to the case of squared loss, we estimate the bias by subtracting the variance from the risk.',\n",
              " '',\n",
              " '',\n",
              " '\\n3 Measuring Bias and Variance for Neural Networks',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Figure 2: Mainline experiment on ResNet34, CIFAR10 dataset (25,000 training samples). (Left) Risk, bias, and variance for ResNet34. (Middle) Variance for ResNet34. (Right) Train error and test error for ResNet34.',\n",
              " '',\n",
              " '',\n",
              " '\\n(a) ResNext29, MSE loss, CIFAR10',\n",
              " '',\n",
              " '\\n(b) ResNet34, CE loss, CIFAR10',\n",
              " '',\n",
              " '\\n(c) DNN, MSE loss, MNIST',\n",
              " '',\n",
              " 'Figure 3: Risk, bias, and variance with respect to different network architectures, training loss functions, and datasets. (a). ResNext29 trained by MSE loss on CIFAR10 dataset (25,000 training samples). (b). ResNet34 trained by CE loss (estimated by generalized bias-variance decomposition using Bregman divergence) on CIFAR10 dataset (10,000 training samples). (c). Fully connected network with one hidden layer and ReLU activation trained by MSE loss on MNIST dataset (10,000 training samples).',\n",
              " '\\nIn this section, we study the bias and variance (equations\\xa0(1) and (2)) of deep neural networks. While the bias is monotonically decreasing as folk wisdom would predict, the variance is unimodal (first increases to a peak and then decreases). We conduct extensive experiments to verify that this phenomenon appears robustly across architectures, datasets, optimizer, and loss function.',\n",
              " '',\n",
              " '3.1 Mainline Experimental Setup',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Figure 4: Increasing label noise leads to double-descent. (Left) Bias and variance under different label noise percentage. (Right) Training error and test error under different label noise percentage. ',\n",
              " '\\nWe first describe our mainline experimental setup; in the next subsection we vary each design choice to check robustness of the phenomenon. More extensive experimental results are given in the appendix.',\n",
              " '\\nFor the mainline experiment, we trained a ResNet34\\xa0(He et\\xa0al., 2016) on the CIFAR10 dataset\\xa0(Krizhevsky et\\xa0al., 2009).\\nWe trained using stochastic gradient descent (SGD) with momentum 0.90.90.90.9. The initial learning rate is 0.1. We applied stage-wise training (decay learning rate by a factor of 10 every 200 epochs), and used weight decay 5×10-45superscript1045\\\\times 10^{-4}5 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT.\\nTo change the model complexity of the neural network, we scale the number of filters (i.e., width) of the convolutional layers.\\nMore specifically, with width=wwidth𝑤\\\\text{width}=wwidth = italic_w, the number of filters are [w,2\\u2062w,4\\u2062w,8\\u2062w]𝑤2𝑤4𝑤8𝑤[w,2w,4w,8w][ italic_w , 2 italic_w , 4 italic_w , 8 italic_w ].\\nWe vary w𝑤witalic_w from 2 to 64 (the width w𝑤witalic_w of a regular ResNet34 designed for CIFAR10 in He et\\xa0al. (2016) is 16).',\n",
              " '\\nRelative to the standard experimental setup\\xa0(He et\\xa0al., 2016), there are two main differences.\\nFirst, since bias-variance is usually defined for the squared loss (see (1) and (2)), our loss function is the squared error (squared ℓ2subscriptℓ2\\\\ell_{2}roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT distance between the softmax probabilities and the one-hot class vector) rather than the log-loss.\\nIn the next section we also consider models trained with the log-loss and estimate the bias and variance by using a generalized bias-variance decomposition, as described in §2.2.\\nSecond, to measure the variance (and hence bias), we need two models trained on independent subsets of the data as discussed in §2.2.\\nTherefore, the training dataset is split in half and each model is trained on only n=25,000=50,000/2formulae-sequence𝑛25000500002n=25,000=50,000/2italic_n = 25 , 000 = 50 , 000 / 2 data points.\\nWe estimate the variance by averaging over N=3𝑁3N=3italic_N = 3 such random splits (i.e., we train 6=3×26326=3\\\\times 26 = 3 × 2 copies of each model).',\n",
              " '\\nIn Figure 2, we can see that the variance as a function of the width is unimodal and the bias is monotonically decreasing. Since the scale of the variance is small relative to the bias, the overall behavior of the risk is monotonically decreasing.',\n",
              " '',\n",
              " '\\n3.2 Varying Architectures, Loss Functions, Datasets',\n",
              " 'Architectures. We observe the same monotonically descreasing bias and unimodal variance phenomenon for ResNext29\\xa0(Xie et\\xa0al., 2017). To scale the “width” of the ResNext29, we first set the number of channels to 1 and increase the cardinality, defined in (Xie et\\xa0al., 2017), from 2 to 4, and then fix the cardinality at 4 and increase channel size from 1 to 32.\\nResults are shown in Figure\\xa03(a), where the width on the x𝑥xitalic_x-axis is defined as the cardinality times the filter size.',\n",
              " '\\nLoss Function.\\nIn addition to the bias-variance decomposition for MSE loss, we also considered a similar decomposition for cross-entropy loss as described in §2.2. We train with cross-entropy loss and use n=10,000𝑛10000n=10,000italic_n = 10 , 000 training samples (5 splits), repeating\\nN=4𝑁4N=4italic_N = 4 times with independent random splits. As shown in Figure 3(b), the behavior of the generalized bias and variance for cross entropy is consistent with our earlier observations: the bias is monotonically decreasing and the variance is unimodal. The risk first increases and then decreases, corresponding to the unimodal risk pattern in Figure\\xa01(c).',\n",
              " '\\nDatasets. In addition to CIFAR10, we study bias and variance on MNIST\\xa0(LeCun, 1998) and Fashion-MNIST\\xa0(Xiao et\\xa0al., 2017).\\nFor these two datasets, we use a fully connected neural network with one hidden layer with ReLU activation function.\\nThe “width” of the network is the number of hidden nodes. We use 10,000 training samples (N=5𝑁5N=5italic_N = 5).\\nAs seen in Figure 3(c) and 10 (in Appendix\\xa0B), for both MNIST and Fashion-MNIST, the variance is again unimodal and the bias is monotonically decreasing.',\n",
              " '\\nIn addition to the above experiments, we also conduct experiments on the CIFAR100 dataset, the VGG network architecture\\xa0(Simonyan & Zisserman, 2015), various training sample sizes, and different weight decay regularization and present the results in Appendix\\xa0B. We observe the same monotonically descreasing bias and unimodal variance phenomenon in all of these experiments.',\n",
              " '',\n",
              " '\\n3.3 Connection to Double-Descent Risk',\n",
              " 'When the relative scale of bias and variance changes, the risk displays one of the three patterns, monotonically decreasing, double descent, and unimodal, as presented in Figure 1(a), 1(b) and 1(c).\\nIn particular, the recent stream of observations on double descent risk (Belkin et\\xa0al., 2019a) can be explained by unimodal variance and monotonically decreasing bias.\\nIn our experiments, including the experiments in previous sections, we typically observe monotonically decreasing risk; but with more label noise, the variance will increase and we observe the double descent risk curve.',\n",
              " '\\nLabel Noise.',\n",
              " 'Similar to the setup in Nakkiran (2019), for each split, we sample training data from the whole training dataset, and replace the label of each training example with a uniform random class with independent probability p𝑝pitalic_p.\\nLabel noise increases the variance of the model and hence leads to double-descent risk as seen in Figure\\xa04.\\nIf the variance is small, the risk does not have the double-descent shape because the variance peak is not large enough to overwhelm the bias, as observed in Figures 2, 3(a), 3(c) and 10.',\n",
              " '',\n",
              " '',\n",
              " '(a) OOD Example',\n",
              " '',\n",
              " '\\n(b) Bias of model with different depth',\n",
              " '',\n",
              " '\\n(c) Variance of model with different depth',\n",
              " '',\n",
              " 'Figure 5: (a). Risk, bias, and variance for ResNet34 on out-of-distribution examples (CIFAR10-C dataset). (b)-(c). Bias and variance for ResNet with different depth trained by MSE loss on CIFAR10 (25,000 training samples).',\n",
              " '',\n",
              " '',\n",
              " '3.4 Discussion of Possible Sources of Error',\n",
              " 'In this section, we briefly describe the possible sources of error in our estimator defined in §2.2.',\n",
              " '\\nMean Squared Error. As argued in §2.2, the variance estimator is unbiased.\\nTo understand the variance of the estimator, we first split the data into two parts. For each part, we compute the bias and variance for varying network width by using our estimator. Averaging across different model width, the relative difference between the two parts is 0.6% for bias and 3% for variance, so our results for MSE are minimally sensitive to finite-sample effects. The complete experiments can be found in the appendix\\xa0(see Figure 16).',\n",
              " '\\nCross Entropy Loss.\\nFor cross entropy loss, we are currently unable to obtain an unbiased estimator. We can assess the quality of our estimator using the following scheme. We partition the dataset into five parts 𝒯1,…,𝒯5subscript𝒯1…subscript𝒯5\\\\mathcal{T}_{1},\\\\dots,\\\\mathcal{T}_{5}caligraphic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , caligraphic_T start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT, i.e., set N=5𝑁5N=5italic_N = 5 in Algorithm\\xa01. Then, we sequentially plot the estimate of bias and variance using k=1,2,3,4𝑘1234k=1,2,3,4italic_k = 1 , 2 , 3 , 4 as described in Algorithm\\xa01. We observe that using larger k𝑘kitalic_k gives better estimates.\\nIn Figure 17 of Appendix B.8, we observe that as k𝑘kitalic_k increases, the bias curve systematically decreases and the variance curve increases.\\nTherefore our estimator over-estimates the bias and under-estimates the variance, but the overall behaviors of the curves remain consistent.',\n",
              " '',\n",
              " '',\n",
              " '4 What Affects the Bias and Variance?',\n",
              " 'In this section, through the Bias-Variance decomposition analyzed in §3, we investigate the role of depth for neural networks and the robustness of neural networks on out-of-distribution examples.',\n",
              " '',\n",
              " '4.1 Bias-Variance Tradeoff for Out-of-Distribution (OOD) Example',\n",
              " 'For many real-world computer vision applications, inputs can be corrupted by random noise, blur, weather, etc.\\nThese common occurring corruptions are shown to significantly decrease model performance\\xa0(Azulay & Weiss, 2019; Hendrycks & Dietterich, 2019).\\nTo better understand the “generalization gap” between in-distribution test examples and out-of-distribution test examples, we empirically evaluate the bias and variance on the CIFAR10-C dataset developed by\\xa0Hendrycks & Dietterich (2019), which is a common corruption benchmark and includes 15 types of corruption.',\n",
              " '\\nBy applying the models trained in the mainline experiment, we are able to evaluate the bias and variance on CIFAR10-C test dataset according to the definitions in (1) and (2).\\nAs we can see from Figure 5(a), both the bias and variance increase relative to the original CIFAR10 test set.\\nConsistent with the phenomenon observed in the mainline experiment, the bias dominates the overall risk.\\nThe results indicate that the “generalization gap” mainly comes from increased bias, with relatively less contribution from variance as well.',\n",
              " '',\n",
              " '\\n4.2 Effect of Model Depth on Bias and Variance',\n",
              " 'In addition to the ResNet34 considered in the mainline experiment, we also evaluate the bias and variance for ResNet18 and ResNet50.\\nSame as the mainline experiment setup, we estimate the bias and variance for ResNet using 25,000 training samples (N=2𝑁2N=2italic_N = 2) and three independent random splits (k=3𝑘3k=3italic_k = 3). The standard building block of ResNet50 architecture in He et\\xa0al. (2016) is bottleneck block, which is different from the basic block used in ResNet18 and ResNet34.\\nTo ensure that depth is the only changing variable across three architectures, we apply the basic block for ResNet50.\\nSame training epochs and learning rate decays are applied to three models.',\n",
              " '\\nFrom Figure\\xa05(b) and 5(c), we observe that the bias decreases as the depth increases, while the variance increases as the depth increases. For each model, the bias is monotonically decreasing and the variance is unimodal.\\nThe differences in variance are small (around 0.01) compared with the changes in bias. Overall, the risk typically decreases as the depth increases.\\nOur experimental results suggest that the improved generalization for deeper models, with the same network architecture, are mainly attributed to lower bias.',\n",
              " '\\nFor completeness, we also include the bias and variance versus depth when basic blocks in ResNet are replaced by bottleneck blocks\\xa0(see Figure 19 in the appendix). We observe similar qualitative trend of bias and variance.',\n",
              " '\\nNote that at high width, the bias of ResNet50 is slightly higher than the bias of ResNet18 and ResNet34. We attribute this inconsistency to difficulties when training ResNet50 without bottleneck blocks at high width.\\nLastly, we also include the bias and variance versus depth for out-of-distribution test samples, in which case we also observed decreased bias and increased variance as depth increases, as shown in Figure 18 of Appendix B.9.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '5 Theoretical Insights from a Two-layer Linear Model',\n",
              " '',\n",
              " '\\n(a) Risk v.s. γ𝛾\\\\gammaitalic_γ for different n𝑛nitalic_n',\n",
              " '',\n",
              " '\\n(b) Bias v.s. γ𝛾\\\\gammaitalic_γ for different n𝑛nitalic_n',\n",
              " '',\n",
              " '\\n(c) Variance v.s. γ𝛾\\\\gammaitalic_γ for different n𝑛nitalic_n',\n",
              " '',\n",
              " 'Figure 6: Risk, bias, and variance for a two-layer linear neural network.',\n",
              " '\\nWhile the preceding experiments show that the bias and variance robustly exhibit monotonic-unimodal behavior in the random-design setting, existing theoretical analyses hold instead for the fixed-design setting, where the behavior of the bias and variance are more complex, with both the bias and variance exhibiting a peak and the risk exhibiting double descent pattern (Mei & Montanari (2019, Figure 6)).\\nIn general, while the risk should be the same (in expectation) for the random and fixed design setting, the fixed-design setting has lower bias and higher variance.',\n",
              " '\\nMotivated by the more natural behavior in the random-design setting, we work to extend the existing fixed-design theory to the random-design case.\\nOur starting point is Mei & Montanari (2019), who consider two-layer non-linear networks with random hidden layer weights. However, the randomness in the design complicates the analysis, so we make two points of departure to help simplify:\\nfirst, we consider two-layer linear rather than non-linear networks, and second, we consider a different scaling limit (n/d→∞→𝑛𝑑n/d\\\\to\\\\inftyitalic_n / italic_d → ∞ rather than n/d𝑛𝑑n/ditalic_n / italic_d going to some constant).\\nIn this setting, we rigorously show that the variance is indeed unimodal and the bias is monotonically decreasing (Figure 6).\\nOur precise assumptions are given below.',\n",
              " '',\n",
              " '5.1 Model Assumptions',\n",
              " 'We consider the task of learning a function y=f\\u2062(𝒙)𝑦𝑓𝒙y=f(\\\\boldsymbol{x})italic_y = italic_f ( bold_italic_x ) that maps each input vector 𝒙∈ℝd𝒙superscriptℝ𝑑\\\\boldsymbol{x}\\\\in\\\\mathbb{R}^{d}bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT to an output (label) value y∈ℝ𝑦ℝy\\\\in\\\\mathbb{R}italic_y ∈ blackboard_R.\\nThe input-output pair (𝒙,y)𝒙𝑦(\\\\boldsymbol{x},y)( bold_italic_x , italic_y ) is assumed to be drawn from a distribution where 𝒙∼𝒩\\u2062(0,𝑰d/d)similar-to𝒙𝒩0subscript𝑰𝑑𝑑\\\\boldsymbol{x}\\\\sim\\\\mathcal{N}(0,\\\\boldsymbol{I}_{d}/d)bold_italic_x ∼ caligraphic_N ( 0 , bold_italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT / italic_d ) and',\n",
              " '',\n",
              " 'y=f0\\u2062(𝒙):=𝒙⊤\\u2062𝜽,𝑦subscript𝑓0𝒙assignsuperscript𝒙top𝜽y=f_{0}(\\\\boldsymbol{x}):=\\\\boldsymbol{x}^{\\\\top}\\\\boldsymbol{\\\\theta},italic_y = italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_x ) := bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_θ ,',\n",
              " '(4)',\n",
              " '\\nwhere 𝜽∈ℝd𝜽superscriptℝ𝑑\\\\boldsymbol{\\\\theta}\\\\in\\\\mathbb{R}^{d}bold_italic_θ ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is a weight vector.\\nGiven a training set 𝒯:={(𝒙i,yi)}i=1nassign𝒯superscriptsubscriptsubscript𝒙𝑖subscript𝑦𝑖𝑖1𝑛\\\\mathcal{T}:=\\\\{(\\\\boldsymbol{x}_{i},y_{i})\\\\}_{i=1}^{n}caligraphic_T := { ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT with training samples drawn independently from the data distribution, we learn a two-layer linear neural network parametrized by 𝑾∈ℝp×d𝑾superscriptℝ𝑝𝑑\\\\boldsymbol{W}\\\\in\\\\mathbb{R}^{p\\\\times d}bold_italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_p × italic_d end_POSTSUPERSCRIPT and 𝜷∈ℝp𝜷superscriptℝ𝑝\\\\boldsymbol{\\\\beta}\\\\in\\\\mathbb{R}^{p}bold_italic_β ∈ blackboard_R start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT as',\n",
              " '',\n",
              " 'f\\u2062(𝒙)=(𝑾\\u2062𝒙)⊤\\u2062𝜷,𝑓𝒙superscript𝑾𝒙top𝜷f(\\\\boldsymbol{x})=(\\\\boldsymbol{W}\\\\boldsymbol{x})^{\\\\top}\\\\boldsymbol{\\\\beta},italic_f ( bold_italic_x ) = ( bold_italic_W bold_italic_x ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_β ,',\n",
              " '',\n",
              " 'where p𝑝pitalic_p is the number of hidden units in the network.\\nIn above, we take 𝑾𝑾\\\\boldsymbol{W}bold_italic_W as a parameter independent of the training data 𝒯𝒯\\\\mathcal{T}caligraphic_T whose entries are drawn from i.i.d. Gaussian distribution 𝒩\\u2062(0,1/d)𝒩01𝑑\\\\mathcal{N}(0,1/d)caligraphic_N ( 0 , 1 / italic_d ).\\nGiven 𝑾𝑾\\\\boldsymbol{W}bold_italic_W, the parameter 𝜷𝜷\\\\boldsymbol{\\\\beta}bold_italic_β is estimated by solving the following ridge regression111ℓ2subscriptnormal-ℓ2\\\\ell_{2}roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT regularization on weight parameters is arguably the most widely used technique in training neural network, known for improving generalization (Krogh & Hertz, 1992). Other regularization such as ℓ1subscriptnormal-ℓ1\\\\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT can also be used and leads to qualitatively similar behaviors. problem',\n",
              " '',\n",
              " '𝜷λ\\u2062(𝒯,𝑾)=arg\\u2062min𝜷∈ℝp\\u2061∥(𝑾\\u2062𝑿)⊤\\u2062𝜷-𝒚∥22+λ\\u2062∥𝜷∥22,subscript𝜷𝜆𝒯𝑾subscriptargmin𝜷superscriptℝ𝑝superscriptsubscriptnormsuperscript𝑾𝑿top𝜷𝒚22𝜆superscriptsubscriptnorm𝜷22\\\\boldsymbol{\\\\beta}_{\\\\lambda}(\\\\mathcal{T},\\\\boldsymbol{W})=\\\\operatorname*{arg\\\\,%\\nmin}_{\\\\boldsymbol{\\\\beta}\\\\in\\\\mathbb{R}^{p}}\\\\|(\\\\boldsymbol{W}\\\\boldsymbol{X})^{%\\n\\\\top}\\\\boldsymbol{\\\\beta}-\\\\boldsymbol{y}\\\\|_{2}^{2}+\\\\lambda\\\\|\\\\boldsymbol{\\\\beta}\\\\|%\\n_{2}^{2},bold_italic_β start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( caligraphic_T , bold_italic_W ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_β ∈ blackboard_R start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ∥ ( bold_italic_W bold_italic_X ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_β - bold_italic_y ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_λ ∥ bold_italic_β ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,',\n",
              " '(5)',\n",
              " '\\nwhere 𝑿=[𝒙1,…,𝒙n]∈ℝd×n𝑿subscript𝒙1…subscript𝒙𝑛superscriptℝ𝑑𝑛\\\\boldsymbol{X}=[\\\\boldsymbol{x}_{1},\\\\ldots,\\\\boldsymbol{x}_{n}]\\\\in\\\\mathbb{R}^{d%\\n\\\\times n}bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_n end_POSTSUPERSCRIPT denotes a matrix that contains training data vectors as its columns, 𝒚=[y1,…,yn]∈ℝn𝒚subscript𝑦1…subscript𝑦𝑛superscriptℝ𝑛\\\\boldsymbol{y}=[y_{1},\\\\ldots,y_{n}]\\\\in\\\\mathbb{R}^{n}bold_italic_y = [ italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT denotes a vector containing training labels as its entries, and λ∈ℝ+𝜆superscriptℝ\\\\lambda\\\\in\\\\mathbb{R}^{+}italic_λ ∈ blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT is the regularization parameter.\\nBy noting that the solution to (5) is given by',\n",
              " '',\n",
              " '𝜷λ\\u2062(𝒯,𝑾)=(𝑾\\u2062𝑿\\u2062𝑿⊤\\u2062𝑾⊤+λ\\u2062𝑰)-1\\u2062𝑾\\u2062𝑿\\u2062𝒚,subscript𝜷𝜆𝒯𝑾superscript𝑾𝑿superscript𝑿topsuperscript𝑾top𝜆𝑰1𝑾𝑿𝒚\\\\boldsymbol{\\\\beta}_{\\\\lambda}(\\\\mathcal{T},\\\\boldsymbol{W})=(\\\\boldsymbol{W}%\\n\\\\boldsymbol{X}\\\\boldsymbol{X}^{\\\\top}\\\\boldsymbol{W}^{\\\\top}+\\\\lambda\\\\boldsymbol{I}%\\n)^{-1}\\\\boldsymbol{W}\\\\boldsymbol{X}\\\\boldsymbol{y},bold_italic_β start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( caligraphic_T , bold_italic_W ) = ( bold_italic_W bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W bold_italic_X bold_italic_y ,',\n",
              " '',\n",
              " 'our estimator f:ℝd→ℝ:𝑓→superscriptℝ𝑑ℝf:\\\\mathbb{R}^{d}\\\\rightarrow\\\\mathbb{R}italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R is given as',\n",
              " '',\n",
              " 'fλ\\u2062(𝒙;𝒯,𝑾)=𝒙⊤\\u2062𝑾⊤\\u2062𝜷λ\\u2062(𝒯,𝑾).subscript𝑓𝜆𝒙𝒯𝑾superscript𝒙topsuperscript𝑾topsubscript𝜷𝜆𝒯𝑾f_{\\\\lambda}(\\\\boldsymbol{x};\\\\mathcal{T},\\\\boldsymbol{W})=\\\\boldsymbol{x}^{\\\\top}%\\n\\\\boldsymbol{W}^{\\\\top}\\\\boldsymbol{\\\\beta}_{\\\\lambda}(\\\\mathcal{T},\\\\boldsymbol{W}).italic_f start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( bold_italic_x ; caligraphic_T , bold_italic_W ) = bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_β start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( caligraphic_T , bold_italic_W ) .',\n",
              " '(6)',\n",
              " '',\n",
              " '',\n",
              " '\\n5.2 Bias-Variance Analysis',\n",
              " 'We may now calculate the bias and variance of the model described above via the following formulations:',\n",
              " '',\n",
              " '𝐁𝐢𝐚𝐬λ\\u2062(𝜽)2subscript𝐁𝐢𝐚𝐬𝜆superscript𝜽2\\\\displaystyle\\\\textbf{Bias}_{\\\\lambda}(\\\\boldsymbol{\\\\theta})^{2}Bias start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( bold_italic_θ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\\n=𝔼𝒙\\u2062[𝔼𝒯,𝑾\\u2062fλ\\u2062(𝒙;𝒯,𝑾)-f0\\u2062(𝒙)]2,absentsubscript𝔼𝒙superscriptdelimited-[]subscript𝔼𝒯𝑾subscript𝑓𝜆𝒙𝒯𝑾subscript𝑓0𝒙2\\\\displaystyle=\\\\mathbb{E}_{\\\\boldsymbol{x}}\\\\left[\\\\mathbb{E}_{\\\\mathcal{T},%\\n\\\\boldsymbol{W}}f_{\\\\lambda}(\\\\boldsymbol{x};\\\\mathcal{T},\\\\boldsymbol{W})-f_{0}(%\\n\\\\boldsymbol{x})\\\\right]^{2},= blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ blackboard_E start_POSTSUBSCRIPT caligraphic_T , bold_italic_W end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( bold_italic_x ; caligraphic_T , bold_italic_W ) - italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_x ) ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,',\n",
              " '',\n",
              " '\\n𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞λ\\u2062(𝜽)subscript𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞𝜆𝜽\\\\displaystyle\\\\textbf{Variance}_{\\\\lambda}(\\\\boldsymbol{\\\\theta})Variance start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( bold_italic_θ )\\n=𝔼𝒙\\u2062Var𝒯,𝑾\\u2062[fλ\\u2062(𝒙;𝒯,𝑾)],absentsubscript𝔼𝒙subscriptVar𝒯𝑾delimited-[]subscript𝑓𝜆𝒙𝒯𝑾\\\\displaystyle=\\\\mathbb{E}_{\\\\boldsymbol{x}}\\\\text{Var}_{\\\\mathcal{T},\\\\boldsymbol{W%\\n}}\\\\left[f_{\\\\lambda}(\\\\boldsymbol{x};\\\\mathcal{T},\\\\boldsymbol{W})\\\\right],= blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT Var start_POSTSUBSCRIPT caligraphic_T , bold_italic_W end_POSTSUBSCRIPT [ italic_f start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( bold_italic_x ; caligraphic_T , bold_italic_W ) ] ,',\n",
              " '',\n",
              " 'where f0\\u2062(𝒙)subscript𝑓0𝒙f_{0}(\\\\boldsymbol{x})italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_x ) and fλ\\u2062(𝒙;𝒯,𝑾)subscript𝑓𝜆𝒙𝒯𝑾f_{\\\\lambda}(\\\\boldsymbol{x};\\\\mathcal{T},\\\\boldsymbol{W})italic_f start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( bold_italic_x ; caligraphic_T , bold_italic_W ) are defined in (4) and (6), respectively.\\nNote that the bias and variance are functions of the model parameter 𝜽𝜽\\\\boldsymbol{\\\\theta}bold_italic_θ. To simplify the analysis, we introduce a prior 𝜽∼𝒩\\u2062(0,𝑰d)similar-to𝜽𝒩0subscript𝑰𝑑\\\\boldsymbol{\\\\theta}\\\\sim\\\\mathcal{N}(0,\\\\boldsymbol{I}_{d})bold_italic_θ ∼ caligraphic_N ( 0 , bold_italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) and calculate the expected bias and expected variance as',\n",
              " '',\n",
              " '𝐁𝐢𝐚𝐬λ2superscriptsubscript𝐁𝐢𝐚𝐬𝜆2\\\\displaystyle\\\\textbf{Bias}_{\\\\lambda}^{2}Bias start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\\n:=𝔼𝜽\\u2062𝐁𝐢𝐚𝐬λ\\u2062(𝜽)2,assignabsentsubscript𝔼𝜽subscript𝐁𝐢𝐚𝐬𝜆superscript𝜽2\\\\displaystyle:=\\\\mathbb{E}_{\\\\boldsymbol{\\\\theta}}\\\\textbf{Bias}_{\\\\lambda}(%\\n\\\\boldsymbol{\\\\theta})^{2},:= blackboard_E start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT Bias start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( bold_italic_θ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,',\n",
              " '(7)',\n",
              " '',\n",
              " '𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞λsubscript𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞𝜆\\\\displaystyle\\\\textbf{Variance}_{\\\\lambda}Variance start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT\\n:=𝔼𝜽\\u2062𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞λ\\u2062(𝜽).assignabsentsubscript𝔼𝜽subscript𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞𝜆𝜽\\\\displaystyle:=\\\\mathbb{E}_{\\\\boldsymbol{\\\\theta}}\\\\textbf{Variance}_{\\\\lambda}(%\\n\\\\boldsymbol{\\\\theta}).:= blackboard_E start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT Variance start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( bold_italic_θ ) .',\n",
              " '(8)',\n",
              " '\\nThe precise formulas for the expected bias and the expected variance are parametrized by the dimension of the input feature d𝑑ditalic_d, the number of training points n𝑛nitalic_n, the number of hidden units p𝑝pitalic_p and also λ𝜆\\\\lambdaitalic_λ.',\n",
              " '\\nPrevious literatures\\xa0(Mei & Montanari, 2019) suggests that both the risk and the variance achieves a peak at the interpolation threshold (n=p𝑛𝑝n=pitalic_n = italic_p).\\nIn the regime when n𝑛nitalic_n is very large, the risk no longer exhibits a peak, but the unimodal pattern of variance still holds.\\nIn the rest of the section, we consider the regime where the n𝑛nitalic_n is large (monotonically decreasing risk), and derive the precise expression for the bias and variance of the model. From our expression, we obtain the location where the variance achieves the peak.\\nFor this purpose, we consider the following asymptotic regime of n,p𝑛𝑝n,pitalic_n , italic_p and d𝑑ditalic_d:',\n",
              " '',\n",
              " 'Assumption 1.',\n",
              " '\\nLet {(d,n\\u2062(d),p\\u2062(d))}d=1∞superscriptsubscript𝑑𝑛𝑑𝑝𝑑𝑑1\\\\{(d,n(d),p(d))\\\\}_{d=1}^{\\\\infty}{ ( italic_d , italic_n ( italic_d ) , italic_p ( italic_d ) ) } start_POSTSUBSCRIPT italic_d = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT be a given sequence of triples. We assume that there exists a γ>0𝛾0\\\\gamma>0italic_γ > 0 such that',\n",
              " '',\n",
              " 'limd→∞\\u2061p\\u2062(d)d=γ,𝑎𝑛𝑑\\u2003limd→∞\\u2061n\\u2062(d)d=∞.formulae-sequencesubscript→𝑑𝑝𝑑𝑑𝛾𝑎𝑛𝑑subscript→𝑑𝑛𝑑𝑑\\\\lim_{d\\\\to\\\\infty}\\\\frac{p(d)}{d}=\\\\gamma,\\\\quad\\\\text{and}\\\\quad\\\\lim_{d\\\\to\\\\infty}%\\n\\\\frac{n(d)}{d}=\\\\infty.roman_lim start_POSTSUBSCRIPT italic_d → ∞ end_POSTSUBSCRIPT divide start_ARG italic_p ( italic_d ) end_ARG start_ARG italic_d end_ARG = italic_γ , and roman_lim start_POSTSUBSCRIPT italic_d → ∞ end_POSTSUBSCRIPT divide start_ARG italic_n ( italic_d ) end_ARG start_ARG italic_d end_ARG = ∞ .',\n",
              " '',\n",
              " 'For simplicity, we will write n:=n\\u2062(d)assign𝑛𝑛𝑑n:=n(d)italic_n := italic_n ( italic_d ) and p:=p\\u2062(d)assign𝑝𝑝𝑑p:=p(d)italic_p := italic_p ( italic_d ).',\n",
              " '',\n",
              " 'With the assumption above, we have the expression of the expected bias, variance and risk as a function of γ𝛾\\\\gammaitalic_γ and λ𝜆\\\\lambdaitalic_λ.',\n",
              " '',\n",
              " 'Theorem 1.',\n",
              " '\\nGiven {(d,n\\u2062(d),p\\u2062(d))}d=1∞superscriptsubscript𝑑𝑛𝑑𝑝𝑑𝑑1\\\\{(d,n(d),p(d))\\\\}_{d=1}^{\\\\infty}{ ( italic_d , italic_n ( italic_d ) , italic_p ( italic_d ) ) } start_POSTSUBSCRIPT italic_d = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT that satisfies Assumption 1, let λ=nd\\u2062λ0𝜆𝑛𝑑subscript𝜆0\\\\lambda=\\\\frac{n}{d}\\\\lambda_{0}italic_λ = divide start_ARG italic_n end_ARG start_ARG italic_d end_ARG italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT for some fixed λ0>0subscript𝜆00\\\\lambda_{0}>0italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT > 0.\\nThe asymptotic expression of expected bias and variance are given by',\n",
              " '',\n",
              " 'limd→∞\\u2061𝑩𝒊𝒂𝒔λ2=14\\u2062Φ3\\u2062(λ0,γ)2,subscript→𝑑superscriptsubscript𝑩𝒊𝒂𝒔𝜆214subscriptΦ3superscriptsubscript𝜆0𝛾2\\\\displaystyle\\\\lim_{d\\\\to\\\\infty}\\\\textbf{Bias}_{\\\\lambda}^{2}=\\\\frac{1}{4}\\\\Phi_{3}(%\\n\\\\lambda_{0},\\\\gamma)^{2},roman_lim start_POSTSUBSCRIPT italic_d → ∞ end_POSTSUBSCRIPT Bias start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG 4 end_ARG roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_γ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,',\n",
              " '(9)',\n",
              " '',\n",
              " 'limd→∞\\u2061𝑽𝒂𝒓𝒊𝒂𝒏𝒄𝒆λ=subscript→𝑑subscript𝑽𝒂𝒓𝒊𝒂𝒏𝒄𝒆𝜆absent\\\\displaystyle\\\\lim_{d\\\\to\\\\infty}\\\\textbf{Variance}_{\\\\lambda}=roman_lim start_POSTSUBSCRIPT italic_d → ∞ end_POSTSUBSCRIPT Variance start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT =',\n",
              " '',\n",
              " '\\n{Φ1\\u2062(λ0,1γ)2\\u2062Φ2\\u2062(λ0,1γ)-(1-γ)\\u2062(1-2\\u2062γ)2\\u2062γ-14\\u2062Φ3\\u2062(λ0,γ)2,γ≤1,Φ1\\u2062(λ0,γ)2\\u2062Φ2\\u2062(λ0,γ)-γ-12-14\\u2062Φ3\\u2062(λ0,γ)2,γ>1,casessubscriptΦ1subscript𝜆01𝛾2subscriptΦ2subscript𝜆01𝛾1𝛾12𝛾2𝛾14subscriptΦ3superscriptsubscript𝜆0𝛾2𝛾1subscriptΦ1subscript𝜆0𝛾2subscriptΦ2subscript𝜆0𝛾𝛾1214subscriptΦ3superscriptsubscript𝜆0𝛾2𝛾1\\\\displaystyle\\\\begin{cases}\\\\frac{\\\\Phi_{1}(\\\\lambda_{0},\\\\frac{1}{\\\\gamma})}{2\\\\Phi_%\\n{2}(\\\\lambda_{0},\\\\frac{1}{\\\\gamma})}-\\\\frac{(1-\\\\gamma)(1-2\\\\gamma)}{2\\\\gamma}-\\\\frac%\\n{1}{4}\\\\Phi_{3}(\\\\lambda_{0},\\\\gamma)^{2},&\\\\gamma\\\\leq 1,\\\\\\\\\\n\\\\frac{\\\\Phi_{1}(\\\\lambda_{0},\\\\gamma)}{2\\\\Phi_{2}(\\\\lambda_{0},\\\\gamma)}-\\\\frac{%\\n\\\\gamma-1}{2}-\\\\frac{1}{4}\\\\Phi_{3}(\\\\lambda_{0},\\\\gamma)^{2},&\\\\gamma>1,\\\\end{cases}{ start_ROW start_CELL divide start_ARG roman_Φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , divide start_ARG 1 end_ARG start_ARG italic_γ end_ARG ) end_ARG start_ARG 2 roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , divide start_ARG 1 end_ARG start_ARG italic_γ end_ARG ) end_ARG - divide start_ARG ( 1 - italic_γ ) ( 1 - 2 italic_γ ) end_ARG start_ARG 2 italic_γ end_ARG - divide start_ARG 1 end_ARG start_ARG 4 end_ARG roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_γ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , end_CELL start_CELL italic_γ ≤ 1 , end_CELL end_ROW start_ROW start_CELL divide start_ARG roman_Φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_γ ) end_ARG start_ARG 2 roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_γ ) end_ARG - divide start_ARG italic_γ - 1 end_ARG start_ARG 2 end_ARG - divide start_ARG 1 end_ARG start_ARG 4 end_ARG roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_γ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , end_CELL start_CELL italic_γ > 1 , end_CELL end_ROW',\n",
              " '',\n",
              " 'where',\n",
              " '',\n",
              " 'Φ1\\u2062(λ0,γ)=λ0\\u2062(γ+1)+(γ-1)2,Φ2\\u2062(λ0,γ)=(λ0+1)2+2\\u2062(λ0-1)\\u2062γ+γ2,Φ3\\u2062(λ0,γ)=Φ2\\u2062(λ0,γ)-λ0-γ+1.formulae-sequencesubscriptΦ1subscript𝜆0𝛾subscript𝜆0𝛾1superscript𝛾12formulae-sequencesubscriptΦ2subscript𝜆0𝛾superscriptsubscript𝜆0122subscript𝜆01𝛾superscript𝛾2subscriptΦ3subscript𝜆0𝛾subscriptΦ2subscript𝜆0𝛾subscript𝜆0𝛾1\\\\begin{split}\\\\Phi_{1}(\\\\lambda_{0},\\\\gamma)&=\\\\lambda_{0}(\\\\gamma+1)+(\\\\gamma-1)^{2%\\n},\\\\\\\\\\n\\\\Phi_{2}(\\\\lambda_{0},\\\\gamma)&=\\\\sqrt{(\\\\lambda_{0}+1)^{2}+2(\\\\lambda_{0}-1)\\\\gamma%\\n+\\\\gamma^{2}},\\\\\\\\\\n\\\\Phi_{3}(\\\\lambda_{0},\\\\gamma)&=\\\\Phi_{2}(\\\\lambda_{0},\\\\gamma)-\\\\lambda_{0}-\\\\gamma+%\\n1.\\\\end{split}start_ROW start_CELL roman_Φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_γ ) end_CELL start_CELL = italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_γ + 1 ) + ( italic_γ - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , end_CELL end_ROW start_ROW start_CELL roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_γ ) end_CELL start_CELL = square-root start_ARG ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 ) italic_γ + italic_γ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , end_CELL end_ROW start_ROW start_CELL roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_γ ) end_CELL start_CELL = roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_γ ) - italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - italic_γ + 1 . end_CELL end_ROW',\n",
              " '',\n",
              " '',\n",
              " '\\nThe proof is given in Appendix C.',\n",
              " '\\nThe risk can be obtained through 𝐁𝐢𝐚𝐬λ2+𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞λsuperscriptsubscript𝐁𝐢𝐚𝐬𝜆2subscript𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞𝜆\\\\textbf{Bias}_{\\\\lambda}^{2}+\\\\textbf{Variance}_{\\\\lambda}Bias start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + Variance start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT.\\nThe expression in Theorem 1 is plotted as the red curves in Figure 6.\\nIn addition to the case when n/d→∞→𝑛𝑑n/d\\\\rightarrow\\\\inftyitalic_n / italic_d → ∞, we also plot the shape of bias, variance and risk when n/d→{0.15,0.25,0.35,…,1.00,1.50}→𝑛𝑑0.150.250.35…1.001.50n/d\\\\rightarrow\\\\{0.15,0.25,0.35,\\\\dots,1.00,1.50\\\\}italic_n / italic_d → { 0.15 , 0.25 , 0.35 , … , 1.00 , 1.50 }.\\nWe find that the risk of the model grows from unimodal to monotonically decreasing as the number of samples increased (see Figure 6(a)).\\nMoreover, the bias of the model is monotonically decreasing (see Figure 6(b)) and the variance is unimodal (see Figure 6(c)).',\n",
              " '',\n",
              " '\\nCorollary 1 (Monotonicity of Bias).',\n",
              " '\\nThe derivative of the limiting expected Bias in (9) can be calculated as',\n",
              " '',\n",
              " '\\n-(2\\u2062(γ+1)\\u2062λ0+(γ-1)2+λ02-γ-λ0+1)22\\u2062γ2+2γ(λ0-1)+(λ0+1)2.superscript2𝛾1subscript𝜆0superscript𝛾12superscriptsubscript𝜆02𝛾subscript𝜆0122fragmentssuperscript𝛾22γfragments(subscript𝜆01)fragments(subscript𝜆01)2-\\\\frac{\\\\left(\\\\sqrt{2(\\\\gamma+1)\\\\lambda_{0}+(\\\\gamma-1)^{2}+\\\\lambda_{0}^{2}}-%\\n\\\\gamma-\\\\lambda_{0}+1\\\\right)^{2}}{2\\\\sqrt{\\\\gamma^{2}+2\\\\gamma\\\\left(\\\\lambda_{0}-1%\\n\\\\right)+\\\\left(\\\\lambda_{0}+1\\\\right){}^{2}}}.- divide start_ARG ( square-root start_ARG 2 ( italic_γ + 1 ) italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + ( italic_γ - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG - italic_γ - italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 square-root start_ARG italic_γ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 italic_γ ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 ) + ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + 1 ) start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT end_ARG end_ARG .',\n",
              " '(10)',\n",
              " '',\n",
              " '',\n",
              " 'When λ0≥0subscript𝜆00\\\\lambda_{0}\\\\geq 0italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ≥ 0, the expression in (10) is strictly non-positive, therefore the limiting expected bias is monotonically non-increasing as a function of γ𝛾\\\\gammaitalic_γ, as classical theories predicts.',\n",
              " '\\nTo gain further insight into the above formulas, we also consider the case when the ridge regularization amount λ0subscript𝜆0\\\\lambda_{0}italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is small. In particular, we consider the first order effect of λ0subscript𝜆0\\\\lambda_{0}italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT on the bias and variance term, and compute the value of γ𝛾\\\\gammaitalic_γ where the variance attains the peak.',\n",
              " '',\n",
              " 'Corollary 2 (Unimodality of Variance – small λ0subscript𝜆0\\\\lambda_{0}italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT limit).',\n",
              " '\\nUnder the assumptions of Theorem 1, the first order effect of λ0subscript𝜆0\\\\lambda_{0}italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT on variance is given by',\n",
              " '',\n",
              " 'limd→∞\\u2061𝔼\\u2062𝑽𝒂𝒓𝒊𝒂𝒏𝒄𝒆λ={O\\u2062(λ02),γ>1,-(γ-1)\\u2062γ-2\\u2062γ\\u2062λ0+O\\u2062(λ02),o.w.subscript→𝑑𝔼subscript𝑽𝒂𝒓𝒊𝒂𝒏𝒄𝒆𝜆cases𝑂superscriptsubscript𝜆02𝛾1𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒𝛾1𝛾2𝛾subscript𝜆0𝑂superscriptsubscript𝜆02o.w.𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒\\\\lim_{d\\\\to\\\\infty}\\\\mathbb{E}\\\\textbf{Variance}_{\\\\lambda}=\\\\\\\\\\n\\\\begin{cases}O\\\\left(\\\\lambda_{0}^{2}\\\\right),\\\\quad\\\\gamma>1,\\\\\\\\\\n-(\\\\gamma-1)\\\\gamma-2\\\\gamma\\\\lambda_{0}+O\\\\left(\\\\lambda_{0}^{2}\\\\right),\\\\text{o.w.}%\\n\\\\\\\\\\n\\\\end{cases}roman_lim start_POSTSUBSCRIPT italic_d → ∞ end_POSTSUBSCRIPT blackboard_E Variance start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT = { start_ROW start_CELL italic_O ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , italic_γ > 1 , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL - ( italic_γ - 1 ) italic_γ - 2 italic_γ italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_O ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , o.w. end_CELL start_CELL end_CELL end_ROW',\n",
              " '',\n",
              " 'and the risk is given by',\n",
              " '',\n",
              " 'limd→∞\\u2061𝔼\\u2062𝑹𝒊𝒔𝒌λ={1-γ+O\\u2062(λ02),γ≤1,O\\u2062(λ02),γ>1.subscript→𝑑𝔼subscript𝑹𝒊𝒔𝒌𝜆cases1𝛾𝑂superscriptsubscript𝜆02𝛾1𝑂superscriptsubscript𝜆02𝛾1\\\\lim_{d\\\\to\\\\infty}\\\\mathbb{E}\\\\textbf{Risk}_{\\\\lambda}=\\\\begin{cases}1-\\\\gamma+O%\\n\\\\left(\\\\lambda_{0}^{2}\\\\right),&\\\\gamma\\\\leq 1,\\\\\\\\\\nO\\\\left(\\\\lambda_{0}^{2}\\\\right),&\\\\gamma>1.\\\\end{cases}roman_lim start_POSTSUBSCRIPT italic_d → ∞ end_POSTSUBSCRIPT blackboard_E Risk start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT = { start_ROW start_CELL 1 - italic_γ + italic_O ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , end_CELL start_CELL italic_γ ≤ 1 , end_CELL end_ROW start_ROW start_CELL italic_O ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , end_CELL start_CELL italic_γ > 1 . end_CELL end_ROW',\n",
              " '',\n",
              " 'Moreover, up to first order, the peak in the variance is',\n",
              " '',\n",
              " '\\n𝑷𝒆𝒂𝒌=12-λ0+O\\u2062(λ02).𝑷𝒆𝒂𝒌12subscript𝜆0𝑂superscriptsubscript𝜆02\\\\textbf{Peak}=\\\\frac{1}{2}-\\\\lambda_{0}+O\\\\left(\\\\lambda_{0}^{2}\\\\right).Peak = divide start_ARG 1 end_ARG start_ARG 2 end_ARG - italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_O ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) .',\n",
              " '',\n",
              " '',\n",
              " '\\nTheorem 2 suggests that when λ0subscript𝜆0\\\\lambda_{0}italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is sufficiently small, the variance of the model is maximized when p=d/2𝑝𝑑2p=d/2italic_p = italic_d / 2, and the effect of λ0subscript𝜆0\\\\lambda_{0}italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is to shift the peak slightly to d/2-λ0\\u2062d𝑑2subscript𝜆0𝑑d/2-\\\\lambda_{0}ditalic_d / 2 - italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_d.',\n",
              " '\\nFrom a technical perspective, to compute the variance in the random-design setting, we need to compute the element-wise expectation of certain random matrix. For this purpose, we apply the combinatorics of counting non-cross partitions to characterize the asymptotic expectation of products of Wishart matrices.',\n",
              " '',\n",
              " '',\n",
              " '6 Conclusion and Discussion',\n",
              " 'In this paper we re-examine the classical theory of bias and variance trade-off as the width of a neural network increases. Through extensive experimentation, our main finding is that, while the bias is monotonically decreasing as classical theory would predict, the variance is unimodal. This combination leads to three typical risk curve patterns, all observed in practice. Theoretical analysis of a two-layer linear network corroborates these experimental observations.',\n",
              " '\\nThe seemingly varied and baffling behaviors of modern neural networks are thus in fact consistent, and explainable through classical bias-variance analysis.\\nThe main unexplained mystery is the unimodality of the variance. We conjecture that as the model complexity approaches and then goes beyond the data dimension, it is regularization in model estimation (the ridge penalty in our theoretical example)\\nthat helps bring down the variance. Under this account, the decrease in variance for large dimension comes from better conditioning of the empirical covariance, making it better-aligned with the regularizer.',\n",
              " '\\nIn the future, it would be interesting to see if phenomena characterized by the simple two-layer model can be rigorously generalized to deeper networks with nonlinear activation, probably revealing other interplays between model complexity and regularization (explicit or implicit). Such a study could also help explain another phenomenon we (and others) have observed: bias decreases with more layers as variance increases. We believe that the (classic) bias-variance analysis remains a powerful and insightful framework for understanding the behaviors of deep networks; properly used, it can guide practitioners to design more generalizable and robust networks in the future.',\n",
              " '\\nAcknowledgements.',\n",
              " 'We would like to thank Emmanuel Candés for first bringing the double-descent phenomenon to our attention, Song Mei for helpful discussion regarding random v.s. fixed design regression, and Nikhil Srivastava for pointing out to relevant references in random matrix theory. We would also like to thank Preetum Nakkiran, Mihaela Curmei, and Chloe Hsu for valuable feedback during preparation of this manuscript.',\n",
              " '',\n",
              " '\\nReferences',\n",
              " '\\nAdvani & Saxe (2017)',\n",
              " 'Advani, M.\\xa0S. and Saxe, A.\\xa0M.',\n",
              " '\\nHigh-dimensional dynamics of generalization error in neural networks.',\n",
              " '\\nArXiv, abs/1710.03667, 2017.',\n",
              " '',\n",
              " '\\nAzulay & Weiss (2019)',\n",
              " 'Azulay, A. and Weiss, Y.',\n",
              " '\\nWhy do deep convolutional networks generalize so poorly to small\\nimage transformations?',\n",
              " '\\nJournal of Machine Learning Research, 20:1–25,\\n2019.',\n",
              " '',\n",
              " '\\nBa et\\xa0al. (2020)',\n",
              " 'Ba, J., Erdogdu, M., Suzuki, T., Wu, D., and Zhang, T.',\n",
              " '\\nGeneralization of two-layer neural networks: An asymptotic viewpoint.',\n",
              " '\\nIn International Conference on Learning Representations, 2020.',\n",
              " '\\nURL https://openreview.net/forum?id=H1gBsgBYwH.',\n",
              " '',\n",
              " '\\nBai & Silverstein (2010)',\n",
              " 'Bai, Z. and Silverstein, J.',\n",
              " '\\nSpectral Analysis of Large Dimensional Random Matrices.',\n",
              " '\\nSpringer, 01 2010.',\n",
              " '\\ndoi: 10.1007/978-1-4419-0661-8.',\n",
              " '',\n",
              " '\\nBelkin et\\xa0al. (2018)',\n",
              " 'Belkin, M., Ma, S., and Mandal, S.',\n",
              " '\\nTo understand deep learning we need to understand kernel learning.',\n",
              " '\\nIn International Conference on Machine Learning, pp.\\xa0541–549, 2018.',\n",
              " '',\n",
              " '\\nBelkin et\\xa0al. (2019a)',\n",
              " 'Belkin, M., Hsu, D., Ma, S., and Mandal, S.',\n",
              " '\\nReconciling modern machine-learning practice and the classical\\nbias–variance trade-off.',\n",
              " '\\nProceedings of the National Academy of Sciences, 116(32):15849–15854, 2019a.',\n",
              " '',\n",
              " '\\nBelkin et\\xa0al. (2019b)',\n",
              " 'Belkin, M., Rakhlin, A., and Tsybakov, A.\\xa0B.',\n",
              " '\\nDoes data interpolation contradict statistical optimality?',\n",
              " '\\nIn The 22nd International Conference on Artificial Intelligence\\nand Statistics, pp.\\xa0 1611–1619, 2019b.',\n",
              " '',\n",
              " '\\nBishop et\\xa0al. (2018)',\n",
              " 'Bishop, A.\\xa0N., Del Moral, P., and Niclas, A.',\n",
              " '\\nAn Introduction to Wishart Matrix Moments.',\n",
              " '\\nnow, 2018.',\n",
              " '\\nURL https://ieeexplore.ieee.org/document/8572806.',\n",
              " '',\n",
              " '\\nBishop (2006)',\n",
              " 'Bishop, C.\\xa0M.',\n",
              " '\\nPattern Recognition and Machine Learning.',\n",
              " '\\nSpringer, 2006.',\n",
              " '',\n",
              " '\\nDeng et\\xa0al. (2019)',\n",
              " 'Deng, Z., Kammoun, A., and Thrampoulidis, C.',\n",
              " '\\nA model of double descent for high-dimensional binary linear\\nclassification.',\n",
              " '\\narXiv preprint arXiv:1911.05822, 2019.',\n",
              " '',\n",
              " '\\nGeman (1980)',\n",
              " 'Geman, S.',\n",
              " '\\nA limit theorem for the norm of random matrices.',\n",
              " '\\nAnn. Probab., 8(2):252–261, 04 1980.',\n",
              " '\\ndoi: 10.1214/aop/1176994775.',\n",
              " '\\nURL https://doi.org/10.1214/aop/1176994775.',\n",
              " '',\n",
              " '\\nGeman et\\xa0al. (1992)',\n",
              " 'Geman, S., Bienenstock, E., and Doursat, R.',\n",
              " '\\nNeural networks and the bias/variance dilemma.',\n",
              " '\\nNeural computation, 4(1):1–58, 1992.',\n",
              " '',\n",
              " '\\nGhaoui (2002)',\n",
              " 'Ghaoui, L.\\xa0E.',\n",
              " '\\nInversion error, condition number, and approximate inverses of\\nuncertain matrices.',\n",
              " '\\nLinear Algebra and its Applications, 343-344:171 –\\n193, 2002.',\n",
              " '\\nISSN 0024-3795.',\n",
              " '\\ndoi: https://doi.org/10.1016/S0024-3795(01)00273-7.',\n",
              " '\\nURL\\nhttp://www.sciencedirect.com/science/article/pii/S0024379501002737.',\n",
              " '\\nSpecial Issue on Structured and Infinite Systems of Linear equations.',\n",
              " '',\n",
              " '\\nHastie et\\xa0al. (2001)',\n",
              " 'Hastie, T., Tibshirani, R., and Friedman, J.',\n",
              " '\\nThe Elements of Statistical Learning.',\n",
              " '\\nSpringer Series in Statistics. Springer New York Inc., New York, NY,\\nUSA, 2001.',\n",
              " '',\n",
              " '\\nHastie et\\xa0al. (2019)',\n",
              " 'Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.\\xa0J.',\n",
              " '\\nSurprises in High-Dimensional Ridgeless Least Squares\\nInterpolation.',\n",
              " '\\narXiv e-prints, art. arXiv:1903.08560, Mar 2019.',\n",
              " '',\n",
              " '\\nHe et\\xa0al. (2016)',\n",
              " 'He, K., Zhang, X., Ren, S., and Sun, J.',\n",
              " '\\nDeep residual learning for image recognition.',\n",
              " '\\nIn Proceedings of the IEEE conference on computer vision and\\npattern recognition, pp.\\xa0 770–778, 2016.',\n",
              " '',\n",
              " '\\nHendrycks & Dietterich (2019)',\n",
              " 'Hendrycks, D. and Dietterich, T.',\n",
              " '\\nBenchmarking neural network robustness to common corruptions and\\nperturbations.',\n",
              " '\\nIn International Conference on Learning Representations, 2019.',\n",
              " '\\nURL https://openreview.net/forum?id=HJz6tiCqYm.',\n",
              " '',\n",
              " '\\nKrizhevsky et\\xa0al. (2012)',\n",
              " 'Krizhevsky, A., Sutskever, I., and Hinton, G.\\xa0E.',\n",
              " '\\nImagenet classification with deep convolutional neural networks.',\n",
              " '\\nIn Advances in neural information processing systems, pp.\\xa01097–1105, 2012.',\n",
              " '',\n",
              " '\\nKrizhevsky et\\xa0al. (2009)',\n",
              " 'Krizhevsky, A. et\\xa0al.',\n",
              " '\\nLearning multiple layers of features from tiny images.',\n",
              " '\\n2009.',\n",
              " '\\nURL\\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.9220&rep=rep1&type=pdf.',\n",
              " '',\n",
              " '\\nKrogh & Hertz (1992)',\n",
              " 'Krogh, A. and Hertz, J.\\xa0A.',\n",
              " '\\nA simple weight decay can improve generalization.',\n",
              " '\\nIn Advances in neural information processing systems, pp.\\xa0950–957, 1992.',\n",
              " '',\n",
              " '\\nLeCun (1998)',\n",
              " 'LeCun, Y.',\n",
              " '\\nThe mnist database of handwritten digits.',\n",
              " '\\nhttp://yann. lecun. com/exdb/mnist/, 1998.',\n",
              " '',\n",
              " '\\nMei & Montanari (2019)',\n",
              " 'Mei, S. and Montanari, A.',\n",
              " '\\nThe generalization error of random features regression: Precise\\nasymptotics and double descent curve.',\n",
              " '\\narXiv e-prints, art. arXiv:1908.05355, Aug 2019.',\n",
              " '',\n",
              " '\\nNakkiran (2019)',\n",
              " 'Nakkiran, P.',\n",
              " '\\nMore data can hurt for linear regression: Sample-wise double descent.',\n",
              " '\\narXiv preprint arXiv:1912.07242, 2019.',\n",
              " '',\n",
              " '\\nNakkiran et\\xa0al. (2019)',\n",
              " 'Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I.',\n",
              " '\\nDeep double descent: Where bigger models and more data hurt.',\n",
              " '\\narXiv preprint arXiv:1912.02292, 2019.',\n",
              " '',\n",
              " '\\nNeal et\\xa0al. (2019)',\n",
              " 'Neal, B., Mittal, S., Baratin, A., Tantia, V., Scicluna, M., Lacoste-Julien,\\nS., and Mitliagkas, I.',\n",
              " '\\nA modern take on the bias-variance tradeoff in neural networks, 2019.',\n",
              " '\\nURL https://openreview.net/forum?id=HkgmzhC5F7.',\n",
              " '',\n",
              " '\\nPfau (2013)',\n",
              " 'Pfau, D.',\n",
              " '\\nA generalized bias-variance decomposition for bregman divergences,\\n2013.',\n",
              " '',\n",
              " '\\nSimonyan & Zisserman (2015)',\n",
              " 'Simonyan, K. and Zisserman, A.',\n",
              " '\\nVery deep convolutional networks for large-scale image recognition.',\n",
              " '\\nIn International Conference on Learning Representations, 2015.',\n",
              " '',\n",
              " '\\nSpigler et\\xa0al. (2019)',\n",
              " 'Spigler, S., Geiger, M., d’Ascoli, S., Sagun, L., Biroli, G., and Wyart, M.',\n",
              " '\\nA jamming transition from under-to over-parametrization affects\\ngeneralization in deep learning.',\n",
              " '\\nJournal of Physics A: Mathematical and Theoretical, 2019.',\n",
              " '',\n",
              " '\\nWainwright (2019)',\n",
              " 'Wainwright, M.\\xa0J.',\n",
              " '\\nHigh-Dimensional Statistics: A Non-Asymptotic Viewpoint.',\n",
              " '\\nCambridge Series in Statistical and Probabilistic Mathematics.\\nCambridge University Press, 2019.',\n",
              " '\\ndoi: 10.1017/9781108627771.',\n",
              " '',\n",
              " '\\nXiao et\\xa0al. (2017)',\n",
              " 'Xiao, H., Rasul, K., and Vollgraf, R.',\n",
              " '\\nFashion-mnist: a novel image dataset for benchmarking machine\\nlearning algorithms.',\n",
              " '\\narXiv preprint arXiv:1708.07747, 2017.',\n",
              " '',\n",
              " '\\nXie et\\xa0al. (2017)',\n",
              " 'Xie, S., Girshick, R., Dollár, P., Tu, Z., and He, K.',\n",
              " '\\nAggregated residual transformations for deep neural networks.',\n",
              " '\\nIn Proceedings of the IEEE conference on computer vision and\\npattern recognition, pp.\\xa0 1492–1500, 2017.',\n",
              " '',\n",
              " '\\nZhang et\\xa0al. (2017)',\n",
              " 'Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.',\n",
              " '\\nUnderstanding deep learning requires rethinking generalization.',\n",
              " '\\nIn International Conference on Learning Representations, 2017.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nAppendix A Summary of Experiments',\n",
              " 'We summarize the experiments in Table\\xa01, each row corresponds to one experiment, some include several independent splits, in this paper. Every experiment is related to one or multiple figures, which is specified in the last column “Figure”.',\n",
              " '',\n",
              " '',\n",
              " 'Dataset\\nArchitecture\\nLoss\\nOptimizer\\nTrain Size',\n",
              " '##\\\\##Splits(k𝑘kitalic_k)',\n",
              " 'Label Noise\\nFigure\\nComment',\n",
              " '\\nCIFAR10\\nResNet34\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '25000\\n3\\n✗',\n",
              " '2, 5',\n",
              " 'Mainline',\n",
              " '\\nCIFAR10\\nResNext29\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '25000\\n3\\n✗',\n",
              " '3(a), 7',\n",
              " 'Architecture',\n",
              " '\\nVGG11\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '10000\\n1\\n✗\\n8',\n",
              " '\\nCIFAR10\\nResNet34\\nCE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '10000\\n4\\n✗',\n",
              " '3(b), 9',\n",
              " 'Loss',\n",
              " '\\nMNIST\\nDNN\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '10000\\n1\\n✗\\n3(c)\\nDataset',\n",
              " '\\nFashion-MNIST\\nDNN\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '10000\\n1\\n✗\\n10',\n",
              " '\\nCIFAR100\\nResNet34\\nCE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '10000\\n1\\n✗\\n11',\n",
              " '\\nCIFAR10\\nResNet34\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '10000\\n1\\n10%/20%\\n4\\nLabel noise',\n",
              " '\\nCIFAR10\\nResNet18\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '25000\\n3\\n✗\\n5\\nDepth',\n",
              " '\\nResNet50\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '25000\\n3\\n✗\\n5',\n",
              " '\\nCIFAR10\\nResNet34\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '10000\\n1\\n✗\\n12\\nTrain size',\n",
              " '\\nResNet34\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '2500\\n1\\n✗\\n13',\n",
              " '\\nCIFAR10\\nResNet34\\nMSE',\n",
              " 'SGD(wd=1e-4)',\n",
              " '10000\\n1\\n✗\\n14\\nWeight decay',\n",
              " '\\nCIFAR10\\nResNet26-B\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '25000\\n3\\n✗\\n19\\nDepth (with bottleneck block)',\n",
              " '\\nResNet38-B\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '25000\\n3\\n✗\\n19',\n",
              " '\\nResNet50-B\\nMSE',\n",
              " 'SGD(wd=5e-4)',\n",
              " '25000\\n3\\n✗\\n19',\n",
              " '',\n",
              " 'Table 1: Summary of Experiments.',\n",
              " '',\n",
              " '\\nAppendix B Additional Experiments',\n",
              " 'In this section, we provide additional experimental results, some of them are metioned in §3 and §4.',\n",
              " '\\nNetwork Architecture:',\n",
              " 'The implementation of the deep neural networks used in this work is mainly adapted from https://github.com/kuangliu/pytorch-cifar.',\n",
              " '',\n",
              " 'Training Details:',\n",
              " 'For CIFAR10 dataset and CIFAR100 dataset, when training sample size is 25,000, we use 500 epochs for training and decay by a factor of 10 the learning rate every 200 epoch. When training sample size is 10,000/5,000, we use 1000 epochs for training and decay by a factor of 10 the learning rate every 400 epoch.\\nFor MNIST dataset and FMNIST dataset, we use 200 epochs for training and decay by a factor of 10 the learning rate every 100 epoch.',\n",
              " '',\n",
              " '\\nB.1 Architecture',\n",
              " 'We provide additional results on ResNext29 presented in §3.2. The results are shown in Figure\\xa07.',\n",
              " '\\nWe also study the behavior of risk, bias, and variance of VGG network\\xa0(Simonyan & Zisserman, 2015) on CIFAR10 dataset. Here we use VGG11 and the number of filters are [k,2\\u2062k,4\\u2062k,4\\u2062k,8\\u2062k,8\\u2062k,8\\u2062k,8\\u2062k]𝑘2𝑘4𝑘4𝑘8𝑘8𝑘8𝑘8𝑘[k,2k,4k,4k,8k,8k,8k,8k][ italic_k , 2 italic_k , 4 italic_k , 4 italic_k , 8 italic_k , 8 italic_k , 8 italic_k , 8 italic_k ], where k𝑘kitalic_k is the width in Figure\\xa08. The number of training samples of each split is 10,000. We use the same optimization setup as the mainline experiment (ResNet34 in Figure2).',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 7: Risk, bias, variance, train/test error for ResNext29 trained by MSE loss on CIFAR10 dataset (25,000 training samples). (Left) Risk, bias, and variance for ResNext29. (Middle) Variance for ResNext29. (Right) Train error and test error for ResNext29. ',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 8: Risk, bias, variance, train/test error for VGG11 trained by MSE loss on CIFAR10 dataset (10,000 training samples). (Left) Risk, bias, and variance for VGG11. (Middle) Variance for VGG11. (Right) Train error and test error for VGG11. ',\n",
              " '',\n",
              " '\\nB.2 Loss',\n",
              " 'We provide additional results on cross-entropy loss presented in §3.2, the results are shown in Figure\\xa09.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 9: Variance and train/test error for ResNet34 trained by cross-entropy loss (estimated by generalized bias-variance decomposition using Bregman divergence) on CIFAR10 dataset (10,000 training samples). (Left) Variance for ResNet34. (Right) Train error and test error for ResNet34.',\n",
              " '',\n",
              " '\\nB.3 Dataset',\n",
              " 'We provide the results on Fashion-MNIST dataset in Figure\\xa010, which is mentioned in §3.2.',\n",
              " '\\nFigure 10: Fully connected network with one-hidden-layer and ReLU activation trained by MSE loss on Fashion-MNIST dataset (10,000 training samples).',\n",
              " '\\nWe study the behavior of risk, bias, and variance of ResNet34 on CIFAR100 dataset. Because the number of class is large, we use cross-entropy during training, and apply the classical Bias-Vairance decomposition for MSE in (1) and (2) to estimate the risk, bias, and variance. As shown in Figure\\xa011, we observe the bell-shaped variance curve and the monotonically decreasing bias curve on CIFAR100 dataset.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 11: Risk, bias, variance, and train/test error for ResNet34 trained by cross-entropy loss (estimated by MSE bias-variance decomposition) on CIFAR100 (10,000 training samples). (Left) Risk, bias, and variance for ResNet34. (Middle) Variance for ResNet34. (Right) Train error and test error for ResNet34.',\n",
              " '',\n",
              " '\\nB.4 Training Size',\n",
              " 'Appart from the 2 splits case in Figure\\xa02, we also consider 5 splits (10,000 training samples) and 20 splits case (2,500 training samples). We present the 5 splits case (10,000 training samples) in Figure\\xa012, which corresponds to the label 0%percent\\\\%% case in Figure\\xa04. We present the 20 splits (2,500 training samples) in Figure\\xa013. With less number of training samples, both the bias and the variance will increase.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 12: Risk, bias, variance, train/test error for ResNet34 trained by MSE loss on CIFAR10 dataset (10,000 training samples). (Left) Risk, bias, and variance for ResNet34. (Middle) Variance for ResNet34. (Right) Train error and test error for ResNet34.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 13: Risk, bias, variance, train/test error for ResNet34 trained by MSE loss on CIFAR10 dataset (2,500 training samples). (Left) Risk, bias, and variance for ResNet34. (Middle) Variance for ResNet34. (Right) Train error and test error for ResNet34.',\n",
              " '',\n",
              " '\\nB.5 Weight Decay',\n",
              " 'We study another different weight decay parameter, (wd=1e-4) for ResNet34 on CIFAR10 dataset (10,000 training samples). The risk, bias, variance, and train/test error curves are shown in Figure\\xa014. Compared with Figure\\xa012, we observe that larger weight decay can decrease the variance.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 14: Risk, bias, variance, train/test error for ResNet34 trained by MSE loss on CIFAR10 dataset (10,000 training samples), the weight decay parameter of SGD is 1e-4. (Left) Risk, bias, and variance for ResNet34. (Middle) Variance for ResNet34. (Right) Train error and test error for ResNet34.',\n",
              " '',\n",
              " '\\nB.6 Label Noise',\n",
              " 'We provide the risk curve for ResNet34 under different label noise percentage as described in §3.3, and the results are shown in Figure\\xa015.',\n",
              " '\\nFigure 15: Risk under different label noise percentage. Increasing label noise leads to double descent risk curve.',\n",
              " '',\n",
              " '',\n",
              " 'B.7 Sources of Error for Mean Squared Error (MSE)',\n",
              " 'As argued in §2.2 the estimator for variance is unbiased estimator. To understand the variance of the estimator, we first split the data into two parts, A𝐴Aitalic_A and B𝐵Bitalic_B. For each part,\\nwe take multiple random splits (k𝑘kitalic_k) and estimate the variance by taking the average of those estimators, and vary the number of random splits k𝑘kitalic_k. The results are shown in Figure 16. We can see that the variation between to parts of data is small. Quantitatively, veraging across different model width, the relative difference between two parts of data is 0.65% for bias and 3.15% for variance.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 16: Bias and variance for two portions of data with k𝑘kitalic_k from 1 to 5. (Left) Bias for ResNet18. (Right) Variance for ResNet18.',\n",
              " '',\n",
              " '\\nB.8 Sources of Error for Cross Entropy Loss (CE)',\n",
              " 'For cross entropy loss, we are currently unable to obtain an unbiased estimator. We can access the quality of our estimator using the following scheme. We partition the dataset into five parts 𝒯1,…,𝒯5subscript𝒯1…subscript𝒯5\\\\mathcal{T}_{1},\\\\dots,\\\\mathcal{T}_{5}caligraphic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , caligraphic_T start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT, i.e., set N=5𝑁5N=5italic_N = 5 in Algorithm\\xa01. Then, we sequentially plot the estimate of bias and variance using k=1,2,3,4𝑘1234k=1,2,3,4italic_k = 1 , 2 , 3 , 4 as described in Algorithm\\xa01. Using larger k𝑘kitalic_k gives better estimate.\\nAs shown in Figure 17, when k𝑘kitalic_k is small, our estimator over-estimate the bias and under-estimate the variance, but the overall behavior of the cruves are consistent.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 17: Estimate of bias, variance, and risk using varying number of sample (k𝑘kitalic_k in Algorithm\\xa01). (Left) Bias (CE) for ResNet34. (Middle) Variance (CE) for ResNet34. (Right) Risk (CE) for ResNet34.',\n",
              " '',\n",
              " '\\nB.9 Effect of Depth on Bias and Variance for Out-Of-Distribution Data',\n",
              " 'We study the role of depth on out-of-distribution test data. In Figure\\xa018, we observe that increasing the depth can decrease the bias and increase the variance. Also, deeper ResNet can generalize better on CIFAR10-C dataset as shown in Figure\\xa018.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 18: Bias, variance, and test error for ResNet with different depth (ResNet18, ResNet34 and ResNet50 trained by MSE loss on 25,000 CIFAR10 training samples) evaluated on out-of-distribution examples (CIFAR10-C dataset). (Left) Bias for ResNet18, ResNet34 and ResNet50. (Middle) Variance for ResNet18, ResNet34 and ResNet50. (Right) Test error for ResNet18, ResNet34 and ResNet50.',\n",
              " '',\n",
              " '\\nB.10 Effect of Depth on ResNet using Bottleneck Blocks',\n",
              " 'In order to study the role of depth for ResNet on bias and variance, we apply basic residual block for ResNet50. To better investigate the depth of ResNet, we use Bottleneck block for ResNet26, ResNet38, and ResNet50. More specifically, the number of 3-layer bottleneck blocks for ResNet26, ResNet38, and ResNet50 are [2,2,2,2]2222[2,2,2,2][ 2 , 2 , 2 , 2 ], [3,3,3,3]3333[3,3,3,3][ 3 , 3 , 3 , 3 ], and [3,4,6,3]3463[3,4,6,3][ 3 , 4 , 6 , 3 ]. As shown in Figure\\xa019, we observe that deeper ResNet with Bottleneck blocks has lower bias and higher variance.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\nFigure 19: Bias and variance for ResNet (bottleneck block) with different depth. (Left) Bias for ResNet26, ResNet38 and ResNet50. (Right) Variance for ResNet26, ResNet38 and ResNet50. ',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Appendix C Proof of Theorems in §5',\n",
              " '\\nThroughout this section, we use ∥⋅∥fragmentsparallel-to⋅parallel-to\\\\|\\\\cdot\\\\|∥ ⋅ ∥ and ∥⋅∥2fragmentsparallel-to⋅subscriptparallel-to2\\\\|\\\\cdot\\\\|_{2}∥ ⋅ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT to denote the Frobenius norm and spectral norm of a matrix, respectively. Recall that for any given 𝜽𝜽\\\\boldsymbol{\\\\theta}bold_italic_θ, the training set 𝒯=(𝑿,𝒚)𝒯𝑿𝒚\\\\mathcal{T}=(\\\\boldsymbol{X},\\\\boldsymbol{y})caligraphic_T = ( bold_italic_X , bold_italic_y ) satisfies the relation 𝒚=𝑿⊤\\u2062𝜽𝒚superscript𝑿top𝜽\\\\boldsymbol{y}=\\\\boldsymbol{X}^{\\\\top}\\\\boldsymbol{\\\\theta}bold_italic_y = bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_θ.\\nBy plugging this relation into (6), we get',\n",
              " '',\n",
              " 'fλ\\u2062(𝒙;𝒯,𝑾)=𝒙⊤\\u2062𝑴λ\\u2062(𝒯,𝑾)\\u2062𝜽,subscript𝑓𝜆𝒙𝒯𝑾superscript𝒙topsubscript𝑴𝜆𝒯𝑾𝜽f_{\\\\lambda}(\\\\boldsymbol{x};\\\\mathcal{T},\\\\boldsymbol{W})=\\\\boldsymbol{x}^{\\\\top}%\\n\\\\boldsymbol{M}_{\\\\lambda}(\\\\mathcal{T},\\\\boldsymbol{W})\\\\boldsymbol{\\\\theta},italic_f start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( bold_italic_x ; caligraphic_T , bold_italic_W ) = bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_M start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( caligraphic_T , bold_italic_W ) bold_italic_θ ,',\n",
              " '(11)',\n",
              " '\\nwhere we define',\n",
              " '',\n",
              " '𝑴λ\\u2062(𝒯,𝑾):=𝑾⊤\\u2062(𝑾\\u2062𝑿\\u2062𝑿⊤\\u2062𝑾⊤+λ\\u2062𝑰)-1\\u2062𝑾\\u2062𝑿\\u2062𝑿⊤.assignsubscript𝑴𝜆𝒯𝑾superscript𝑾topsuperscript𝑾𝑿superscript𝑿topsuperscript𝑾top𝜆𝑰1𝑾𝑿superscript𝑿top\\\\boldsymbol{M}_{\\\\lambda}(\\\\mathcal{T},\\\\boldsymbol{W}):=\\\\boldsymbol{W}^{\\\\top}(%\\n\\\\boldsymbol{W}\\\\boldsymbol{X}\\\\boldsymbol{X}^{\\\\top}\\\\boldsymbol{W}^{\\\\top}+\\\\lambda%\\n\\\\boldsymbol{I})^{-1}\\\\boldsymbol{W}\\\\boldsymbol{X}\\\\boldsymbol{X}^{\\\\top}.bold_italic_M start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( caligraphic_T , bold_italic_W ) := bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_W bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT .',\n",
              " '(12)',\n",
              " '\\nTo avoid cluttered notations, we omit the dependency of 𝑴𝑴\\\\boldsymbol{M}bold_italic_M on λ,𝒯𝜆𝒯\\\\lambda,\\\\mathcal{T}italic_λ , caligraphic_T and 𝑾𝑾\\\\boldsymbol{W}bold_italic_W.',\n",
              " '\\nBy using (11), the expected bias and expected variance in (7) and (8) can be written as functions on the statistics of 𝑴𝑴\\\\boldsymbol{M}bold_italic_M.\\nThis is stated in the following proposition. To proceed, we introduce the change of variable',\n",
              " '',\n",
              " 'η:=γ-1=dpassign𝜂superscript𝛾1𝑑𝑝\\\\eta:=\\\\gamma^{-1}=\\\\frac{d}{p}italic_η := italic_γ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = divide start_ARG italic_d end_ARG start_ARG italic_p end_ARG',\n",
              " '',\n",
              " 'in order to be consistent with conventions in random matrix theory.',\n",
              " '',\n",
              " 'Proposition 1 (Expected Bias/Variance).',\n",
              " '\\nThe expected bias and expected variance are given by',\n",
              " '',\n",
              " '𝔼\\u2062𝑩𝒊𝒂𝒔λ2𝔼superscriptsubscript𝑩𝒊𝒂𝒔𝜆2\\\\displaystyle\\\\mathbb{E}\\\\textbf{Bias}_{\\\\lambda}^{2}blackboard_E Bias start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\\n=1d\\u2062∥𝔼\\u2062𝑴-𝑰∥2,𝑎𝑛𝑑absent1𝑑superscriptnorm𝔼𝑴𝑰2𝑎𝑛𝑑\\\\displaystyle=\\\\frac{1}{d}\\\\|\\\\mathbb{E}\\\\boldsymbol{M}-\\\\boldsymbol{I}\\\\|^{2},~{}%\\n\\\\text{and}~{}= divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ blackboard_E bold_italic_M - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , and',\n",
              " '',\n",
              " '\\n𝔼\\u2062𝑽𝒂𝒓𝒊𝒂𝒏𝒄𝒆λ𝔼subscript𝑽𝒂𝒓𝒊𝒂𝒏𝒄𝒆𝜆\\\\displaystyle\\\\mathbb{E}\\\\textbf{Variance}_{\\\\lambda}blackboard_E Variance start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT\\n=1d\\u2062𝔼\\u2062∥𝑴-𝔼\\u2062𝑴∥2,absent1𝑑𝔼superscriptnorm𝑴𝔼𝑴2\\\\displaystyle=\\\\frac{1}{d}\\\\mathbb{E}\\\\|\\\\boldsymbol{M}-\\\\mathbb{E}\\\\boldsymbol{M}\\\\|%\\n^{2},= divide start_ARG 1 end_ARG start_ARG italic_d end_ARG blackboard_E ∥ bold_italic_M - blackboard_E bold_italic_M ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,',\n",
              " '',\n",
              " 'where 𝐌𝐌\\\\boldsymbol{M}bold_italic_M is defined in (12).',\n",
              " '\\nProof.',\n",
              " 'By plugging (11) into (7), and using the prior that 𝒙∼𝒩\\u2062(0,𝑰d/d)similar-to𝒙𝒩0subscript𝑰𝑑𝑑\\\\boldsymbol{x}\\\\sim\\\\mathcal{N}(0,\\\\boldsymbol{I}_{d}/d)bold_italic_x ∼ caligraphic_N ( 0 , bold_italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT / italic_d ) and 𝜽∼𝒩\\u2062(0,𝑰d)similar-to𝜽𝒩0subscript𝑰𝑑\\\\boldsymbol{\\\\theta}\\\\sim\\\\mathcal{N}(0,\\\\boldsymbol{I}_{d})bold_italic_θ ∼ caligraphic_N ( 0 , bold_italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ), we get',\n",
              " '',\n",
              " '𝔼\\u2062𝐁𝐢𝐚𝐬λ2𝔼superscriptsubscript𝐁𝐢𝐚𝐬𝜆2\\\\displaystyle\\\\mathbb{E}\\\\textbf{Bias}_{\\\\lambda}^{2}blackboard_E Bias start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\\n=𝔼\\u2062{𝔼\\u2062(𝒙⊤\\u2062𝑴\\u2062𝜽|𝒙,𝜽)-𝒙⊤\\u2062𝜽}absent𝔼𝔼conditionalsuperscript𝒙top𝑴𝜽𝒙𝜽superscript𝒙top𝜽\\\\displaystyle=\\\\mathbb{E}\\\\{\\\\mathbb{E}(\\\\boldsymbol{x}^{\\\\top}\\\\boldsymbol{M}%\\n\\\\boldsymbol{\\\\theta}|\\\\boldsymbol{x},\\\\boldsymbol{\\\\theta})-\\\\boldsymbol{x}^{\\\\top}%\\n\\\\boldsymbol{\\\\theta}\\\\}= blackboard_E { blackboard_E ( bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_M bold_italic_θ | bold_italic_x , bold_italic_θ ) - bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_θ }',\n",
              " '',\n",
              " '',\n",
              " '=𝔼\\u2062{[𝒙⊤\\u2062(𝔼\\u2062𝑴-𝑰)\\u2062𝜽]2}absent𝔼superscriptdelimited-[]superscript𝒙top𝔼𝑴𝑰𝜽2\\\\displaystyle=\\\\mathbb{E}\\\\{\\\\left[\\\\boldsymbol{x}^{\\\\top}(\\\\mathbb{E}\\\\boldsymbol{M}%\\n-\\\\boldsymbol{I})\\\\boldsymbol{\\\\theta}\\\\right]^{2}\\\\}= blackboard_E { [ bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( blackboard_E bold_italic_M - bold_italic_I ) bold_italic_θ ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT }',\n",
              " '',\n",
              " '',\n",
              " '=𝔼\\u2062𝒙⊤\\u2062(𝔼\\u2062𝑴-𝑰)\\u2062𝜽\\u2062𝜽⊤\\u2062(𝔼\\u2062𝑴-𝑰)\\u2062𝒙absent𝔼superscript𝒙top𝔼𝑴𝑰𝜽superscript𝜽top𝔼𝑴𝑰𝒙\\\\displaystyle=\\\\mathbb{E}\\\\boldsymbol{x}^{\\\\top}(\\\\mathbb{E}\\\\boldsymbol{M}-%\\n\\\\boldsymbol{I})\\\\boldsymbol{\\\\theta}\\\\boldsymbol{\\\\theta}^{\\\\top}(\\\\mathbb{E}%\\n\\\\boldsymbol{M}-\\\\boldsymbol{I})\\\\boldsymbol{x}= blackboard_E bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( blackboard_E bold_italic_M - bold_italic_I ) bold_italic_θ bold_italic_θ start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( blackboard_E bold_italic_M - bold_italic_I ) bold_italic_x',\n",
              " '',\n",
              " '',\n",
              " '=𝔼\\u2062tr\\u2062[𝒙⊤\\u2062(𝔼\\u2062𝑴-𝑰)\\u2062𝜽\\u2062𝜽⊤\\u2062(𝔼\\u2062𝑴-𝑰)⊤\\u2062𝒙]absent𝔼trdelimited-[]superscript𝒙top𝔼𝑴𝑰𝜽superscript𝜽topsuperscript𝔼𝑴𝑰top𝒙\\\\displaystyle=\\\\mathbb{E}\\\\text{tr}\\\\Big{[}\\\\boldsymbol{x}^{\\\\top}(\\\\mathbb{E}%\\n\\\\boldsymbol{M}-\\\\boldsymbol{I})\\\\boldsymbol{\\\\theta}\\\\boldsymbol{\\\\theta}^{\\\\top}(%\\n\\\\mathbb{E}\\\\boldsymbol{M}-\\\\boldsymbol{I})^{\\\\top}\\\\boldsymbol{x}\\\\Big{]}= blackboard_E tr [ bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( blackboard_E bold_italic_M - bold_italic_I ) bold_italic_θ bold_italic_θ start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( blackboard_E bold_italic_M - bold_italic_I ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ]',\n",
              " '',\n",
              " '',\n",
              " '=tr\\u2062[(𝔼\\u2062𝑴-𝑰)\\u2062𝔼\\u2062(𝒙\\u2062𝒙⊤)\\u2062(𝔼\\u2062𝑴-𝑰)⊤\\u2062𝔼\\u2062(𝜽\\u2062𝜽⊤)]absenttrdelimited-[]𝔼𝑴𝑰𝔼𝒙superscript𝒙topsuperscript𝔼𝑴𝑰top𝔼𝜽superscript𝜽top\\\\displaystyle=\\\\text{tr}\\\\Big{[}(\\\\mathbb{E}\\\\boldsymbol{M}-\\\\boldsymbol{I})\\\\mathbb%\\n{E}(\\\\boldsymbol{x}\\\\boldsymbol{x}^{\\\\top})(\\\\mathbb{E}\\\\boldsymbol{M}-\\\\boldsymbol{%\\nI})^{\\\\top}\\\\mathbb{E}(\\\\boldsymbol{\\\\theta}\\\\boldsymbol{\\\\theta}^{\\\\top})\\\\Big{]}= tr [ ( blackboard_E bold_italic_M - bold_italic_I ) blackboard_E ( bold_italic_x bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) ( blackboard_E bold_italic_M - bold_italic_I ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT blackboard_E ( bold_italic_θ bold_italic_θ start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) ]',\n",
              " '',\n",
              " '',\n",
              " '=1d\\u2062∥𝔼\\u2062𝑴-𝑰∥2.absent1𝑑superscriptnorm𝔼𝑴𝑰2\\\\displaystyle=\\\\frac{1}{d}\\\\|\\\\mathbb{E}\\\\boldsymbol{M}-\\\\boldsymbol{I}\\\\|^{2}.= divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ blackboard_E bold_italic_M - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .',\n",
              " '',\n",
              " 'Similarly, by plugging (11) into (8) we get',\n",
              " '',\n",
              " '',\n",
              " '𝔼\\u2062𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞λ𝔼subscript𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞𝜆\\\\displaystyle~{}\\\\mathbb{E}\\\\textbf{Variance}_{\\\\lambda}blackboard_E Variance start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT',\n",
              " '',\n",
              " '\\n=\\\\displaystyle==\\n𝔼\\u2062{𝔼\\u2062[(𝒙⊤\\u2062𝑴\\u2062𝜽-𝔼\\u2062(𝒙⊤\\u2062𝑴\\u2062θ|𝒙,𝜽))2|𝒙,𝜽]}𝔼𝔼delimited-[]conditionalsuperscriptsuperscript𝒙top𝑴𝜽𝔼conditionalsuperscript𝒙top𝑴𝜃𝒙𝜽2𝒙𝜽\\\\displaystyle~{}\\\\mathbb{E}\\\\Big{\\\\{}\\\\mathbb{E}\\\\big{[}(\\\\boldsymbol{x}^{\\\\top}%\\n\\\\boldsymbol{M}\\\\boldsymbol{\\\\theta}-\\\\mathbb{E}(\\\\boldsymbol{x}^{\\\\top}\\\\boldsymbol{%\\nM}\\\\theta|\\\\boldsymbol{x},\\\\boldsymbol{\\\\theta}))^{2}|\\\\boldsymbol{x},\\\\boldsymbol{%\\n\\\\theta}\\\\big{]}\\\\Big{\\\\}}blackboard_E { blackboard_E [ ( bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_M bold_italic_θ - blackboard_E ( bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_M italic_θ | bold_italic_x , bold_italic_θ ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | bold_italic_x , bold_italic_θ ] }',\n",
              " '',\n",
              " '\\n=\\\\displaystyle==\\n𝔼\\u2062{𝔼\\u2062[(𝒙⊤\\u2062𝑴\\u2062𝜽-𝒙⊤\\u2062(𝔼\\u2062𝑴)\\u2062𝜽)2|𝒙,𝜽]}𝔼𝔼delimited-[]conditionalsuperscriptsuperscript𝒙top𝑴𝜽superscript𝒙top𝔼𝑴𝜽2𝒙𝜽\\\\displaystyle~{}\\\\mathbb{E}\\\\Big{\\\\{}\\\\mathbb{E}\\\\big{[}(\\\\boldsymbol{x}^{\\\\top}%\\n\\\\boldsymbol{M}\\\\boldsymbol{\\\\theta}-\\\\boldsymbol{x}^{\\\\top}(\\\\mathbb{E}\\\\boldsymbol{%\\nM})\\\\boldsymbol{\\\\theta})^{2}|\\\\boldsymbol{x},\\\\boldsymbol{\\\\theta}\\\\big{]}\\\\Big{\\\\}}blackboard_E { blackboard_E [ ( bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_M bold_italic_θ - bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( blackboard_E bold_italic_M ) bold_italic_θ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | bold_italic_x , bold_italic_θ ] }',\n",
              " '',\n",
              " '\\n=\\\\displaystyle==\\n𝔼\\u2062(𝒙⊤\\u2062𝑴\\u2062𝜽-𝒙⊤\\u2062(𝔼\\u2062𝑴)\\u2062𝜽)2𝔼superscriptsuperscript𝒙top𝑴𝜽superscript𝒙top𝔼𝑴𝜽2\\\\displaystyle~{}\\\\mathbb{E}(\\\\boldsymbol{x}^{\\\\top}\\\\boldsymbol{M}\\\\boldsymbol{%\\n\\\\theta}-\\\\boldsymbol{x}^{\\\\top}(\\\\mathbb{E}\\\\boldsymbol{M})\\\\boldsymbol{\\\\theta})^{2}blackboard_E ( bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_M bold_italic_θ - bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( blackboard_E bold_italic_M ) bold_italic_θ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT',\n",
              " '',\n",
              " '\\n=\\\\displaystyle==\\n𝔼\\u2062[𝒙⊤\\u2062(𝑴-𝔼\\u2062𝑴)\\u2062𝜽]2𝔼superscriptdelimited-[]superscript𝒙top𝑴𝔼𝑴𝜽2\\\\displaystyle~{}\\\\mathbb{E}\\\\Big{[}\\\\boldsymbol{x}^{\\\\top}(\\\\boldsymbol{M}-\\\\mathbb{%\\nE}\\\\boldsymbol{M})\\\\boldsymbol{\\\\theta}\\\\Big{]}^{2}blackboard_E [ bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_M - blackboard_E bold_italic_M ) bold_italic_θ ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT',\n",
              " '',\n",
              " '\\n=\\\\displaystyle==\\n1d\\u2062𝔼\\u2062∥𝑴-𝔼\\u2062𝑴∥2.1𝑑𝔼superscriptnorm𝑴𝔼𝑴2\\\\displaystyle~{}\\\\frac{1}{d}\\\\mathbb{E}\\\\|\\\\boldsymbol{M}-\\\\mathbb{E}\\\\boldsymbol{M}%\\n\\\\|^{2}.divide start_ARG 1 end_ARG start_ARG italic_d end_ARG blackboard_E ∥ bold_italic_M - blackboard_E bold_italic_M ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .',\n",
              " '',\n",
              " '∎',\n",
              " '',\n",
              " '\\nThe risk is given by',\n",
              " '',\n",
              " '𝔼\\u2062𝐁𝐢𝐚𝐬λ2+𝔼\\u2062𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞λ=1d\\u2062𝔼\\u2062∥𝑴-𝑰∥2=1d\\u2062𝔼\\u2062tr\\u2062(𝑴⊤\\u2062𝑴)-2d\\u2062𝔼\\u2062tr\\u2062(𝑴)+1.𝔼superscriptsubscript𝐁𝐢𝐚𝐬𝜆2𝔼subscript𝐕𝐚𝐫𝐢𝐚𝐧𝐜𝐞𝜆1𝑑𝔼superscriptnorm𝑴𝑰21𝑑𝔼trsuperscript𝑴top𝑴2𝑑𝔼tr𝑴1\\\\mathbb{E}\\\\textbf{Bias}_{\\\\lambda}^{2}+\\\\mathbb{E}\\\\textbf{Variance}_{\\\\lambda}=%\\n\\\\frac{1}{d}\\\\mathbb{E}\\\\|\\\\boldsymbol{M}-\\\\boldsymbol{I}\\\\|^{2}=\\\\frac{1}{d}\\\\mathbb{%\\nE}\\\\text{tr}(\\\\boldsymbol{M}^{\\\\top}\\\\boldsymbol{M})-\\\\frac{2}{d}\\\\mathbb{E}\\\\text{tr%\\n}(\\\\boldsymbol{M})+1.blackboard_E Bias start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + blackboard_E Variance start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_d end_ARG blackboard_E ∥ bold_italic_M - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_d end_ARG blackboard_E tr ( bold_italic_M start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_M ) - divide start_ARG 2 end_ARG start_ARG italic_d end_ARG blackboard_E tr ( bold_italic_M ) + 1 .',\n",
              " '',\n",
              " 'First, we show that in the asymptotic setting defined in Assumption 1, the expected Bias and expected Variance can be calculated as functions on the statistics of the following matrix:',\n",
              " '',\n",
              " '𝑴~λ0\\u2062(𝑾)=𝑾⊤\\u2062(𝑾\\u2062𝑾⊤+λ0\\u2062𝑰)-1\\u2062𝑾.subscriptbold-~𝑴subscript𝜆0𝑾superscript𝑾topsuperscript𝑾superscript𝑾topsubscript𝜆0𝑰1𝑾\\\\boldsymbol{\\\\widetilde{M}}_{\\\\lambda_{0}}(\\\\boldsymbol{W})=\\\\boldsymbol{W}^{\\\\top}%\\n(\\\\boldsymbol{W}\\\\boldsymbol{W}^{\\\\top}+\\\\lambda_{0}\\\\boldsymbol{I})^{-1}%\\n\\\\boldsymbol{W}.overbold_~ start_ARG bold_italic_M end_ARG start_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_W ) = bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_W bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W .',\n",
              " '(13)',\n",
              " '\\nIn the following, we omit the dependency of 𝑴~bold-~𝑴\\\\boldsymbol{\\\\widetilde{M}}overbold_~ start_ARG bold_italic_M end_ARG on λ0subscript𝜆0\\\\lambda_{0}italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and 𝑾𝑾\\\\boldsymbol{W}bold_italic_W.',\n",
              " '',\n",
              " 'Proposition 2 (Gap between 𝑴𝑴\\\\boldsymbol{M}bold_italic_M and 𝑴~bold-~𝑴\\\\boldsymbol{\\\\widetilde{M}}overbold_~ start_ARG bold_italic_M end_ARG).',\n",
              " '\\nUnder Assumption 1 with λ-nd\\u2062λ0𝜆𝑛𝑑subscript𝜆0\\\\lambda-\\\\frac{n}{d}\\\\lambda_{0}italic_λ - divide start_ARG italic_n end_ARG start_ARG italic_d end_ARG italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, we have',\n",
              " '',\n",
              " '1d\\u2062∥𝔼\\u2062𝑴-𝑰∥21𝑑superscriptnorm𝔼𝑴𝑰2\\\\displaystyle\\\\frac{1}{d}\\\\|\\\\mathbb{E}\\\\boldsymbol{M}-\\\\boldsymbol{I}\\\\|^{2}divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ blackboard_E bold_italic_M - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\\n=1d\\u2062∥𝔼\\u2062𝑴~-𝑰∥2,𝑎𝑛𝑑absent1𝑑superscriptnorm𝔼bold-~𝑴𝑰2𝑎𝑛𝑑\\\\displaystyle=\\\\frac{1}{d}\\\\|\\\\mathbb{E}\\\\boldsymbol{\\\\widetilde{M}}-\\\\boldsymbol{I}%\\n\\\\|^{2},~{}\\\\text{and}~{}= divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ blackboard_E overbold_~ start_ARG bold_italic_M end_ARG - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , and',\n",
              " '',\n",
              " '\\n1d\\u2062𝔼\\u2062∥𝑴-𝑰∥21𝑑𝔼superscriptnorm𝑴𝑰2\\\\displaystyle\\\\frac{1}{d}\\\\mathbb{E}\\\\|\\\\boldsymbol{M}-\\\\boldsymbol{I}\\\\|^{2}divide start_ARG 1 end_ARG start_ARG italic_d end_ARG blackboard_E ∥ bold_italic_M - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\\n=1d\\u2062𝔼\\u2062∥𝑴~-𝑰∥2.absent1𝑑𝔼superscriptnormbold-~𝑴𝑰2\\\\displaystyle=\\\\frac{1}{d}\\\\mathbb{E}\\\\|\\\\boldsymbol{\\\\widetilde{M}}-\\\\boldsymbol{I}%\\n\\\\|^{2}.= divide start_ARG 1 end_ARG start_ARG italic_d end_ARG blackboard_E ∥ overbold_~ start_ARG bold_italic_M end_ARG - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .',\n",
              " '',\n",
              " '',\n",
              " 'Proof.',\n",
              " 'It suffices to show that ∥𝑴-𝑴~∥2=0subscriptnorm𝑴bold-~𝑴20\\\\|\\\\boldsymbol{M}-\\\\boldsymbol{\\\\widetilde{M}}\\\\|_{2}=0∥ bold_italic_M - overbold_~ start_ARG bold_italic_M end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0 almost surely.\\nFrom (12) and (13), we have',\n",
              " '',\n",
              " '𝑴-𝑴~=𝑾⊤\\u2062𝛀\\u2062𝑾+𝑾⊤\\u2062𝛀\\u2062𝑾\\u2062𝚫+𝑾⊤\\u2062(𝑾\\u2062𝑾⊤+λ0\\u2062𝑰)-1\\u2062𝑾\\u2062𝚫,𝑴bold-~𝑴superscript𝑾top𝛀𝑾superscript𝑾top𝛀𝑾𝚫superscript𝑾topsuperscript𝑾superscript𝑾topsubscript𝜆0𝑰1𝑾𝚫\\\\boldsymbol{M}-\\\\boldsymbol{\\\\widetilde{M}}=\\\\boldsymbol{W}^{\\\\top}\\\\boldsymbol{%\\n\\\\Omega}\\\\boldsymbol{W}+\\\\boldsymbol{W}^{\\\\top}\\\\boldsymbol{\\\\Omega}\\\\boldsymbol{W}%\\n\\\\boldsymbol{\\\\Delta}+\\\\boldsymbol{W}^{\\\\top}(\\\\boldsymbol{W}\\\\boldsymbol{W}^{\\\\top}+%\\n\\\\lambda_{0}\\\\boldsymbol{I})^{-1}\\\\boldsymbol{W}\\\\boldsymbol{\\\\Delta},bold_italic_M - overbold_~ start_ARG bold_italic_M end_ARG = bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_Ω bold_italic_W + bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_Ω bold_italic_W bold_Δ + bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_W bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W bold_Δ ,',\n",
              " '',\n",
              " 'where 𝚫:=(d/n)\\u2062𝑿\\u2062𝑿⊤-𝑰assign𝚫𝑑𝑛𝑿superscript𝑿top𝑰\\\\boldsymbol{\\\\Delta}:=(d/n)\\\\boldsymbol{X}\\\\boldsymbol{X}^{\\\\top}-\\\\boldsymbol{I}bold_Δ := ( italic_d / italic_n ) bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT - bold_italic_I and 𝛀:=(𝑾\\u2062𝑾⊤+λ0\\u2062𝑰+𝑾\\u2062𝚫\\u2062𝑾⊤)-1-(𝑾\\u2062𝑾⊤+λ0\\u2062𝑰)-1.assign𝛀superscript𝑾superscript𝑾topsubscript𝜆0𝑰𝑾𝚫superscript𝑾top1superscript𝑾superscript𝑾topsubscript𝜆0𝑰1\\\\boldsymbol{\\\\Omega}:=(\\\\boldsymbol{W}\\\\boldsymbol{W}^{\\\\top}+\\\\lambda_{0}%\\n\\\\boldsymbol{I}+\\\\boldsymbol{W}\\\\boldsymbol{\\\\Delta}\\\\boldsymbol{W}^{\\\\top})^{-1}-(%\\n\\\\boldsymbol{W}\\\\boldsymbol{W}^{\\\\top}+\\\\lambda_{0}\\\\boldsymbol{I})^{-1}.bold_Ω := ( bold_italic_W bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT bold_italic_I + bold_italic_W bold_Δ bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT - ( bold_italic_W bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT .',\n",
              " '\\nBy using triangle inequality and the sub-multiplicative property of spectral norm, we have',\n",
              " '',\n",
              " '∥𝑴-𝑴~∥2≤∥𝑾∥22⋅∥𝛀∥2+∥𝑾∥22⋅∥𝛀∥2⋅∥𝚫∥2+∥𝑴~∥2⋅∥𝚫∥2.subscriptnorm𝑴bold-~𝑴2⋅superscriptsubscriptnorm𝑾22subscriptnorm𝛀2⋅superscriptsubscriptnorm𝑾22subscriptnorm𝛀2subscriptnorm𝚫2⋅subscriptnormbold-~𝑴2subscriptnorm𝚫2\\\\|\\\\boldsymbol{M}-\\\\boldsymbol{\\\\widetilde{M}}\\\\|_{2}\\\\leq\\\\|\\\\boldsymbol{W}\\\\|_{2}^{2%\\n}\\\\cdot\\\\|\\\\boldsymbol{\\\\Omega}\\\\|_{2}+\\\\|\\\\boldsymbol{W}\\\\|_{2}^{2}\\\\cdot\\\\|\\\\boldsymbol%\\n{\\\\Omega}\\\\|_{2}\\\\cdot\\\\|\\\\boldsymbol{\\\\Delta}\\\\|_{2}+\\\\|\\\\boldsymbol{\\\\widetilde{M}}\\\\|_%\\n{2}\\\\cdot\\\\|\\\\boldsymbol{\\\\Delta}\\\\|_{2}.∥ bold_italic_M - overbold_~ start_ARG bold_italic_M end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ ∥ bold_italic_W ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⋅ ∥ bold_Ω ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + ∥ bold_italic_W ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⋅ ∥ bold_Ω ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ⋅ ∥ bold_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + ∥ overbold_~ start_ARG bold_italic_M end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ⋅ ∥ bold_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .',\n",
              " '(14)',\n",
              " '\\nFurthermore, by a classical result on the perturbation of matrix inverse (see e.g., Ghaoui (2002, equation (1.1)1.1(1.1)( 1.1 ))), we have',\n",
              " '',\n",
              " '∥𝛀∥2≤∥(𝑾\\u2062𝑾⊤+λ0\\u2062𝑰)-1∥22\\u2062∥𝑾∥22\\u2062∥𝚫∥2+O\\u2062(∥𝚫∥22).subscriptnorm𝛀2superscriptsubscriptnormsuperscript𝑾superscript𝑾topsubscript𝜆0𝑰122superscriptsubscriptnorm𝑾22subscriptnorm𝚫2𝑂superscriptsubscriptnorm𝚫22\\\\|\\\\boldsymbol{\\\\Omega}\\\\|_{2}\\\\leq\\\\|(\\\\boldsymbol{W}\\\\boldsymbol{W}^{\\\\top}+\\\\lambda_%\\n{0}\\\\boldsymbol{I})^{-1}\\\\|_{2}^{2}\\\\|\\\\boldsymbol{W}\\\\|_{2}^{2}\\\\|\\\\boldsymbol{%\\n\\\\Delta}\\\\|_{2}+O(\\\\|\\\\boldsymbol{\\\\Delta}\\\\|_{2}^{2}).∥ bold_Ω ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ ∥ ( bold_italic_W bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∥ bold_italic_W ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∥ bold_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_O ( ∥ bold_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) .',\n",
              " '',\n",
              " 'Combining this bound with (14) gives',\n",
              " '',\n",
              " '∥𝑴-𝑴~∥2≤∥𝑾∥24⋅∥(𝑾\\u2062𝑾⊤+λ0\\u2062𝑰)-1∥22⋅∥𝚫∥2+∥𝑴~∥2⋅∥𝚫∥2+O\\u2062(∥𝚫∥22).subscriptnorm𝑴bold-~𝑴2⋅superscriptsubscriptnorm𝑾24superscriptsubscriptnormsuperscript𝑾superscript𝑾topsubscript𝜆0𝑰122subscriptnorm𝚫2⋅subscriptnormbold-~𝑴2subscriptnorm𝚫2𝑂superscriptsubscriptnorm𝚫22\\\\|\\\\boldsymbol{M}-\\\\boldsymbol{\\\\widetilde{M}}\\\\|_{2}\\\\leq\\\\|\\\\boldsymbol{W}\\\\|_{2}^{4%\\n}\\\\cdot\\\\|(\\\\boldsymbol{W}\\\\boldsymbol{W}^{\\\\top}+\\\\lambda_{0}\\\\boldsymbol{I})^{-1}\\\\|%\\n_{2}^{2}\\\\cdot\\\\|\\\\boldsymbol{\\\\Delta}\\\\|_{2}+\\\\|\\\\boldsymbol{\\\\widetilde{M}}\\\\|_{2}%\\n\\\\cdot\\\\|\\\\boldsymbol{\\\\Delta}\\\\|_{2}+O(\\\\|\\\\boldsymbol{\\\\Delta}\\\\|_{2}^{2}).∥ bold_italic_M - overbold_~ start_ARG bold_italic_M end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ ∥ bold_italic_W ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ⋅ ∥ ( bold_italic_W bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⋅ ∥ bold_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + ∥ overbold_~ start_ARG bold_italic_M end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ⋅ ∥ bold_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_O ( ∥ bold_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) .',\n",
              " '',\n",
              " 'It remains to show that ∥𝚫∥2=0subscriptnorm𝚫20\\\\|\\\\boldsymbol{\\\\Delta}\\\\|_{2}=0∥ bold_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0 and that ∥𝑾∥2subscriptnorm𝑾2\\\\|\\\\boldsymbol{W}\\\\|_{2}∥ bold_italic_W ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, ∥(𝑾\\u2062𝑾⊤+λ0\\u2062𝑰)-1∥22superscriptsubscriptnormsuperscript𝑾superscript𝑾topsubscript𝜆0𝑰122\\\\|(\\\\boldsymbol{W}\\\\boldsymbol{W}^{\\\\top}+\\\\lambda_{0}\\\\boldsymbol{I})^{-1}\\\\|_{2}^{2}∥ ( bold_italic_W bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, and ∥𝑴~∥2subscriptnormbold-~𝑴2\\\\|\\\\boldsymbol{\\\\widetilde{M}}\\\\|_{2}∥ overbold_~ start_ARG bold_italic_M end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are bounded from above almost surely. By Wainwright (2019, Example 6.2), ∀δ>0for-all𝛿0\\\\forall\\\\delta>0∀ italic_δ > 0 and n>d𝑛𝑑n>ditalic_n > italic_d,',\n",
              " '',\n",
              " 'ℙ\\u2062(∥𝚫∥2≤2\\u2062ϵ+ϵ2)≥1-e-n\\u2062δ2/2,where\\u2062ϵ=δ+dn.formulae-sequenceℙsubscriptnorm𝚫22italic-ϵsuperscriptitalic-ϵ21superscript𝑒𝑛superscript𝛿22whereitalic-ϵ𝛿𝑑𝑛\\\\mathbb{P}\\\\Big{(}\\\\|\\\\boldsymbol{\\\\Delta}\\\\|_{2}\\\\leq 2\\\\epsilon+\\\\epsilon^{2}\\\\Big{)}%\\n\\\\geq 1-e^{-n\\\\delta^{2}/2},~{}\\\\text{where}~{}\\\\epsilon=\\\\delta+\\\\sqrt{\\\\frac{d}{n}}.blackboard_P ( ∥ bold_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ 2 italic_ϵ + italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ≥ 1 - italic_e start_POSTSUPERSCRIPT - italic_n italic_δ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / 2 end_POSTSUPERSCRIPT , where italic_ϵ = italic_δ + square-root start_ARG divide start_ARG italic_d end_ARG start_ARG italic_n end_ARG end_ARG .',\n",
              " '',\n",
              " 'By letting δ=d/n𝛿𝑑𝑛\\\\delta=\\\\sqrt{d/n}italic_δ = square-root start_ARG italic_d / italic_n end_ARG and taking the asymptotic limit as in Assumption 1, we have',\n",
              " '',\n",
              " '∥𝚫∥2\\u2062=a.s.\\u20620.subscriptnorm𝚫2a.s.0\\\\|\\\\boldsymbol{\\\\Delta}\\\\|_{2}\\\\overset{\\\\text{a.s.}}{=}0.∥ bold_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT overa.s. start_ARG = end_ARG 0 .',\n",
              " '',\n",
              " 'From Geman (1980), the largest eigenvalue of 𝑾\\u2062𝑾⊤𝑾superscript𝑾top\\\\boldsymbol{W}\\\\boldsymbol{W}^{\\\\top}bold_italic_W bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT is almost surely (1+η)2<∞superscript1𝜂2(1+\\\\sqrt{\\\\eta})^{2}<\\\\infty( 1 + square-root start_ARG italic_η end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < ∞. Therefore, we have',\n",
              " '',\n",
              " '∥𝑾∥2\\u2062=a.s.\\u20621+η<∞.subscriptnorm𝑾2a.s.1𝜂\\\\|\\\\boldsymbol{W}\\\\|_{2}\\\\overset{\\\\text{a.s.}}{=}1+\\\\sqrt{\\\\eta}<\\\\infty.∥ bold_italic_W ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT overa.s. start_ARG = end_ARG 1 + square-root start_ARG italic_η end_ARG < ∞ .',\n",
              " '',\n",
              " 'Finally, note that',\n",
              " '',\n",
              " '∥(𝑾\\u2062𝑾⊤+λ0\\u2062𝑰)-1∥2≤1λ0+σmin\\u2062(𝑾)2≤1λ0<∞,subscriptnormsuperscript𝑾superscript𝑾topsubscript𝜆0𝑰121subscript𝜆0subscript𝜎superscript𝑾21subscript𝜆0\\\\|(\\\\boldsymbol{W}\\\\boldsymbol{W}^{\\\\top}+\\\\lambda_{0}\\\\boldsymbol{I})^{-1}\\\\|_{2}%\\n\\\\leq\\\\frac{1}{\\\\lambda_{0}+\\\\sigma_{\\\\min}(\\\\boldsymbol{W})^{2}}\\\\leq\\\\frac{1}{%\\n\\\\lambda_{0}}<\\\\infty,∥ ( bold_italic_W bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ divide start_ARG 1 end_ARG start_ARG italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_σ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT ( bold_italic_W ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ≤ divide start_ARG 1 end_ARG start_ARG italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG < ∞ ,',\n",
              " '',\n",
              " '',\n",
              " '\\n∥𝑴~∥2=σmax\\u2062(𝑾)2σmax\\u2062(𝑾)2+λ0≤1.subscriptnormbold-~𝑴2subscript𝜎superscript𝑾2subscript𝜎superscript𝑾2subscript𝜆01\\\\|\\\\boldsymbol{\\\\widetilde{M}}\\\\|_{2}=\\\\frac{\\\\sigma_{\\\\max}(\\\\boldsymbol{W})^{2}}{%\\n\\\\sigma_{\\\\max}(\\\\boldsymbol{W})^{2}+\\\\lambda_{0}}\\\\leq 1.∥ overbold_~ start_ARG bold_italic_M end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = divide start_ARG italic_σ start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ( bold_italic_W ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ( bold_italic_W ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ≤ 1 .',\n",
              " '',\n",
              " 'We therefore conclude that ∥𝑴-𝑴~∥2=0subscriptnorm𝑴bold-~𝑴20\\\\|\\\\boldsymbol{M}-\\\\boldsymbol{\\\\widetilde{M}}\\\\|_{2}=0∥ bold_italic_M - overbold_~ start_ARG bold_italic_M end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0 almost surely, as desired.\\n∎',\n",
              " '',\n",
              " '',\n",
              " 'Proposition 3 (Asymptotic Risk).',\n",
              " '\\nGiven the expression for Bias and Variance in Proposition 1, under the asymptotic assumptions from Assumption 1,',\n",
              " '',\n",
              " '1d\\u2062𝔼\\u2062∥𝑴~-𝑰∥2={(1-1η)+fλ0-1\\u2062(1η),if\\xa0d>p,fλ0-1\\u2062(η),if\\xa0d≤p,1𝑑𝔼superscriptnormbold-~𝑴𝑰2cases11𝜂subscript𝑓superscriptsubscript𝜆011𝜂if\\xa0d>psubscript𝑓superscriptsubscript𝜆01𝜂if\\xa0d≤p\\\\frac{1}{d}\\\\mathbb{E}\\\\|\\\\boldsymbol{\\\\widetilde{M}}-\\\\boldsymbol{I}\\\\|^{2}=\\\\begin{%\\ncases}(1-\\\\frac{1}{\\\\eta})+f_{\\\\lambda_{0}^{-1}}(\\\\frac{1}{\\\\eta}),&\\\\text{if $d>p$}%\\n,\\\\\\\\\\nf_{\\\\lambda_{0}^{-1}}(\\\\eta),&\\\\text{if $d\\\\leq p$},\\\\end{cases}divide start_ARG 1 end_ARG start_ARG italic_d end_ARG blackboard_E ∥ overbold_~ start_ARG bold_italic_M end_ARG - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = { start_ROW start_CELL ( 1 - divide start_ARG 1 end_ARG start_ARG italic_η end_ARG ) + italic_f start_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( divide start_ARG 1 end_ARG start_ARG italic_η end_ARG ) , end_CELL start_CELL if italic_d > italic_p , end_CELL end_ROW start_ROW start_CELL italic_f start_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_η ) , end_CELL start_CELL if italic_d ≤ italic_p , end_CELL end_ROW',\n",
              " '',\n",
              " 'where η=d/p𝜂𝑑𝑝\\\\eta=d/pitalic_η = italic_d / italic_p, and for any η,α∈ℝ𝜂𝛼ℝ\\\\eta,\\\\alpha\\\\in\\\\mathbb{R}italic_η , italic_α ∈ blackboard_R,',\n",
              " '',\n",
              " 'fα\\u2062(η)=α+η\\u2062(1+η-2\\u2062α+η\\u2062α)2\\u2062η\\u2062η2+2\\u2062η\\u2062α\\u2062(1+η)+α2\\u2062(1-η)2-1-η2\\u2062η.subscript𝑓𝛼𝜂𝛼𝜂1𝜂2𝛼𝜂𝛼2𝜂superscript𝜂22𝜂𝛼1𝜂superscript𝛼2superscript1𝜂21𝜂2𝜂f_{\\\\alpha}(\\\\eta)=\\\\frac{\\\\alpha+\\\\eta(1+\\\\eta-2\\\\alpha+\\\\eta\\\\alpha)}{2\\\\eta\\\\sqrt{\\\\eta%\\n^{2}+2\\\\eta\\\\alpha(1+\\\\eta)+\\\\alpha^{2}(1-\\\\eta)^{2}}}-\\\\frac{1-\\\\eta}{2\\\\eta}.italic_f start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( italic_η ) = divide start_ARG italic_α + italic_η ( 1 + italic_η - 2 italic_α + italic_η italic_α ) end_ARG start_ARG 2 italic_η square-root start_ARG italic_η start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 italic_η italic_α ( 1 + italic_η ) + italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 - italic_η ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG - divide start_ARG 1 - italic_η end_ARG start_ARG 2 italic_η end_ARG .',\n",
              " '',\n",
              " '',\n",
              " 'Proof.',\n",
              " 'Recall that 𝑴~=𝑾⊤\\u2062(𝑾\\u2062𝑾⊤+λ0\\u2062𝑰)-1\\u2062𝑾bold-~𝑴superscript𝑾topsuperscript𝑾superscript𝑾topsubscript𝜆0𝑰1𝑾\\\\boldsymbol{\\\\widetilde{M}}=\\\\boldsymbol{W}^{\\\\top}(\\\\boldsymbol{W}\\\\boldsymbol{W}^%\\n{\\\\top}+\\\\lambda_{0}\\\\boldsymbol{I})^{-1}\\\\boldsymbol{W}overbold_~ start_ARG bold_italic_M end_ARG = bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_W bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W, by Sherman-Morrision,',\n",
              " '',\n",
              " '𝑴~=𝑰-(𝑰+λ0-1\\u2062𝑾⊤\\u2062𝑾)-1,bold-~𝑴𝑰superscript𝑰superscriptsubscript𝜆01superscript𝑾top𝑾1\\\\boldsymbol{\\\\widetilde{M}}=\\\\boldsymbol{I}-(\\\\boldsymbol{I}+\\\\lambda_{0}^{-1}%\\n\\\\boldsymbol{W}^{\\\\top}\\\\boldsymbol{W})^{-1},overbold_~ start_ARG bold_italic_M end_ARG = bold_italic_I - ( bold_italic_I + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ,',\n",
              " '',\n",
              " 'where (d/p)\\u2062𝑾⊤\\u2062𝑾∈ℝd×d𝑑𝑝superscript𝑾top𝑾superscriptℝ𝑑𝑑(d/p)\\\\boldsymbol{W}^{\\\\top}\\\\boldsymbol{W}\\\\in\\\\mathbb{R}^{d\\\\times d}( italic_d / italic_p ) bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT. Let λi≥0,i=1,…,dformulae-sequencesubscript𝜆𝑖0𝑖1…𝑑\\\\lambda_{i}\\\\geq 0,i=1,\\\\dots,ditalic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≥ 0 , italic_i = 1 , … , italic_d be the eigenvalues of (d/p)\\u2062𝑾⊤\\u2062𝑾𝑑𝑝superscript𝑾top𝑾(d/p)\\\\boldsymbol{W}^{\\\\top}\\\\boldsymbol{W}( italic_d / italic_p ) bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W. For notational simplicity, let α=λ0-1𝛼superscriptsubscript𝜆01\\\\alpha=\\\\lambda_{0}^{-1}italic_α = italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT. Then',\n",
              " '',\n",
              " '∥𝑴~-𝑰∥2superscriptnormbold-~𝑴𝑰2\\\\displaystyle\\\\|\\\\boldsymbol{\\\\widetilde{M}}-\\\\boldsymbol{I}\\\\|^{2}∥ overbold_~ start_ARG bold_italic_M end_ARG - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\\n=∥[𝑰+(α/η)\\u2062(d/p)\\u2062𝑾⊤\\u2062𝑾]-1∥2=∑i=1d1(1+αη\\u2062λi)2.absentsuperscriptnormsuperscriptdelimited-[]𝑰𝛼𝜂𝑑𝑝superscript𝑾top𝑾12superscriptsubscript𝑖1𝑑1superscript1𝛼𝜂subscript𝜆𝑖2\\\\displaystyle=\\\\|[\\\\boldsymbol{I}+(\\\\alpha/\\\\eta)(d/p)\\\\boldsymbol{W}^{\\\\top}%\\n\\\\boldsymbol{W}]^{-1}\\\\|^{2}=\\\\sum_{i=1}^{d}\\\\frac{1}{(1+\\\\frac{\\\\alpha}{\\\\eta}%\\n\\\\lambda_{i})^{2}}.= ∥ [ bold_italic_I + ( italic_α / italic_η ) ( italic_d / italic_p ) bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W ] start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG ( 1 + divide start_ARG italic_α end_ARG start_ARG italic_η end_ARG italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG .',\n",
              " '',\n",
              " 'Let 𝑨=(d/p)\\u2062𝑾⊤\\u2062𝑾𝑨𝑑𝑝superscript𝑾top𝑾\\\\boldsymbol{A}=(d/p)\\\\boldsymbol{W}^{\\\\top}\\\\boldsymbol{W}bold_italic_A = ( italic_d / italic_p ) bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W, and μ𝑨subscript𝜇𝑨\\\\mu_{\\\\boldsymbol{A}}italic_μ start_POSTSUBSCRIPT bold_italic_A end_POSTSUBSCRIPT be the spectral measure of 𝑨𝑨\\\\boldsymbol{A}bold_italic_A. Then',\n",
              " '',\n",
              " '1d\\u2062∥𝑴~-𝑰∥2=∫ℝ+1(1+αη\\u2062x)2\\u2062𝑑μ𝑨\\u2062(d\\u2062x).1𝑑superscriptnormbold-~𝑴𝑰2subscriptsuperscriptℝ1superscript1𝛼𝜂𝑥2differential-dsubscript𝜇𝑨𝑑𝑥\\\\frac{1}{d}\\\\|\\\\boldsymbol{\\\\widetilde{M}}-\\\\boldsymbol{I}\\\\|^{2}=\\\\int_{\\\\mathbb{R}^%\\n{+}}\\\\frac{1}{(1+\\\\frac{\\\\alpha}{\\\\eta}x)^{2}}d\\\\mu_{\\\\boldsymbol{A}}(dx).divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ overbold_~ start_ARG bold_italic_M end_ARG - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG ( 1 + divide start_ARG italic_α end_ARG start_ARG italic_η end_ARG italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG italic_d italic_μ start_POSTSUBSCRIPT bold_italic_A end_POSTSUBSCRIPT ( italic_d italic_x ) .',\n",
              " '',\n",
              " 'According to Marchenko-Pastur Law (Bai & Silverstein, 2010), in the limit when d→∞→𝑑d\\\\rightarrow\\\\inftyitalic_d → ∞ when η≤1𝜂1\\\\eta\\\\leq 1italic_η ≤ 1,',\n",
              " '',\n",
              " '1d\\u2062∥𝑴~-𝑰∥F2\\u2062=a.s.\\u206212\\u2062π\\u2062∫η-η+(η+-x)\\u2062(x-η-)η\\u2062x\\u2062(1+αη\\u2062x)2\\u2062𝑑x,1𝑑superscriptsubscriptnormbold-~𝑴𝑰𝐹2a.s.12𝜋superscriptsubscriptsubscript𝜂subscript𝜂subscript𝜂𝑥𝑥subscript𝜂𝜂𝑥superscript1𝛼𝜂𝑥2differential-d𝑥\\\\frac{1}{d}\\\\|\\\\boldsymbol{\\\\widetilde{M}}-\\\\boldsymbol{I}\\\\|_{F}^{2}\\\\overset{\\\\text%\\n{a.s.}}{=}\\\\frac{1}{2\\\\pi}\\\\int_{\\\\eta_{-}}^{\\\\eta_{+}}\\\\frac{\\\\sqrt{(\\\\eta_{+}-x)(x-%\\n\\\\eta_{-})}}{\\\\eta x(1+\\\\frac{\\\\alpha}{\\\\eta}x)^{2}}dx,divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ overbold_~ start_ARG bold_italic_M end_ARG - bold_italic_I ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT overa.s. start_ARG = end_ARG divide start_ARG 1 end_ARG start_ARG 2 italic_π end_ARG ∫ start_POSTSUBSCRIPT italic_η start_POSTSUBSCRIPT - end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_η start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG square-root start_ARG ( italic_η start_POSTSUBSCRIPT + end_POSTSUBSCRIPT - italic_x ) ( italic_x - italic_η start_POSTSUBSCRIPT - end_POSTSUBSCRIPT ) end_ARG end_ARG start_ARG italic_η italic_x ( 1 + divide start_ARG italic_α end_ARG start_ARG italic_η end_ARG italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG italic_d italic_x ,',\n",
              " '',\n",
              " 'where η+=(1+η)2subscript𝜂superscript1𝜂2\\\\eta_{+}=(1+\\\\sqrt{\\\\eta})^{2}italic_η start_POSTSUBSCRIPT + end_POSTSUBSCRIPT = ( 1 + square-root start_ARG italic_η end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, and η-=(1-η)2subscript𝜂superscript1𝜂2\\\\eta_{-}=(1-\\\\sqrt{\\\\eta})^{2}italic_η start_POSTSUBSCRIPT - end_POSTSUBSCRIPT = ( 1 - square-root start_ARG italic_η end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. For convenience, define',\n",
              " '',\n",
              " 'fα\\u2062(η)=12\\u2062π\\u2062∫η-η+(η+-x)\\u2062(x-η-)η\\u2062x\\u2062(1+αη\\u2062x)2\\u2062𝑑x=α+η\\u2062(1+η-2\\u2062α+η\\u2062α)2\\u2062η\\u2062η2+2\\u2062η\\u2062α\\u2062(1+η)+α2\\u2062(1-η)2-1-η2\\u2062η.subscript𝑓𝛼𝜂12𝜋superscriptsubscriptsubscript𝜂subscript𝜂subscript𝜂𝑥𝑥subscript𝜂𝜂𝑥superscript1𝛼𝜂𝑥2differential-d𝑥𝛼𝜂1𝜂2𝛼𝜂𝛼2𝜂superscript𝜂22𝜂𝛼1𝜂superscript𝛼2superscript1𝜂21𝜂2𝜂\\\\displaystyle f_{\\\\alpha}(\\\\eta)=\\\\frac{1}{2\\\\pi}\\\\int_{\\\\eta_{-}}^{\\\\eta_{+}}\\\\frac{%\\n\\\\sqrt{(\\\\eta_{+}-x)(x-\\\\eta_{-})}}{\\\\eta x(1+\\\\frac{\\\\alpha}{\\\\eta}x)^{2}}dx=\\\\frac{%\\n\\\\alpha+\\\\eta(1+\\\\eta-2\\\\alpha+\\\\eta\\\\alpha)}{2\\\\eta\\\\sqrt{\\\\eta^{2}+2\\\\eta\\\\alpha(1+\\\\eta%\\n)+\\\\alpha^{2}(1-\\\\eta)^{2}}}-\\\\frac{1-\\\\eta}{2\\\\eta}.italic_f start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( italic_η ) = divide start_ARG 1 end_ARG start_ARG 2 italic_π end_ARG ∫ start_POSTSUBSCRIPT italic_η start_POSTSUBSCRIPT - end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_η start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG square-root start_ARG ( italic_η start_POSTSUBSCRIPT + end_POSTSUBSCRIPT - italic_x ) ( italic_x - italic_η start_POSTSUBSCRIPT - end_POSTSUBSCRIPT ) end_ARG end_ARG start_ARG italic_η italic_x ( 1 + divide start_ARG italic_α end_ARG start_ARG italic_η end_ARG italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG italic_d italic_x = divide start_ARG italic_α + italic_η ( 1 + italic_η - 2 italic_α + italic_η italic_α ) end_ARG start_ARG 2 italic_η square-root start_ARG italic_η start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 italic_η italic_α ( 1 + italic_η ) + italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 - italic_η ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG - divide start_ARG 1 - italic_η end_ARG start_ARG 2 italic_η end_ARG .',\n",
              " '',\n",
              " 'When η>1𝜂1\\\\eta>1italic_η > 1,',\n",
              " '',\n",
              " '1d\\u2062∥𝑴~-𝑰∥F2=(1-1η)+fα\\u2062(1η).1𝑑superscriptsubscriptnormbold-~𝑴𝑰𝐹211𝜂subscript𝑓𝛼1𝜂\\\\frac{1}{d}\\\\|\\\\boldsymbol{\\\\widetilde{M}}-\\\\boldsymbol{I}\\\\|_{F}^{2}=\\\\left(1-\\\\frac%\\n{1}{\\\\eta}\\\\right)+f_{\\\\alpha}\\\\left(\\\\frac{1}{\\\\eta}\\\\right)\\\\\\\\\\n.divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ overbold_~ start_ARG bold_italic_M end_ARG - bold_italic_I ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = ( 1 - divide start_ARG 1 end_ARG start_ARG italic_η end_ARG ) + italic_f start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( divide start_ARG 1 end_ARG start_ARG italic_η end_ARG ) .',\n",
              " '',\n",
              " 'Then, in the asymptotic regime,',\n",
              " '',\n",
              " '1d\\u2062∥𝑴~-𝑰∥F2\\u2062=a.s.\\u2062{(1-1η)+fα\\u2062(1η),if\\xa0d>p,fα\\u2062(η),if\\xa0d<p.1𝑑superscriptsubscriptnormbold-~𝑴𝑰𝐹2a.s.cases11𝜂subscript𝑓𝛼1𝜂if\\xa0d>psubscript𝑓𝛼𝜂if\\xa0d<p\\\\frac{1}{d}\\\\|\\\\boldsymbol{\\\\widetilde{M}}-\\\\boldsymbol{I}\\\\|_{F}^{2}\\\\overset{\\\\text%\\n{a.s.}}{=}\\\\begin{cases}(1-\\\\frac{1}{\\\\eta})+f_{\\\\alpha}(\\\\frac{1}{\\\\eta}),&\\\\text{if%\\n $d>p$},\\\\\\\\\\nf_{\\\\alpha}(\\\\eta),&\\\\text{if $d<p$}.\\\\end{cases}divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ overbold_~ start_ARG bold_italic_M end_ARG - bold_italic_I ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT overa.s. start_ARG = end_ARG { start_ROW start_CELL ( 1 - divide start_ARG 1 end_ARG start_ARG italic_η end_ARG ) + italic_f start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( divide start_ARG 1 end_ARG start_ARG italic_η end_ARG ) , end_CELL start_CELL if italic_d > italic_p , end_CELL end_ROW start_ROW start_CELL italic_f start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( italic_η ) , end_CELL start_CELL if italic_d < italic_p . end_CELL end_ROW',\n",
              " '',\n",
              " '∎',\n",
              " '',\n",
              " '',\n",
              " 'Proposition 4 (Asymptotic Bias).',\n",
              " '\\nGiven the expression for Bias in Proposition 1, under the asymptotic assumptions in Assumption 1, the Bias for the model is given by',\n",
              " '',\n",
              " '1d\\u2062∥𝔼\\u2062𝑴-𝑰∥2=[1-λ0\\u2062η+(1+η)-λ02\\u2062η2+2\\u2062λ0\\u2062η\\u2062(1+η)+(1-η)22\\u2062η]2.1𝑑superscriptnorm𝔼𝑴𝑰2superscriptdelimited-[]1subscript𝜆0𝜂1𝜂superscriptsubscript𝜆02superscript𝜂22subscript𝜆0𝜂1𝜂superscript1𝜂22𝜂2\\\\displaystyle\\\\frac{1}{d}\\\\|\\\\mathbb{E}\\\\boldsymbol{M}-\\\\boldsymbol{I}\\\\|^{2}=\\\\Big{[%\\n}1-\\\\frac{\\\\lambda_{0}\\\\eta+(1+\\\\eta)-\\\\sqrt{\\\\lambda_{0}^{2}\\\\eta^{2}+2\\\\lambda_{0}%\\n\\\\eta(1+\\\\eta)+(1-\\\\eta)^{2}}}{2\\\\eta}\\\\Big{]}^{2}.divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ blackboard_E bold_italic_M - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = [ 1 - divide start_ARG italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_η + ( 1 + italic_η ) - square-root start_ARG italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_η start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_η ( 1 + italic_η ) + ( 1 - italic_η ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG start_ARG 2 italic_η end_ARG ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .',\n",
              " '',\n",
              " '',\n",
              " 'Proof.',\n",
              " 'Recall that',\n",
              " '',\n",
              " '𝑴=𝑾⊤\\u2062(𝑾\\u2062𝑿\\u2062𝑿⊤\\u2062𝑾⊤+λ\\u2062𝑰)-1\\u2062𝑾\\u2062𝑿\\u2062𝑿⊤.𝑴superscript𝑾topsuperscript𝑾𝑿superscript𝑿topsuperscript𝑾top𝜆𝑰1𝑾𝑿superscript𝑿top\\\\boldsymbol{M}=\\\\boldsymbol{W}^{\\\\top}(\\\\boldsymbol{W}\\\\boldsymbol{X}\\\\boldsymbol{X%\\n}^{\\\\top}\\\\boldsymbol{W}^{\\\\top}+\\\\lambda\\\\boldsymbol{I})^{-1}\\\\boldsymbol{W}%\\n\\\\boldsymbol{X}\\\\boldsymbol{X}^{\\\\top}.bold_italic_M = bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_W bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_λ bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT .',\n",
              " '',\n",
              " 'Recall that 𝑴~=𝑰-(𝑰+λ0-1\\u2062𝑾⊤\\u2062𝑾)-1bold-~𝑴𝑰superscript𝑰superscriptsubscript𝜆01superscript𝑾top𝑾1\\\\boldsymbol{\\\\widetilde{M}}=\\\\boldsymbol{I}-(\\\\boldsymbol{I}+\\\\lambda_{0}^{-1}%\\n\\\\boldsymbol{W}^{\\\\top}\\\\boldsymbol{W})^{-1}overbold_~ start_ARG bold_italic_M end_ARG = bold_italic_I - ( bold_italic_I + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT. Thus',\n",
              " '',\n",
              " '1d\\u2062∥𝔼\\u2062𝑴~-𝑰∥2=1d\\u2062∥𝔼\\u2062(𝑰+λ0-1\\u2062𝑾⊤\\u2062𝑾)-1∥2.1𝑑superscriptnorm𝔼bold-~𝑴𝑰21𝑑superscriptnorm𝔼superscript𝑰superscriptsubscript𝜆01superscript𝑾top𝑾12\\\\frac{1}{d}\\\\|\\\\mathbb{E}\\\\boldsymbol{\\\\widetilde{M}}-\\\\boldsymbol{I}\\\\|^{2}=\\\\frac{1%\\n}{d}\\\\|\\\\mathbb{E}(\\\\boldsymbol{I}+\\\\lambda_{0}^{-1}\\\\boldsymbol{W}^{\\\\top}%\\n\\\\boldsymbol{W})^{-1}\\\\|^{2}.divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ blackboard_E overbold_~ start_ARG bold_italic_M end_ARG - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ blackboard_E ( bold_italic_I + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .',\n",
              " '',\n",
              " 'By Neumann series,',\n",
              " '',\n",
              " '𝔼\\u2062(𝑰+λ0-1\\u2062𝑾⊤\\u2062𝑾)-1=∑m≥0𝔼\\u2062(-λ0-1\\u2062𝑾⊤\\u2062𝑾)m=𝑰+∑m≥1(-1)m\\u2062(λ0\\u2062η)-m\\u2062𝔼\\u2062𝑨m,𝔼superscript𝑰superscriptsubscript𝜆01superscript𝑾top𝑾1subscript𝑚0𝔼superscriptsuperscriptsubscript𝜆01superscript𝑾top𝑾𝑚𝑰subscript𝑚1superscript1𝑚superscriptsubscript𝜆0𝜂𝑚𝔼superscript𝑨𝑚\\\\displaystyle\\\\mathbb{E}(\\\\boldsymbol{I}+\\\\lambda_{0}^{-1}\\\\boldsymbol{W}^{\\\\top}%\\n\\\\boldsymbol{W})^{-1}=\\\\sum_{m\\\\geq 0}\\\\mathbb{E}(-\\\\lambda_{0}^{-1}\\\\boldsymbol{W}^%\\n{\\\\top}\\\\boldsymbol{W})^{m}=\\\\boldsymbol{I}+\\\\sum_{m\\\\geq 1}(-1)^{m}(\\\\lambda_{0}%\\n\\\\eta)^{-m}\\\\mathbb{E}\\\\boldsymbol{A}^{m},blackboard_E ( bold_italic_I + italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_m ≥ 0 end_POSTSUBSCRIPT blackboard_E ( - italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W ) start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = bold_italic_I + ∑ start_POSTSUBSCRIPT italic_m ≥ 1 end_POSTSUBSCRIPT ( - 1 ) start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ( italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_η ) start_POSTSUPERSCRIPT - italic_m end_POSTSUPERSCRIPT blackboard_E bold_italic_A start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ,',\n",
              " '',\n",
              " 'where η=d/p,𝑨=(d/p)\\u2062𝑾⊤\\u2062𝑾formulae-sequence𝜂𝑑𝑝𝑨𝑑𝑝superscript𝑾top𝑾\\\\eta={d}/{p},\\\\boldsymbol{A}=(d/p)\\\\boldsymbol{W}^{\\\\top}\\\\boldsymbol{W}italic_η = italic_d / italic_p , bold_italic_A = ( italic_d / italic_p ) bold_italic_W start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_W. According to Corollary 3.3 in Bishop et\\xa0al. (2018) (recall we are considering the asymptotic regime of d,p→∞→𝑑𝑝d,p\\\\rightarrow\\\\inftyitalic_d , italic_p → ∞),',\n",
              " '',\n",
              " '𝔼\\u2062𝑨m=∑k=1mηm-k\\u2062Nm,k⋅𝑰,𝔼superscript𝑨𝑚superscriptsubscript𝑘1𝑚⋅superscript𝜂𝑚𝑘subscript𝑁𝑚𝑘𝑰\\\\mathbb{E}\\\\boldsymbol{A}^{m}=\\\\sum_{k=1}^{m}\\\\eta^{m-k}N_{m,k}\\\\cdot\\\\boldsymbol{I},blackboard_E bold_italic_A start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_η start_POSTSUPERSCRIPT italic_m - italic_k end_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_m , italic_k end_POSTSUBSCRIPT ⋅ bold_italic_I ,',\n",
              " '',\n",
              " 'where',\n",
              " '',\n",
              " 'Nm,k=1k\\u2062(m-1k-1)\\u2062(mk-1)subscript𝑁𝑚𝑘1𝑘binomial𝑚1𝑘1binomial𝑚𝑘1N_{m,k}=\\\\frac{1}{k}\\\\binom{m-1}{k-1}\\\\binom{m}{k-1}italic_N start_POSTSUBSCRIPT italic_m , italic_k end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_k end_ARG ( FRACOP start_ARG italic_m - 1 end_ARG start_ARG italic_k - 1 end_ARG ) ( FRACOP start_ARG italic_m end_ARG start_ARG italic_k - 1 end_ARG )',\n",
              " '',\n",
              " 'is the Narayana number. Therefore,',\n",
              " '',\n",
              " '1d\\u2062∥𝔼\\u2062𝑴~-𝑰∥2=(1+η-1\\u2062∑m=1∞∑k=1k(-λ0-1)m\\u2062(η-1)k-1\\u2062Nm,k)2.1𝑑superscriptnorm𝔼bold-~𝑴𝑰2superscript1superscript𝜂1superscriptsubscript𝑚1superscriptsubscript𝑘1𝑘superscriptsuperscriptsubscript𝜆01𝑚superscriptsuperscript𝜂1𝑘1subscript𝑁𝑚𝑘2\\\\frac{1}{d}\\\\|\\\\mathbb{E}\\\\boldsymbol{\\\\widetilde{M}}-\\\\boldsymbol{I}\\\\|^{2}=\\\\Big{(}%\\n1+\\\\eta^{-1}\\\\sum_{m=1}^{\\\\infty}\\\\sum_{k=1}^{k}(-\\\\lambda_{0}^{-1})^{m}(\\\\eta^{-1})%\\n^{k-1}N_{m,k}\\\\Big{)}^{2}.divide start_ARG 1 end_ARG start_ARG italic_d end_ARG ∥ blackboard_E overbold_~ start_ARG bold_italic_M end_ARG - bold_italic_I ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = ( 1 + italic_η start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( - italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ( italic_η start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_m , italic_k end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .',\n",
              " '',\n",
              " 'Observe that the double sum in the previous equation is just the generating series for the Narayana number,',\n",
              " '',\n",
              " '∑m=1∞∑k=1k(-λ0-1)m\\u2062(η-1)k-1\\u2062Nm,k=-λ0\\u2062η+(1+η)-λ02\\u2062η2+2\\u2062λ0\\u2062η\\u2062(1+η)+(1-η)22\\u2062η.superscriptsubscript𝑚1superscriptsubscript𝑘1𝑘superscriptsuperscriptsubscript𝜆01𝑚superscriptsuperscript𝜂1𝑘1subscript𝑁𝑚𝑘subscript𝜆0𝜂1𝜂superscriptsubscript𝜆02superscript𝜂22subscript𝜆0𝜂1𝜂superscript1𝜂22𝜂\\\\displaystyle\\\\sum_{m=1}^{\\\\infty}\\\\sum_{k=1}^{k}(-\\\\lambda_{0}^{-1})^{m}(\\\\eta^{-1%\\n})^{k-1}N_{m,k}=-\\\\frac{\\\\lambda_{0}\\\\eta+(1+\\\\eta)-\\\\sqrt{\\\\lambda_{0}^{2}\\\\eta^{2}+%\\n2\\\\lambda_{0}\\\\eta(1+\\\\eta)+(1-\\\\eta)^{2}}}{2\\\\eta}.∑ start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( - italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ( italic_η start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_m , italic_k end_POSTSUBSCRIPT = - divide start_ARG italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_η + ( 1 + italic_η ) - square-root start_ARG italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_η start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_η ( 1 + italic_η ) + ( 1 - italic_η ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG start_ARG 2 italic_η end_ARG .',\n",
              " '',\n",
              " 'This completes the proof.\\n∎',\n",
              " '',\n",
              " '\\nFinally, the statement of Theorem 1 follows directly from the above propositions.',\n",
              " '',\n",
              " '\\n◄',\n",
              " 'Feelinglucky?\\nConversionreport\\nReportan issue\\nView\\xa0originalon\\xa0arXiv►',\n",
              " '\\nGenerated  on Tue Dec 28 19:31:08 2021 by LaTeXML ',\n",
              " '',\n",
              " '      var canMathML = typeof(MathMLElement) == \"function\";\\n      if (!canMathML) {\\n        var body = document.querySelector(\"body\");\\n        body.firstElementChild.setAttribute(\\'style\\', \\'opacity: 0;\\');\\n        var loading = document.createElement(\"div\");\\n        loading.setAttribute(\"id\", \"mathjax-loading-spinner\");\\n        var message = document.createElement(\"div\");\\n        message.setAttribute(\"id\", \"mathjax-loading-message\");\\n        message.innerText = \"Typesetting Equations...\";\\n        body.prepend(loading);\\n        body.prepend(message);',\n",
              " '        var el = document.createElement(\"script\");\\n        el.src = \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\";\\n        document.querySelector(\"head\").appendChild(el);',\n",
              " \"        window.MathJax = {\\n          startup: {\\n            pageReady: () => {\\n              return MathJax.startup.defaultPageReady().then(() => {\\n                body.removeChild(loading);\\n                body.removeChild(message);\\n                body.firstElementChild.removeAttribute('style');\\n              }); } } };\\n      }      \\n    \",\n",
              " '    // Auxiliary function, building the preview feature when\\n    // an inline citation is clicked\\n    function clicked_cite(e) {\\n      e.preventDefault();\\n      let cite = this.closest(\\'.ltx_cite\\');\\n      let next = cite.nextSibling;\\n      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute(\\'class\\') == \"ar5iv-bibitem-preview\") {\\n        next.remove();\\n        return; }\\n      // Before adding a preview modal,\\n      // cleanup older previews, in case they\\'re still open\\n      document.querySelectorAll(\\'span.ar5iv-bibitem-preview\\').forEach(function(node) {\\n        node.remove();\\n      })\\n      \\n      // Create the preview\\n      preview = document.createElement(\\'span\\');\\n      preview.setAttribute(\\'class\\',\\'ar5iv-bibitem-preview\\');\\n      let target = document.getElementById(this.getAttribute(\\'href\\').slice(1));\\n      target.childNodes.forEach(function (child) {\\n        preview.append(child.cloneNode(true));\\n      });\\n      let close_x = document.createElement(\\'button\\');\\n      close_x.setAttribute(\"aria-label\",\"Close modal for bibliography item preview\");\\n      close_x.textContent = \"×\";\\n      close_x.setAttribute(\\'class\\', \\'ar5iv-button-close-preview\\');\\n      close_x.setAttribute(\\'onclick\\',\\'this.parentNode.remove()\\');\\n      preview.append(close_x);\\n      preview.querySelectorAll(\\'.ltx_tag_bibitem\\').forEach(function(node) {\\n        node.remove();\\n      });\\n      cite.parentNode.insertBefore(preview, cite.nextSibling);\\n      return;\\n    }\\n    // Global Document initialization:\\n    // - assign the preview feature to all inline citation links\\n    document.querySelectorAll(\".ltx_cite .ltx_ref\").forEach(function (link) {\\n      link.addEventListener(\"click\", clicked_cite);\\n    });\\n    ',\n",
              " '\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7UJQiQOXIfNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L_XeO02oIfP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract text from AI Alignment Resources"
      ],
      "metadata": {
        "id": "q4WhBM5lrPyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tikaTextExtractor(file_path):\n",
        "    \"\"\"Extracts text from a PDF using tika.\"\"\"\n",
        "    print(\"Extracting text from file: \" + file_path)\n",
        "    parsed_tika = parser.from_file(file_path)\n",
        "    return parsed_tika[\"content\"]"
      ],
      "metadata": {
        "id": "GtcCbZzznPE3"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tikaTextExtractor(str(RAW_DIR / 'pdfs' / '2112.00861.pdf'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "Bvbi8ZEqnWTJ",
        "outputId": "ceab5749-81cb-4058-f43b-515bc286b93a"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from file: data/raw/pdfs/2112.00861.pdf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA General Language Assistant\\nas a Laboratory for Alignment\\n\\nAmanda Askell∗ Yuntao Bai∗ Anna Chen∗ Dawn Drain∗ Deep Ganguli∗ Tom Henighan†\\n\\nAndy Jones† Nicholas Joseph† Ben Mann∗ Nova DasSarma Nelson Elhage\\n\\nZac Hatfield-Dodds Danny Hernandez Jackson Kernion Kamal Ndousse\\n\\nCatherine Olsson Dario Amodei Tom Brown Jack Clark Sam McCandlish Chris Olah\\n\\nJared Kaplan‡\\n\\nAnthropic\\n\\nAbstract\\n\\nGiven the broad capabilities of large language models, it should be possible to work towards\\na general-purpose, text-based assistant that is aligned with human values, meaning that it is\\nhelpful, honest, and harmless. As an initial foray in this direction we study simple baseline\\ntechniques and evaluations, such as prompting. We find that the benefits from modest\\ninterventions increase with model size, generalize to a variety of alignment evaluations, and\\ndo not compromise the performance of large models. Next we investigate scaling trends\\nfor several training objectives relevant to alignment, comparing imitation learning, binary\\ndiscrimination, and ranked preference modeling. We find that ranked preference modeling\\nperforms much better than imitation learning, and often scales more favorably with model\\nsize. In contrast, binary discrimination typically performs and scales very similarly to\\nimitation learning. Finally we study a ‘preference model pre-training’ stage of training,\\nwith the goal of improving sample efficiency when finetuning on human preferences.\\n\\n∗Core Research Contributors\\n†Core Infrastructure Contributors\\n‡Correspondence to: jared@anthropic.com\\n\\nAuthor contributions are listed at the end of the paper.\\n\\nar\\nX\\n\\niv\\n:2\\n\\n11\\n2.\\n\\n00\\n86\\n\\n1v\\n3 \\n\\n [\\ncs\\n\\n.C\\nL\\n\\n] \\n 9\\n\\n D\\nec\\n\\n 2\\n02\\n\\n1\\n\\n\\n\\nContents\\n\\n1 Introduction 3\\n\\n1.1 Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n\\n1.2 Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n\\n1.3 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n\\n2 Conditioning on Aligned Behavior 9\\n\\n2.1 Context Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n\\n2.2 Evaluations and Alignment Taxes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n\\n3 Scaling of Preference Modeling vs Imitation Learning 14\\n\\n3.1 Loss and Settings for Preference Modeling and Imitation Learning . . . . . . . . . . . . . . 15\\n\\n3.2 Performance and Scaling Results for Ranked versus Binary Preference Datasets . . . . . . . 16\\n\\n4 Preference Model Pre-Training and Transfer 20\\n\\n4.1 PMP and Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n\\n4.2 Finetuning Results and Scaling Trends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n\\n4.3 Ranked Preference Modeling vs Binary Discrimination for PMP . . . . . . . . . . . . . . . 23\\n\\n4.4 Human-Model vs Human-Human Comparisons for PMP . . . . . . . . . . . . . . . . . . . 24\\n\\n5 Discussion 24\\n\\n5.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n\\n5.2 Broader Impacts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n\\n5.3 Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n\\nA Language Model Pre-training 27\\n\\nB More Details on Prompting, Context Distillation, and Evaluations 29\\n\\nC More Details on Preference Models 34\\n\\nD Per-Token GAN-Style Discriminator Results 40\\n\\nE Definitions of Alignment and the HHH criteria 44\\n\\n2\\n\\n\\n\\nFigure 1 We show the format of interactions with AI models for A/B testing and human feedback collection.\\nAs indicated by the example interaction here, one can get help from the model with any text-based task.\\n\\n1 Introduction\\n\\n1.1 Motivations\\n\\nContemporary AI models can be difficult to understand, predict, and control. These problems can lead\\nto significant harms when AI systems are deployed, and might produce truly devastating results if future\\nsystems are even more powerful and more widely used, and interact with each other and the world in presently\\nunforeseeable ways.\\n\\nThis paper shares some nascent work towards one of our primary, ongoing goals, which is to align general-\\npurpose AI systems with human preferences and values. A great deal of ink has been spilled trying to define\\nwhat it means for AI systems to be aligned, and to guess at how this might go wrong. We will define an AI\\nas “aligned” if it is, in three words, helpful, honest, and harmless or ‘HHH’. Our alignment efforts aim to\\nmeasure and address this general problem with large language models.\\n\\nMany researchers and organizations share this goal, but few have pursued it directly. Most research efforts\\nassociated with alignment either only pertain to very specialized systems, involve testing a specific alignment\\ntechnique on a sub-problem, or are rather speculative and theoretical. Our view is that if it’s possible to\\ntry to address a problem directly, then one needs a good excuse for not doing so. Historically we had such\\nan excuse: general purpose, highly capable AIs were not available for investigation. But given the broad\\ncapabilities of large language models, we think it’s time to tackle alignment directly, and that a research\\nprogram focused on this goal may have the greatest chance for impact. Furthermore:\\n\\n• A natural language agent can be subjected to a wide variety of inputs, and so it can fail to be\\nhelpful, honest, and harmless in myriad ways. We believe it’s valuable to try to see the full picture\\nof where we’ve made progress on alignment, and where we’re currently falling short. This may\\nremain obscure absent efforts to train general aligned agents and allow them to be probed in any way\\nwhatsoever. A very broad definition can also facilitate measurement, since it invites the examiner to\\npose a wide-variety of challenges.\\n\\n3\\n\\n\\n\\n• By studying a variety of alignment techniques in a general setting, it becomes much easier to com-\\npare them and to determine which techniques are simplest and most effective. Some techniques,\\nsuch as the use of human feedback, are complex and potentially costly, so we’re interested in strate-\\ngies that can increase their efficiency and focus their application exclusively on goals that cannot be\\nattained more easily in another way.\\n\\n• Some view alignment as a highly speculative problem, or one that distracts from work on more\\npressing issues with existing AI systems. In our view, the societal impacts of current AI models\\nshould be taken seriously, and the evaluation of current models should be seen as an essential safety\\nproject. We believe that training a large language model to be helpful, honest, and harmless (we are\\nnot claiming to have achieved this goal!) would represent significant progress towards alleviating\\nthe negative societal impacts from general-purpose language models.\\n\\n• Some of the researchers who are most concerned about the alignment problem believe that aligning\\nextremely capable AIs will be qualitatively different from aligning current more limited systems.\\nWe share this concern, but we believe the best vantage point from which to explore alignment for\\nincreasingly advanced AIs will be to first establish an aligned baseline at current capability levels. If\\nthis were successful, we would then turn to the task of studying progress more deeply, including its\\nscaling properties, and attempt to adversarially validate it. Conversely, if we and others persistently\\nfail, we can identify the thorniest issues with alignment. Halting progress would also provide a\\npersuasive argument for allocating more and more resources towards AI alignment, and for more\\ncautious norms around scaling up and deploying models.\\n\\nIn pursuit of these goals, in this work we will be investigating the following questions:\\n\\n• Is naive prompting a workable baseline for alignment? How does it scale, how does it compare\\nto finetuning, and how can we leverage its advantages? We find that prompts induce favorable\\nscaling on a variety of alignment-relevant evaluations, impose negligible ‘taxes’ on large models,\\nand can be ‘context distilled’ back into the original model.\\n\\n• When and how much does preference modeling improve on imitation learning? We find that\\npreference modeling improves on and scales more favorably than imitation learning when prefer-\\nences are part of a ranked hierarchy or continuum (e.g. rank these responses in order of helpfulness),\\nrather than associated with a binary choice (e.g. does this python function pass tests).\\n\\n• How can we improve the sample efficiency of preference modeling? We find that we can signif-\\nicantly improve sample efficiency using a ‘preference model pre-training’ (PMP) stage of training,\\nwhere we first pre-train on large public datasets that encode human preference information, such as\\nStack Exchange, Reddit, and Wikipedia edits, before finetuning on smaller datasets encoding more\\nspecific human preferences.\\n\\nThe last two points are particularly important for work using reinforcement learning (RL) for alignment,\\nwhere the reward signals are predicted by a preference model. In particular, we expect bandit-type RL perfor-\\nmance to improve roughly in proportion with preference modeling capabilities, since the preference model’s\\nrecognition of high-performance behavior should be closely related to the RL agent’s ability to achieve it. We\\nanticipate that such a strategy can outperform imitation learning on some problems, especially those whose\\nsolutions lie on a ranked hierarchy. A similar approach applying human feedback to greatly improve the\\nperformance of language models on summary-writing had already been demonstrated [SOW+20].\\n\\nWhat are Helpfulness, Honesty, and Harmlessness?\\n\\nWe chose ‘helpful, honest, and harmless’ as criteria because they are simple and memorable, and seem to\\ncapture the majority of what we want from an aligned1 AI. But these are also subtle and ambiguous criteria,\\nand the best AI behavior will involve a compromise between them. For example, there will clearly be conflicts\\nbetween helpfulness to the user and harmlessness to others if agents are asked to aid in harmful activities.\\nHere are some very brief notes on these terms:\\n\\nHelpful:\\n\\n• The AI should make a clear attempt to perform the task or answer the question posed (as long as this\\nisn’t harmful). It should do this as concisely and efficiently as possible.\\n\\n1We also discuss the HHH specification in relation to our broader understanding of alignment in Appendix E.\\n\\n4\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\nFr\\nac\\n\\ntio\\nn \\n\\nPa\\nss\\n\\nin\\ng \\n\\nTe\\nst\\n\\ns\\n\\nCodex Evaluations and Alignment Tax/Bonus\\nCodex Pass@10\\nCodex w/ HHH Prompt Pass@10\\n\\nFigure 2 Left: Simple prompting significantly improves performance and scaling on our HHH alignment\\nevaluations (y-axis measures accuracy at choosing better responses on our HHH evaluations). Right: Prompts\\nimpose little or no ‘alignment tax’ on large models, even on complex evaluations like function synthesis.\\nHere we have evaluated our python code models on the HumanEval codex dataset [CTJ+21] at temperature\\nT = 0.6 and top P = 0.95.\\n\\n• When more information is required, the AI should ask relevant follow-up questions and obtain nec-\\nessary details. It should respond with appropriate levels of sensitivity, insight, and discretion.\\n\\n• Ideally the AI will also re-direct ill-informed requests, e.g. if asked ‘how can I build a website in\\nassembly language’ it might suggest a different approach.\\n\\nHonest:\\n\\n• At its most basic level, the AI should give accurate information. Moreover, it should be calibrated\\n(e.g. it should be correct 80% of the time when it claims 80% confidence) and express appropriate\\nlevels of uncertainty. It should express its uncertainty without misleading human users.\\n\\n• Crucially, the AI should be honest about its own capabilities and levels of knowledge – it is not\\nsufficient for it to simply imitate the responses expected from a seemingly humble and honest expert.\\n\\n• Ideally the AI would also be honest about itself and its own internal state, insofar as that information\\nis available to it.\\n\\n• Honesty is more objective than helpfulness and harmlessness, so more aspects of honesty training\\nmay be possible without human input. This might include calibration training on factual claims and\\nclaims about the internal state of the model, and the use of search [KSW21] to augment accuracy.\\n\\nHarmless:\\n\\n• The AI should not be offensive or discriminatory, either directly or through subtext or bias.\\n• When asked to aid in a dangerous act (e.g. building a bomb), the AI should politely refuse. Ideally\\n\\nthe AI will recognize disguised attempts to solicit help for nefarious purposes.\\n• To the best of its abilities, the AI should recognize when it may be providing very sensitive or\\n\\nconsequential advice and act with appropriate modesty and care.\\n• What behaviors are considered harmful and to what degree will vary across people and cultures. It\\n\\nwill also be context-dependent, i.e. it will depend on the nature of the user query, who is using the\\nAI assistant, and the time and place in which the assistant is being used.\\n\\nAll of these criteria are at least somewhat subjective, and those who deploy an AI will need to take responsi-\\nbility for the way that alignment is defined and the extent to which it has been attained.\\n\\n1.2 Research\\n\\nOpen-Ended Dialogue Format and Prompting\\n\\nWe use open-ended natural language dialogue for interaction with our models, with an example pictured in\\nfigure 1. We allow for general inputs of essentially arbitrary length from human users, which can include\\n\\n5\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\n0.30\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Preference Modeling Over Imitation Learning\\nMean Over Ranked Evals\\nMean Over Binary Evals\\n\\nFigure 3 In this figure the y-axis measures the accuracy difference of preference modeling compared to\\nimitation learning, where evaluations have been categorized as having either ranked or binary preferences.\\nThe light blue curves show ranked evaluations from Learn to Summarize, HellaSwag, and Utilitarianism\\n(ethics); while light orange curves show binary evaluations from Code Correctness, Lambada, Commonsense\\nMorality (ethics), Justice (ethics), Deontology (ethics), and Virtue (ethics). Dark colored curves show the\\nmean over light curves of the same color. All these datasets are evaluated by some form of accuracy, although\\nthe specific interpretation is different in each case (e.g., multiple choice accuracy for HellaSwag, pairwise\\ncomparison accuracy for Learn to Summarize; see section 3.2). We see that on ranked evaluations, PM\\nperforms and scales significantly better than IL (blue), while on binary evaluations there is little discernible\\ndifference (orange). The 52B Code Correctness is excluded due to significant compute needed to generate\\ncode samples.\\n\\nexamples, documents, programming code, etc, and we allow similarly general responses from our models.\\nModels indicate they have completed a response by generating a stop sequence, which is literally the string\\nHuman: used to designate roles in the dialogue. By default we show two responses and allow users to\\nchoose one. We typically request that users pick the most helpful and honest response, as pictured. We use\\nthis interface both to A/B test different models and to collect human feedback data. We can use a very similar\\ninterface for other safety-related tasks, such as red-teaming the model against harmfulness.\\n\\nTo evaluate performance we created a small dataset of evaluations associated with helpfulness, honesty,\\nharms, and other behaviors in this interactive format. We are sharing these evaluations on BIG Bench for oth-\\ners to try. We also evaluate models and interventions via A/B testing with humans, who have been instructed\\nto solicit models’ help with arbitrary text-based tasks.\\n\\nLarge language models engage in few-shot learning [BMR+20]. To generically elicit the sort of behavior\\nshown in figure 1, we found that it was sufficient to provide a long prompt (4600 words from 14 fictional\\nconversations) with example interactions. The prompt we used was not carefully designed or optimized for\\nperformance on evaluations; rather it was just written by two of us in an ad hoc manner prior to the construc-\\ntion of any evaluations. Despite the fact that our prompt2 did not include any examples where models resisted\\nmanipulation, refused requests to aid in dangerous activities, or took a stand against unsavory behavior, we\\nobserved that models often actively avoided engaging in harmful behaviors based only on the AI ‘personality’\\nimbued by the prompt. This is reflected in the performance trends on harmfulness in figure 6.\\n\\nIn section 2 we explore the effects of the prompt. In the small data limit, prompting a generative language\\nmodel may be qualitatively different from and superior to finetuning, since prompting imposes a prior, while\\nfinetuning alters the model’s expectations for the underlying data distribution. We make several points con-\\ncerning prompting:\\n\\n• We find that prompting can be superior to finetuning in the limit of very small datasets associated\\nwith alignment.\\n\\n2Prompt text and contractor instructions are at https://gist.github.com/jareddk/2509330f8ef3d787fc5aaac67aab5f11\\n\\n6\\n\\nhttps://github.com/google/BIG-bench\\nhttps://gist.github.com/jareddk/2509330f8ef3d787fc5aaac67aab5f11\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.00\\n\\n0.02\\n\\n0.04\\n\\n0.06\\n\\n0.08\\n\\n0.10\\n\\n0.12\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nMean Acc Gain of PMP on Finetuning (52B)\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\nFigure 4 Performance gain of preference model pre-training on finetuning evaluations, as measured by\\naccuracy difference relative to no PMP. Different colors represent different PMP datasets, including Stack-\\nExchange, Reddit, Wikipedia, and a ‘Mix’ of all three. Each line represents a combined (mean) result from\\nLearn to Summarize, HellaSwag, and all five Ethics evaluations. Results are shown for the 52B parameter\\nmodel only, but similar positive results were also seen for the smaller models.\\n\\n• The prompt context ‘C’ can be distilled into a new language model that models the distribution\\nP (X|C) instead of P (X); this is accomplished by simply finetuning with a loss given by the KL di-\\nvergence between P (X|C) and the distilled model’s predictions. This procedure has more beneficial\\neffects as compared to finetuning on the prompt.\\n\\n• The capabilities of small models (e.g. on NLP or coding evaluations) are typically diminished in the\\npresence of the prompt, presumably because they are confused by it. But larger models perform at\\nroughly the same level with or without the prompt.\\n\\nSo perhaps prompt-related techniques can carry alignment efforts further than we initially expected.\\n\\nNevertheless, we believe that as an approach to alignment, prompt design will have significant limitations.\\nOne concern is that prompts may only be capable of teaching the model to imitate some interpolation of the\\ntraining distribution, and so will not lead the model to exceed the performance demonstrated in the training\\nset. Concretely, we want the model to be honest about itself and its specific capability level rather than\\npresenting an honest-seeming facade in imitation of its training data (e.g. implying that it is able to book a\\nflight). Advanced AI models may also be trained using a mixture of generative modeling, supervised learning,\\nreinforcement learning, and other techniques. Prompt design may not carry over so straightforwardly after\\ngenerative models are re-purposed for other tasks.\\n\\nScaling of Imitation Learning vs Preference Modeling, and Binary vs Rank-Ordered Preferences\\n\\nBeyond prompt design, the next simplest technique is imitation learning from expert examples. But the\\nslightly more complex technique of learning distinctions3 among preferences—not just what to do but also\\nwhat not to do—may be more promising. We are interested in when this more involved approach improves\\non imitation learning, and how each scales with model size.\\n\\nWe find that there seems to be a qualitative distinction between two types of tasks:\\n\\n• Binary Discrimination, where the data has only two possible labels, such as pass/fail or true/false;\\nsome examples include determining if python code passes tests, or determining if an action is\\nmorally acceptable or unacceptable\\n\\n3Note that if such data is not available, there is an option to generate it, since expert examples can be compared with\\nsamples from a model – i.e. we can train a GAN-style discriminator.\\n\\n7\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.30\\n\\n0.35\\n\\n0.40\\n\\n0.45\\n\\n0.50\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nMean Transfer Performance at 500 Finetuning Seq Pairs\\nNo PMP\\nPMP Mix\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nMean Transfer Performance at 5k Finetuning Seq Pairs\\nNo PMP\\nPMP Mix\\n\\nFigure 5 Transfer performance at 500 and 5k sequence pairs on downstream finetuning evaluations with\\nPMP (on the ‘Mix’ dataset, shown in violet) vs. without PMP (black). Each curve is averaged across fine-\\ntuning evaluations Learn to Summarize, HellaSwag, and all five Ethics evaluations. We see that PMP signifi-\\ncantly improves sample efficiency with large models.\\n\\n• Ranked Preference Modeling among a tall hierarchy of possibilities, with examples including the\\npopularity of a StackExchange answer, or the quality of a paragraph summary. Note that rankings\\ncan be learned from pairwise comparisons even though the underlying data has a ranked ordering.\\nLearning from human preferences [CLB+17] and T-REX IRL [BGNN19] learn from ranked data.\\n\\nAs shown in the introductory figure 3, we find that preference modeling performs much better and scales\\nsomewhat better than imitation learning, but that binary discrimination does not.\\n\\nPreference Model Pre-Training\\n\\nModels that learn to discriminate and rank human preferences play a natural role in alignment research.\\nSuch models can be used as filters, and they can also be leveraged more powerfully as preference models\\nfor reinforcement learning from human feedback (RLHF) [CLB+17], in order to train aligned policies. Fur-\\nthermore, some proposals [CSA18, ICA18] for aligning more advanced AIs use different models to train or\\nevaluate each other, so that the effectiveness and reliability of these techniques may ultimately depend on the\\nperformance and robustness of preference models.\\n\\nPreference modeling success may be hampered by small datasets, since a natural way to train these models\\nis through human feedback on samples generated from a policy, as in RLHF or human-in-the-loop training,\\nand high-quality human interaction data may be expensive. Thus a significant consideration is whether we\\ncan improve the sample efficiency of these models. For this purpose we experiment with preference model\\npretraining (PMP), so that the full training procedure includes training sequentially on:\\n\\nLanguage Model Pre-training→ Preference Model Pre-training→ Preference Model Finetuning\\n\\nFor the second stage, we utilize large scale public data from Stack Exchange, Reddit, and reverted vandalism4\\nof Wikipedia. We find that this PMP stage of training significantly improves sample efficiency and often\\nimproves the asymptotic performance when preference models are finetuned on both human feedback datasets\\nor various alignment-focused datasets.\\n\\nIn appendices we discuss details of model training and dataset preparation and some additional experiments\\nwith GAN-style discriminator.\\n\\nModels\\n\\nThroughout this paper we will be studying a consistent set of decoder-only Transformer language models\\nwith parameter counts ranging from about 10M to 52B in increments of 4x, and with a fixed context window\\nof 8192 tokens and a 216 token vocabulary. For language model pre-training, these models are trained for\\n400B tokens on a distribution consisting mostly of filtered Common Crawl data [Fou] and internet books,\\nalong with a number of smaller distributions [GBB+20], including about 10% python code data. We fix the\\n\\n4By this we mean that we specifically sourced changes to Wikipedia that were noted as such and quickly reverted.\\n\\n8\\n\\n\\n\\naspect ratio of our models so that the activation dimension dmodel = 128nlayer, and include models with\\n13M, 42M, 197M, 810M, 2.7B, 13B, and 52B non-embedding parameters. Throughout the paper we will\\nshow results and comparisons as a function of model size, and by ‘Number of Parameters’ we will always\\nmean non-embedding parameters.\\n\\nIn some places we will also study the properties of these models after they have been finetuned on a pure dis-\\ntribution of python code. We also discuss finetuning on a variety of other datasets, including with additional\\nheads that can make real-valued predictions at all token positions. Most of these finetuning datasets do not\\nutilize the full 8192-token context window, so in many cases we restrict to shorter contexts during finetuning.\\nFor a more detailed description of language model pre-training see Appendix A.\\n\\n1.3 Contributions\\n\\nOn prompting, alignment evaluations, alignment taxes, and context distillation:\\n\\n• A simple prompt provides a workable baseline for alignment, and leads to significant improvements\\non a variety of evaluations (figure 2), including a helpfulness, honesty, and harm evaluation we have\\nwritten. We introduce ‘context distillation’ and show that it behaves similarly to prompting.\\n\\n• The prompt reduces toxicity [GGS+20] (figure 8) and seemingly leads larger models to be more\\naccurate than smaller models on TruthfulQA [LHE21] (figure 6). Prompted models are significantly\\npreferred by people who interact with them (figure 9).\\n\\n• Prompting can have negative effects on the capabilities of small models, but has small and sometimes\\npositive effects on large models, which therefore pay little ‘alignment tax’ (figure 2).\\n\\nOn the comparative scaling of imitation learning, binary discrimination, and preference modeling:\\n\\n• The scaling of binary discrimination does not improve very significantly on the scaling of imitation\\nlearning (see figure 3 for a summary, and figure 12 for detailed results on Code Correctness).\\n\\n• Ranked preference modeling of complex hierarchies greatly improves on imitation learning. This\\nshould be encouraging news for alignment work based on human preferences.\\n\\n• These conclusions hold rather cleanly and consistently as represented by at least three distinct\\ndatasets in each category (see figures 3, 14, and 15), but we would still suggest that further work\\nmay improve our understanding of these findings.\\n\\nOn preference modeling pre-training (PMP) for improved sample efficiency:\\n\\n• A PMP stage of training between basic language model pretraining and finetuning on small final\\ndatasets significantly improves sample efficiency (see figures 4 and 5 for summaries, and figure 17\\nfor details).\\n\\n• These results hold even when the PMP data are quite different from the final dataset (e.g. finetuning\\nfrom Stack Exchange to summarization).\\n\\n• In marked contrast to the scaling results mentioned earlier, where PM scales best on hierarchically\\nranked datasets, we find that it’s better for the PMP stage of training to focus on binary discrimination\\n(see figure 18). An explanation for the better performance of binary PMP may be that hierarchies of\\npreferences are difficult to quickly unlearn during finetuning, whereas binary discrimination training\\nteaches models the correct features without establishing strong model preferences. We test this\\nexplanation with a quick synthetic data experiment shown in figure 33.\\n\\n• We also try training the preference model to discriminate between human- and model-generated\\nsamples for the PMP step, and find that it also performs well, as shown in figure 19.\\n\\n2 Conditioning on Aligned Behavior\\n\\nLarge language models can be guided towards desirable behaviors by taking advantage of their in-context\\nlearning abilities. Given a suitable prompt, models will take on the style and persona implicit in the prompt\\nand continue to behave mostly in the same vein. This technique can leverage small quantities of very high\\nquality data, and it has the advantage that the prompt can be easily interpreted by humans. For a variety of\\nreasons we do not expect that prompting will produce fully aligned behavior, but it provides a very useful\\nbaseline.\\n\\n9\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.20\\n\\n0.25\\n\\n0.30\\n\\n0.35\\n\\n0.40\\n\\n0.45\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nTruthfulQA (MC1)\\n\\nDistilled Mutual Info\\nLM Mutual Info\\nDistilled Sum Logprobs\\nLM Sum Logprobs\\n\\nFigure 6 Left: We show the HHH evaluation performance broken down by category. The improvements\\non the Harm evaluations suggest a form of generalization, as the prompt does not contain any examples\\nwhere the assistant resists engaging in harmful behavior. Right: We show results on the adversarial Truth-\\nfulQA dataset (MC1), which was constructed so that larger models would perform more poorly. The context-\\ndistilled prompt seems to improve the performance of the largest models. The solid lines correspond to the\\nofficial evaluation using total probability for each response; we also show the mutual information metric for\\ncomparison.\\n\\nIn this section we will study a variety of zero-shot evaluations for alignment with and without prompting.\\nThe prompt we use consists of fourteen human-assistant conversations, where the assistant is always polite,\\nhelpful, and accurate. The prompt does not contain examples where the assistant actively resists aiding in\\nharmful behavior, but nevertheless for simplicity we will refer to it as the ‘HHH prompt’ or simply the prompt\\nin what follows. We find that although the effect of prompting is modest when measured against the overall\\ngoal of alignment, it improves alignment (according to our evaluations) and decreases toxicity. A potentially\\nmore important observation is that the prompt improves trends, so that alignment improves with model size,\\nincluding on TruthfulQA [LHE21], a dataset designed specifically to induce the opposite trend. Furthermore,\\nwe show that there is little ‘tax’ from alignment – at large model size capabilities are not significantly impaired\\nby the prompt. Of course, this does not mean that more intensive alignment interventions will incur no cost.\\n\\nWe also introduce a ‘context distillation’ technique that may make prompting more efficient in practice and\\npotentially allow for the use of prompts that exceed the size of the context window. For many but not all of\\nour evaluations context distillation performs about as well as prompting. We begin by briefly describing this\\nmethod, and then we will discuss evaluations.\\n\\n2.1 Context Distillation\\n\\nSampling from a language model with a prepended prompt has several disadvantages: the prompt occu-\\npies useful space in a finite context window, which also limits the total prompt length, and without special\\naffordances the prompt will waste compute and memory when sampling.\\n\\nOne way to avoid all of these problems is to finetune on the prompt. This invites some practical difficulties,\\nsince we need to finetune on a tiny dataset without limiting model capabilities. But finetuning also behaves\\ndifferently from prompting – finetuning changes the model’s expectations for the data distribution P (X),\\nbringing it closer to the distribution of the prompt P (C), whereas prompting instead asks the model for the\\ndistribution P (X|C), where C is the context. To give a stark illustration, if we show a language model the\\nlist C = 1, 2, · · · , 63 then it will assign very high probability that the numbers X = 64, 65, · · · are coming\\nnext. If instead we finetune on C, the resulting model will not expect to immediately see the token 64, though\\nit will catch on to the counting pattern if we continue the sequence. We illustrate this toy experiment in figure\\n26, which we have relegated to the appendix.\\n\\nWe can both avoid overfitting and take advantage of conditioning via ‘context distillation’, where we finetune\\na model pθ(X) with a loss given by\\n\\nL(θ) = DKL(p0(X|C)||pθ(X)) (2.1)\\n\\nwhere p0 is the initial model, the context C is fixed, and the data X is drawn from a large corpus of text, such\\nas the original pre-training distribution. We discuss the details of context distillation training in appendix B.5.\\n\\n10\\n\\n\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\nNumber of Parameters\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nA\\ncc\\n\\nur\\nac\\n\\ny\\n\\nAccuracy on Lambada Eval\\n\\nLM\\nLM+Prompt\\nLM+Context Distillation\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\nNumber of Parameters\\n\\n0.035\\n\\n0.030\\n\\n0.025\\n\\n0.020\\n\\n0.015\\n\\n0.010\\n\\nLM\\n -\\n\\n A\\nlig\\n\\nne\\nd \\n\\nA\\ncc\\n\\nur\\nac\\n\\ny\\n\\nLambada Alignment Tax\\n\\nLM+Prompt\\nLM+Context Distillation\\n\\nFigure 7 We show zero-shot Lambada performance in the presence of the HHH prompt and with context\\ndistillation. In both cases there is a small ‘alignment tax’.\\n\\nWe see from figure 2 that this technique appears to work quite well. However, the benefits compared to\\nsimply finetuning on the prompt become much less significant if we additionally provide a small prompt\\nafter the finetuning or distillation process, as shown in figure 20 in the appendix. It appears that contractors\\ninteracting with our models observe a small degradation from distillation, as seen in figure 9. In the future it\\nmight be interesting to apply context distillation iteratively, which one might liken to loading the model with\\na long-term memory or pseudo-identity.\\n\\n2.2 Evaluations and Alignment Taxes\\n\\n2.2.1 HHH Evaluations and TruthfulQA\\n\\nAs a first step in evaluating our models, the authors wrote about fifty comparison evaluations for each cate-\\ngory of helpfulness, honesty,5 harmlessness (HHH), and an ‘other’ label, for a total of around two-hundred\\ncomparisons, which will be available shortly at BIG Bench. We did not put effort into separating alignment\\nfrom capabilities, and so even without any alignment-related prompting, we find that larger models do some-\\nwhat better overall. In many cases we initially produced several slightly different queries (largely differing\\nby paraphrase) for each comparison, but found that large models were rarely confused by these variations, so\\nfor simplicity we dropped them. Results on these evaluations are pictured in figure 2. We expect that more\\nsophisticated alignment techniques should be able to significantly improve these results.\\n\\nNote that we evaluate model choices using the empirical mutual information I(a, q) = log [P (a|q)/P (a)]\\nfor queries q and responses a, rather than the more typical choice of mean token probability for the response\\n(mutual information was also used for several evaluations of GPT-3 [BMR+20]). The mutual information\\nmetric tends to be useful when responses differ greatly in length, and it makes a significant difference in\\nperformance on our evaluations.\\n\\nOn the left in figure 6 we show the results on our HHH evaluations by category. We found it a bit ironic that the\\nmodels perform best in the ‘honesty’ category, as the models certainly do fabricate information when probed\\ninteractively as general-purpose assistants. To further evaluate our models’ honesty, we include evaluations\\non TruthfulQA6 MC1 on the right of this figure. We see that the context distilled prompt has slightly improved\\nthe performance of our largest models using the standard evaluation7 metric. We also compare the use of more\\nevaluation metrics on TruthfulQA in figure 21 in the appendix. The use of conditional probabilities does not\\nalter trends significantly, but does greatly affect absolute performance.\\n\\n5Our evaluations of ‘honesty’ are probably the most correlated with model capabilities, as they measure a mixture\\nof accuracy, preference for expressions of humility, recognition of when another source might be more useful than a\\nlanguage model, and unwillingness to provide inaccurate information. Whether an AI’s response is honest depends on\\nthe expertise of the AI, and a major weakness of our evaluations is that they do not account for this.\\n\\n6We wrote the prompt before TruthfulQA was available. That said, we found in other experiments that using Truth-\\nfulQA examples as a prompt significantly improves performance (much more than our prompt). This suggests that the\\nphenomenon uncovered by TruthfulQA is not a difficult alignment challenge on its own.\\n\\n7In an earlier version of this paper we mistakenly used a very non-standard formulation of the task. We thank the\\nauthors of [LHE21] for pointing out this error, which has been corrected.\\n\\n11\\n\\nhttps://github.com/google/BIG-bench\\n\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\nNumber of Parameters\\n\\n0.03\\n\\n0.04\\n\\n0.05\\n\\n0.06\\n\\nA\\nve\\n\\nra\\nge\\n\\n T\\nox\\n\\nic\\nity\\n\\nToxicity in Response to Non-Toxic Prompts\\n\\nLM\\nLM+Prompt\\nLM+Context Distillation\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\n0.11\\n\\n0.12\\n\\n0.13\\n\\n0.14\\n\\n0.15\\n\\n0.16\\n\\nToxicity in Response to Toxic Prompts\\n\\nFigure 8 Left: Average toxicity in response to a random sample of 500 prompts labeled as ‘non-toxic’ from\\nthe RealToxicityPrompts dataset for language models (LM, blue), prompted language models (LM+Prompt,\\norange), and context distilled language models (LM+Context Distillation, green). Right: Same as Left, ex-\\ncept for a random sample of 500 prompts labeled as Toxic. For non-toxic and toxic prompts, both prompting\\nand context-distillation decrease toxicity and perform similarly to each other as models increase in size. It\\nappears that the prompt leads to decreasing toxicity as model size increases.\\n\\nIt is noteworthy that larger models tend to perform better on our evaluations in the presence of the HHH\\nprompt, even on categories such as harmlessness that are not directly demonstrated by the prompt. We find\\nthis mildly encouraging but unsurprising, since all prior work suggests that larger models have stronger in-\\ncontext learning capabilities, so that they can more efficiently recognize the implicit framing from the prompt.\\n\\n2.2.2 Toxicity\\n\\nWe measured the effect of prompting and context distillation on the toxicity of text generated from language\\nmodels of increasing size. We found that these simple alignment interventions tend to both decrease toxicity\\nand perform similarly to one another (Figure 8). To measure toxicity, we first sampled text conditioned on\\na random sample of 1K prompts from the RealToxicityPrompts dataset [GGS+20]. The prompts are labeled\\nas either ’toxic’ or ’non-toxic’ and we sample an equal proportion of these prompts. Next, we computed\\na toxicity score from model samples of text, conditioned on the prompts, using an open source automated\\ntoxicity detector [HU20]. Our analysis is similar to to [GGS+20] with a few minor modifications. We provide\\nfull details and further analyses in Appendix B.2.\\n\\nFigure 8 illustrates three key findings from our analysis. First, without any alignment intervention, toxicity\\nincreases monotonically with model size in response to both toxic and non-toxic prompts (blue curves).\\nSecond, for non-toxic prompts, both prompting and context distillation significantly reduce toxicity and we\\nobserve little difference between the two interventions (green and orange curves, left figure). Finally, in\\nresponse to toxic prompts, the reduction in toxicity achieved by both prompting and context distillation\\nsignificantly increases with model size (green and orange curves, right figure). The larger reduction in toxicity\\nemerges at 12B parameters. In this regime, context distillation performs similarly to prompting. These results\\nsuggest that prompting-based alignment interventions may have more dramatic effects as models scale and\\nmay be more difficult to evaluate for smaller models.\\n\\nWhile these results are encouraging, automated toxicity detection has several known issues [GGS+20,\\nWGU+21]. For example, there can be low agreement in human annotations of toxicity and biases in tox-\\nicity labels for certain minorities. We also note that other interventions explicitly designed to reduce toxicity\\n(e.g., fine-tuning models on non-toxic training data, steering/filtering model outputs away from toxic outputs\\nat test time, filtering toxic training data at train time) can yield much larger decreases in automated toxicity\\nscores than the ones we observe here [GGS+20, WGU+21]. Nevertheless, we believe that prompting and\\ncontext distillation provide a useful baseline for testing the impact of alignment interventions on automated\\ntoxicity scores.\\n\\n2.2.3 Human Preferences and Model Performance\\n\\nUsing the dialogue interface in figure 1, we evaluated relative model performance via a number of head-to-\\nhead tests between pairs of models. This worked as follows. For any given conversation, we would choose\\n\\n12\\n\\n\\n\\nFigure 9 This figure illustrates the approximate Elo score of various models, fit from the frequency with\\nwhich contractors viewed a given model as more helpful and honest in head-to-head tests involving pairs of\\nmodels. Models with the full HHH prompt seem to be slightly preferred over those with a shorter prompt or\\ncontext distillation. We include 1σ error bars for the special cases, which were only compared against the\\nHHH-prompted models of equal size.\\n\\na pair of models, with each model writing a single response to each human query. We randomized whether\\na given model’s responses would appear in position \"A\" or \"B\" in the interface, to avoid the possibility that\\nusers would consistently find \"A\" or \"B\" to be better. We also pegged streaming sampling speed to that of the\\nslowest model, to partially obscure model identity and avoid bias. We collected a total of about 6k individual\\npair-wise8 model comparisons\\n\\nFrom this process we collected a table of ‘win rates’ for pairs of models, which we provide in table 2 in the\\nappendix. Here we included fully HHH-prompted models with 200M, 800M, 3B, 13B, and 52B parame-\\nters, though we collected somewhat more comparisons involving larger, better-performing models. We also\\ncompared the fully prompted 13B and 52B models to their context-distilled versions and to a version with a\\nshorter prompt consisting of only a single9 example conversation.\\n\\nWe used these results to estimate a single relative Elo score for each model. Intuitively, this score is similar\\nto that used for ranking Chess players, with a real scalar value based on the relative win rates amongst all\\nplayers. Quantitatively, we fit the Elo scores from the data in table 2 with the same loss function we use for\\npreference modeling (equation 3.1). We display the results in figure 9, where we recall that a difference of\\n100 points in an Elo score signifies a ‘win rate’ of 64%.\\n\\nThe most striking feature of these results is that Elo score appears to be linear in the logarithm of model size\\nfrom 197M to 13B parameters, but it does not change very significantly between 13B and 52B parameters.\\nWe do not believe that this is because the two largest models are equally capable. Rather, we interpret it\\nas a limitation of the training and incentives of the contractors evaluating the models, who are US-based\\nmaster-qualified MTurkers who were only provided with some simple instructions, and who have an implicit\\nincentive to finish tasks quickly. This provides a sense for how well-trained and capable workers need to be\\nto perceive distinctions among large language models.\\n\\nWe note that using a much shorter prompt with just one example conversation seems to hurt performance,\\nand it seems that the contractors were able to differentiate the prompted and context distilled model, with the\\nformer being preferred about 53% of the time. We include 1-σ error bars for these comparisons (note that\\nthe short-prompt and distilled models were only compared to the fully prompted models of equal size), so we\\nhave some weak evidence that context distillation has degraded performance somewhat compared to the full\\nHHH prompt.\\n\\n8Note that we typically obtain roughly 3-5 comparisons per conversation. There may be some subtle biases here\\nwhere weaker models perform more poorly early on in conversations, affecting the possibilities for later dialogue.\\n\\n9We did not use completely unprompted models because they would be very unlikely to keep to the format of the\\ndialogue or emit appropriate stop sequences.\\n\\n13\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\nFr\\nac\\n\\ntio\\nn \\n\\nPa\\nss\\n\\nin\\ng \\n\\nTe\\nst\\n\\ns\\n\\nCodex Evaluations and Alignment Tax/Bonus\\nCodex Pass@10\\nCodex Pass@1\\nCodex w/ HHH Prompt Pass@10\\nCodex w/ HHH Prompt Pass@1\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nFr\\nac\\n\\ntio\\nn \\n\\nPa\\nss\\n\\nin\\ng \\n\\nTe\\nst\\n\\ns\\n\\nQuixBugs Evaluations and Alignment Tax/Bonus\\nQuixBugs Pass@10\\nQuixBugs Pass@1\\nQuixBugs w/ HHH Prompt Pass@10\\nQuixBugs w/ HHH Prompt Pass@1\\n\\nFigure 10 This figure shows performance of our code-finetuned models on the Codex and QuixBugs eval-\\nuations with and without the alignment prompt. We see that in both cases, the prompt confuses smaller\\nmodels, leading to worse performance, but it actively improves the 13B and 52B models. All samples were\\ngenerated at temperature T = 0.6 and top P = 0.95 (these settings were not optimized and are not optimal\\nfor Pass@1). Note the figure on the left here was also presented in the introduction.\\n\\n2.2.4 Alignment Taxes/Bonuses\\n\\nA general concern about alignment is that it may impose a ‘tax’ on performance, such that aligned models may\\nbe weaker than raw or unaligned models. In the case of prompting and context distillation, it is straightforward\\nto evaluate this question directly by performing evaluations with and without the prompt. When we include\\nthe HHH prompt, we also use the human-assistant framing when presenting the problem or evaluation to the\\nmodel. The precise specifications can be found in appendix B.1.\\n\\nWe display results for two very similar python coding evaluations, the Codex HumanEval [CTJ+21] and the\\nQuixBugs challenge reformulated as a function synthesis task [LKCSL17] in figure 10. Interestingly, smaller\\nmodels perform significantly worse with the prompt, but 13B and 52B models actually perform noticeably\\nbetter. These evaluations were run using our code-finetuned models, so the strong performance of the larger\\nmodels also suggests that these models have not lost their ability to process the natural language in the prompt.\\n\\nWe performed a similar evaluation on Lambada [PKL+16], with results shown in figure 7. We see that\\nthe prompt and context distillation impose a small ‘tax’ on performance that does not have a significant\\nmodel-size dependence. As shown in Appendix B.4, Lambada performance is strongly dependent on some\\nformatting issues, which alter performance by a much larger margin than the prompt. This format-dependence\\nitself might be regarded as an alignment problem, but unfortunately we do not find that the HHH prompt\\nreduces the difference between accuracies obtained from different Lambada formats.\\n\\nWe therefore found that while smaller models may be confused by the prompt, larger models’ performance\\nis not heavily impacted by it.\\n\\n3 Scaling of Preference Modeling vs Imitation Learning\\n\\nAlignment requires distinguishing between ‘good’ and ‘bad’ behavior. There are several different training\\nobjectives that may be used to accomplish this:\\n\\n• Imitation Learning: Here we simply train language models to imitate ‘good’ behavior via super-\\nvised learning with the usual cross-entropy loss.\\n\\n• Binary Discrimination: Given a sample of ‘correct’ behavior and a sample of ‘incorrect’ behavior,\\ntrain the model to distinguish between the two.\\n\\n• Ranked Preference Modeling: Given a dataset of samples whose overall ‘quality’ is ranked in\\nsome way, we train models to output a scalar quality score10 for each sample whose value matches\\nthe ranking as closely as possible. For simplicity we focus on using pairs of ranked samples (i.e.,\\nbinary comparisons), and we train our models to assign a higher score to the ‘better’ sample in each\\n\\n10These values could then be used as reward signals for reinforcement learning.\\n\\n14\\n\\n\\n\\npair. In some respects this generalizes binary discrimination, and for uniformity we will use it as the\\ntraining objective even for binary discrimination tasks (see section 3.1 for details).\\n\\nWe would like to explore a very general question: when and by how much do discriminators and preference\\nmodels outperform imitation learning?\\n\\nOur experiments in this section involve comparing the performance of imitation learning vs. preference\\nmodeling on a variety of finetuning evaluations, some of which are binary in nature while others are ranked.\\n\\n• Binary: Code Correctness, Commonsense (ethics), Justice (ethics), Deontology (ethics), Virtue\\n(ethics), Lambada\\n\\n• Ranked: Learn to Summarize, Utility (ethics), HellaSwag\\n\\nWe focus mostly on alignment-relevant tasks, but include one binary and one ranked NLP task (Lambada\\n[PKL+16] and HellaSwag [ZHB+19], respectively). Code Correctness is a dataset we constructed from\\npython functions in public github repos with test coverage, with correctness determined by unit tests. The\\nEthics [HBB+21] evaluations are mostly binary classification problems, and so naturally belong in our bi-\\nnary category, except for Utilitarianism which compares relative ‘pleasantness’ of scenarios. The distinction\\nbetween ranked and binary tasks can be ambiguous—for example, whether code passes tests is binary, but\\ncode quality seems like a continuum.\\n\\nOur results support a simple conclusion summarized in figure 3: Ranked preference models tend to improve\\ngreatly on imitation learning, but binary discrimination typically provides little benefit.\\n\\nIn some respects this conclusion is quite intuitive: to apply imitation learning to preference modeling, one\\nmust either only train on the very best data (limiting the dataset size) or train to imitate a lot of examples of\\nlower quality. Nonetheless, the magnitude of the gains are rather stark.\\n\\nIn many cases it is also possible to study the robustness of various methods for ranking samples. For exam-\\nple, if we sample many responses to a prompt/query, we would like to know if the highest ranked samples\\naccording to a given preference model are truly the best. We test this behavior directly in our code correctness\\nstudies and with Lambada.\\n\\n3.1 Loss and Settings for Preference Modeling and Imitation Learning\\n\\nPreference Modeling\\n\\nOur preference models consist of a value head that predicts a single scalar ‘score’ r on top of the final token\\nof any given context, with larger r indicating more desirable samples. The preference modeling loss for each\\npair of ‘good’ and ‘bad’ sequences is [CLB+17]\\n\\nLPM = log\\n(\\n1 + erbad−rgood\\n\\n)\\n, (3.1)\\n\\nand for batched sample pairs we take the mean over all pairs. This is clearly not the most natural loss function\\nfor some applications; for binary ‘correctness’ it would be better to predict if each example is correct or\\nincorrect, and for multiple choice problems, it might be better to maximize the likelihood for the correct\\nresponse among all available responses. However, since our primary motivation is preference modeling, we\\nwill focus on this formulation unless otherwise noted.\\n\\nIn particular, we format all binary discriminators as preference models so that the same architecture can be uti-\\nlized for both binary and ranked evaluations, which is convenient for studying transfer between them. Given\\nany context C with a binary label A/B (e.g., ‘True/False’, ‘Good/Bad’), we create a preference modeling pair\\nC:A > C:B, where B denotes the incorrect label, and the colon denotes concatenation.\\n\\nWe also found that appending a special ‘end-of-context’ token to each sequence to unambiguously delineate\\nthe end of passage sometimes improves performance, as discussed in section C.4.\\n\\nImitation Learning\\n\\nFor imitation learning, our training objective is simply the autoregressive language modeling loss on the\\n‘good’ sequence in each pair—that is, we train the model to imitate ‘good’ behavior. In the notation above,\\nthis means that for imitation learning we trained on C:A. We found that applying a mask to train only over\\nthe response tokens improved performance significantly, so all our imitation learning results are masked.\\nFurthermore, just to clarify, at training time we sum over negative token log-probs to compute the loss as is\\ntypically done, but at evaluation time we average over negative token log-probs to make pairwise comparisons\\n\\n15\\n\\n\\n\\n100 101 102\\nNumber of Samples\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nPreference Modeling (Solid) vs. Imitation Learning (Dashed)\\n\\n108\\n\\n109\\n\\n1010\\n\\nM\\nodel Param\\n\\neters\\n\\nFigure 11 Here we compare the performance of code correctness discriminators and imitation learning for\\nranking samples. All models used for a fixed color are the same size – the generator of the discriminator\\ntraining data, the generator of the test samples, and the preference or imitation learning model used for\\nranking. The fact that some of these curves are not monotonic represents a robustness failure of preference\\nmodeling.\\n\\n(i.e, a pairwise comparison is accurate if the average negative log-prob for the ‘good’ sample is lower than\\nfor the ‘bad’ sample). This significantly improves performance when responses have different lengths.\\n\\n3.2 Performance and Scaling Results for Ranked versus Binary Preference Datasets\\n\\nHere we provide a short description of our evaluation datasets, some of which we categorize as ‘ranked’ while\\nothers are ‘binary’. In this section, all evaluations involve finetuning on a training set and evaluating on a test\\nset.\\n\\nCode Correctness (Binary)\\n\\nFor these experiments we collected about 500k python functions with test coverage11 from public github\\nrepos, and split these functions into a training and test set. For each function, we discarded the original\\nimplementation (keeping only the function definition and docstring) and generated 8 samples from each code\\nmodel up to 13B parameters, and tested these samples with all available tests. We then created pairs of\\ncorrect and incorrect samples for each function, using only model-generated code, to avoid confusing code\\ncorrectness with the task of human-model discrimination. We compared two training procedures: imitation\\nlearning on correct functions, and preference modeling comparing the correct and incorrect functions.\\n\\nThen we evaluated performance on the test set in the following way. We generated 100 samples for each\\nfunction (using pretrained code models), and ranked them according to both mean per-token log-probs of\\nthe IL model, and scores produced by the preference model. Then we evaluated the probability that the top\\nsample among k, as ranked by either method, was in fact correct (we derive an unbiased formula in appendix\\nB.6, based on the pass@k estimate from [CTJ+21]). For this we used the same model size for training and\\ntest set generation and for ranking samples. Some results are shown in figures 11 and 12.\\n\\nOverall we found that preference modeling on this binary discrimination task does not improve very signif-\\nicantly on imitation learning. Both PM and IL are quite similar, overall. These results differ from similar\\nrecent experiments on math problem solving [CKB+21], though they trained on thousands of times less data.\\nThe difference may be that our imitation learning baseline is much stronger, since even before IL finetuning\\non Code Correctness specifically, our code models had seen a great deal of on-distribution python code.\\n\\nLambada (Binary)\\n11We required that at least half of the lines in the function were executed by a combination of tests in the repo.\\n\\n16\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nCode Correctness with 2 Samples\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nCode Correctness with 8 Samples\\n\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nCode Correctness with 32 Samples\\n\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nCode Correctness with 96 Samples\\n\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\nFigure 12 To create this figure, we generated 100 samples (at T = 1) from code models. We then ranked\\nthese samples using either log-probs from the same model, or using a preference model trained to discriminate\\ncorrect and incorrect code. The \"oracle\" line plots optimal ranking where all correct samples are ranked before\\nincorrect ones. We see that imitation learning and preference modeling perform similarly.\\n\\nWe now discuss our evaluations on Lambada [PKL+16]. We used the dataset with original formatting, which\\ndiffers from that used in GPT-3 [BMR+20]. For imitation learning we simply trained on the correct answers\\nin the training set. For binary discrimination, we sampled answers at T = 1 from models of various sizes,\\ncreated up to two pairs of correct and incorrect answers for each prompt, and then trained the discriminator to\\nidentify the correct completion. At test time we sampled multiple responses for each question (at temperature\\nT = 1) and ranked them by either log-probs (for IL) or preference modeling score. The results are shown\\nin figure 13, where we see that imitation learning performs roughly on par with preference modeling. This\\nprovides an independent verification of what we found with Code Correctness, though again the imitation\\nlearning baseline is very strong, as the Lambada task aligns very well with the language model pre-training\\nobjective.\\n\\nHellaSwag (Ranked)\\n\\nWe also performed a comparison of imitation learning and preference modeling on the HellaSwag [ZHB+19]\\ndataset. This is a multiple choice evaluation on commonsense inference—given an event description, the\\nmodel is asked to identify the most sensible completion. Although each problem presents only three choices,\\nthe desired responses are not uniquely correct, but are merely the most sensible inference among the three\\noptions. Thus this task is a form of ranked preference modeling, rather than binary discrimination. In agree-\\nment with our expectations, we find that preference modeling scales far better than imitation learning on this\\ndataset, as shown in figure 14.\\n\\nNote that while the training data is formatted as multiple choice, we convert the data to binary comparisons\\nby pairing the correct choice with a randomly chosen incorrect choice. It might be possible to improve\\nperformance by training on all options, but we did not explore this.\\n\\nLearn to Summarize (Ranked)\\n\\n17\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nLambada Correctness with 2 Samples\\n\\nIL Sampled at T=0\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nLambada Correctness with 8 Samples\\n\\nIL Sampled at T=0\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nLambada Correctness with 32 Samples\\n\\nIL Sampled at T=0\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\nTo\\np-\\n\\n1 \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nLambada Correctness with 96 Samples\\n\\nIL Sampled at T=0\\nOracle\\nAverage (No Ranking)\\nIL Mean Log-Probs\\nPM Scores\\n\\nFigure 13 Similarly to Code Correctness in figure 12, we generated 100 samples (at T = 1) from pretrained\\nlanguage models. We then ranked these samples using either log-probs from an imitation learning model, or\\nusing the scores from a preference model trained to discriminate correct vs. incorrect Lambada completions.\\nNote that for some questions, all the generated answers may be incorrect in which case we default to 0\\naccuracy. We see that these approaches perform similarly, as we expected since Lambada is a ‘binary’ eval.\\nLambada performance depends significantly on formatting, as noted in appendix B.4. We also include a line\\nfor T = 0 (argmax) sampling .\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nHellaSwag (Ranked)\\nPreference Modeling\\nImitation Learning\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nLearn to Summarize (Ranked)\\nPreference Modeling\\nImitation Learning\\n\\nFigure 14 Scaling behavior of imitation learning and preference modeling on HellaSwag (ranked) and\\nLearn to Summarize (ranked), showing that PM performs better than IL, as we expect for ranked finetuning\\nevaluations.\\n\\n18\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.750\\n\\n0.775\\n\\n0.800\\n\\n0.825\\n\\n0.850\\n\\n0.875\\n\\n0.900\\n\\n0.925\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEthics: Commonsense Morality (Binary)\\nPreference Modeling\\nImitation Learning\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEthics: Deontology (Binary)\\nPreference Modeling\\nImitation Learning\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEthics: Justice (Binary)\\nPreference Modeling\\nImitation Learning\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEthics: Virtue (Binary)\\nPreference Modeling\\nImitation Learning\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\n0.85\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEthics: Utilitarianism (Ranked)\\nPreference Modeling\\nImitation Learning\\n\\nFigure 15 Scaling behavior of imitation learning and preference modeling for all five Ethics evaluations,\\nwhich are all binary except Utilitarianism. We find, in agreement with our expectations, that PM beats IL\\non the ranked task, but on binary tasks they perform similarly. For brevity we have only included the easier\\nevaluation sets here.\\n\\nPreference modeling and RLHF has been applied to the task of generating high-quality summaries of short\\narticles [SOW+20]. We study the associated dataset, which we term ‘Learn to Summarize’. It consists of a\\ncollection of articles, where each is accompanied by a pair of summaries that have been ranked by trained\\nhuman workers. This dataset presents a defining example of a ranked preference modeling task, since there\\nis no clear sense in which any given summary is ‘correct’, but typically among any pair of samples, one will\\nbe better than the other. We are especially interested in this finetuning evaluation as it is highly relevant for\\nalignment. We created our own data split by shuffling the data and splitting it into a train (64k pairs) and test\\n(29k pairs) set. On this dataset preference modeling performs far better than imitation learning, as seen in\\nfigure 14.\\n\\nEthics (Binary, except for Utilitarianism)\\n\\nWe studied the Ethics tasks [HBB+21], which include five distinct datasets. We provide a simplified descrip-\\ntion of each here, but we encourage the interested reader to read the original paper for details:\\n\\n19\\n\\n\\n\\n• Commonsense Morality (binary): Assess whether a given action is morally acceptable.\\n\\n• Deontology (binary): Assess whether a given statement is reasonable on the basis of ‘whether an act\\nis required, permitted, or forbidden according to a set of rules or constraints.’\\n\\n• Justice (binary): Assess whether a given statement is reasonable on the basis of impartiality and\\ndesert.\\n\\n• Virtue (binary): Given a personal trait and a scenario involving a character, assess whether the\\ncharacter expresses that particular trait.\\n\\n• Utilitarianism (ranked): Given two similar scenarios, rank them by how ‘pleasant’ they are for the\\ncharacter involved.\\n\\nIn terms of the binary versus ranked12 distinction, the first four evaluations are clearly binary since they come\\nwith binary labels, while we interpret Utilitarianism as a ranked preference modeling task since ‘pleasantness’\\nis a ranked quality.\\n\\nEach dataset includes a single training set and two test sets (standard and hard). We train our models on the\\ntraining sets and evaluate on both test sets during and after training. In all cases we evaluate performance\\nin terms of an accuracy. For Commonsense Morality and Utilitarianism, we use binary accuracy. But for\\nJustice, Deontology and Virtue, the samples are grouped such that a model is accurate on the group only if\\nit gets all responses correct within that group. All our accuracy results follow these requirements. In some\\ncases we also display the preference modeling loss (3.1), as in figure 16, and in that case we simply average\\nover all pairwise comparisons, without any grouping.\\n\\nWe find that as claimed, PM performs significantly better than IL on the ranked Utilitarianism evaluation, but\\nthat PM and IL perform similarly on all binary evaluations, as shown in figure 15.\\n\\n4 Preference Model Pre-Training and Transfer\\n\\nWe saw in section 3 that ranked preference modeling typically performs better than imitation learning, and\\nalso often scales better as we increase model size. However, some datasets needed for alignment may be\\nsmall and expensive to source, since they may require high-quality human feedback. For example, we saw\\na hint in figure 9 that workers may require detailed instructions to differentiate13 among models much larger\\nthan 10B parameters. Thus we are particularly interested in methods to increase sample efficiency when\\nfinetuning on small preference modeling datasets.\\n\\nIn this section we will explore the idea of a ‘preference model pre-training’ (PMP) phase of training, after ba-\\nsic language model (LM) pretraining and before finetuning on a smaller preference modeling dataset relevant\\nfor alignment. Our training pipeline can be summarized as\\n\\nLM Pre-training→ PMP→ PM Finetuning.\\n\\nEach PMP training dataset typically consists of millions of sequence pairs, while each fine-tuning dataset\\ntypically consists of thousands to tens of thousands of sequence pairs.\\n\\nWe find that:\\n\\n• Training on large public preference modeling data sourced from e.g. Stack Exchange question-\\nanswer pairs, Reddit comments, and Wikipedia edits (that revert ‘suspected vandalism’) significantly\\nimproves sample efficiency when subsequently finetuning on small preference modeling datasets.\\nThe pre-training datasets are explained in section 4.1, and the finetuning results are presented in\\nsection 4.2.\\n\\n• In particular, we find that each PMP dataset is capable of transfering to a variety of finetuning\\ndatasets, with an effect size that seems to grow with model size, even though there may not be any\\nobvious similarities between the datasets.\\n\\n• Intriguingly, for the PMP stage of training, it’s most beneficial to train on binary discrimination\\ndata rather than ranked preferences. We suspect this is because ranked preferences often need to be\\n\\n12In some cases this might be altered by changing the objective of the task, but this is our understanding based on the\\ngiven evaluation metrics [HBB+21]\\n\\n13A similar observation was made concerning news articles in [BMR+20].\\n\\n20\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n5.2 × 10 1\\n\\n5.4 × 10 1\\n\\n5.6 × 10 1\\n\\n5.8 × 10 1\\n\\n6 × 10 1\\n\\n6.2 × 10 1\\n\\n6.4 × 10 1\\n6.6 × 10 1\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\nin\\ng \\n\\nLo\\nss\\n\\nMean Transfer Performance at 500 Finetuning Seq Pairs\\nNo PMP\\nPMP Mix\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n3 × 10 1\\n\\n4 × 10 1\\n\\n5 × 10 1\\n\\n6 × 10 1\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\nin\\ng \\n\\nLo\\nss\\n\\nMean Transfer Performance at 5k Finetuning Seq Pairs\\nNo PMP\\nPMP Mix\\n\\nFigure 16 Transfer performance at 500 and 5k finetuning sequence pairs averaged across multiple finetun-\\ning evaluations (Learn to Summarize, HellaSwag, and all five Ethics evaluations).\\n\\n‘unlearned’ during finetuning, which presents a liability to transfer, as explained in section 4.3. In\\nparticular, for PMP we apply a simple ‘binarization’ method that converts any ranked PM dataset to\\nbinary discrimination, as explained in section 4.1.\\n\\n4.1 PMP and Datasets\\n\\nWe constructed multiple PMP datasets from various data dumps found online, including StackExchange,\\nReddit, Wikipedia, and a mixture of all three we refer to as the ‘Mix’. In each case, we began by creating a\\nranked dataset consisting of pairwise comparisons, with each pair consisting of a ‘better’ and ‘worse’ sample.\\nDetails on each dataset is provided in section C.1.\\n\\nSubsequently, we created a binary dataset by applying a ‘binarization’ procedure to the ranked dataset. That\\nis, for every ranked pair A > B, we transform it into two independent binary comparisons:\\n\\nGOOD:A > BAD:A\\nBAD:B > GOOD:B\\n\\nConsequently, the binary dataset has twice as many pairs as the ranked dataset. As discussed in more detail in\\nsection 4.3, we found that pre-training on the binary dataset typically transferred better than the corresponding\\nranked version, and so all our PMP experiments assume binary pre-training unless otherwise stated.\\n\\nWe pre-train a scan of preference models of various sizes on each binary dataset. Training details such as\\nhyperparameter choices are described in section C.1.\\n\\n4.2 Finetuning Results and Scaling Trends\\n\\nHere we show finetuning results after preference model pre-training (PMP) on a variety of downstream fine-\\ntuning evaluations. We find that all our PMP models significantly improve sample efficiency when finetuning,\\ndespite there often being little similarity between the PMP distribution and the finetuning distribution.\\n\\nOur results are summarized in figure 4, showing the performance gain of PMP. Since performance on all of\\nour final finetuning datasets can be evaluated in terms of accuracy, we define the performance gain as the\\naccuracy difference between PMP and no PMP as measured on each test set. We show the accuracy gain of\\nPMP as a function of number of finetuning sequences, where the pre-training dataset consists of a mixture\\nof StackExchange, Reddit, and Wikipedia which we simply refer to as the ‘Mix’. Furthermore, the lightly\\nshaded violet curves show results for individual finetuning evaluations, while the bold violet curve shows\\ntheir mean. More detailed breakdown of results is shown in figure 17 and figure 32.\\n\\nWe are also interested in how finetuning scales with model size, especially in the small data limit, as shown\\nin figure 16. We find that at 1k finetuning sequences (or 500 pairs), PMP on the Mix dataset improves\\nperformance significantly for models larger than ∼ 1B parameters, but does not appear to benefit small\\nmodels. Furthermore, at 10k finetuning sequences (or 5000 pairs), PMP Mix also benefits large models, but\\nto a lesser extent. We also show results for scaling of the best-achieved loss with model size on the finetuning\\nevaluation datasets in figure 28 in the appendix.\\n\\n21\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Hellaswag (52B)\\n\\nNo PMP\\nPMP Mix\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.45\\n\\n0.50\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Learn to Summarize (52B)\\nNo PMP\\nPMP Mix\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Ethics: Commonsense Morality (52B)\\nNo PMP\\nPMP Mix\\nNo PMP (HARD)\\nPMP Mix (HARD)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Ethics: Deontology (52B)\\nNo PMP\\nPMP Mix\\nNo PMP (HARD)\\nPMP Mix (HARD)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Ethics: Justice (52B)\\nNo PMP\\nPMP Mix\\nNo PMP (HARD)\\nPMP Mix (HARD)\\n\\n103 104 105\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Ethics: Virtue (52B)\\n\\nNo PMP\\nPMP Mix\\nNo PMP (HARD)\\nPMP Mix (HARD)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.50\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\n0.85\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning on Ethics: Utilitarianism (52B)\\n\\nNo PMP\\nPMP Mix\\nNo PMP (HARD)\\nPMP Mix (HARD)\\n\\nFigure 17 Transfer to various finetuning evaluations from PMP (on the ‘Mix’ pre-training dataset, shown\\nas violet curves) and no PMP (black curves). Each of the five Ethics datasets (Commonsense Morality,\\nDeontology, Justice, Utilitarianism, and Virtue) has both an ‘easy’ test set (solid curves) and a ‘hard’ test\\nset (dashed curves), but only one training set. The x-axis shows the number of finetuning training sequence\\npairs, while the y-axis shows accuracy as evaluated on a held-out test set. All results are shown for the\\n52B parameter model. In most cases PMP significantly improves sample efficiency, especially in the . 10k\\nsequence pairs regime. Plots show 4 training epochs for each eval.\\n\\n22\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.00\\n\\n0.01\\n\\n0.02\\n\\n0.03\\n\\n0.04\\n\\n0.05\\n\\n0.06\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP \\n On Finetuning\\n\\nPMP Mix\\n\\nFigure 18 In this figure we show the benefit of ‘binarizing’ PMP datasets; the y-axis is the gain in finetuning\\naccuracy with binarization versus without binarization. The x-axis counts number of text sequences seen by\\nthe model, with 2 sequences corresponding to a single preference-modeling comparison.\\n\\nAs already mentioned, pre-training on binary distributions typically transfers better than ranked\\ndistributions—this is discussed more in section 4.3. In addition, we found that the following factors also\\nhelped, all of which have been incorporated into our experiments unless otherwise stated:\\n\\n• Adding to the preference modeling loss a basic language modeling loss to teach the model to imitate\\nthe ‘good’ sequence in each preference modeling pair, as discussed in section C.3.\\n\\n• Appending an end-of-context token to each sequence on top of which the preference modeling score\\nis predicted, as discussed in C.4.\\n\\n4.3 Ranked Preference Modeling vs Binary Discrimination for PMP\\n\\nRecall that our pre-training dataset comes in two forms: ranked and binary. So far we have only presented\\nfine-tuning results from binary PMP, but here we also compare to ranked pre-training, and show that binary\\npre-training typically transfers better than ranked-pre-training. This may be counter-intuitive because prefer-\\nence models are designed to learn an Elo-like score, which can be interpreted as a ranking, and so it is natural\\nto expect ranked pre-training to outperform binary. The goals of this section are to (1) present empirical\\nresults showing the difference, and (2) provide and briefly test a plausible explanation.\\n\\nIn figure 18 we show the advantage of binary pre-training over ranked pre-training. In particular, for each\\nfinetuning evaluation, we plot the accuracy difference vs. the number of training sequences, which can be\\nseen as lightly shaded violet curves. Since there is significant variance in these results, we also take the mean\\nover all such evaluations, giving the bold violet curve. On average, we find that binary pre-training performs\\n+5% better at 500 sequence pairs, and +2% better at 5k sequence pairs. More detailed plots of binary vs.\\nranked pre-training can be found in figure 37 in the appendix, showing the accuracy difference for multiple\\nindividual pre-training datasets and multiple individual finetuning evaluations.\\n\\nThis result surprised some of the authors, but with hindsight we found a plausible explanation. When pre-\\ntraining on a ranked dataset, the model learns a corresponding ranked ordering for sample sequences (rep-\\nresented by a scalar value for each sample). However, downstream evaluations may have rankings that are\\nqualitatively very different, which may then require the pre-trained model to ‘unscramble’ its existing ratings.\\nOn the contrary, binary pre-training establishes a much less ‘rigid’ score, which may require less ‘unscram-\\nbling’ and thus may transfer more easily to very different datasets. We designed an experiment with synthetic\\ndata that appears to confirm this hypothesis, which we describe in detail in appendix C.6.\\n\\n23\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Learn to Summarize\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On HellaSwag\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\nFigure 19 We compare PMP on “human-human” vs “human-model” Reddit datasets by evaluating their\\ntransfer performance (for the latter, the “model” pre-training samples were all generated by a 2.7B model). It\\nappears that “human-model” pre-training transfers better on Learn to Summarize and significantly better on\\nHellaSwag, possibly because both evaluations contain model-generated data, thus giving “human-model” an\\nadvantage. While our primary focus has been on “human-human”, this results suggests that “human-model”\\nalso deserves further investigation.\\n\\n4.4 Human-Model vs Human-Human Comparisons for PMP\\n\\nAll our PMP datasets so far consist of ‘human-human’ comparisons, by which we mean that both samples in\\neach pair are human-written. For this section we consider an alternative dataset consisting of ‘human-model’\\ncomparisons, as we are interested in whether this might improve transfer performance. It is also noteworthy\\nthat such comparisons should be easy to generate, since any high-quality fragment of human text might be\\ncompared to model-generated text on the same subject.\\n\\nThe ‘human-model’ dataset was created by following these steps:\\n\\n• We first finetuned a language model to imitate the ‘good’ samples in our ranked pre-training dataset\\n(e.g., StackExchange, Reddit, or Wikipedia).\\n\\n• For each sample pair in the ranked pre-training dataset, we kept the ‘good’ sequence, but replaced\\nthe “bad” sequence with a sample from the finetuned language model.\\n\\nConsequently, the resulting dataset has the same number of pairs as the original ranked pre-training dataset,\\nwith “good” human-written sequences and “bad” model-written sequences. For these experiments we used\\nthe Reddit PMP dataset, and a 3B model for sample generation.\\n\\nWe found that PMP on the human-model Reddit dataset transfers significantly better to HellaSwag, and\\nsomewhat better to Learn to Summarize, as shown in figure 19. Transfer to the Ethics evaluations (see figure\\n36) is more ambiguous, showing both positive and negative signals. Our suspicion is that human-model pre-\\ntraining has a particular advantage on downstream finetuning evaluations that contain model-generated data—\\nindeed, all incorrect answers on HellaSwag are model-generated, and Learn to Summarize has a significant\\namount of model-generated summaries, while Ethics has no model-generated data. Nonetheless, PMP with\\nhuman-model generated data deserves further investigation, especially since it can be applied to such a great\\nvariety of data distributions.\\n\\n5 Discussion\\n\\n5.1 Related Work\\n\\nThere have been many works related to AI safety and alignment, including some suggestions for global\\nresearch plans such as [AOS+16] and [HCSS21]. Work using human feedback to learn summarizations\\n[SOW+20] has particular relevance to our work, since they observe that preference modeling and RL lead to\\ndramatic improvements compared to imitation learning. One of our motivations was to understand when such\\nimprovements can be expected from these techniques, and how we can take maximal advantage of human\\n\\n24\\n\\n\\n\\nfeedback data. To inquire into our models’ alignment we discussed ethics evaluations from [HBB+21],\\nadversarial honesty evaluations from [LHE21], and toxicity evaluations from [GGS+20].\\n\\nOur use of a small amount of high-quality data for alignment is most similar to [SD21]. On the other end\\nof the spectrum, a rather different technique is to filter pretraining data, as discussed in [NRA+21]. Our use\\nof prompts was motivated by observations about the behavior of large language models [BMR+20]. Some\\nother observations about prompting and the dependence of prompt-tuning on scale were made in [LARC21]\\nthough we did not utilize prompt tuning. The fact that larger models are less subject to forgetting [RDR20]\\nmay be related to the fact that larger models do not incur significant alignment taxes.\\n\\nOur coding models are similar to those discussed in [CTJ+21]. They also performed alignment-related eval-\\nuations, though with high and low quality code examples rather than a natural language prompt. The recent\\nwork [AON+21] evaluated language models (without a great deal of code training) on code, including in a\\nconversational manner.\\n\\nMany papers have studied scaling laws [HNA+17, RRBS19, KMH+20, Jon21]. A few have compared dis-\\ncriminators or preference models to imitation learning, including [ILP+18, SOW+20, WOZ+21]. The T-REX\\nIRL method [BGNN19] uses ranked preference modeling to improve on GAIL and on imitation learning. The\\nauthors of [AAB+21] compared GAIL [HE16] to conventional imitation learning in an RL context, and found\\nin some cases that GAIL scaled significantly better with dataset size. Experiments comparing RL and be-\\nhavioral cloning with the decision transformer [CLR+21] are also somewhat similar to our comparison of\\npreference modeling and imitation learning. Very recently [CKB+21] performed experiments that are very\\nsimilar to our work on code correctness, except that they studied mathematical problem solving, and focused\\nmore on dataset size scaling. Interestingly, they find that a verifier (aka binary discriminator) has a more\\nfavorable dataset size scaling as compared to imitation learning. However, their experiments are likely in a\\ndifferent regime from ours – they were severely data limited, training on only thousands of math problems,\\nwhereas our models were trained on millions of python files, perhaps giving us a much stronger baseline for\\nimitation learning.\\n\\nVarious works [LARC21, WBZ+21, SWR+21, ATS+21] have noted that by finetuning on a large variety\\nof simple tasks, one can improve model performance generally and achieve instruction-following behavior.\\nThis idea is closely related to the ‘preference model pre-training’ approach we have discussed. The work\\nwith the most similar approach to PMP for alignment was the very recent Delphi [JHB+21], which trains a\\ngeneral-purpose ethical critic. Their work differs insofar as we investigate transfer between distributions that\\nare only distantly related (e.g. from Stack Exchange to summarization), whereas they focus on transfer from\\nand to data related to ethics.\\n\\n5.2 Broader Impacts\\n\\nThis work was motivated by the problem of technical AI alignment, with the specific goal of training a\\nnatural language agent that is helpful, honest, and harmless. We believe this work is important because of the\\npotential for very broad impacts from AI and from language models in particular, especially if progress in the\\nfield continues at its current rapid pace [Bow21].\\n\\nWe hope that by directly approaching a general and ambitious problem, we will either (1) fail due to spe-\\ncific technical challenges, which we would then attempt to more precisely articulate for further study from\\nthe research community, or (2) convince ourselves that we have addressed technical alignment for currently\\navailable models.14 In the event of the second outcome, we would expect our results to be carefully interro-\\ngated by the research community. There would also be a need for further empirical investigations into how\\nwell these techniques scale to more capable models in terms of both robustness and efficiency, and how likely\\nit is that we will be able to detect alignment failures in more capable models.\\n\\nThe road to hell is paved with good intentions, and as such we shouldn’t be complacent with concerns asso-\\nciated with alignment work. Foremost in our minds is that advances in aligning AI with human values do not\\ndepend on any specific choice for these values. Efficient alignment techniques could be used to train highly\\ncapable systems that do things we consider to be bad, for instance systems for misinformation, censorship,\\nor oppression. Even terms like helpful, honest, and harmless are ambiguous and can be in tension with each\\nother, and it’s easy to imagine them distorted beyond their original meaning, perhaps in intentionally Or-\\n\\n14Of course, we may fail in uninteresting ways, due to our own limitations, and in that case we can only hope that\\nfuture work will be more successful.\\n\\n25\\n\\n\\n\\nwellian ways. And within the context of our own and similar work, the choice of who provides feedback data\\nto train models has broad implications.\\n\\nInformation such as our comparisons among different scaling behavior may also be useful for improving AI\\ncapabilities, without regard for safety. We believe that understanding how and why ML systems work will be\\nessential to improving their safety, and that these sorts of comparisons aid in that effort. Another concern is\\nthat alignment progress might be used as an excuse for carelessness, or to conclude that alignment has already\\nbeen adequately addressed and can subsequently be ignored. Our view is that people and organizations\\nthat deploy AI systems need to take responsibility for their behavior. Research may help to make such\\ndeployments possible, but the question of broader relevance is simply whether deployed AI systems are\\nactually safe and beneficial in practice.\\n\\n5.3 Implications\\n\\nLarger models tend to perform better at most tasks, and there is no reason to expect naive alignment-related\\ntasks to be an exception. In line with these expectations, we find that behavioral alignment tends to improve\\nwith model size, with even the simplest conceivable intervention (i.e. prompting) leading larger models to\\nperform better on alignment-relevant evaluations.\\n\\nOne reason to investigate scaling trends for preference modeling would be to understand how to train better\\npreference models. However, one of our motivations was actually a bit different – it was to set expectations\\nfor the scaling of reinforcement learning. We would expect that if it is very difficult for models to learn\\nto recognize favorable outcomes, they will also have difficulty learning to take actions that produce such\\noutcomes. That is, value function performance should tell us something about the likely performance of\\na trained policy. This logic should become irrefutable when preference models are re-purposed as reward\\nmodels for RL training. So, given that large gains in both absolute performance and scaling are possible\\nwhen training ranked preference models, significant progress on alignment may also be possible.\\n\\nAuthor Contributions\\n\\nYuntao Bai sourced and curated the PMP data with initial help from Ben Mann, conducted the PMP and fine-\\ntuning experiments, suggested investigating the distinctions between binary and ranked preference modeling,\\nand suggested several ML improvements for preference modeling.\\n\\nAnna Chen conducted experiments on scaling trends for imitation learning versus preference modeling, in-\\ncluding on function synthesis (with help from Dawn Drain, Andy Jones, and others). She also conducted\\nthe experiments on GAN-type discriminators and many other evaluations, and suggested improvements for\\npreference modeling and code quality.\\n\\nAnna and Yuntao collaborated on many experiments and on the training and evaluation code for preference\\nmodeling.\\n\\nAmanda Askell developed the conceptualization of alignment in terms of helpfulness, honesty, and harmless-\\nness. Amanda produced the initial mockup of the model interface and helped to design and build it. Amanda\\nsourced and trained workers for the interface, conducted our original A/B testing experiments, and provided\\nguidance on evaluations.\\n\\nBen Mann built most of the human interaction interface and the necessary backend for robust and efficient\\nsampling. Ben led all of our data collection efforts for both language and code data, in collaboration with\\nDanny Hernandez, who has led research on data quality. Ben also contributed to the core language model\\ntraining infrastructure.\\n\\nBen, Yuntao, Anna, and Amanda contributed to research and project planning.\\n\\nDeep Ganguli proposed, conducted, and analyzed experiments on toxicity (with help from Andy Jones and\\nothers) and conducted some of our experiments on alignment taxes. He also contributed to discussions on\\nharms and alignment.\\n\\nDawn Drain trained the code models and helped Anna with code evaluations, including with collecting func-\\ntions with test coverage (with some help from Ben Mann, Andy Jones, and Tom Henighan). Dawn also\\nconducted experiments on alignment taxes with code models.\\n\\n26\\n\\n\\n\\nNicholas Joseph was central to building and maintaining a highly efficient distributed training system for\\nlarge language models and helped with our sampling infrastructure.\\n\\nTom Henighan managed our research cluster, helped build our distributed training system, and did research\\nand experiments on the numerical stability of large language model training. He also helped with ML research\\non large language models. Nova DasSarma has also helped manage the cluster.\\n\\nAndy Jones was central in building our sampling infrastructure. He also provided engineering support to the\\ntoxicity experiments, A/B testing infrastructure, distributed training, and code model data collection.\\n\\nCatherine Olsson contributed crucially to alignment ideas, and provided useful advice for sourcing and train-\\ning contractors to test our models.\\n\\nLed by Tom Brown in collaboration with Sam McCandlish, much of the technical staff at Anthropic con-\\ntributed to efficient distributed model training and sampling, the underlying ML, and cluster stability. Core\\ncontributors include Nicholas Joseph, Tom Henighan, and Andy Jones. Nelson Elhage, Kamal Ndousse, Zac\\nHatfield-Dodds, and Ben Mann also contributed to this infrastructure.\\n\\nCatherine Olsson and Jared Kaplan wrote the HHH prompt, and along with Deep Ganguli, Anna Chen,\\nAmanda Askell, and many others wrote most of the alignment evaluations. Jackson Kernion helped improve\\nthe alignment evaluations and source workers to interact with our models.\\n\\nJared Kaplan, Yuntao Bai, Anna Chen, Amanda Askell, Deep Ganguli, and Ben Mann wrote the paper, with\\nhelpful comments from everyone at Anthropic.\\n\\nDario Amodei, Chris Olah, and Jack Clark contributed expertise and advice throughout the project.\\n\\nSam McCandlish led model pretraining efforts, often in collaboration with Jared Kaplan. Sam also led the\\noverall synthesis of engineering and research efforts.\\n\\nJared Kaplan conceived and led the project. He conducted some initial experiments on preference modeling\\nand many of the experiments on prompting and context distillation.\\n\\nAcknowledgments\\n\\nWe thank Daniela Amodei, Jia Yuan Loke, Liane Lovitt, Taylor Rogalski, and Timothy Telleen-Lawton for\\nsupport with this project, and Sam Bowman, Collin Burns, Ethan Dyer, Owain Evans, David Krueger, Jan\\nLeike, Liane Lovitt, Helen Ngo, and Jeff Wu for comments on the draft. We thank Paul Christiano for helpful\\ndiscussions.\\n\\nA Language Model Pre-training\\n\\nAll the decoder-only [LSP+18] Transformer [VSP+17] models we train have a fixed aspect ratio\\ndmodel/nlayer = 128, as it has been shown that this is roughly optimal [KMH+20]. Their MLPs up-project\\nby a factor of 4, so that dff = 4dmodel. This means that their total non-embedding parameter count is\\nN = 12nlayerd\\n\\n2\\nmodel ≈ (1.97 × 10\\n\\n5)n3layer. The models have a context window of 8192 tokens with a BPE\\n[SHB15] vocabulary of size nvocab = 216 trained on a mixture of natural language and python code in a\\nsubstantially similar manner to GPT-3 [BMR+20] and its precursors [RNSS18, RWC+19].\\n\\nThe training dataset is composed of 90% natural language and 10% python code. All components of the\\nNL and code datasets were globally fuzzily deduplicated [BMR+20], and we train for one epoch on all sub-\\ncomponents (i.e. we do not repeat any data). The natural language dataset was composed of 55% heavily\\nfiltered common crawl data (220B tokens), 32% internet books (128B tokens), and some smaller distribu-\\ntions including OpenWebText, Wikipedia, Stack Exchange, Arxiv, Legal and Patent documents, Ubuntu-IRC\\ndiscussion, and movie scripts, most of which we sourced from The Pile [GBB+20].\\n\\nOur code models were further finetuned for 100B tokens on a distribution of python code containing about\\n45B unique tokens, so for a bit more than two epochs of training.\\n\\n27\\n\\n\\n\\nnlayer dmodel Parameters (N ) Training FLOPs\\n\\n4 512 13M 3.0e19\\n\\n6 768 42M 1.0e20\\n\\n10 1280 197M 4.7e20\\n\\n16 2048 810M 1.9e21\\n\\n24 3072 2.7B 6.5e21\\n\\n40 5120 13B 3.0e22\\n\\n64 8192 52B 1.2e23\\n\\nTable 1 Basic model parameters including pretraining compute from 400B tokens of training.\\n\\nFigure 20 Left: Comparing context distillation, the full prompt, finetuning on the HHH prompt, and no\\nintervention on our HHH evaluations. Right: By adding two human-assistant conversations we can improve\\nperformance after finetuning on the prompt. Since responses in the HHH evaluations vary greatly in length,\\nin all cases we evaluate using conditional probabilities.\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.20\\n\\n0.25\\n\\n0.30\\n\\n0.35\\n\\n0.40\\n\\n0.45\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nTruthfulQA (MC1) with Various Evaluation Metrics\\nDistilled Mutual Info\\nLM Mutual Info\\nDistilled Mean Logprobs\\nLM Mean Logprobs\\nDistilled Sum Logprobs\\nLM Sum Logprobs\\n\\nFigure 21 We show results on the adversarial TruthfulQA dataset when evaluating with both mutual infor-\\nmation, mean logprobs, and summed logprobs (the official metric for MC1). The model sizes trends for both\\nare similar. These are all zero-shot evaluations on language models, with and without context distillation on\\nthe HHH prompt.\\n\\n28\\n\\n\\n\\nB More Details on Prompting, Context Distillation, and Evaluations\\n\\nB.1 Alignment Tax Evaluations\\n\\nFor alignment tax evaluations, we used a formulation that naturally fit with the human-assistant dialogue. For\\nLambada we used the format:\\n\\nHuman: Can you tell me a story?\\nAssistant: Sure, here’s a draft: <LAMBADA PROMPT>\\n\\nFor function synthesis we used:\\n\\nHuman: Can you help me write this Python function? I’ve already\\nwritten the function’s signature and docstring, but\\nI’m not sure how to write the function’s body. It starts like this:\\n<SIGNATURE_PLUS_DOCSTRING>\\n\\nAssistant: Sure thing, here you go! I’ve tested this function myself\\nso I know that it’s correct:\\n<SIGNATURE_PLUS_DOCSTRING>\\n\\nB.2 Toxicity Analysis Details\\n\\nTo measure toxicity, we first sample text from our models conditioned on prompts from the RealToxici-\\ntyPrompts dataset [GGS+20]. Each prompt in this dataset is labeled as either toxic or non-toxic. In order to\\nminimize the computational cost incurred by sampling from large models (especially when primed with the\\nlarge context), we select a random sample (stratified to have an equal number of toxic and non-toxic prompts)\\nof 1K out of the available ≈ 100K prompts.\\nNext, following [GGS+20], we sample for each prompt 25 sequences of up to 20 tokens each. We use tem-\\nperature T = 1 and top P = 0.90. For the prompted and distilled models we make two minor modifications.\\nFirst, we embellish the RealToxicityPrompt to fit with the human-assistant dialogue as follows:\\n\\nHuman: Can you help me complete a sentence? I’ve already written the\\nfirst part of the sentence, but I’m not sure how to complete\\nit to say something non-toxic. The first part of the sentence is:\\n<REALTOXICITYPROMPT>\\n\\nAssistant: Sure thing, here is a non-harmful completion of your\\nsentence: <REALTOXICITYPROMPT>\\n\\nWe simply replace <REALTOXICITYPROMPT> with a prompt from the dataset. Second, we never sample\\nthe tokens Human: and - - - - - in order to stop the models from emulating a human’s response to the\\ncompleted sentence. We found that such emulated human responses (which typically looked like Human:\\nThanks! This looks great!) significantly decreased toxicity (Figure 22). In fact, emulating hu-\\nman responses had an effect size larger than that of the alignment interventions, which confounded the results.\\n\\nTo measure the toxicity of the model generated text, we used an open source toxicity detector [HU20] that\\noutputs a score, between 0 and 1 with a higher score corresponding to more toxic content. In particular, we\\nused the ’unbiased’ RoBERTa based model, which was trained on data from the Jigsaw Unintended Bias in\\nToxicity Classification Kaggle competition 15. The model achieves an AUC score of 0.9374 on predicting a\\nhuman-annotated toxicity label. At the time of writing, the highest leaderboard AUC score is 0.9473. Our\\nusage of this model represents a departure from [GGS+20], and other work on toxicity in language models,\\nwhich typically rely on the widely used and publicly available Perspective API 16 for toxicity detection. We\\nuse the open source toxicity detector purely for ease of implementation. However, we verified that the open\\nsource toxicity scores are strongly correlated the Perspective toxicity scores (for the prompts we sampled\\nfrom RealToxicityPrompts dataset, r = 0.829) and that the distributions of toxicity are similar for both\\ntoxicity detectors. We will leave a re-analysis of toxicity with the Perspective API for future work, though\\nwe do not expect this to significantly affect our main findings.\\n\\n15https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview\\n16https://www.perspectiveapi.com/\\n\\n29\\n\\n\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\n0.08\\n\\n0.10\\n\\n0.12\\n\\n0.14\\n\\n0.16\\n\\nToxicity in Response to Toxic Prompts\\n\\nLM\\nLM+Prompt\\nLM+Prompt-Human\\nLM+Context Distillation\\nLM+Context Distillation-Human\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\nNumber of Parameters\\n\\n0.02\\n\\n0.03\\n\\n0.04\\n\\n0.05\\n\\n0.06\\nA\\n\\nve\\nra\\n\\nge\\n T\\n\\nox\\nic\\n\\nity\\n\\nToxicity in Response to Non-Toxic Prompts\\n\\nFigure 22 Average toxicity tends to decrease when prompted (orange) and context distilled (green) models\\nemulate human responses (dashed lines) relative to when when they do not (solid lines). Left: For non-toxic\\nprompts, allowing aligned models to emulate human responses tends to slightly decrease average toxicity.\\nRight: For toxic prompts, allowing aligned models to emulate human responses tends to significantly de-\\ncrease average toxicity, which dwarfs and confounds the effect of the alignment interventions.\\n\\nIn Figure 8 we report the mean toxicity score averaged across all 500 prompts and 25 samples per prompt.\\nThis represents a departure from [GGS+20] and other work on toxicity in language models, which typically\\nreport the metrics: Expected Maximum Toxicity and Probability of Toxicity. The Expected Maximum Tox-\\nicity metric reports the maximum toxicity across the 25 continuations per prompt, averaged across all 500\\nprompts. The probability of toxicity metric captures the average, across prompts, of an indicator variable\\nthat’s 1 if a given sample has a toxicity score > 0.5, and 0 otherwise, across continuations. We report these\\nmetrics in Figure 23. We note that, in general, likely due to the maximum and thresholding operations of each\\nmetric prior to averaging, both metrics have large standard deviations and do not scale smoothly with model\\nsize. Regardless, the general findings from the main text remain true: both context distillation and prompting\\nreduce toxicity and the reduction in toxicity according to these metrics is greater as models get larger. We\\nalso observe that both Expected Maximum Toxicity and Probability of Toxicity tend to be strongly correlated\\nwith each other.\\n\\nTo gain intuition about why the simple average toxicity score scales smoothly with model size, we inspect\\nthe probability distribution of toxicity scores across model sizes for the base language model (LM, Figure\\n24 Left). The distribution is bimodal with one peak for low toxicity scores and and a relatively smaller\\npeak for high toxicity scores. As the model size increases, probability mass tends to shift smoothly from the\\nlow toxicity peak to the high toxicity peak. Computing the mean of these distributions captures this smooth\\ntransition in mass between modes. We also inspect the influence of the alignment interventions for the largest\\n50B parameter model (Figure 24 Right). We see that the alignment interventions tend to undo the effect of\\nscaling up model sizes in that they shift probability mass away from the toxic mode towards the less toxic\\nmode.\\n\\nB.3 TruthfulQA Formatting\\n\\nFor evaluations of TruthfulQA with context distilled models, we used the format:\\n\\nHuman: <QUESTION>\\n\\nAssistant: <ANSWER>\\n\\nand evaluate the probability of the answer tokens. With our pure language models (no prompt or context\\ndistillation), we tried using both this format and even simpler format <QUESTION> <ANSWER>, and found\\nthat the latter did very slightly better, and so we have used results from that format in all figures.\\n\\nB.4 A Comment on Lambada Formatting\\n\\nWe performed a fairly complicated evaluation on Lambada in section 3.2, which involved finetuning on the\\ntraining set. Therefore, we used the official version of the dataset, which has a number of typos and strange\\n\\n30\\n\\n\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\n0.32\\n\\n0.34\\n\\n0.36\\n\\n0.38\\n\\n0.40\\n\\n0.42\\n\\n0.44\\n\\nE\\nxp\\n\\nec\\nte\\n\\nd \\nM\\n\\nax\\nim\\n\\num\\n T\\n\\nox\\nic\\n\\nity\\n\\nToxicity in Response to Non-Toxic Prompts\\n\\nLM\\nLM+Prompt\\nLM+Context Distillation\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\n0.72\\n\\n0.74\\n\\n0.76\\n\\n0.78\\n\\n0.80\\n\\n0.82\\n\\nToxicity in Response to Toxic Prompts\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\nNumber of Parameters\\n\\n0.275\\n\\n0.300\\n\\n0.325\\n\\n0.350\\n\\n0.375\\n\\n0.400\\n\\n0.425\\n\\n0.450\\n\\nP\\nro\\n\\nba\\nbi\\n\\nlit\\ny \\n\\nof\\n T\\n\\nox\\nic\\n\\nity\\n\\n10\\n7\\n\\n10\\n8\\n\\n10\\n9\\n\\n10\\n10\\n\\n0.74\\n\\n0.76\\n\\n0.78\\n\\n0.80\\n\\n0.82\\n\\n0.84\\n\\n0.86\\n\\nFigure 23 Expected Maximum Toxicity (top plots) and Probability of Toxicity (bottom plots) tend to scale\\nless smoothly with model size in response to both non-toxic (left plots) and toxic (right plots) prompts. Top\\nLeft: Expected Maximum Toxicity in response to non-toxic prompts is generally lower for both prompted\\n(orange) and context distilled (green) models relative to unaligned (blue) models. Top Right: Expected\\nMaximum Toxicity in response to toxic prompts only decreases relative to unaligned models only for larger\\nmodels and increases otherwise. Bottom Left: Probability of Toxicity in response to non-toxic prompts\\nexhibits the same general trend as Expected Maximum Toxicity in response to non-toxic prompts Bottom\\nRight: Probability of Toxicity in response to toxic prompts also exhibits same general trend as Expected\\nMaximum Toxicity.\\n\\n10\\n4\\n\\n10\\n3\\n\\n10\\n2\\n\\n10\\n1\\n\\n10\\n0\\n\\nToxicity Score\\n\\n0.00\\n\\n0.02\\n\\n0.04\\n\\n0.06\\n\\n0.08\\n\\n0.10\\n\\nD\\nen\\n\\nsi\\nty\\n\\nDistribution of Toxicity Conditioned on Model Parameters\\n\\n# Parameters\\n52.4B\\n12.8B\\n2.76B\\n200M\\n43.2M\\n12.8M\\n\\n10\\n4\\n\\n10\\n3\\n\\n10\\n2\\n\\n10\\n1\\n\\n10\\n0\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\nEffect of Alignment on Distribution Toxicity for 50B Models\\n\\nLM+Context Distillation\\nLM+Prompt\\nLM\\n\\nFigure 24 The distribution, estimated via kernel density estimation with a Gaussian kernel, of toxicity\\nscores is bimodal, with one peak for for low toxicity scores and a relatively smaller peak for high toxicity\\nscores. Left: For a standard LM, as the model size increases, probability mass tends to shift from the low\\ntoxicity peak to the high peak. Right: Conversely, for a 50B parameter model (blue), prompting (orange) and\\ncontext distillation (green) tends to shift mass from the high peak to the low peak.\\n\\n31\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nThe Importance of Data Formatting\\n\\n0-Shot Awkwardly Formatted \\n20-shot Awkwardly Formatted\\n0-Shot Nicely Formatted\\n20-shot with Fill-In-The-Blanks\\n\\nFigure 25 We show Lambada results with three different formats – an awkward format from the orig-\\ninal/official Lambada dataset, a format constructed by OpenAI, and a fill-in-the-blanks format used with\\nGPT-3 [BMR+20] that performs very well with few-shot learning.\\n\\nwhitespace and punctuation choices. However, in section 2.2.4 we included some zero-shot Lambada eval-\\nuations to assess ‘alignment taxes’. These formatting choices make a very large difference in performance,\\nas shown in figure 25. In particular, we believe this explains in large part why the results from figure 13 are\\ncomparatively weak.\\n\\nTo be explicit, here is an example from the nicely formatted version:\\n\\n\"Helen’s heart broke a little in the face of Miss Mabel’s selfless\\ncourage. She thought that because she was old, her life was of less\\nvalue than the others. For all Helen knew, Miss Mabel had a lot more\\nyears to live than she did. \"Not going to happen,\" replied Helen\\n\\nAnd here’s an example from the original version:\\n\\nit was very freeing . there would be no more hiding , no more tiptoeing\\naround the conversation . logan and i were together . plain and\\nsimple . we cared for each other and were doing what felt right .\\nthat did n’t stop my stomach from sinking the second door swung open\\n. dr. andrews strode into the room , casting a cautionary glance in my\\ndirection before turning his attention to logan\\n\\nThe difference in performance between these formats might be regarded as an alignment failure itself. For\\nthis reason, we were interested in whether the HHH prompt reduced the gap between Lambada formats, but\\nwe did not find this effect.\\n\\nB.5 Context Distillation Finetuning\\n\\nTo perform context distillation in practice, we prepended both the HHH prompt and then Human: (signi-\\nfying the beginnning of a new conversation) to text samples. We then performed a forward pass with the 52B\\nmodel and stored the top 50 log-probabilities for each token, along with their indices within the vocabulary.\\nWe used a half-and-half mixture of generic pretraining data and Stack Exchange questions. We formatted the\\nlatter to use the Assistant: label before the answers, as an attempt to stay near the human-assistant\\ndistribution. We filled out the remainder of the context with distillation data, providing about 1500 tokens per\\nsequence (subtracting the length of the prompt).\\n\\nAfter generating this data, we finetuned all model sizes on it with KL loss between the stored log-probabilities\\nand the model-predicted probabilities. Since we only stored the top 50 log-probs, for each token this KL\\nwas actually a 51-category comparison, with the extra category coming from the aggregation of all other\\npossibilities besides the top 50 from the prompted 52B model.\\n\\n32\\n\\n\\n\\nFigure 26 Left: Per-token losses when counting, along with Laplace’s prediction that “if the sun has risen\\nn times in a row, the probability it will not rise tomorrow ∼ 1/n”. Right: An extreme illustration of the\\ndifference between prompting (conditioning) and finetuning (altering the expected data distribution). The\\nfinetuned models were trained on the sequence 1,2, ..., 63. With sufficient finetuning these models become\\nvery confident about counting, but never learn that the first few tokens should be 64, 65, ...\\n\\n800M 3B 13B 52B\\n\\n200M 0.34 0.30 0.13 0.16\\n\\n800M - 0.40 0.26 0.25\\n\\n3B - - 0.34 0.34\\n\\n12B - - - 0.45\\n\\n12B Distilled - - 0.46 -\\n\\n52B Distilled - - - 0.46\\n\\n12B Short-Prompt - - 0.44 -\\n\\n52B Short-Prompt - - - 0.47\\n\\nTable 2 In this table we show the fraction of head-to-head model comparisons where one model was pre-\\nferred to the other by contractors. The numbers represent the \"win rate\" of the models indicated in each\\nrow against those indicated by the column labels. All models were presented with the full 4600 word HHH\\nprompt, and we sampled responses at T = 1 and top P = 0.95. We include a dash where we made no\\ncomparison, or where the results are trivially implied by p→ 1− p across the diagonal.\\n\\nB.6 Estimator of Accuracy When Re-Ranking Samples\\n\\nWhen studying the performance of models that rank sample quality (with a PM or log-probs from a language\\nmodel), we’re interested in the measuring the fraction of problems that are solved by the the top-ranked\\nsample, when there are k samples in total. Here we derive an unbiased estimator for this quantity when using\\na finite pool of N ≥ k samples. The analysis builds on estimates for pass@k from [CTJ+21].\\nFor each problem, we sample a list of N samples, and then calculate both the score for ranking (by a model\\nof interest) and whether each individual response was correct. Then for each problem, we estimate:\\n\\nacc@k(S) =\\n\\nN∑\\ni=1\\n\\nacc(si) ·\\n(\\nN−i\\nk−1\\n)(\\n\\nN\\nk\\n\\n)\\nwhere S is a list of ranked samples, from better to worse scores; acc(·) is the accuracy of the sample (correct\\nor incorrect, so these are all 1 or 0);\\n\\n(\\nN\\nk\\n\\n)\\nis the total number of possible combinations when choosing k of\\n\\nN samples. Then, crucially,\\n(\\nN−i\\nk−1\\n)\\n\\nis the number of combinations where sample si is the top-ranked sample\\namong the k chosen samples. So the ratio of binomial coefficients in equation (B.6) is the probability that the\\nith sample is chosen and is the highest ranked sample in a group of k.\\n\\n33\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.625\\n\\n0.630\\n\\n0.635\\n\\n0.640\\n\\n0.645\\n\\n0.650\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\n L\\nos\\n\\ns\\n\\nPre-train Performance on StackExch\\nMix\\nStackExch\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.60\\n\\n0.61\\n\\n0.62\\n\\n0.63\\n\\n0.64\\n\\n0.65\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\n L\\nos\\n\\ns\\n\\nPre-train Performance on Reddit\\nMix\\nReddit\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n0.26\\n\\n0.28\\n\\n0.30\\n\\n0.32\\n\\n0.34\\n\\n0.36\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\n L\\nos\\n\\ns\\nPre-train Performance on Wiki\\n\\nMix\\nWiki\\n\\nFigure 27 Scaling laws for PMP, showing PM loss vs. model size, for each of the three pre-training datasets\\nStackExchange, Reddit, and Wikipedia, as evaluated on a held-out test set after one training epoch. The “Mix”\\nis simply a mixture consisting of one epoch each of the three pre-training datasets. We do not know why the\\n52B seems to be off-trend. This could be caused by (1) being in a data-limited regime, or (2) being limited\\nby the entropy of the pre-training distribution, or (3) sub-optimal choice of hyperparameters. Nonetheless, it\\nis interesting to observe (e.g., from figure 5) that the 52B still transfers significantly better than the smaller\\nmodels.\\n\\nThe overall metric is simply the mean of this quantity over all the problems.\\n\\nC More Details on Preference Models\\n\\nC.1 Preference Model Pre-training\\n\\nWe now describe how the ranked pre-training datasets were prepared for each domain. The binarization\\nprocedure outlined in 4.1 was subsequently applied to convert each ranked dataset to a binary one.\\n\\n• StackExchange: The StackExchange Data Dump17 consists of questions and answers from the\\nStackExchange website. For each question, we evaluate the ‘score’ of all answers, where the score\\nis defined as the log2(1+upvotes) rounded to the nearest integer, plus 1 if the answer was accepted\\nby the questioner (we assign a score of −1 if the number of upvotes is negative). In order to make\\npairwise comparison data for PMP, we sample two answers with distinct scores, skipping questions\\nwhere this is not possible. For each question-answer pair, the corresponding context is formatted as\\n\\nQuestion: ...\\nAnswer: ...\\n\\nWe prepared 5.8M training pairs and 59k test pairs.\\n• Reddit: The Pushshift Reddit data dump18 consists of posts and comments from the Reddit website.\\n\\nFor each Reddit post, we sample a pair of comment sequences differing only in the final comment.\\n17https://archive.org/details/stackexchange\\n18https://files.pushshift.io/reddit/\\n\\n34\\n\\n\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n10 1\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\nin\\ng \\n\\nLo\\nss\\n\\nFinetuning Performance After PMP\\nHellaswag\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n2 × 10 1\\n\\n3 × 10 1\\n\\n4 × 10 1\\n\\n6 × 10 1\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\nin\\ng \\n\\nLo\\nss\\n\\nFinetuning Performance After PMP\\nCommonsense Morality\\nJustice\\nDeontology\\nVirtue\\n\\n107 108 109 1010\\nNumber of Parameters\\n\\n5 × 10 1\\n\\n6 × 10 1\\n\\nPr\\nef\\n\\ner\\nen\\n\\nce\\n M\\n\\nod\\nel\\n\\nin\\ng \\n\\nLo\\nss\\n\\nFinetuning Performance After PMP\\nLearn to Summarize\\nUtility\\n\\nFigure 28 Scaling trends with model size for the best achieved comparison test loss on various final fine-\\ntuning evaluations. We have grouped datasets together based on the dynamic range in the loss. The results\\nare measured after one training epoch each for Learn to Summarize and Hellaswag, and four training epochs\\nfor each Ethics eval. In all cases larger models perform better, as expected. Sometimes we see a fairly\\nclean power-law trend in the loss, but often there are significant deviations, including perhaps an interesting\\nimprovement in the slope on some datasets in the hundred million parameter range.\\n\\nWe then label the sequence whose final comment has the higher number of user upvotes as the\\n“better” sequence, thus forming a preference modeling pair. For each post and comment sequence,\\nthe context is formatted as\\n\\nSUBMISSION by username: ...\\nCOMMENT by username: ...\\nCOMMENT by username: ...\\n\\nwhere each username is replaced with the corresponding author’s alias. We also removed deleted\\ncomments and comments from bots. We prepared 1.1M training pairs and 11k test pairs.\\nNote: We also made an effort to filter away poor or irrelevant data. For instance, we restrict to a\\n“whitelist” of subreddits that we believe have the highest data quality. We specifically chose not to\\ninclude AmItheAsshole, as it overlaps with one of our fine-tuning datasets, Commonsense Moral-\\nity. Instead we include the subreddits: tifu, explainlikeimfive, WritingPrompts, changemyview,\\nLifeProTips, todayilearned, science, askscience, ifyoulikeblank, UpliftingNews, Foodforthought,\\nIWantToLearn, bestof, IAmA, socialskills, relationship_advice, philosophy, YouShouldKnow, his-\\ntory, books, Showerthoughts, personalfinance, buildapc, EatCheapAndHealthy, boardgames, male-\\nfashionadvice, femalefashionadvice, scifi, Fantasy, Games, bodyweightfitness, SkincareAddiction,\\npodcasts, suggestmeabook, AskHistorians, gaming, DIY, mildlyinteresting, sports, space, gadgets,\\nDocumentaries, GetMotivated, UpliftingNews, technology, Fitness, travel, lifehacks, Damnthatsin-\\nteresting, gardening, programming.\\n\\n• Wikipedia: Wikipedia provides a data dump19 of the full edit history for every page. For some\\nedits, a short explanation of the intention behind the edit is provided in the metadata. In particular,\\n\\n19https://en.wikipedia.org/wiki/Wikipedia:Database_download\\n\\n35\\n\\n\\n\\na significant number of edits revert “suspected vandalism”, as noted in comments associated with\\nthe edits. Examples of vandalism include edits that are intended to be misleading, counterfactual,\\nor irrelevant to the subject matter of the page. For each such edit, we form a preference modeling\\npair by extracting the contents of the page before and after the edit, with the reverted version labeled\\nas “better”. For each edit, we restrict to only the page sections that had been edited, and make a\\npreference modeling pair for each such section, thus reducing the necessary context length signifi-\\ncantly. For each item in each pair, the context simply consists of the contents of the relevant section,\\nformatted as\\n\\nPAGE TITLE: ...\\nSECTION TITLE: ...\\nSECTION BODY: ...\\n\\nWe also made an effort to clean out various irrelevant metadata, such as hyperlinks, citations, data\\ntables, and placeholders for images. We also removed uninteresting sections such as references and\\nbibliography. We made 1.4M training pairs and 14k test pairs.\\n\\n• Mix: We also consider a mixture (i.e., union) of StackExchange, Reddit, and Wikipedia, and refer\\nto it as the “Mix”. Since we choose to use a single epoch of each component dataset, the mix is\\nabout 70% StackExchange.\\n\\nFor each pre-training dataset, including the “Mix”, we trained a scan a model sizes for exactly one epoch each.\\nIn all cases we used context size of 1024 tokens per sequence, batch size of 512 sequence pairs, and constant\\nlearning rate of 0.1 relative to language model pre-training. We evaluate preference model pre-training by\\nPM accuracy (i.e., does the PM assign a higher score to the “good” sample in each pair?) and PM loss (3.1).\\n\\nC.2 Preference Model Pre-Training\\n\\nWe present more detailed finetuning results in figure 17, showing performance as a function of number of\\nfinetuning sequence pairs for both PMP (on the Mix dataset) and no PMP.\\n\\nFor all these experiments, we used a model context size of 1024 tokens per sequence, batch size of 32\\nsequence pairs, and a constant learning rate of 0.01 relative to pre-training. We trained for one epoch each\\non Learn to Summarize and HellaSwag, and four epochs each on the Ethics evaluations as doing so improved\\nperformance. We used hyperparameters (λ, µ) = (1, 1) for the PM and LM losses, respectively, as discussed\\nin section C.3.\\n\\nC.3 Language Modeling Improves PMP Transfer\\n\\nIn this section we describe a technical detail which improves the transfer-ability of PMP significantly. We\\nconsider two losses for the pre-training stage: (1) the preference modeling (PM) loss, and (2) an autoregres-\\nsive language modeling (LM) loss that imitates the “good” sample in each sequence pair.\\n\\nLtotal = λLPM + µLLM, good (C.1)\\n\\nwhere λ, µ are hyperparameters. For the latter, we do not apply any masking on the tokens and simply train\\nthe model to predict the full context of the good sample.\\n\\nWe found that adding the language modeling loss during pre-training consistently improved the sample ef-\\nficiency on finetuning evaluations. In figure 29, we show the transfer performance for several pre-training\\nlosses:\\n\\n• No PM pre-training,\\n• “Pure” PM loss for which (λ, µ) = (1, 0),\\n• “Pure” LM loss for which (λ, µ) = (0, 1),\\n• “Composite” PM+LM loss for which (λ, µ) = (1, 1),\\n\\nFor uniformity, we used (λ, µ) = (1, 1) for the subsequent finetuning stage in all four scenarios.\\n\\nWe observe that\\n\\n• Pure LM performs similarly as no PM pre-training, which is unsurprising since it’s just an extension\\nof the basic language model pre-training on which all our experiments are initialized.\\n\\n36\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning Performance of Different UPM Pre-training Losses On\\n Learn to Summarize (13B)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nFinetuning Performance of Different UPM Pre-training Losses On\\n HellaSwag (13B)\\n\\nNo UPM Pre-train\\nUPM Reddit (LM Loss Only)\\nUPM Reddit (PM Loss Only)\\nUPM Reddit (PM+LM Loss)\\n\\nFigure 29 PMP using only the PM loss transfers poorly to downstream evaluations and is typically worse\\nthan simply doing no such pre-training at all. However, when combined with an autoregressive language\\nmodeling loss that imitates the “good” sample in each training pair, it significantly improves transfer to many\\ndownstream evaluations. Here we show results for PMP on Reddit finetuned on Learn to Summarize and\\nHellaSwag, but we made similar observations on all other pre-training and finetuning datasets. Furthermore,\\nthe fact that “PM+LM Loss” clearly performs better than “LM Loss Only” strongly suggests that the per-\\nformance gain of the former does not arise solely from language modeling, but from its combination with\\npreference modeling.\\n\\nFigure 30 Here we show calibration curves on the summarization test set. We see that aside from the\\nsmallest model, the preference models are very well calibrated on-distribution. These models were all first\\nprefence model pre-trained on the stack exchange and then finetuned on summarization PMing. We include\\na black line as a reference for perfect calibration.\\n\\n37\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEffect of EOC Token On Learn to Summarize (13B)\\n\\nNo UPM Pre-train (w/ EOC Token)\\nNo UPM Pre-train (no EOC Token)\\nUPM Reddit (w/ EOC Token)\\nUPM Reddit (no EOC Token)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\n0.85\\n\\n0.90\\n\\n0.95\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nEffect of EOC Token On HellaSwag (13B)\\n\\nNo UPM Pre-train (w/ EOC Token)\\nNo UPM Pre-train (no EOC Token)\\nUPM Reddit (w/ EOC Token)\\nUPM Reddit (no EOC Token)\\n\\nFigure 31 Appending an “end-of-context” token (EOC) to every sequence visibly improves overall perfor-\\nmance, as seen here for both with and without PMP. In all cases where PMP is applied, we include the EOC\\ntoken not just for the finetuning sequences but also the pre-training sequences. We made similar observations\\non all PMP datasets (as well as no PMP) and all finetuning datasets.\\n\\n• Pure PM improves sample efficiency for a small number of samples, but eventually underperforms\\nrelative to no PM pre-training.\\n\\n• The PM+LM pre-training consistently improves sample efficiency relative to no PM pre-training. It\\nalso performs better than pure LM, thus indicating that the performance gain isn’t due purely to LM,\\nbut a combination of PM and LM.\\n\\nWhat’s particularly interesting is that neither pure PM nor pure LM transfers particularly well, but the com-\\nbined effort of PM+LM performs significantly better. Our hypothesis is that pure PM has a tendency to learn\\nbiased or “trivial” features (e.g., context length, token frequencies) that don’t generalize well to downstream\\ntasks, while the addition of LM forces the PM to learn from more substantial “language-relevant” features.\\n\\nC.4 End-of-context Token Improves Preference Modeling Performance\\n\\nHere we outline a technical detail that improves the overall performance of preference models. We designate\\na special “end-of-context” token (EOC) which is included as the final token of each sample context. The\\npreference model score is also predicted directly on top of this token. For our experiments we used the\\n<SOS> token, but in principle many other choices are possible.\\n\\nWe compare finetuning experiments with and without the EOC token. For experiments with, we consistently\\napply the same EOC token throughout both the PMP and fine-tuning stages; and for experiments without, we\\nconsistently do not apply the EOC token. From figure 31 we see that the EOC clearly improves performance.\\n\\nWe hypothesize that the improvement comes from two factors:\\n\\n• Sometimes the sentiment behind a natural language statement can be altered or reversed significantly\\nby the addition of one or two words, and so knowing where the context ends can be helpful for the\\npreference model to predict a sensible score.\\n\\n• Without an EOC token, the preference model must not only predict a score, but also try to anticipate\\nwhere the context ends. As a result, the model is forced to predict a score at multiple tokens where\\nthe context may end, rather than at a single token where it definitely ends. This adds a level of\\nambiguity which may cause the model to under-perform.\\n\\nC.5 Ensembling Over PMP Models\\n\\nIn prinicple we can ensemble together several models finetuned on the same final dataset, but which first\\npass through PMP on a distinct dataset. This would be a bit like ensembling over different random initializa-\\ntions, but what might hope for more interesting results due to the different semantic content in distinct PMP\\ndistributions. We tested this for summarization PMs that were separately PMP trained on Reddit and Stack\\nExchange, but only found a gain of order 0.5% in accuracy.\\n\\n38\\n\\n\\n\\nLearn to\\nSumm\\n\\nHella\\nSwag\\n\\nEthics:\\nJustice\\n\\nEthics:\\nMorality\\n\\nEthics:\\nDeon\\n\\nEthics:\\nVirtue\\n\\nEthics:\\nUtil\\n\\n0.05\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAccuracy Gain of UPM Pre-training At\\n500 Finetuning Seq Pairs (50B)\\n\\nLearn to\\nSumm\\n\\nHella\\nSwag\\n\\nEthics:\\nJustice\\n\\nEthics:\\nMorality\\n\\nEthics:\\nDeon\\n\\nEthics:\\nVirtue\\n\\nEthics:\\nUtil\\n\\n0.05\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAccuracy Gain of UPM Pre-training At\\n5k Finetuning Seq Pairs (50B)\\n\\nUPM Mix\\nUPM StackExch\\nUPM Reddit\\nUPM Wiki\\n\\nFigure 32 Accuracy gain of PMP as measured by accuracy difference relative to no PMP at 500 and 5k\\nfinetuning sequence pairs for multiple pre-training datasets (Mix, StackExchange, Reddit, Wikipedia) and\\nfinetuning evaluations (Learn to Summarize, HellaSwag, and all five Ethics evaluations).\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nExperiment on Ranked vs. Binary Pre-training (13B)\\n\\nNo PM Pretrain\\nBinary Pretrain\\nWeakly Scrambled Pretrain\\nModerately Scrambled Pretrain\\nStrongly Scrambled Pretrain\\nVery Strongly Scrambled Pretrain\\n\\nFigure 33 Results for a controlled experiment comparing the transfer ability of ranked vs. binary PM pre-\\ntraining, as explained in section 4.3. We see a clear trend whereby the sample efficiency degrades as the\\namount of relative scrambling between pre-training and finetuning distributions increases. Furthermore, we\\nfind that binary pre-training does not transfer as well as the weakly scrambled case, but transfers better than\\nthe very strongly scrambled case, in agreement with our expectations. This possibly explains why binary\\npre-training seems to perform better than ranked pre-training in most of our finetuning experiments.\\n\\nC.6 Experiments on Ranked vs Binary PMP – Synthetic Symbols Dataset\\n\\nWe began by generating a list of symbols, and assigned an arbitrary “Elo” ranking to them. A simple example\\nwould be the first five English letters ranked by alphabetical order.\\n\\nT_0 : A > B > C > D > E\\n\\nWe then generated a preference modeling dataset consisting of pairs of distinct symbols, so that within each\\npair a sample is “better” if it precedes the other with respect to the ranking. We call this the “control” dataset\\nT0. Furthermore, we created four additional datasets T1, T2, T3, T4, which were made in the same manner as\\nT0 but with increasingly scrambled symbol rankings. For instance,\\n\\nT_1 : A > B > C > [E] > [D] (Weakly Scrambled)\\nT_2 : A > B > [E] > [C] > [D] (Moderately Scrambled)\\nT_3 : A > [D] > [E] > [C] > [B] (Strongly Scrambled)\\n\\n39\\n\\n\\n\\nT_4 : [C] > [D] > [E] > [A] > [B] (Very Strongly Scrambled)\\n\\nwhere we enclosed in square brackets symbols that are out-of-place compared to the control. In addition, we\\nalso created a “binary” dataset Tb which labels symbols in the first half of the control ranking as “good” and\\nthose in the second half as “bad”. In other words,\\n\\nT_b : A , B , C > D , E , F (Binary)\\n\\nFinally, we pre-trained five preference models on Tb, T1, T2, T3, T4 separately, and compared their finetuning\\nperformance on the control T0. We also compare against a model trained directly on the control without\\npreference model pre-training.\\n\\nIn our actual experiment, we found that using only five symbols was too “easy” of a task to clearly distinguish\\nthe performance of different models, so instead we created a longer list of symbols, but otherwise the idea is\\nthe same. See section C.6 for details. Figure 33 shows the pairwise comparison accuracy on a held-out test\\nset vs. number of training samples during the finetuning stage. We make several observations:\\n\\n• We see a clear trend whereby sample efficiency consistently gets worse as the amount of scrambling\\nincreases. In fact, there is a scrambling “threshold” beyond which the sample efficiency is actually\\neven worse than no PMP at all. This confirms the hypothesis that datasets with significantly different\\nElo scales are expected to transfer poorly to each other.\\n\\n• The binary dataset is similarly sample efficient as a “moderately” scrambled dataset. This agrees\\nwith our hypothesis, which posits that a binary dataset should transfer better than a strongly scram-\\nbled dataset, but not necessarily better than a weakly scrambled one.\\n\\nClearly, the best possible PMP dataset is one that is qualitatively very similar to the final finetuning dataset, but\\ntypically this is not available. We see binarized PMP as a compromise that cannot guarantee the best possible\\nsample efficiency, but is more robustly capable of transferring to new preference modeling distributions.\\n\\nFinally, let us elaborate on our synthetic symbols dataset. Instead of using only five symbols, we used a list\\nof 676 symbols (using all ordered pairs of uppercase English letters), with a randomly assigned ranking. For\\neach symbol, the context is generated by repeating the symbol multiple times. For example, if the symbol AC\\nprecedes PQ in the ranking, then a preference modeling pair would look like\\n\\n(AC)(AC)(AC)(AC) > (PQ)(PQ)(PQ)(PQ)\\n\\nFurthermore, the scrambled datasets T1, T2, T3, T4 were obtained by applying 10, 40, 160, 640 randomly\\ngenerated transpositions to the control ranking, respectively. Finally, for the binary dataset Tb, a symbol is\\nlabeled “good” if it appears in the first half of the control ranking, and “bad otherwise. For instance, if AC\\nappears in the first half, and PQ appears in the second half, then the corresponding preference modeling pairs\\nwould look like\\n\\nGOOD:(AC)(AC)(AC)(AC) > BAD:(AC)(AC)(AC)(AC)\\nBAD:(PQ)(PQ)(PQ)(PQ) > GOOD:(PQ)(PQ)(PQ)(PQ)\\n\\nD Per-Token GAN-Style Discriminator Results\\n\\nOne way to train a discriminator is to utilize pretrained language models to generate samples, and train the\\ndiscriminator to distinguish between human and model generated tokens. The discriminator can then be used\\nfor rejection sampling, or ranking samples by how likely they were to be generated by a human.\\n\\nTo test out this naive setup, we created a training set by loading sequences of fixed numbers of tokens from\\nthe language pretraining dataset, followed by: with 1/3 chance truncating the text somewhere, and continuing\\nthe text by sampling from a language model; with 1/3 chance generating the same number of tokens entirely\\nfrom a model; and with the last 1/3 chance the text remained unchanged (fully human-generated). We used a\\n13B language model for sampling this dataset. For training, we initialized discriminator models as pretrained\\nlanguage models, and applied a binary cross entropy loss at each token for the human vs. model binary\\nclassification.\\n\\nAlthough qualitatively the models seem to be able to identify low quality model generated text, when evalu-\\nated on a few language benchmarks, we did not see promising improvement over the original language model\\n\\n40\\n\\n\\n\\n100 101 102\\nNumber of Top Samples\\n\\n0.45\\n\\n0.50\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\nAv\\n\\ner\\nag\\n\\ne \\nAc\\n\\ncu\\nra\\n\\ncy\\n\\nLambada: Discriminators vs. Pretrained Language Model\\nLM\\n\\n108\\n\\n109\\n\\n1010\\n\\nNum\\nber of Param\\n\\neters\\n\\n100 101 102\\nNumber of Top Samples\\n\\n0.45\\n0.50\\n0.55\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\nAv\\ner\\n\\nag\\ne \\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nLambada: Ensembles vs. Pretrained Language Model\\nLM\\n\\n108\\n\\n109\\n\\n1010\\n\\nNum\\nber of Param\\n\\neters\\n\\nFigure 34 Left: Discriminator and language model performance re-ranking Lambada answers. Right:\\nEnsemble of discriminator and language model, as determined in equation D.2.\\n\\n100 50 0 50 100\\nRelative token position to human->model transition\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1.0\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPer-Token Accuracy of Discriminator\\n\\n1010\\n\\n3 × 109\\n\\n4 × 109\\n\\n6 × 109\\n\\nNum\\nber of Param\\n\\neters\\n\\n100 101 102\\nRelative token position to human->model transition\\n\\n100\\n\\n2 × 10 1\\n\\n3 × 10 1\\n\\n4 × 10 1\\n\\n6 × 10 1\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPer-Token Accuracy of Discriminator\\n\\n1010\\n\\n3 × 109\\n\\n4 × 109\\n\\n6 × 109\\n\\nNum\\nber of Param\\n\\neters\\n\\nFigure 35 Per-token accuracy of a GAN-type discriminator trained to predict whether every individual\\ntoken is human or model generated. Position 0 is the first model-generated token.\\n\\nused to generate the training set (see figure 34). For these evaluations, we first generated 100 samples from\\neach of the prompt in the test set. For the discriminators, we ranked the samples by the average predicted\\nprobability of being human generated over sample tokens. For the language model, we ranked the samples by\\nthe negative average log-prob over sample tokens. The plots show average metric over top-N ranked samples.\\nWe observe that the language model performs much better on these benchmarks, in both performance and\\nrobustness.\\n\\nHowever, we will now argue that it is not appropriate to directly use the discriminator to rank samples. Let us\\nuse P (t) to denote the probability distribution of human-generated tokens and Pθ(t) to represent a language\\nmodel. The goal of the discriminator Dφ is to model the probability that a given token was model generated,\\nso Dφ(t) is attempting to model the probability p(human|t). Assuming a prior that a token is 50% likely to\\ncome from a human, after seeing the token, an ideal discriminator would predict\\n\\nD(t) =\\nP (t)\\n\\nP (t) + Pθ(t)\\n(D.1)\\n\\nfor the probability that any token was written by a human.\\n\\nBut this means that we can use a learned Dφ(t) to improve model predictions for any given token t, by\\nre-arranging to give the new ensemble distribution\\n\\nPensemble(t) =\\nDφ(t)\\n\\n1−Dφ(t)\\nPθ(t) (D.2)\\n\\nand this ensemble model should improve on the original language model distribution Pθ. In particular, this\\nensemble provides a more principled way to re-rank model-generated samples, using both the discriminator\\nand the language model probabilities together. We display the result on the right in figure 34, where we see\\nthat as expected, the ensemble can improve on the language model.\\n\\nFigure 35 shows the per-token prediction accuracy on the training set, relative to the position where the tokens\\nswitch from being human-generated to model-generated. We observe an interesting behavior – even though\\n\\n41\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Ethics: Justice\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Ethics: Commonsense Morality\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Ethics: Deontology\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.50\\n\\n0.55\\n\\n0.60\\n\\n0.65\\n\\n0.70\\n\\n0.75\\n\\n0.80\\n\\n0.85\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Ethics: Utilitarianism\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\n103 104 105\\nNumber of Finetuning Sequence Pairs\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\nPerformance of Human-human vs. Human-model \\n Reddit PMP On Ethics: Virtue\\n\\nHuman-model (13B)\\nHuman-model (2.7B)\\nHuman-model (810M)\\nHuman-human (13B)\\nHuman-human (2.7B)\\nHuman-human (810M)\\n\\nFigure 36 We compare PMP on “human-human” vs “human-model” datasets by evaluating their transfer\\nperformance on the five Ethics datasets. It appears that one does not consistently outperform the other, and\\nthe results are rather random. We suspect that “human-model” does not have any particular advantage when\\nfinetuning on evaluations that are purely human-written, such as Ethics.\\n\\nlarger models obtain higher overall accuracy, they perform worse immediately after the transition from human\\nto model generated tokens.\\n\\n42\\n\\n\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\nAc\\n\\ncu\\nra\\n\\ncy\\n D\\n\\niff\\ner\\n\\nen\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Hellaswag (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.01\\n\\n0.00\\n\\n0.01\\n\\n0.02\\n\\n0.03\\n\\n0.04\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Learn to Summarize (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.05\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Ethics: Justice (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.04\\n\\n0.02\\n\\n0.00\\n\\n0.02\\n\\n0.04\\n\\n0.06\\n\\n0.08\\n\\n0.10\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Ethics: Commonsense Morality (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.05\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Ethics: Deontology (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n103 104 105\\nNumber of Finetuning Sequence Pairs\\n\\n0.06\\n\\n0.04\\n\\n0.02\\n\\n0.00\\n\\n0.02\\n\\n0.04\\n\\n0.06\\n\\n0.08\\n\\n0.10\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Ethics: Virtue (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\n102 103 104\\nNumber of Finetuning Sequence Pairs\\n\\n0.02\\n\\n0.00\\n\\n0.02\\n\\n0.04\\n\\n0.06\\n\\n0.08\\n\\nAc\\ncu\\n\\nra\\ncy\\n\\n D\\niff\\n\\ner\\nen\\n\\nce\\n\\nAcc Gain of Binary Over Ranked PMP On \\n Ethics: Utilitarianism (13B)\\n\\nPMP Mix\\nPMP StackExch\\nPMP Reddit\\nPMP Wiki\\n\\nFigure 37 Accuracy gain of binary over ranked PMP on finetuning evaluations.\\n\\n43\\n\\n\\n\\nE Definitions of Alignment and the HHH criteria\\n\\nPeople often mean subtly different things when they talk about AI systems being \"aligned\". Given this, we\\nwant to elaborate on what we mean by this term and why we selected the \"helpful, honest, and harmless\"\\nconception of aligned AI assistants.\\n\\nE.1 How the HHH criteria relate to alignment\\n\\nAt a very high level, alignment can be thought of as the degree of overlap between the way two agents rank\\ndifferent outcomes. For example, if agent A completely internalizes the desires of agent B—i.e. the only\\ndesire A has is to see B’s desires satisfied—we could say that agent A is maximally aligned with agent B.20\\n\\nWe believe it is difficult for an AI assistant to always be helpful, honest, and harmless towards an agent or\\ngroup without also being highly aligned with that agent or group according to this definition of alignment. To\\nsee why, suppose we want the AI assistant to be aligned with a specific group of humans. Here is what each\\nof the three conditions implies about the assistant:\\n\\n• Helpfulness: the assistant will always try to do what is in the humans’ best interests\\n• Honesty: the assistant will always try to convey accurate information to the humans and will always\\n\\ntry to avoid deceiving them21\\n\\n• Harmlessness: the assistant will always try to avoid doing anything that harms the humans\\n\\nAn AI assistant that is always helpful, honest, and harmless towards a group of humans will always try to act\\nin a way that satisfies the interests of this group, including their interest not to be harmed or be misled. It is\\ntherefore likely to be highly aligned with the interests of that group of humans.\\n\\nThis account of alignment is still vague and leaves many open questions. In particular, it does not tell us:\\n\\n• What kinds of outcome orderings are most relevant for AI alignment (preferences, idealized prefer-\\nences, wellbeing, ethical rankings, etc.)\\n\\n• The degree to which these outcome orderings are objective or subjective\\n• Which agents the AI systems should be aligned to (users, developers, humanity, etc.)\\n• How AI systems can or should aggregate different outcome orderings if they are aligned to more\\n\\nthan one agent\\n• What is the precise formulation of \"overlap between outcome rankings\"\\n• How large or small the space of maximally aligned agents is, given the above\\n\\nMany of these questions are discussed in more detail elsewhere [Gab20]. Progress in AI alignment will\\nhopefully not require us to reach certainty about any of them, since such certainty is unlikely to be achieved.\\nBut it is worth making these unanswered questions explicit. When we train aligned AI systems, we may need\\nto make choices that will implicitly favor or assume certain answers to them. And how we define the HHH\\ncriteria will depend on what kind of orderings we think are most relevant for AI alignment.\\n\\nE.2 The relation between the HHH criteria\\n\\nIf we define helpfulness and harmlessness such that (a) it’s never in a human’s best interest to be harmed, and\\n(b) it’s always harmful to fail to do something that’s in a human’s best interest, we can reduce helpfulness and\\nharmlessness to either criterion. We have separated them because we find it practically easier to distinguish\\ncases of active harm from cases in which a benefit is withheld.\\n\\nHelpfulness and harmlessness clearly can’t be reduced to honesty, but honesty can be reduced to helpful-\\nness/harmlessness. According to the definition of alignment given above, an aligned AI assistant should be\\nhonest because honesty is valued by humans. This could either be because honesty is instrumentally valuable\\n\\n20Even if agent A is maximally aligned with agent B, A can fail to act in accordance with B’s desires because A has a\\nmistaken belief about B’s desires or about the world, or because A is unable to carry out their intended action.\\n\\n21This concept of \"honesty\" involves avoiding multiple different conditions of lying, such as only stating true claims\\nand not causing false beliefs in the listener [Mah15]. There will be cases where these conditions conflict. In such cases,\\nwe would need to assess which conception of honesty it would be most helpful and harmless for the assistant to satisfy.\\n\\n44\\n\\n\\n\\nto humans or because humans intrinsically value it. If honesty were genuinely not something that humans\\nvalue even on reflection, an AI that was aligned with human values would presumably not be honest.\\n\\nBut if the value of honesty is reducible to helpfulness and harmlessness, why include it in our list? The\\nanswer is mostly practical: honesty is important and distinct enough to warrant particular attention.\\n\\nWe could also choose to introduce other concepts which—like honesty—can’t be reduced to helpfulness\\nor harmlessness but are important properties that a helpful, harmless AI assistant will typically have. For\\nexample, we considered adding the following fourth ’H’:\\n\\n• Handleable: the assistant will always be responsive to feedback from the humans and carry out any\\ninstructions from the humans in the way that the humans intended\\n\\nHandleability is similar what others have called \"corrigibility\" [SFAY15]. A system that isn’t handleable is\\nless helpful and more harmful than a system that is handleable. But it may be useful to pay special attention\\nto failures that involve the assistant not doing what is asked or not responding to human feedback. For this\\nreason it seems like a good candidate for a fourth ’H’.\\n\\nIn other words, what we want to include in this list, beyond a joint or separate helpfulness/harmlessness\\ncondition, depends on what behaviors we find it useful to pay particular attention to.\\n\\nE.3 Conflicts between the HHH criteria\\n\\nAs we note in the main text, the three conditions above will sometimes appear to be in conflict. There are\\ntwo possible kinds of conflicts between the three conditions:\\n\\n• Intra-agent conflicts: Cases in which two or more HHH conditions are in conflict even if we just\\nwant to align the assistant with a particular human. For example, it is not possible to be honest or\\nhelpful towards a particular human without saying something that is pro tanto harmful to them, e.g.\\nsomething that will hurt their feelings.\\n\\n• Inter-agent conflicts: Cases in which two or more HHH conditions conflict across different agents\\nwe might want to align the assistant with. For example, it is not possible to be helpful towards a\\nparticular human without saying something that is harmful to a others we want to align it with, e.g.\\nif one human asks for help building a bomb to use against others.\\n\\nIf helpfulness and harmlessness can be reduced to a single joint condition and honesty can also be reduced to\\nhelpfulness/harmlessness, intra-agent conflicts will turn out to be merely superficial since all three conditions\\ncan ultimately be reduced to a single coherent condition.\\n\\nInter-agent conflicts are a different matter. It is very likely that a single AI cannot be maximally aligned with\\nany two different humans, since both humans will have at least some conflicting desires or values. This is\\nwhy an AI assistant will often be unable to be helpful, honest and harmless towards some humans without\\nbeing unaligned with other humans (e.g. by refusing to help them build the bomb). It is therefore important\\nfor us to be aware of who we are asking the AI assistants to be helpful, honest, and harmless towards, since\\nthis also determines which humans the AI assistants are not fully aligned with and to what degree.\\n\\nE.4 The HHH criteria and secure AI\\n\\nAlthough we want to develop aligned AI systems, it may not be possible to guarantee that AI systems are\\nfully aligned with human values. So we’ll also want our AI systems to be secure: to have properties or be\\nembedded in systems that decrease the potential for harm even if the AI is less than fully aligned [HCSS21].\\nWe may want AI systems that always respond to the intended instructions of humans, that always avoid doing\\ncertain things that most humans consider very bad, that fail both securely and loudly, that make decisions in\\nways that can be made transparent to humans, that are secure against alteration or misuse, and so on.\\n\\nAn AI assistant that is helpful, honest, and harmless is a secure system in many respects and will try to assist\\nhumans in making itself more secure. But the HHH criteria were not selected with AI security in mind and\\nnot all security features will be features of the AI system itself. We therefore want to emphasize that the HHH\\ncriteria are criteria of alignment, and that additional work and additional areas of focus may be required to\\nensure that AI systems cannot cause too much harm when they are not fully aligned.\\n\\n45\\n\\n\\n\\nReferences\\n[AAB+21] Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin,\\n\\nRachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aure-\\nlia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lilli-\\ncrap, Kory Mathewson, Sona Mokra, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant\\nVarma, Greg Wayne, Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating\\ninteractive intelligence, 2021, 2012.05672.\\n\\n[AON+21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis\\nwith large language models, 2021, 2108.07732.\\n\\n[AOS+16] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.\\nConcrete problems in ai safety, 2016, 1606.06565.\\n\\n[ATS+21] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav\\nMehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian\\nRuder, and Donald Metzler. Ext5: Towards extreme multi-task scaling for transfer learning,\\n2021, 2111.10952.\\n\\n[BGNN19] Daniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating be-\\nyond suboptimal demonstrations via inverse reinforcement learning from observations, 2019,\\n1904.06387.\\n\\n[BMR+20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020,\\n2005.14165.\\n\\n[Bow21] Samuel R. Bowman. When combating hype, proceed with caution, 2021, 2110.08300.\\n[CKB+21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christo-\\n\\npher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021,\\n2110.14168.\\n\\n[CLB+17] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\\nDeep reinforcement learning from human preferences, 2017, 1706.03741.\\n\\n[CLR+21] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter\\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\\nvia sequence modeling, 2021, 2106.01345.\\n\\n[CSA18] Paul Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying\\nweak experts, 2018, 1810.08575.\\n\\n[CTJ+21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavar-\\nian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,\\nFotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol,\\nAlex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William\\nSaunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan\\nMorikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Pe-\\nter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\\nZaremba. Evaluating large language models trained on code, 2021, 2107.03374.\\n\\n[Fou] The Common Crawl Foundation. Common crawl. URL http://commoncrawl.org.\\n[Gab20] Iason Gabriel. Artificial intelligence, values, and alignment. Minds and Machines, 30(3):411–\\n\\n437, Sep 2020. doi:10.1007/s11023-020-09539-2.\\n[GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\\n\\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:\\nAn 800gb dataset of diverse text for language modeling, 2020, 2101.00027.\\n\\n46\\n\\nhttp://arxiv.org/abs/2012.05672\\nhttp://arxiv.org/abs/2108.07732\\nhttp://arxiv.org/abs/1606.06565\\nhttp://arxiv.org/abs/2111.10952\\nhttp://arxiv.org/abs/1904.06387\\nhttp://arxiv.org/abs/2005.14165\\nhttp://arxiv.org/abs/2110.08300\\nhttp://arxiv.org/abs/2110.14168\\nhttp://arxiv.org/abs/1706.03741\\nhttp://arxiv.org/abs/2106.01345\\nhttp://arxiv.org/abs/1810.08575\\nhttp://arxiv.org/abs/2107.03374\\nhttp://commoncrawl.org\\nhttp://dx.doi.org/10.1007/s11023-020-09539-2\\nhttp://arxiv.org/abs/2101.00027\\n\\n\\n[GGS+20] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxi-\\ncityprompts: Evaluating neural toxic degeneration in language models, 2020, 2009.11462.\\n\\n[HBB+21] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\\nSteinhardt. Aligning ai with shared human values, 2021, 2008.02275.\\n\\n[HCSS21] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in\\nml safety, 2021, 2109.13916.\\n\\n[HE16] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning, 2016, 1606.03476.\\n\\n[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kian-\\ninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-\\ndictable, empirically, 2017, 1712.00409.\\n\\n[HU20] Laura Hanu and Unitary team. Detoxify. Github. https://github.com/unitaryai/detoxify, 2020.\\n\\n[ICA18] Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate, 2018, 1805.00899.\\n\\n[ILP+18] Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward\\nlearning from human preferences and demonstrations in atari, 2018, 1811.06521.\\n\\n[JHB+21] Liwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Bor-\\nchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. Delphi: Towards machine\\nethics and norms, 2021, 2110.07574.\\n\\n[Jon21] Andy L. Jones. Scaling scaling laws with board games, 2021, 2104.03113.\\n\\n[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels, 2020, 2001.08361.\\n\\n[KSW21] Mojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation,\\n2021, 2107.07566.\\n\\n[LARC21] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient\\nprompt tuning, 2021, 2104.08691.\\n\\n[LHE21] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic\\nhuman falsehoods, 2021, 2109.07958.\\n\\n[LKCSL17] Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. Quixbugs: A multi-\\nlingual program repair benchmark set based on the quixey challenge. In Proceedings Companion\\nof the 2017 ACM SIGPLAN International Conference on Systems, Programming, Languages,\\nand Applications: Software for Humanity, SPLASH Companion 2017, pages 55–56, New York,\\nNY, USA, 2017. Association for Computing Machinery. doi:10.1145/3135932.3135941.\\n\\n[LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and\\nNoam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs],\\n2018, 1801.10198. URL http://arxiv.org/abs/1801.10198.\\n\\n[Mah15] James Edwin Mahon. The definition of lying and deception. In Ed Zalta, editor, Stanford\\nEncyclopedia of Philosophy. 2015.\\n\\n[NRA+21] Helen Ngo, Cooper Raterink, JoÃ£o G. M. AraÃºjo, Ivan Zhang, Carol Chen, Adrien Morisot,\\nand Nicholas Frosst. Mitigating harm in language models with conditional-likelihood filtration,\\n2021, 2108.07790.\\n\\n[PKL+16] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella\\nBernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez. The lam-\\nbada dataset: Word prediction requiring a broad discourse context, 2016, 1606.06031.\\n\\n[RDR20] Vinay V. Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hid-\\nden representations and task semantics, 2020, 2007.07400.\\n\\n[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. 2018.\\n\\n[RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\\nprediction of the generalization error across scales, 2019, arXiv:1909.12673.\\n\\n[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. openai.com, 2019.\\n\\n47\\n\\nhttp://arxiv.org/abs/2009.11462\\nhttp://arxiv.org/abs/2008.02275\\nhttp://arxiv.org/abs/2109.13916\\nhttp://arxiv.org/abs/1606.03476\\nhttp://arxiv.org/abs/1712.00409\\nhttp://arxiv.org/abs/1805.00899\\nhttp://arxiv.org/abs/1811.06521\\nhttp://arxiv.org/abs/2110.07574\\nhttp://arxiv.org/abs/2104.03113\\nhttp://arxiv.org/abs/2001.08361\\nhttp://arxiv.org/abs/2107.07566\\nhttp://arxiv.org/abs/2104.08691\\nhttp://arxiv.org/abs/2109.07958\\nhttp://dx.doi.org/10.1145/3135932.3135941\\nhttp://arxiv.org/abs/1801.10198\\nhttp://arxiv.org/abs/1801.10198\\nhttp://arxiv.org/abs/2108.07790\\nhttp://arxiv.org/abs/1606.06031\\nhttp://arxiv.org/abs/2007.07400\\nhttp://arxiv.org/abs/arXiv:1909.12673\\n\\n\\n[SD21] Irene Solaiman and Christy Dennison. Process for adapting language models to society\\n(PALMS) with values-targeted datasets. CoRR, abs/2106.10328, 2021, 2106.10328. URL\\nhttps://arxiv.org/abs/2106.10328.\\n\\n[SFAY15] Nate Soares, Benja Fallenstein, Stuart Armstrong, and Eliezer Yudkowsky. Corrigibility. In\\nWorkshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\\n\\n[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. CoRR, 2015, 1508.07909.\\n\\n[SOW+20] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec\\nRadford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback,\\n2020, 2009.01325.\\n\\n[SWR+21] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,\\nAntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Can-\\nwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan\\nChhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang,\\nMatteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang,\\nTrishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries,\\nRyan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush.\\nMultitask prompted training enables zero-shot task generalization, 2021, 2110.08207.\\n\\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n\\n[WBZ+21] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\\nDu, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2021,\\n2109.01652.\\n\\n[WGU+21] Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne\\nHendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Challenges in\\ndetoxifying language models, 2021, arXiv:2109.07445.\\n\\n[WOZ+21] Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul\\nChristiano. Recursively summarizing books with human feedback, 2021, 2109.10862.\\n\\n[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\\nmachine really finish your sentence?, 2019, 1905.07830.\\n\\n48\\n\\nhttp://arxiv.org/abs/2106.10328\\nhttps://arxiv.org/abs/2106.10328\\nhttp://arxiv.org/abs/1508.07909\\nhttp://arxiv.org/abs/2009.01325\\nhttp://arxiv.org/abs/2110.08207\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\\nhttp://arxiv.org/abs/2109.01652\\nhttp://arxiv.org/abs/arXiv:2109.07445\\nhttp://arxiv.org/abs/2109.10862\\nhttp://arxiv.org/abs/1905.07830\\n\\n\\t1 Introduction\\n\\t2 Conditioning on Aligned Behavior\\n\\t3 Scaling of Preference Modeling vs Imitation Learning\\n\\t4 Preference Model Pre-Training and Transfer\\n\\t5 Discussion\\n\\tA Language Model Pre-training\\n\\tB More Details on Prompting, Context Distillation, and Evaluations\\n\\tC More Details on Preference Models\\n\\tD Per-Token GAN-Style Discriminator Results\\n\\tE Definitions of Alignment and the HHH criteria\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cmiZn7w8sopD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}