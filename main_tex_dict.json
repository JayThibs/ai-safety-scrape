{
    "2111.13872v1": {
        "main_tex_filename": "coopai_workshop_2021.tex"
    },
    "2102.05008v1": {
        "main_tex_filename": "ms.tex"
    },
    "2012.10800v1": {
        "main_tex_filename": "pdg-arXiv-stripped.tex"
    },
    "2002.11089v1": {
        "main_tex_filename": "relabelling.tex"
    },
    "2012.02671v2": {
        "main_tex_filename": "main_v1.tex"
    },
    "1911.04266v3": {
        "main_tex_filename": "main_SafeAI.tex"
    },
    "2006.15191v2": {
        "main_tex_filename": "sample.tex"
    },
    "1907.03843v2": {
        "main_tex_filename": "Main_Text.tex"
    },
    "2011.08820v1": {
        "main_tex_filename": "realab2.tex"
    },
    "1705.09990v1": {
        "main_tex_filename": "ijcai17.tex"
    },
    "1802.03493v2": {
        "main_tex_filename": "main_MRDR.tex"
    },
    "1705.04226v2": {
        "main_tex_filename": "CogsciHRI.tex"
    },
    "1907.00452v1": {
        "main_tex_filename": "ijcai19.tex"
    },
    "2011.04483v1": {
        "main_tex_filename": "trichotomy.tex"
    },
    "1707.08747v1": {
        "main_tex_filename": "LogicalNonOmniscience.tex"
    },
    "1812.02795v1": {
        "main_tex_filename": "nips_2018_prob_verify.tex"
    },
    "1812.01647v1": {
        "main_tex_filename": "iclr2019_conference_cleaned.tex"
    },
    "2007.02382v2": {
        "main_tex_filename": "icml_main.tex"
    },
    "2010.14603v1": {
        "main_tex_filename": "0-main.tex"
    },
    "1910.13369v2": {
        "main_tex_filename": "root.tex"
    },
    "1707.05173v1": {
        "main_tex_filename": "draft_arxiv.tex"
    },
    "1801.08757v1": {
        "main_tex_filename": "safety_layer_v5.tex"
    },
    "2011.08541v1": {
        "main_tex_filename": "boirl_draft.tex"
    },
    "1805.10265v2": {
        "main_tex_filename": "nips_2018.tex"
    },
    "1911.00497v1": {
        "main_tex_filename": "SC2_ICML.tex"
    },
    "2001.07118v2": {
        "main_tex_filename": "the-incentives-that-shape-behaviour.tex"
    },
    "2002.11328v1": {
        "main_tex_filename": "ms.tex"
    },
    "2011.08512v1": {
        "main_tex_filename": "pdg-arXiv-stripped.tex"
    },
    "2004.09044v1": {
        "main_tex_filename": "dark.tex"
    },
    "2004.10802v1": {
        "main_tex_filename": "ScalingLawsDataManifoldDimensionFinal.tex"
    },
    "1901.08654v1": {
        "main_tex_filename": "conference_041818.tex"
    },
    "1806.02404v1": {
        "main_tex_filename": "RSPA_Author_tex.tex"
    },
    "1902.09980v7": {
        "main_tex_filename": "incentives1.tex"
    },
    "1807.05185v1": {
        "main_tex_filename": "final.tex"
    },
    "1906.09624v1": {
        "main_tex_filename": "conference_041818.tex"
    },
    "1802.01604v1": {
        "main_tex_filename": "corl_2018.tex"
    },
    "1805.12573v5": {
        "main_tex_filename": "ird_nips17.tex"
    },
    "1807.05037v1": {
        "main_tex_filename": "minimax-longcorr.tex"
    },
    "2006.04635v4": {
        "main_tex_filename": "agile.tex"
    },
    "1807.00196v1": {
        "main_tex_filename": "coopai_workshop_2021.tex"
    },
    "1902.02767v2": {
        "main_tex_filename": "safety_icml.tex"
    },
    "1810.09136v3": {
        "main_tex_filename": "ms.tex"
    },
    "2010.07877v1": {
        "main_tex_filename": "arxiv2.tex"
    },
    "1802.05666v2": {
        "main_tex_filename": "ai-acp.tex"
    },
    "1811.04017v2": {
        "main_tex_filename": "emnlp-ijcnlp-2019.tex"
    },
    "1805.01109v2": {
        "main_tex_filename": "haibrid_chess_kdd.tex"
    },
    "1507.01986v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "2001.00078v1": {
        "main_tex_filename": "neurips_2021.tex"
    },
    "2102.02503v1": {
        "main_tex_filename": "arxiv_submission.tex"
    },
    "1607.08289v4": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1908.07613v3": {
        "main_tex_filename": "root.tex"
    },
    "1106.2657v1": {
        "bibliography": "\\begin{thebibliography}{10}\n\n\\bibitem{bengio_lm}\nYoshua Bengio, R{\\'e}jean Ducharme, Pascal Vincent, and Christian Janvin.\n\\newblock A neural probabilistic language model.\n\\newblock {\\em The Journal of Machine Learning Research}, 3:1137--1155, 2003.\n\n\\bibitem{collobert}\nRonan Collobert and Jason Weston.\n\\newblock A unified architecture for natural language processing: deep neural\n  networks with multitask learning.\n\\newblock In {\\em Proceedings of the 25th international conference on Machine\n  learning}, pages 160--167. ACM, 2008.\n\n\\bibitem{bengio_sentiment}\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n\\newblock Domain adaptation for large-scale sentiment classification: A deep\n  learning approach.\n\\newblock In {\\em ICML}, 513--520, 2011.\n\n\\bibitem{nce}\nMichael~U Gutmann and Aapo Hyv{\\\"a}rinen.\n\\newblock Noise-contrastive estimation of unnormalized statistical models, with\n  applications to natural image statistics.\n\\newblock {\\em The Journal of Machine Learning Research}, 13:307--361, 2012.\n\n\\bibitem{mikolov3}\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev\n  Khudanpur.\n\\newblock Extensions of recurrent neural network language model.\n\\newblock In {\\em Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE\n  International Conference on}, pages 5528--5531. IEEE, 2011.\n\n\\bibitem{largernnlmforasr}\nTomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky.\n\\newblock Strategies for Training Large Scale Neural Network Language Models.\n\\newblock In Proc. {\\em Automatic Speech Recognition and Understanding}, 2011.\n\n\\bibitem{mikolov4}\nTomas Mikolov.\n\\newblock Statistical Language Models Based on Neural Networks.\n\\newblock {\\em PhD thesis, PhD Thesis, Brno University of Technology}, 2012.\n\n\\bibitem{mikolov}\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n\\newblock Efficient estimation of word representations in vector space.\n\\newblock {\\em ICLR Workshop}, 2013.\n\n\\bibitem{linreg}\nTomas Mikolov, Wen-tau Yih and Geoffrey Zweig.\n\\newblock Linguistic Regularities in Continuous Space Word Representations.\n\\newblock In {\\em Proceedings of NAACL HLT}, 2013.\n\n\\bibitem{mnih}\nAndriy Mnih and Geoffrey~E Hinton.\n\\newblock A scalable hierarchical distributed language model.\n\\newblock {\\em Advances in neural information processing systems},\n  21:1081--1088, 2009.\n\n\\bibitem{mnih-nce}\nAndriy Mnih and Yee~Whye Teh.\n\\newblock A fast and simple algorithm for training neural probabilistic\n  language models.\n\\newblock {\\em arXiv preprint arXiv:1206.6426}, 2012.\n\n\\bibitem{hsoft_first}\nFrederic Morin and Yoshua Bengio.\n\\newblock Hierarchical probabilistic neural network language model.\n\\newblock In {\\em Proceedings of the international workshop on artificial\n  intelligence and statistics}, pages 246--252, 2005.\n\n\\bibitem{nature}\nDavid~E Rumelhart, Geoffrey~E Hintont, and Ronald~J Williams.\n\\newblock Learning representations by back-propagating errors.\n\\newblock {\\em Nature}, 323(6088):533--536, 1986.\n\n\\bibitem{Schwenk}\nHolger Schwenk.\n\\newblock Continuous space language models.\n\\newblock {\\em Computer Speech and Language}, vol. 21, 2007.\n\n\\bibitem{socher}\nRichard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning.\n\\newblock Parsing natural scenes and natural language with recursive neural\n  networks.\n\\newblock In {\\em Proceedings of the 26th International Conference on Machine\n  Learning (ICML)}, volume~2, 2011.\n\n\\bibitem{socher2}\nRichard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.\n\\newblock Semantic Compositionality Through Recursive Matrix-Vector Spaces.\n\\newblock In {\\em Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2012.\n\n\\bibitem{turian}\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\n\\newblock Word representations: a simple and general method for semi-supervised\n  learning.\n\\newblock In {\\em Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics}, pages 384--394. Association for Computational\n  Linguistics, 2010.\n\n\\bibitem{turney}\nPeter D. Turney and Patrick Pantel.\n\\newblock From frequency to meaning: Vector space models of semantics.\n\\newblock In {\\em Journal of Artificial Intelligence Research}, 37:141-188, 2010.\n\n\\bibitem{turney2}\nPeter D. Turney.\n\\newblock Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\n\\newblock In {\\em Transactions of the Association for Computational Linguistics (TACL)}, 353--366, 2013.\n\n\\bibitem{wsabie}\nJason Weston, Samy Bengio, and Nicolas Usunier.\n\\newblock Wsabie: Scaling up to large vocabulary image annotation.\n\\newblock In {\\em Proceedings of the Twenty-Second international joint\n  conference on Artificial Intelligence-Volume Volume Three}, pages 2764--2770.\n  AAAI Press, 2011.\n\n\\end{thebibliography}\n"
    },
    "1805.08347v3": {
        "main_tex_filename": "aihuman_main.tex"
    },
    "1912.05743v2": {
        "main_tex_filename": "main_arxiv.tex"
    },
    "1901.08579v2": {
        "main_tex_filename": "acl2020.tex"
    },
    "2002.08484v3": {
        "main_tex_filename": "0-main.tex"
    },
    "1704.02882v2": {
        "main_tex_filename": "root.tex"
    },
    "1308.3778v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1810.05157v4": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "2005.01831v1": {
        "main_tex_filename": "acl2020.tex"
    },
    "1805.08882v2": {
        "main_tex_filename": "manuscript.tex"
    },
    "2002.01059v2": {
        "main_tex_filename": "draft_camera.tex"
    },
    "1806.01946v4": {
        "main_tex_filename": "agile.tex"
    },
    "2006.14804v5": {
        "main_tex_filename": "0-main.tex"
    },
    "1807.08364v3": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "2102.01685v2": {
        "main_tex_filename": "ai-acp.tex"
    },
    "1909.01492v2": {
        "main_tex_filename": "emnlp-ijcnlp-2019.tex"
    },
    "2109.10862v2": {
        "main_tex_filename": "neurips_2021.tex"
    },
    "1901.01291v2": {
        "main_tex_filename": "conference_041818.tex"
    },
    "2109.07958v1": {
        "main_tex_filename": "arxiv_submission.tex"
    },
    "2010.00581v3": {
        "main_tex_filename": "sociallearning.tex"
    },
    "2006.01855v3": {
        "main_tex_filename": "haibrid_chess_kdd.tex"
    },
    "1711.02827v2": {
        "main_tex_filename": "ird_nips17.tex"
    },
    "2007.09540v1": {
        "main_tex_filename": "final.tex"
    },
    "2011.08827v1": {
        "main_tex_filename": "arxiv2.tex"
    },
    "1901.10031v2": {
        "main_tex_filename": "safety_icml.tex"
    },
    "1604.00289v3": {
        "main_tex_filename": "core.tex"
    },
    "1802.01780v1": {
        "main_tex_filename": "goal-inference-improves.tex"
    },
    "1904.01318v1": {
        "main_tex_filename": "main_arxiv.tex"
    },
    "1811.01267v1": {
        "main_tex_filename": "draft_aies.tex"
    },
    "1805.12387v1": {
        "main_tex_filename": "finding-agents.tex"
    },
    "1804.04268v1": {
        "main_tex_filename": "incomplete_contracts_nips_fullpaper_v2.tex"
    },
    "1709.06166v1": {
        "main_tex_filename": "root.tex"
    },
    "1905.01034v1": {
        "main_tex_filename": "arxiv-20190502.tex"
    },
    "2009.09266v1": {
        "main_tex_filename": "aihuman_main.tex"
    },
    "1805.00899v2": {
        "main_tex_filename": "debate.tex"
    },
    "1312.5602v1": {
        "bibliography": "\\begin{thebibliography}{10}\n\n\\bibitem{bengio_lm}\nYoshua Bengio, R{\\'e}jean Ducharme, Pascal Vincent, and Christian Janvin.\n\\newblock A neural probabilistic language model.\n\\newblock {\\em The Journal of Machine Learning Research}, 3:1137--1155, 2003.\n\n\\bibitem{collobert}\nRonan Collobert and Jason Weston.\n\\newblock A unified architecture for natural language processing: deep neural\n  networks with multitask learning.\n\\newblock In {\\em Proceedings of the 25th international conference on Machine\n  learning}, pages 160--167. ACM, 2008.\n\n\\bibitem{bengio_sentiment}\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n\\newblock Domain adaptation for large-scale sentiment classification: A deep\n  learning approach.\n\\newblock In {\\em ICML}, 513--520, 2011.\n\n\\bibitem{nce}\nMichael~U Gutmann and Aapo Hyv{\\\"a}rinen.\n\\newblock Noise-contrastive estimation of unnormalized statistical models, with\n  applications to natural image statistics.\n\\newblock {\\em The Journal of Machine Learning Research}, 13:307--361, 2012.\n\n\\bibitem{mikolov3}\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev\n  Khudanpur.\n\\newblock Extensions of recurrent neural network language model.\n\\newblock In {\\em Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE\n  International Conference on}, pages 5528--5531. IEEE, 2011.\n\n\\bibitem{largernnlmforasr}\nTomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky.\n\\newblock Strategies for Training Large Scale Neural Network Language Models.\n\\newblock In Proc. {\\em Automatic Speech Recognition and Understanding}, 2011.\n\n\\bibitem{mikolov4}\nTomas Mikolov.\n\\newblock Statistical Language Models Based on Neural Networks.\n\\newblock {\\em PhD thesis, PhD Thesis, Brno University of Technology}, 2012.\n\n\\bibitem{mikolov}\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n\\newblock Efficient estimation of word representations in vector space.\n\\newblock {\\em ICLR Workshop}, 2013.\n\n\\bibitem{linreg}\nTomas Mikolov, Wen-tau Yih and Geoffrey Zweig.\n\\newblock Linguistic Regularities in Continuous Space Word Representations.\n\\newblock In {\\em Proceedings of NAACL HLT}, 2013.\n\n\\bibitem{mnih}\nAndriy Mnih and Geoffrey~E Hinton.\n\\newblock A scalable hierarchical distributed language model.\n\\newblock {\\em Advances in neural information processing systems},\n  21:1081--1088, 2009.\n\n\\bibitem{mnih-nce}\nAndriy Mnih and Yee~Whye Teh.\n\\newblock A fast and simple algorithm for training neural probabilistic\n  language models.\n\\newblock {\\em arXiv preprint arXiv:1206.6426}, 2012.\n\n\\bibitem{hsoft_first}\nFrederic Morin and Yoshua Bengio.\n\\newblock Hierarchical probabilistic neural network language model.\n\\newblock In {\\em Proceedings of the international workshop on artificial\n  intelligence and statistics}, pages 246--252, 2005.\n\n\\bibitem{nature}\nDavid~E Rumelhart, Geoffrey~E Hintont, and Ronald~J Williams.\n\\newblock Learning representations by back-propagating errors.\n\\newblock {\\em Nature}, 323(6088):533--536, 1986.\n\n\\bibitem{Schwenk}\nHolger Schwenk.\n\\newblock Continuous space language models.\n\\newblock {\\em Computer Speech and Language}, vol. 21, 2007.\n\n\\bibitem{socher}\nRichard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning.\n\\newblock Parsing natural scenes and natural language with recursive neural\n  networks.\n\\newblock In {\\em Proceedings of the 26th International Conference on Machine\n  Learning (ICML)}, volume~2, 2011.\n\n\\bibitem{socher2}\nRichard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.\n\\newblock Semantic Compositionality Through Recursive Matrix-Vector Spaces.\n\\newblock In {\\em Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2012.\n\n\\bibitem{turian}\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\n\\newblock Word representations: a simple and general method for semi-supervised\n  learning.\n\\newblock In {\\em Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics}, pages 384--394. Association for Computational\n  Linguistics, 2010.\n\n\\bibitem{turney}\nPeter D. Turney and Patrick Pantel.\n\\newblock From frequency to meaning: Vector space models of semantics.\n\\newblock In {\\em Journal of Artificial Intelligence Research}, 37:141-188, 2010.\n\n\\bibitem{turney2}\nPeter D. Turney.\n\\newblock Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\n\\newblock In {\\em Transactions of the Association for Computational Linguistics (TACL)}, 353--366, 2013.\n\n\\bibitem{wsabie}\nJason Weston, Samy Bengio, and Nicolas Usunier.\n\\newblock Wsabie: Scaling up to large vocabulary image annotation.\n\\newblock In {\\em Proceedings of the Twenty-Second international joint\n  conference on Artificial Intelligence-Volume Volume Three}, pages 2764--2770.\n  AAAI Press, 2011.\n\n\\end{thebibliography}\n"
    },
    "1312.6114v10": {
        "bibliography": "\\begin{thebibliography}{10}\n\n\\bibitem{bengio_lm}\nYoshua Bengio, R{\\'e}jean Ducharme, Pascal Vincent, and Christian Janvin.\n\\newblock A neural probabilistic language model.\n\\newblock {\\em The Journal of Machine Learning Research}, 3:1137--1155, 2003.\n\n\\bibitem{collobert}\nRonan Collobert and Jason Weston.\n\\newblock A unified architecture for natural language processing: deep neural\n  networks with multitask learning.\n\\newblock In {\\em Proceedings of the 25th international conference on Machine\n  learning}, pages 160--167. ACM, 2008.\n\n\\bibitem{bengio_sentiment}\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n\\newblock Domain adaptation for large-scale sentiment classification: A deep\n  learning approach.\n\\newblock In {\\em ICML}, 513--520, 2011.\n\n\\bibitem{nce}\nMichael~U Gutmann and Aapo Hyv{\\\"a}rinen.\n\\newblock Noise-contrastive estimation of unnormalized statistical models, with\n  applications to natural image statistics.\n\\newblock {\\em The Journal of Machine Learning Research}, 13:307--361, 2012.\n\n\\bibitem{mikolov3}\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev\n  Khudanpur.\n\\newblock Extensions of recurrent neural network language model.\n\\newblock In {\\em Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE\n  International Conference on}, pages 5528--5531. IEEE, 2011.\n\n\\bibitem{largernnlmforasr}\nTomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky.\n\\newblock Strategies for Training Large Scale Neural Network Language Models.\n\\newblock In Proc. {\\em Automatic Speech Recognition and Understanding}, 2011.\n\n\\bibitem{mikolov4}\nTomas Mikolov.\n\\newblock Statistical Language Models Based on Neural Networks.\n\\newblock {\\em PhD thesis, PhD Thesis, Brno University of Technology}, 2012.\n\n\\bibitem{mikolov}\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n\\newblock Efficient estimation of word representations in vector space.\n\\newblock {\\em ICLR Workshop}, 2013.\n\n\\bibitem{linreg}\nTomas Mikolov, Wen-tau Yih and Geoffrey Zweig.\n\\newblock Linguistic Regularities in Continuous Space Word Representations.\n\\newblock In {\\em Proceedings of NAACL HLT}, 2013.\n\n\\bibitem{mnih}\nAndriy Mnih and Geoffrey~E Hinton.\n\\newblock A scalable hierarchical distributed language model.\n\\newblock {\\em Advances in neural information processing systems},\n  21:1081--1088, 2009.\n\n\\bibitem{mnih-nce}\nAndriy Mnih and Yee~Whye Teh.\n\\newblock A fast and simple algorithm for training neural probabilistic\n  language models.\n\\newblock {\\em arXiv preprint arXiv:1206.6426}, 2012.\n\n\\bibitem{hsoft_first}\nFrederic Morin and Yoshua Bengio.\n\\newblock Hierarchical probabilistic neural network language model.\n\\newblock In {\\em Proceedings of the international workshop on artificial\n  intelligence and statistics}, pages 246--252, 2005.\n\n\\bibitem{nature}\nDavid~E Rumelhart, Geoffrey~E Hintont, and Ronald~J Williams.\n\\newblock Learning representations by back-propagating errors.\n\\newblock {\\em Nature}, 323(6088):533--536, 1986.\n\n\\bibitem{Schwenk}\nHolger Schwenk.\n\\newblock Continuous space language models.\n\\newblock {\\em Computer Speech and Language}, vol. 21, 2007.\n\n\\bibitem{socher}\nRichard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning.\n\\newblock Parsing natural scenes and natural language with recursive neural\n  networks.\n\\newblock In {\\em Proceedings of the 26th International Conference on Machine\n  Learning (ICML)}, volume~2, 2011.\n\n\\bibitem{socher2}\nRichard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.\n\\newblock Semantic Compositionality Through Recursive Matrix-Vector Spaces.\n\\newblock In {\\em Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2012.\n\n\\bibitem{turian}\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\n\\newblock Word representations: a simple and general method for semi-supervised\n  learning.\n\\newblock In {\\em Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics}, pages 384--394. Association for Computational\n  Linguistics, 2010.\n\n\\bibitem{turney}\nPeter D. Turney and Patrick Pantel.\n\\newblock From frequency to meaning: Vector space models of semantics.\n\\newblock In {\\em Journal of Artificial Intelligence Research}, 37:141-188, 2010.\n\n\\bibitem{turney2}\nPeter D. Turney.\n\\newblock Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\n\\newblock In {\\em Transactions of the Association for Computational Linguistics (TACL)}, 353--366, 2013.\n\n\\bibitem{wsabie}\nJason Weston, Samy Bengio, and Nicolas Usunier.\n\\newblock Wsabie: Scaling up to large vocabulary image annotation.\n\\newblock In {\\em Proceedings of the Twenty-Second international joint\n  conference on Artificial Intelligence-Volume Volume Three}, pages 2764--2770.\n  AAAI Press, 2011.\n\n\\end{thebibliography}\n"
    },
    "1409.3215v3": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1503.07619v2": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1512.05832v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1606.01540v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1606.03137v3": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1609.08144v2": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1611.08219v3": {
        "main_tex_filename": "osg_ijcai17.tex"
    },
    "1703.03717v2": {
        "main_tex_filename": "ms.tex"
    },
    "1703.06856v3": {
        "main_tex_filename": "ricardo_draft.tex"
    },
    "1706.03762v5": {
        "main_tex_filename": "ms.tex"
    },
    "1707.06354v2": {
        "main_tex_filename": "NoisyPedagogy.tex"
    },
    "1709.10163v2": {
        "main_tex_filename": "root.tex"
    },
    "1711.07356v3": {
        "main_tex_filename": "iclr2019_conference.tex"
    },
    "1802.01421v4": {
        "main_tex_filename": "advers.tex"
    },
    "1803.03453v4": {
        "main_tex_filename": "full_article.tex"
    },
    "1803.04765v1": {
        "main_tex_filename": "arxiv.tex"
    },
    "1803.05859v4": {
        "main_tex_filename": "copy.tex"
    },
    "1803.06373v1": {
        "main_tex_filename": "example_paper.tex"
    },
    "1804.02485v1": {
        "main_tex_filename": "example_paper.tex"
    },
    "1804.05464v3": {
        "main_tex_filename": "arxiv.tex"
    },
    "1804.09160v2": {
        "main_tex_filename": "acl2018.tex"
    },
    "1804.09170v4": {
        "main_tex_filename": "neurips2018.tex"
    },
    "1805.01772v1": {
        "main_tex_filename": "ms.tex"
    },
    "1805.06826v3": {
        "main_tex_filename": "latent_confounder.tex"
    },
    "1805.08263v4": {
        "main_tex_filename": "root.tex"
    },
    "1805.08336v2": {
        "main_tex_filename": "mcteil.tex"
    },
    "1805.11592v2": {
        "main_tex_filename": "combined.tex"
    },
    "1806.00667v3": {
        "main_tex_filename": "nips_2017.tex"
    },
    "1806.05502v5": {
        "main_tex_filename": "bmvc_review.tex"
    },
    "1806.08340v1": {
        "main_tex_filename": "icml18-interp-images-workshop.tex"
    },
    "1806.09795v3": {
        "main_tex_filename": "jair_paper_formatting.tex"
    },
    "1806.10019v2": {
        "main_tex_filename": "example.tex"
    },
    "1806.11146v2": {
        "main_tex_filename": "adv_reprog_2018.tex"
    },
    "1807.00366v2": {
        "main_tex_filename": "arxiv.tex"
    },
    "1807.01672v3": {
        "main_tex_filename": "rankrew.tex"
    },
    "1807.03571v2": {
        "main_tex_filename": "journal.tex"
    },
    "1807.04723v1": {
        "main_tex_filename": "example_paper.tex"
    },
    "1808.04730v3": {
        "main_tex_filename": "ms.tex"
    },
    "1809.01560v2": {
        "main_tex_filename": "formatting-instructions-latex-2019.tex"
    },
    "1809.02925v2": {
        "main_tex_filename": "iclr2019_conference.tex"
    },
    "1809.05188v3": {
        "main_tex_filename": "iclr2020_conference.tex"
    },
    "1810.00619v3": {
        "main_tex_filename": "smartchoices.tex"
    },
    "1810.00821v4": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "1810.00869v1": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "1810.01032v4": {
        "main_tex_filename": "aaai20.tex"
    },
    "1810.02541v9": {
        "main_tex_filename": "00-Main.tex"
    },
    "1810.06530v5": {
        "main_tex_filename": "neurips.tex"
    },
    "1810.09591v2": {
        "main_tex_filename": "wsdm.tex"
    },
    "1810.11181v2": {
        "main_tex_filename": "main_eqa_modular.tex"
    },
    "1810.11545v2": {
        "main_tex_filename": "CoL_sUAS_AAAI19_main.tex"
    },
    "1811.01439v1": {
        "main_tex_filename": "sample-sigconf-authordraft.tex"
    },
    "1811.02625v2": {
        "main_tex_filename": "mixed_train.tex"
    },
    "1811.03531v1": {
        "main_tex_filename": "arxiv_main.tex"
    },
    "1811.04251v4": {
        "main_tex_filename": "mmi_limit_combined.tex"
    },
    "1811.04784v1": {
        "main_tex_filename": "nips_2018.tex"
    },
    "1811.07882v2": {
        "main_tex_filename": "iclr2019_conference.tex"
    },
    "1811.09720v1": {
        "main_tex_filename": "nips_2018_imginfluence.tex"
    },
    "1811.12231v2": {
        "main_tex_filename": "iclr2019_conference.tex"
    },
    "1812.11118v2": {
        "main_tex_filename": "arxiv-full.tex"
    },
    "1901.00596v4": {
        "main_tex_filename": "Tnnls_template.tex"
    },
    "1901.01365v2": {
        "main_tex_filename": "adInfoHRL_iclr19.tex"
    },
    "1901.05761v2": {
        "main_tex_filename": "iclr2019_conference.tex"
    },
    "1902.05542v1": {
        "main_tex_filename": "main_arxiv.tex"
    },
    "1902.06162v1": {
        "main_tex_filename": "SelfSupervised.tex"
    },
    "1902.08265v1": {
        "main_tex_filename": "perceptual_aes.tex"
    },
    "1902.10186v3": {
        "main_tex_filename": "naaclhlt2019.tex"
    },
    "1903.01973v2": {
        "main_tex_filename": "main2.tex"
    },
    "1903.03096v4": {
        "main_tex_filename": "meta_dataset.tex"
    },
    "1903.08894v1": {
        "main_tex_filename": "preqn.tex"
    },
    "1903.10396v1": {
        "main_tex_filename": "LogBarrier_Arxiv_Adam.tex"
    },
    "1903.11680v3": {
        "main_tex_filename": "outlier.tex"
    },
    "1904.06387v5": {
        "main_tex_filename": "learning_from_learning.tex"
    },
    "1904.11455v1": {
        "main_tex_filename": "main-arxiv.tex"
    },
    "1905.01320v1": {
        "main_tex_filename": "dynamics.tex"
    },
    "1905.03030v2": {
        "main_tex_filename": "metalearning.tex"
    },
    "1905.11108v3": {
        "main_tex_filename": "master.tex"
    },
    "1905.12686v4": {
        "main_tex_filename": "mom_ICML_2021.tex"
    },
    "1906.03218v3": {
        "main_tex_filename": "AShah_RAL_2020.tex"
    },
    "1906.03973v2": {
        "main_tex_filename": "ms.tex"
    },
    "1906.08237v2": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "1907.01657v2": {
        "main_tex_filename": "iclr2020_conference.tex"
    },
    "1907.03976v3": {
        "main_tex_filename": "corl_draft.tex"
    },
    "1907.11932v6": {
        "main_tex_filename": "AAAI-JinD.7014.tex"
    },
    "1908.08016v2": {
        "main_tex_filename": "arxiv_2020.tex"
    },
    "1909.05863v1": {
        "main_tex_filename": "emnlp2019.tex"
    },
    "1911.05722v3": {
        "main_tex_filename": "moco.tex"
    },
    "1911.11132v3": {
        "main_tex_filename": "main_icml.tex"
    },
    "1912.05671v4": {
        "main_tex_filename": "instability-icml-submission.tex"
    },
    "1912.06680v1": {
        "main_tex_filename": "ocean-paper.tex"
    },
    "1912.07768v1": {
        "main_tex_filename": "sample.tex"
    },
    "2002.05379v1": {
        "main_tex_filename": "arxiv.tex"
    },
    "2002.09089v4": {
        "main_tex_filename": "safeirl_main.tex"
    },
    "2002.09758v3": {
        "main_tex_filename": "emnlp2020.tex"
    },
    "2002.11174v1": {
        "main_tex_filename": "root.tex"
    },
    "2003.03384v2": {
        "main_tex_filename": "paper_part0_main.tex"
    },
    "2003.04297v1": {
        "main_tex_filename": "mocov2.tex"
    },
    "2003.05012v4": {
        "main_tex_filename": "minerl.tex"
    },
    "2003.13350v1": {
        "main_tex_filename": "agent57.tex"
    },
    "2005.00582v1": {
        "main_tex_filename": "ijcai20.tex"
    },
    "2005.07648v2": {
        "main_tex_filename": "example.tex"
    },
    "2005.10243v3": {
        "main_tex_filename": "infomin.tex"
    },
    "2006.04734v3": {
        "main_tex_filename": "example_paper.tex"
    },
    "2006.13208v2": {
        "main_tex_filename": "HRI2021.tex"
    },
    "2006.14796v5": {
        "main_tex_filename": "neurips_2020.tex"
    },
    "2007.03244v1": {
        "main_tex_filename": "neurips_2019.tex"
    },
    "2008.03525v1": {
        "main_tex_filename": "arxiv.tex"
    },
    "2010.05150v2": {
        "main_tex_filename": "neurips_2019.tex"
    },
    "2010.15920v2": {
        "main_tex_filename": "0-main.tex"
    },
    "2012.01557v2": {
        "main_tex_filename": "vav_icml.tex"
    },
    "2012.07532v1": {
        "main_tex_filename": "11_proposals.tex"
    },
    "2101.08153v2": {
        "main_tex_filename": "PAPER.tex"
    },
    "2101.11038v1": {
        "main_tex_filename": "acl2020.tex"
    },
    "2103.03872v1": {
        "main_tex_filename": "icml2021.tex"
    },
    "2103.05247v2": {
        "main_tex_filename": "arxiv.tex"
    },
    "2103.12656v2": {
        "main_tex_filename": "neurips_2021.tex"
    },
    "2104.03946v2": {
        "main_tex_filename": "iclr2021_conference.tex"
    },
    "2104.04670v5": {
        "main_tex_filename": "emnlp2021.tex"
    },
    "2104.08691v2": {
        "main_tex_filename": "emnlp2021.tex"
    },
    "2104.13733v1": {
        "main_tex_filename": "emnlp2021.tex"
    },
    "2105.04857v1": {
        "main_tex_filename": "arxiv.tex"
    },
    "2105.12938v1": {
        "main_tex_filename": "XMarioCoG.tex"
    },
    "2106.02039v4": {
        "main_tex_filename": "ms.tex"
    },
    "2107.06882v1": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "2108.01634v1": {
        "main_tex_filename": "main_ICCV.tex"
    },
    "2108.02818v1": {
        "main_tex_filename": "cvpr.tex"
    },
    "2109.01652v5": {
        "main_tex_filename": "iclr2022_conference_final.tex"
    },
    "2109.07445v1": {
        "main_tex_filename": "emnlp2021.tex"
    },
    "2110.07574v1": {
        "main_tex_filename": "_main.tex"
    },
    "2110.07719v1": {
        "main_tex_filename": "arxiv.tex"
    },
    "2110.08058v2": {
        "main_tex_filename": "ms.tex"
    },
    "2110.08514v1": {
        "main_tex_filename": "acl_latex.tex"
    },
    "2110.13136v2": {
        "main_tex_filename": "neurips_data_2021.tex"
    },
    "2111.01705v1": {
        "main_tex_filename": "ai_ethics_statements.tex"
    },
    "2111.06956v1": {
        "main_tex_filename": "paper_template.tex"
    },
    "2111.12797v1": {
        "main_tex_filename": "neurips_2021.tex"
    },
    "2111.14341v2": {
        "main_tex_filename": "00_main.tex"
    },
    "2112.00659v1": {
        "main_tex_filename": "cvpr.tex"
    },
    "2201.12427v1": {
        "main_tex_filename": "icml2022_seditor.tex"
    },
    "2201.12440v1": {
        "main_tex_filename": "arxiv_version.tex"
    },
    "2202.01679v1": {
        "main_tex_filename": "main-arxiv.tex"
    },
    "2202.01747v1": {
        "main_tex_filename": "ms.tex"
    },
    "2202.07785v1": {
        "main_tex_filename": "main_arxiv.tex"
    },
    "2202.11233v1": {
        "main_tex_filename": "RAC.tex"
    },
    "2203.01441v1": {
        "main_tex_filename": "arxiv.tex"
    },
    "2203.02155v1": {
        "main_tex_filename": "neurips_2021.tex"
    },
    "1605.03142v1": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "1311.2901v3": {
        "bibliography": "\\begin{thebibliography}{10}\n\n\\bibitem{bengio_lm}\nYoshua Bengio, R{\\'e}jean Ducharme, Pascal Vincent, and Christian Janvin.\n\\newblock A neural probabilistic language model.\n\\newblock {\\em The Journal of Machine Learning Research}, 3:1137--1155, 2003.\n\n\\bibitem{collobert}\nRonan Collobert and Jason Weston.\n\\newblock A unified architecture for natural language processing: deep neural\n  networks with multitask learning.\n\\newblock In {\\em Proceedings of the 25th international conference on Machine\n  learning}, pages 160--167. ACM, 2008.\n\n\\bibitem{bengio_sentiment}\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n\\newblock Domain adaptation for large-scale sentiment classification: A deep\n  learning approach.\n\\newblock In {\\em ICML}, 513--520, 2011.\n\n\\bibitem{nce}\nMichael~U Gutmann and Aapo Hyv{\\\"a}rinen.\n\\newblock Noise-contrastive estimation of unnormalized statistical models, with\n  applications to natural image statistics.\n\\newblock {\\em The Journal of Machine Learning Research}, 13:307--361, 2012.\n\n\\bibitem{mikolov3}\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev\n  Khudanpur.\n\\newblock Extensions of recurrent neural network language model.\n\\newblock In {\\em Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE\n  International Conference on}, pages 5528--5531. IEEE, 2011.\n\n\\bibitem{largernnlmforasr}\nTomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky.\n\\newblock Strategies for Training Large Scale Neural Network Language Models.\n\\newblock In Proc. {\\em Automatic Speech Recognition and Understanding}, 2011.\n\n\\bibitem{mikolov4}\nTomas Mikolov.\n\\newblock Statistical Language Models Based on Neural Networks.\n\\newblock {\\em PhD thesis, PhD Thesis, Brno University of Technology}, 2012.\n\n\\bibitem{mikolov}\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n\\newblock Efficient estimation of word representations in vector space.\n\\newblock {\\em ICLR Workshop}, 2013.\n\n\\bibitem{linreg}\nTomas Mikolov, Wen-tau Yih and Geoffrey Zweig.\n\\newblock Linguistic Regularities in Continuous Space Word Representations.\n\\newblock In {\\em Proceedings of NAACL HLT}, 2013.\n\n\\bibitem{mnih}\nAndriy Mnih and Geoffrey~E Hinton.\n\\newblock A scalable hierarchical distributed language model.\n\\newblock {\\em Advances in neural information processing systems},\n  21:1081--1088, 2009.\n\n\\bibitem{mnih-nce}\nAndriy Mnih and Yee~Whye Teh.\n\\newblock A fast and simple algorithm for training neural probabilistic\n  language models.\n\\newblock {\\em arXiv preprint arXiv:1206.6426}, 2012.\n\n\\bibitem{hsoft_first}\nFrederic Morin and Yoshua Bengio.\n\\newblock Hierarchical probabilistic neural network language model.\n\\newblock In {\\em Proceedings of the international workshop on artificial\n  intelligence and statistics}, pages 246--252, 2005.\n\n\\bibitem{nature}\nDavid~E Rumelhart, Geoffrey~E Hintont, and Ronald~J Williams.\n\\newblock Learning representations by back-propagating errors.\n\\newblock {\\em Nature}, 323(6088):533--536, 1986.\n\n\\bibitem{Schwenk}\nHolger Schwenk.\n\\newblock Continuous space language models.\n\\newblock {\\em Computer Speech and Language}, vol. 21, 2007.\n\n\\bibitem{socher}\nRichard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning.\n\\newblock Parsing natural scenes and natural language with recursive neural\n  networks.\n\\newblock In {\\em Proceedings of the 26th International Conference on Machine\n  Learning (ICML)}, volume~2, 2011.\n\n\\bibitem{socher2}\nRichard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.\n\\newblock Semantic Compositionality Through Recursive Matrix-Vector Spaces.\n\\newblock In {\\em Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2012.\n\n\\bibitem{turian}\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\n\\newblock Word representations: a simple and general method for semi-supervised\n  learning.\n\\newblock In {\\em Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics}, pages 384--394. Association for Computational\n  Linguistics, 2010.\n\n\\bibitem{turney}\nPeter D. Turney and Patrick Pantel.\n\\newblock From frequency to meaning: Vector space models of semantics.\n\\newblock In {\\em Journal of Artificial Intelligence Research}, 37:141-188, 2010.\n\n\\bibitem{turney2}\nPeter D. Turney.\n\\newblock Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\n\\newblock In {\\em Transactions of the Association for Computational Linguistics (TACL)}, 353--366, 2013.\n\n\\bibitem{wsabie}\nJason Weston, Samy Bengio, and Nicolas Usunier.\n\\newblock Wsabie: Scaling up to large vocabulary image annotation.\n\\newblock In {\\em Proceedings of the Twenty-Second international joint\n  conference on Artificial Intelligence-Volume Volume Three}, pages 2764--2770.\n  AAAI Press, 2011.\n\n\\end{thebibliography}\n"
    },
    "1602.04019v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1202.6177v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1406.2661v1": {
        "bibliography": "\\begin{thebibliography}{10}\n\n\\bibitem{bengio_lm}\nYoshua Bengio, R{\\'e}jean Ducharme, Pascal Vincent, and Christian Janvin.\n\\newblock A neural probabilistic language model.\n\\newblock {\\em The Journal of Machine Learning Research}, 3:1137--1155, 2003.\n\n\\bibitem{collobert}\nRonan Collobert and Jason Weston.\n\\newblock A unified architecture for natural language processing: deep neural\n  networks with multitask learning.\n\\newblock In {\\em Proceedings of the 25th international conference on Machine\n  learning}, pages 160--167. ACM, 2008.\n\n\\bibitem{bengio_sentiment}\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n\\newblock Domain adaptation for large-scale sentiment classification: A deep\n  learning approach.\n\\newblock In {\\em ICML}, 513--520, 2011.\n\n\\bibitem{nce}\nMichael~U Gutmann and Aapo Hyv{\\\"a}rinen.\n\\newblock Noise-contrastive estimation of unnormalized statistical models, with\n  applications to natural image statistics.\n\\newblock {\\em The Journal of Machine Learning Research}, 13:307--361, 2012.\n\n\\bibitem{mikolov3}\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev\n  Khudanpur.\n\\newblock Extensions of recurrent neural network language model.\n\\newblock In {\\em Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE\n  International Conference on}, pages 5528--5531. IEEE, 2011.\n\n\\bibitem{largernnlmforasr}\nTomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky.\n\\newblock Strategies for Training Large Scale Neural Network Language Models.\n\\newblock In Proc. {\\em Automatic Speech Recognition and Understanding}, 2011.\n\n\\bibitem{mikolov4}\nTomas Mikolov.\n\\newblock Statistical Language Models Based on Neural Networks.\n\\newblock {\\em PhD thesis, PhD Thesis, Brno University of Technology}, 2012.\n\n\\bibitem{mikolov}\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n\\newblock Efficient estimation of word representations in vector space.\n\\newblock {\\em ICLR Workshop}, 2013.\n\n\\bibitem{linreg}\nTomas Mikolov, Wen-tau Yih and Geoffrey Zweig.\n\\newblock Linguistic Regularities in Continuous Space Word Representations.\n\\newblock In {\\em Proceedings of NAACL HLT}, 2013.\n\n\\bibitem{mnih}\nAndriy Mnih and Geoffrey~E Hinton.\n\\newblock A scalable hierarchical distributed language model.\n\\newblock {\\em Advances in neural information processing systems},\n  21:1081--1088, 2009.\n\n\\bibitem{mnih-nce}\nAndriy Mnih and Yee~Whye Teh.\n\\newblock A fast and simple algorithm for training neural probabilistic\n  language models.\n\\newblock {\\em arXiv preprint arXiv:1206.6426}, 2012.\n\n\\bibitem{hsoft_first}\nFrederic Morin and Yoshua Bengio.\n\\newblock Hierarchical probabilistic neural network language model.\n\\newblock In {\\em Proceedings of the international workshop on artificial\n  intelligence and statistics}, pages 246--252, 2005.\n\n\\bibitem{nature}\nDavid~E Rumelhart, Geoffrey~E Hintont, and Ronald~J Williams.\n\\newblock Learning representations by back-propagating errors.\n\\newblock {\\em Nature}, 323(6088):533--536, 1986.\n\n\\bibitem{Schwenk}\nHolger Schwenk.\n\\newblock Continuous space language models.\n\\newblock {\\em Computer Speech and Language}, vol. 21, 2007.\n\n\\bibitem{socher}\nRichard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning.\n\\newblock Parsing natural scenes and natural language with recursive neural\n  networks.\n\\newblock In {\\em Proceedings of the 26th International Conference on Machine\n  Learning (ICML)}, volume~2, 2011.\n\n\\bibitem{socher2}\nRichard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.\n\\newblock Semantic Compositionality Through Recursive Matrix-Vector Spaces.\n\\newblock In {\\em Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2012.\n\n\\bibitem{turian}\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\n\\newblock Word representations: a simple and general method for semi-supervised\n  learning.\n\\newblock In {\\em Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics}, pages 384--394. Association for Computational\n  Linguistics, 2010.\n\n\\bibitem{turney}\nPeter D. Turney and Patrick Pantel.\n\\newblock From frequency to meaning: Vector space models of semantics.\n\\newblock In {\\em Journal of Artificial Intelligence Research}, 37:141-188, 2010.\n\n\\bibitem{turney2}\nPeter D. Turney.\n\\newblock Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\n\\newblock In {\\em Transactions of the Association for Computational Linguistics (TACL)}, 353--366, 2013.\n\n\\bibitem{wsabie}\nJason Weston, Samy Bengio, and Nicolas Usunier.\n\\newblock Wsabie: Scaling up to large vocabulary image annotation.\n\\newblock In {\\em Proceedings of the Twenty-Second international joint\n  conference on Artificial Intelligence-Volume Volume Three}, pages 2764--2770.\n  AAAI Press, 2011.\n\n\\end{thebibliography}\n"
    },
    "1409.0813v2": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1510.03370v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1602.04938v3": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1606.05374v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1602.04184v5": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1604.05288v3": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1606.07092v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1105.3821v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1207.0580v1": {
        "bibliography": "\\begin{thebibliography}{10}\n\n\\bibitem{bengio_lm}\nYoshua Bengio, R{\\'e}jean Ducharme, Pascal Vincent, and Christian Janvin.\n\\newblock A neural probabilistic language model.\n\\newblock {\\em The Journal of Machine Learning Research}, 3:1137--1155, 2003.\n\n\\bibitem{collobert}\nRonan Collobert and Jason Weston.\n\\newblock A unified architecture for natural language processing: deep neural\n  networks with multitask learning.\n\\newblock In {\\em Proceedings of the 25th international conference on Machine\n  learning}, pages 160--167. ACM, 2008.\n\n\\bibitem{bengio_sentiment}\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n\\newblock Domain adaptation for large-scale sentiment classification: A deep\n  learning approach.\n\\newblock In {\\em ICML}, 513--520, 2011.\n\n\\bibitem{nce}\nMichael~U Gutmann and Aapo Hyv{\\\"a}rinen.\n\\newblock Noise-contrastive estimation of unnormalized statistical models, with\n  applications to natural image statistics.\n\\newblock {\\em The Journal of Machine Learning Research}, 13:307--361, 2012.\n\n\\bibitem{mikolov3}\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev\n  Khudanpur.\n\\newblock Extensions of recurrent neural network language model.\n\\newblock In {\\em Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE\n  International Conference on}, pages 5528--5531. IEEE, 2011.\n\n\\bibitem{largernnlmforasr}\nTomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky.\n\\newblock Strategies for Training Large Scale Neural Network Language Models.\n\\newblock In Proc. {\\em Automatic Speech Recognition and Understanding}, 2011.\n\n\\bibitem{mikolov4}\nTomas Mikolov.\n\\newblock Statistical Language Models Based on Neural Networks.\n\\newblock {\\em PhD thesis, PhD Thesis, Brno University of Technology}, 2012.\n\n\\bibitem{mikolov}\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n\\newblock Efficient estimation of word representations in vector space.\n\\newblock {\\em ICLR Workshop}, 2013.\n\n\\bibitem{linreg}\nTomas Mikolov, Wen-tau Yih and Geoffrey Zweig.\n\\newblock Linguistic Regularities in Continuous Space Word Representations.\n\\newblock In {\\em Proceedings of NAACL HLT}, 2013.\n\n\\bibitem{mnih}\nAndriy Mnih and Geoffrey~E Hinton.\n\\newblock A scalable hierarchical distributed language model.\n\\newblock {\\em Advances in neural information processing systems},\n  21:1081--1088, 2009.\n\n\\bibitem{mnih-nce}\nAndriy Mnih and Yee~Whye Teh.\n\\newblock A fast and simple algorithm for training neural probabilistic\n  language models.\n\\newblock {\\em arXiv preprint arXiv:1206.6426}, 2012.\n\n\\bibitem{hsoft_first}\nFrederic Morin and Yoshua Bengio.\n\\newblock Hierarchical probabilistic neural network language model.\n\\newblock In {\\em Proceedings of the international workshop on artificial\n  intelligence and statistics}, pages 246--252, 2005.\n\n\\bibitem{nature}\nDavid~E Rumelhart, Geoffrey~E Hintont, and Ronald~J Williams.\n\\newblock Learning representations by back-propagating errors.\n\\newblock {\\em Nature}, 323(6088):533--536, 1986.\n\n\\bibitem{Schwenk}\nHolger Schwenk.\n\\newblock Continuous space language models.\n\\newblock {\\em Computer Speech and Language}, vol. 21, 2007.\n\n\\bibitem{socher}\nRichard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning.\n\\newblock Parsing natural scenes and natural language with recursive neural\n  networks.\n\\newblock In {\\em Proceedings of the 26th International Conference on Machine\n  Learning (ICML)}, volume~2, 2011.\n\n\\bibitem{socher2}\nRichard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.\n\\newblock Semantic Compositionality Through Recursive Matrix-Vector Spaces.\n\\newblock In {\\em Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2012.\n\n\\bibitem{turian}\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\n\\newblock Word representations: a simple and general method for semi-supervised\n  learning.\n\\newblock In {\\em Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics}, pages 384--394. Association for Computational\n  Linguistics, 2010.\n\n\\bibitem{turney}\nPeter D. Turney and Patrick Pantel.\n\\newblock From frequency to meaning: Vector space models of semantics.\n\\newblock In {\\em Journal of Artificial Intelligence Research}, 37:141-188, 2010.\n\n\\bibitem{turney2}\nPeter D. Turney.\n\\newblock Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\n\\newblock In {\\em Transactions of the Association for Computational Linguistics (TACL)}, 353--366, 2013.\n\n\\bibitem{wsabie}\nJason Weston, Samy Bengio, and Nicolas Usunier.\n\\newblock Wsabie: Scaling up to large vocabulary image annotation.\n\\newblock In {\\em Proceedings of the Twenty-Second international joint\n  conference on Artificial Intelligence-Volume Volume Three}, pages 2764--2770.\n  AAAI Press, 2011.\n\n\\end{thebibliography}\n"
    },
    "1608.04112v6": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1609.03543v5": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1412.6980v9": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1411.1373v9": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1512.02595v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1409.1556v6": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1604.06963v2": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1605.03143v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1606.06565v2": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1206.5264v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1604.05280v4": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1506.07359v1": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "1409.0473v7": {
        "bibliography": "\\begin{thebibliography}{10}\n\n\\bibitem{bengio_lm}\nYoshua Bengio, R{\\'e}jean Ducharme, Pascal Vincent, and Christian Janvin.\n\\newblock A neural probabilistic language model.\n\\newblock {\\em The Journal of Machine Learning Research}, 3:1137--1155, 2003.\n\n\\bibitem{collobert}\nRonan Collobert and Jason Weston.\n\\newblock A unified architecture for natural language processing: deep neural\n  networks with multitask learning.\n\\newblock In {\\em Proceedings of the 25th international conference on Machine\n  learning}, pages 160--167. ACM, 2008.\n\n\\bibitem{bengio_sentiment}\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n\\newblock Domain adaptation for large-scale sentiment classification: A deep\n  learning approach.\n\\newblock In {\\em ICML}, 513--520, 2011.\n\n\\bibitem{nce}\nMichael~U Gutmann and Aapo Hyv{\\\"a}rinen.\n\\newblock Noise-contrastive estimation of unnormalized statistical models, with\n  applications to natural image statistics.\n\\newblock {\\em The Journal of Machine Learning Research}, 13:307--361, 2012.\n\n\\bibitem{mikolov3}\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev\n  Khudanpur.\n\\newblock Extensions of recurrent neural network language model.\n\\newblock In {\\em Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE\n  International Conference on}, pages 5528--5531. IEEE, 2011.\n\n\\bibitem{largernnlmforasr}\nTomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky.\n\\newblock Strategies for Training Large Scale Neural Network Language Models.\n\\newblock In Proc. {\\em Automatic Speech Recognition and Understanding}, 2011.\n\n\\bibitem{mikolov4}\nTomas Mikolov.\n\\newblock Statistical Language Models Based on Neural Networks.\n\\newblock {\\em PhD thesis, PhD Thesis, Brno University of Technology}, 2012.\n\n\\bibitem{mikolov}\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n\\newblock Efficient estimation of word representations in vector space.\n\\newblock {\\em ICLR Workshop}, 2013.\n\n\\bibitem{linreg}\nTomas Mikolov, Wen-tau Yih and Geoffrey Zweig.\n\\newblock Linguistic Regularities in Continuous Space Word Representations.\n\\newblock In {\\em Proceedings of NAACL HLT}, 2013.\n\n\\bibitem{mnih}\nAndriy Mnih and Geoffrey~E Hinton.\n\\newblock A scalable hierarchical distributed language model.\n\\newblock {\\em Advances in neural information processing systems},\n  21:1081--1088, 2009.\n\n\\bibitem{mnih-nce}\nAndriy Mnih and Yee~Whye Teh.\n\\newblock A fast and simple algorithm for training neural probabilistic\n  language models.\n\\newblock {\\em arXiv preprint arXiv:1206.6426}, 2012.\n\n\\bibitem{hsoft_first}\nFrederic Morin and Yoshua Bengio.\n\\newblock Hierarchical probabilistic neural network language model.\n\\newblock In {\\em Proceedings of the international workshop on artificial\n  intelligence and statistics}, pages 246--252, 2005.\n\n\\bibitem{nature}\nDavid~E Rumelhart, Geoffrey~E Hintont, and Ronald~J Williams.\n\\newblock Learning representations by back-propagating errors.\n\\newblock {\\em Nature}, 323(6088):533--536, 1986.\n\n\\bibitem{Schwenk}\nHolger Schwenk.\n\\newblock Continuous space language models.\n\\newblock {\\em Computer Speech and Language}, vol. 21, 2007.\n\n\\bibitem{socher}\nRichard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning.\n\\newblock Parsing natural scenes and natural language with recursive neural\n  networks.\n\\newblock In {\\em Proceedings of the 26th International Conference on Machine\n  Learning (ICML)}, volume~2, 2011.\n\n\\bibitem{socher2}\nRichard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.\n\\newblock Semantic Compositionality Through Recursive Matrix-Vector Spaces.\n\\newblock In {\\em Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2012.\n\n\\bibitem{turian}\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\n\\newblock Word representations: a simple and general method for semi-supervised\n  learning.\n\\newblock In {\\em Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics}, pages 384--394. Association for Computational\n  Linguistics, 2010.\n\n\\bibitem{turney}\nPeter D. Turney and Patrick Pantel.\n\\newblock From frequency to meaning: Vector space models of semantics.\n\\newblock In {\\em Journal of Artificial Intelligence Research}, 37:141-188, 2010.\n\n\\bibitem{turney2}\nPeter D. Turney.\n\\newblock Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\n\\newblock In {\\em Transactions of the Association for Computational Linguistics (TACL)}, 353--366, 2013.\n\n\\bibitem{wsabie}\nJason Weston, Samy Bengio, and Nicolas Usunier.\n\\newblock Wsabie: Scaling up to large vocabulary image annotation.\n\\newblock In {\\em Proceedings of the Twenty-Second international joint\n  conference on Artificial Intelligence-Volume Volume Three}, pages 2764--2770.\n  AAAI Press, 2011.\n\n\\end{thebibliography}\n"
    },
    "1310.4546v1": {
        "bibliography": "\\begin{thebibliography}{10}\n\n\\bibitem{bengio_lm}\nYoshua Bengio, R{\\'e}jean Ducharme, Pascal Vincent, and Christian Janvin.\n\\newblock A neural probabilistic language model.\n\\newblock {\\em The Journal of Machine Learning Research}, 3:1137--1155, 2003.\n\n\\bibitem{collobert}\nRonan Collobert and Jason Weston.\n\\newblock A unified architecture for natural language processing: deep neural\n  networks with multitask learning.\n\\newblock In {\\em Proceedings of the 25th international conference on Machine\n  learning}, pages 160--167. ACM, 2008.\n\n\\bibitem{bengio_sentiment}\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n\\newblock Domain adaptation for large-scale sentiment classification: A deep\n  learning approach.\n\\newblock In {\\em ICML}, 513--520, 2011.\n\n\\bibitem{nce}\nMichael~U Gutmann and Aapo Hyv{\\\"a}rinen.\n\\newblock Noise-contrastive estimation of unnormalized statistical models, with\n  applications to natural image statistics.\n\\newblock {\\em The Journal of Machine Learning Research}, 13:307--361, 2012.\n\n\\bibitem{mikolov3}\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev\n  Khudanpur.\n\\newblock Extensions of recurrent neural network language model.\n\\newblock In {\\em Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE\n  International Conference on}, pages 5528--5531. IEEE, 2011.\n\n\\bibitem{largernnlmforasr}\nTomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky.\n\\newblock Strategies for Training Large Scale Neural Network Language Models.\n\\newblock In Proc. {\\em Automatic Speech Recognition and Understanding}, 2011.\n\n\\bibitem{mikolov4}\nTomas Mikolov.\n\\newblock Statistical Language Models Based on Neural Networks.\n\\newblock {\\em PhD thesis, PhD Thesis, Brno University of Technology}, 2012.\n\n\\bibitem{mikolov}\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\n\\newblock Efficient estimation of word representations in vector space.\n\\newblock {\\em ICLR Workshop}, 2013.\n\n\\bibitem{linreg}\nTomas Mikolov, Wen-tau Yih and Geoffrey Zweig.\n\\newblock Linguistic Regularities in Continuous Space Word Representations.\n\\newblock In {\\em Proceedings of NAACL HLT}, 2013.\n\n\\bibitem{mnih}\nAndriy Mnih and Geoffrey~E Hinton.\n\\newblock A scalable hierarchical distributed language model.\n\\newblock {\\em Advances in neural information processing systems},\n  21:1081--1088, 2009.\n\n\\bibitem{mnih-nce}\nAndriy Mnih and Yee~Whye Teh.\n\\newblock A fast and simple algorithm for training neural probabilistic\n  language models.\n\\newblock {\\em arXiv preprint arXiv:1206.6426}, 2012.\n\n\\bibitem{hsoft_first}\nFrederic Morin and Yoshua Bengio.\n\\newblock Hierarchical probabilistic neural network language model.\n\\newblock In {\\em Proceedings of the international workshop on artificial\n  intelligence and statistics}, pages 246--252, 2005.\n\n\\bibitem{nature}\nDavid~E Rumelhart, Geoffrey~E Hintont, and Ronald~J Williams.\n\\newblock Learning representations by back-propagating errors.\n\\newblock {\\em Nature}, 323(6088):533--536, 1986.\n\n\\bibitem{Schwenk}\nHolger Schwenk.\n\\newblock Continuous space language models.\n\\newblock {\\em Computer Speech and Language}, vol. 21, 2007.\n\n\\bibitem{socher}\nRichard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning.\n\\newblock Parsing natural scenes and natural language with recursive neural\n  networks.\n\\newblock In {\\em Proceedings of the 26th International Conference on Machine\n  Learning (ICML)}, volume~2, 2011.\n\n\\bibitem{socher2}\nRichard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.\n\\newblock Semantic Compositionality Through Recursive Matrix-Vector Spaces.\n\\newblock In {\\em Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2012.\n\n\\bibitem{turian}\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\n\\newblock Word representations: a simple and general method for semi-supervised\n  learning.\n\\newblock In {\\em Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics}, pages 384--394. Association for Computational\n  Linguistics, 2010.\n\n\\bibitem{turney}\nPeter D. Turney and Patrick Pantel.\n\\newblock From frequency to meaning: Vector space models of semantics.\n\\newblock In {\\em Journal of Artificial Intelligence Research}, 37:141-188, 2010.\n\n\\bibitem{turney2}\nPeter D. Turney.\n\\newblock Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\n\\newblock In {\\em Transactions of the Association for Computational Linguistics (TACL)}, 353--366, 2013.\n\n\\bibitem{wsabie}\nJason Weston, Samy Bengio, and Nicolas Usunier.\n\\newblock Wsabie: Scaling up to large vocabulary image annotation.\n\\newblock In {\\em Proceedings of the Twenty-Second international joint\n  conference on Artificial Intelligence-Volume Volume Three}, pages 2764--2770.\n  AAAI Press, 2011.\n\n\\end{thebibliography}\n"
    },
    "1609.04994v3": {
        "bibliography": "\\begin{thebibliography}{17}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002UCB}\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\n\\newblock Finite-time analysis of the multiarmed bandit problem.\n\\newblock \\emph{Machine learning}, 47\\penalty0 (2-3):\\penalty0 235--256, 2002.\n\n\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\n  Saxton, and Munos]{BSOSSM:2016explore}\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\n  and Remi Munos.\n\\newblock Unifying count-based exploration and intrinsic motivation.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1606.01868}.\n\n\\bibitem[Dearden et~al.(1999)Dearden, Friedman, and Andre]{Dearden+:1999}\nRichard Dearden, Nir Friedman, and David Andre.\n\\newblock Model based {B}ayesian exploration.\n\\newblock In \\emph{Uncertainty in Artificial Intelligence}, pages 150--159,\n  1999.\n\n\\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and\n  Abbeel]{HCDSDA:2016explore}\nRein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter\n  Abbeel.\n\\newblock Curiosity-driven exploration in deep reinforcement learning via\n  bayesian neural networks.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.09674}.\n\n\\bibitem[Hutter(2005)]{Hutter:2005}\nMarcus Hutter.\n\\newblock \\emph{Universal Artificial Intelligence}.\n\\newblock Springer, 2005.\n\n\\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{JOA:2010UCRL}\nThomas Jaksch, Ronald Ortner, and Peter Auer.\n\\newblock Near-optimal regret bounds for reinforcement learning.\n\\newblock \\emph{Journal of Machine Learning Research}, 11\\penalty0\n  (Apr):\\penalty0 1563--1600, 2010.\n\n\\bibitem[Lattimore(2015)]{Lattimore:2015OCUCB}\nTor Lattimore.\n\\newblock Optimally confident {UCB}: Improved regret for finite-armed bandits.\n\\newblock Technical report, 2015.\n\\newblock \\url{http://arxiv.org/abs/1507.07880}.\n\n\\bibitem[Leike(2016)]{Leike:2016}\nJan Leike.\n\\newblock \\emph{Nonparametric General Reinforcement Learning}.\n\\newblock PhD thesis, Australian National University, 2016.\n\n\\bibitem[Machado and Bowling(2016)]{MB:2016options}\nMarlos~C Machado and Michael Bowling.\n\\newblock Learning purposeful behaviour in the absence of rewards.\n\\newblock Technical report, 2016.\n\\newblock \\url{http://arxiv.org/abs/1605.07700}.\n\n\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\n  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,\n  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{MKSRV+:2015deepQ}\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\n  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,\n  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock \\emph{Nature}, 518\\penalty0 (7540):\\penalty0 529--533, 2015.\n\n\\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,\n  Silver, and Kavukcuoglu]{MBMG+2016DQN}\nVolodymyr Mnih, Adri\u00e0~Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy~P\n  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.\n\\newblock Asynchronous methods for deep reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, 2016.\n\n\\bibitem[Orseau et~al.(2013)Orseau, Lattimore, and Hutter]{OLH:2013ksa}\nLaurent Orseau, Tor Lattimore, and Marcus Hutter.\n\\newblock Universal knowledge-seeking agents for stochastic environments.\n\\newblock In \\emph{Algorithmic Learning Theory}, pages 158--172. Springer,\n  2013.\n\n\\bibitem[Reddy et~al.(2016)Reddy, Celani, and Vergassola]{RCV:2016infomax}\nGautam Reddy, Antonio Celani, and Massimo Vergassola.\n\\newblock Infomax strategies for an optimal balance between exploration and\n  exploitation.\n\\newblock \\emph{Journal of Statistical Physics}, 163\\penalty0 (6):\\penalty0\n  1454--1476, 2016.\n\n\\bibitem[Russo and Van~Roy(2014)]{Russo:2014}\nDan Russo and Benjamin Van~Roy.\n\\newblock Learning to optimize via information-directed sampling.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1583--1591, 2014.\n\n\\bibitem[Schmidhuber(2010)]{Schmidhuber:2010everything}\nJ\u00fcrgen Schmidhuber.\n\\newblock Formal theory of creativity, fun, and intrinsic motivation\n  (1990--2010).\n\\newblock \\emph{IEEE Transactions on Autonomous Mental Development}, 2\\penalty0\n  (3):\\penalty0 230--247, 2010.\n\n\\bibitem[Strens(2000)]{Strens:2000}\nMalcolm Strens.\n\\newblock A {B}ayesian framework for reinforcement learning.\n\\newblock In \\emph{International Conference on Machine Learning}, pages\n  943--950, 2000.\n\n\\bibitem[Sun et~al.(2011)Sun, Gomez, and Schmidhuber]{SGS:2011infogain}\nYi~Sun, Faustino Gomez, and J\u00fcrgen Schmidhuber.\n\\newblock Planning to be surprised: Optimal bayesian exploration in dynamic\n  environments.\n\\newblock In \\emph{Artificial General Intelligence}, pages 41--51. Springer,\n  2011.\n\n\\end{thebibliography}\n"
    },
    "2010.14496v4": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "1811.06521v1": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "2011.06118v2": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "2107.03374v2": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "1906.04358v2": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "2002.05671v2": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "1806.03820v1": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "1904.06866v1": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "2002.09571v2": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    },
    "done_1607.082": {
        "bibliography": "\\begin{thebibliography}{}\n\n\\bibitem[Austerweil {\\em et~al.}(2016)Austerweil, Brawner, Greenwald, Hilliard,\n  Ho, Littman, MacGlashan, and Trimbach]{austerweil2015impact}\nAusterweil, J.~L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman,\n  M.~L., MacGlashan, J., and Trimbach, C. (2016).\n\\newblock How other-regarding preferences can promote cooperation in\n  non-zero-sum grid games.\n\\newblock In {\\em Proceedings of the AAAI Symposium on Challenges and\n  Opportunities in Multiagent Learning for the Real World\\/}.\n\n\\bibitem[Bansal {\\em et~al.}(2018)Bansal, Pachocki, Sidor, Sutskever, and\n  Mordatch]{bansal2017emergent}\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018).\n\\newblock Emergent complexity via multi-agent competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Bengio {\\em et~al.}(2009)Bengio, Louradour, Collobert, and\n  Weston]{bengio2009curriculum}\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\n\\newblock Curriculum learning.\n\\newblock In {\\em Proceedings of the 26th annual international conference on\n  machine learning\\/}, pages 41--48. ACM.\n\n\\bibitem[Bhattacharya {\\em et~al.}(2010)Bhattacharya, Likhachev, and\n  Kumar]{bhattacharya2010multi}\nBhattacharya, S., Likhachev, M., and Kumar, V. (2010).\n\\newblock Multi-agent path planning with multiple tasks and distance\n  constraints.\n\\newblock In {\\em Robotics and Automation (ICRA), 2010 IEEE International\n  Conference on\\/}, pages 953--959. IEEE.\n\n\\bibitem[{Blizzard Entertainment}(2019){Blizzard Entertainment}]{blizzard}\n{Blizzard Entertainment} (2019).\n\\newblock Starcraft ii.\n\\newblock \\url{https://starcraft2.com/en-us/}, Last accessed on 2019-09-07.\n\n\\bibitem[Cao {\\em et~al.}(2013)Cao, Yu, Ren, and Chen]{cao2013overview}\nCao, Y., Yu, W., Ren, W., and Chen, G. (2013).\n\\newblock An overview of recent progress in the study of distributed\n  multi-agent coordination.\n\\newblock {\\em IEEE Transactions on Industrial informatics\\/}, {\\bf 9}(1),\n  427--438.\n\n\\bibitem[Carion {\\em et~al.}(2019)Carion, Synnaeve, Lazaric, and\n  Usunier]{carion2019structured}\nCarion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019).\n\\newblock A structured prediction approach for generalization in cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em Advances in neural information processing systems\\/}.\n\n\\bibitem[Chang {\\em et~al.}(2004)Chang, Ho, and Kaelbling]{chang2004all}\nChang, Y.-H., Ho, T., and Kaelbling, L.~P. (2004).\n\\newblock All learning is local: Multi-agent learning in global reward games.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  807--814.\n\n\\bibitem[Foerster {\\em et~al.}(2018)Foerster, Farquhar, Afouras, Nardelli, and\n  Whiteson]{foerster2018counterfactual}\nFoerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.\n  (2018).\n\\newblock Counterfactual multi-agent policy gradients.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Gupta {\\em et~al.}(2017)Gupta, Egorov, and\n  Kochenderfer]{gupta2017cooperative}\nGupta, J.~K., Egorov, M., and Kochenderfer, M. (2017).\n\\newblock Cooperative multi-agent control using deep reinforcement learning.\n\\newblock In {\\em International Conference on Autonomous Agents and Multiagent\n  Systems\\/}, pages 66--83. Springer.\n\n\\bibitem[Hernandez-Leal {\\em et~al.}(2018)Hernandez-Leal, Kartal, and\n  Taylor]{hernandez2018multiagent}\nHernandez-Leal, P., Kartal, B., and Taylor, M.~E. (2018).\n\\newblock Is multiagent deep reinforcement learning the answer or the question?\n  a brief survey.\n\\newblock {\\em arXiv preprint arXiv:1810.05587\\/}.\n\n\\bibitem[Hu and Wellman(2003)Hu and Wellman]{hu2003nash}\nHu, J. and Wellman, M.~P. (2003).\n\\newblock Nash q-learning for general-sum stochastic games.\n\\newblock {\\em Journal of machine learning research\\/}, {\\bf 4}(Nov),\n  1039--1069.\n\n\\bibitem[Isele {\\em et~al.}(2018)Isele, Rahimi, Cosgun, Subramanian, and\n  Fujimura]{isele2018navigating}\nIsele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018).\n\\newblock Navigating occluded intersections with autonomous vehicles using deep\n  reinforcement learning.\n\\newblock In {\\em 2018 IEEE International Conference on Robotics and Automation\n  (ICRA)\\/}, pages 2034--2039. IEEE.\n\n\\bibitem[Jaderberg {\\em et~al.}(2019)Jaderberg, Czarnecki, Dunning, Marris,\n  Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, {\\em\n  et~al.}]{jaderberg2019human}\nJaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,\n  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., {\\em\n  et~al.} (2019).\n\\newblock Human-level performance in 3d multiplayer games with population-based\n  reinforcement learning.\n\\newblock {\\em Science\\/}, {\\bf 364}(6443), 859.\n\n\\bibitem[Kuefler {\\em et~al.}(2017)Kuefler, Morton, Wheeler, and\n  Kochenderfer]{kuefler2017imitating}\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017).\n\\newblock Imitating driver behavior with generative adversarial networks.\n\\newblock In {\\em 2017 IEEE Intelligent Vehicles Symposium (IV)\\/}, pages\n  204--211. IEEE.\n\n\\bibitem[Li {\\em et~al.}(2019)Li, Wu, Cui, Dong, Fang, and\n  Russell]{li2019robust}\nLi, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019).\n\\newblock Robust multi-agent reinforcement learning via minimax deep\n  deterministic policy gradient.\n\\newblock In {\\em AAAI Conference on Artificial Intelligence (AAAI)\\/}.\n\n\\bibitem[Lillicrap {\\em et~al.}(2016)Lillicrap, Hunt, Pritzel, Heess, Erez,\n  Tassa, Silver, and Wierstra]{lillicrap2016continuous}\nLillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,\n  Silver, D., and Wierstra, D. (2016).\n\\newblock Continuous control with deep reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lin {\\em et~al.}(2018)Lin, Zhao, Xu, and Zhou]{lin2018efficient}\nLin, K., Zhao, R., Xu, Z., and Zhou, J. (2018).\n\\newblock Efficient large-scale fleet management via multi-agent deep\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 24th ACM SIGKDD International Conference\n  on Knowledge Discovery \\& Data Mining\\/}, pages 1774--1783. ACM.\n\n\\bibitem[Littman(1994)Littman]{littman1994markov}\nLittman, M.~L. (1994).\n\\newblock Markov games as a framework for multi-agent reinforcement learning.\n\\newblock In {\\em Machine Learning Proceedings 1994\\/}, pages 157--163.\n  Elsevier.\n\n\\bibitem[Liu {\\em et~al.}(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and\n  Graepel]{liu2019emergent}\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T.\n  (2019).\n\\newblock Emergent coordination through competition.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Lopez {\\em et~al.}(2018)Lopez, Behrisch, Bieker-Walz, Erdmann,\n  Fl{\\\"o}tter{\\\"o}d, Hilbrich, L{\\\"u}cken, Rummel, Wagner, and\n  Wie{\\ss}ner]{SUMO2018}\nLopez, P.~A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl{\\\"o}tter{\\\"o}d,\n  Y.-P., Hilbrich, R., L{\\\"u}cken, L., Rummel, J., Wagner, P., and Wie{\\ss}ner,\n  E. (2018).\n\\newblock Microscopic traffic simulation using {SUMO}.\n\\newblock In {\\em The 21st IEEE International Conference on Intelligent\n  Transportation Systems\\/}. IEEE.\n\n\\bibitem[Lowe {\\em et~al.}(2017)Lowe, Wu, Tamar, Harb, Abbeel, and\n  Mordatch]{lowe2017multi}\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I. (2017).\n\\newblock Multi-agent actor-critic for mixed cooperative-competitive\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  6382--6393.\n\n\\bibitem[Mnih {\\em et~al.}(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness,\n  Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, {\\em\n  et~al.}]{mnih2015human}\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,\n  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., {\\em\n  et~al.} (2015).\n\\newblock Human-level control through deep reinforcement learning.\n\\newblock {\\em Nature\\/}, {\\bf 518}(7540), 529.\n\n\\bibitem[Mordatch and Abbeel(2018)Mordatch and Abbeel]{mordatch2018emergence}\nMordatch, I. and Abbeel, P. (2018).\n\\newblock Emergence of grounded compositional language in multi-agent\n  populations.\n\\newblock In {\\em Thirty-Second AAAI Conference on Artificial Intelligence\\/}.\n\n\\bibitem[Nguyen {\\em et~al.}(2018)Nguyen, Kumar, and Lau]{nguyen2018credit}\nNguyen, D.~T., Kumar, A., and Lau, H.~C. (2018).\n\\newblock Credit assignment for collective multiagent rl with global rewards.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  8112--8123.\n\n\\bibitem[Oliehoek {\\em et~al.}(2008)Oliehoek, Spaan, and\n  Vlassis]{oliehoek2008optimal}\nOliehoek, F.~A., Spaan, M.~T., and Vlassis, N. (2008).\n\\newblock Optimal and approximate q-value functions for decentralized pomdps.\n\\newblock {\\em Journal of Artificial Intelligence Research\\/}, {\\bf 32},\n  289--353.\n\n\\bibitem[Omidshafiei {\\em et~al.}(2017)Omidshafiei, Pazis, Amato, How, and\n  Vian]{omidshafiei2017deep}\nOmidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J. (2017).\n\\newblock Deep decentralized multi-task multi-agent reinforcement learning\n  under partial observability.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  2681--2690.\n\n\\bibitem[Pan and Yang(2010)Pan and Yang]{pan2010survey}\nPan, S.~J. and Yang, Q. (2010).\n\\newblock A survey on transfer learning.\n\\newblock {\\em IEEE Transactions on knowledge and data engineering\\/}, {\\bf\n  22}(10), 1345--1359.\n\n\\bibitem[Panait and Luke(2005)Panait and Luke]{panait2005cooperative}\nPanait, L. and Luke, S. (2005).\n\\newblock Cooperative multi-agent learning: The state of the art.\n\\newblock {\\em Autonomous agents and multi-agent systems\\/}, {\\bf 11}(3),\n  387--434.\n\n\\bibitem[Pynadath and Tambe(2002)Pynadath and Tambe]{pynadath2002communicative}\nPynadath, D.~V. and Tambe, M. (2002).\n\\newblock The communicative multiagent team decision problem: Analyzing\n  teamwork theories and models.\n\\newblock {\\em Journal of artificial intelligence research\\/}, {\\bf 16},\n  389--423.\n\n\\bibitem[Rashid {\\em et~al.}(2018)Rashid, Samvelyan, Schroeder, Farquhar,\n  Foerster, and Whiteson]{rashid2018a}\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and\n  Whiteson, S. (2018).\n\\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent\n  reinforcement learning.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 4295--4304.\n\n\\bibitem[Rusu {\\em et~al.}(2016)Rusu, Rabinowitz, Desjardins, Soyer,\n  Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}\nRusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,\n  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).\n\\newblock Progressive neural networks.\n\\newblock {\\em arXiv preprint arXiv:1606.04671\\/}.\n\n\\bibitem[Schaul {\\em et~al.}(2015)Schaul, Horgan, Gregor, and\n  Silver]{schaul2015universal}\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).\n\\newblock Universal value function approximators.\n\\newblock In {\\em International Conference on Machine Learning\\/}, pages\n  1312--1320.\n\n\\bibitem[Shoham {\\em et~al.}(2003)Shoham, Powers, and\n  Grenager]{shoham2003multi}\nShoham, Y., Powers, R., and Grenager, T. (2003).\n\\newblock Multi-agent reinforcement learning: a critical survey.\n\\newblock Technical report, Technical report, Stanford University.\n\n\\bibitem[Shu and Tian(2019)Shu and Tian]{shu2019m3rl}\nShu, T. and Tian, Y. (2019).\n\\newblock M3rl: Mind-aware multi-agent management reinforcement learning.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Silver {\\em et~al.}(2014)Silver, Lever, Heess, Degris, Wierstra, and\n  Riedmiller]{silver2014deterministic}\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.\n  (2014).\n\\newblock Deterministic policy gradient algorithms.\n\\newblock In {\\em ICML\\/}.\n\n\\bibitem[Singh {\\em et~al.}(2019)Singh, Jain, and\n  Sukhbaatar]{singh2018learning}\nSingh, A., Jain, T., and Sukhbaatar, S. (2019).\n\\newblock Learning when to communicate at scale in multiagent cooperative and\n  competitive tasks.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Son {\\em et~al.}(2019)Son, Kim, Kang, Hostallero, and\n  Yi]{son2019qtran}\nSon, K., Kim, D., Kang, W.~J., Hostallero, D., and Yi, Y. (2019).\n\\newblock Qtran: Learning to factorize with transformation for cooperative\n  multi-agent reinforcement learning.\n\\newblock In {\\em International Conference on Machine Learning\\/}.\n\n\\bibitem[Srinivasan {\\em et~al.}(2018)Srinivasan, Lanctot, Zambaldi,\n  P{\\'e}rolat, Tuyls, Munos, and Bowling]{srinivasan2018actor}\nSrinivasan, S., Lanctot, M., Zambaldi, V., P{\\'e}rolat, J., Tuyls, K., Munos,\n  R., and Bowling, M. (2018).\n\\newblock Actor-critic policy optimization in partially observable multiagent\n  environments.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  3426--3439.\n\n\\bibitem[Stone and Veloso(2000)Stone and Veloso]{stone2000multiagent}\nStone, P. and Veloso, M. (2000).\n\\newblock Multiagent systems: A survey from a machine learning perspective.\n\\newblock {\\em Autonomous Robots\\/}, {\\bf 8}(3), 345--383.\n\n\\bibitem[Sukhbaatar {\\em et~al.}(2016)Sukhbaatar, Fergus, {\\em\n  et~al.}]{sukhbaatar2016learning}\nSukhbaatar, S., Fergus, R., {\\em et~al.} (2016).\n\\newblock Learning multiagent communication with backpropagation.\n\\newblock In {\\em Advances in Neural Information Processing Systems\\/}, pages\n  2244--2252.\n\n\\bibitem[Sunehag {\\em et~al.}(2018)Sunehag, Lever, Gruslys, Czarnecki,\n  Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, {\\em\n  et~al.}]{sunehag2018value}\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,\n  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., {\\em et~al.} (2018).\n\\newblock Value-decomposition networks for cooperative multi-agent learning\n  based on team reward.\n\\newblock In {\\em Proceedings of the 17th International Conference on\n  Autonomous Agents and MultiAgent Systems\\/}, pages 2085--2087. International\n  Foundation for Autonomous Agents and Multiagent Systems.\n\n\\bibitem[Sutton {\\em et~al.}(1999)Sutton, Precup, and Singh]{sutton1999between}\nSutton, R.~S., Precup, D., and Singh, S. (1999).\n\\newblock Between mdps and semi-mdps: A framework for temporal abstraction in\n  reinforcement learning.\n\\newblock {\\em Artificial intelligence\\/}, {\\bf 112}(1-2), 181--211.\n\n\\bibitem[Sutton {\\em et~al.}(2000)Sutton, McAllester, Singh, and\n  Mansour]{sutton2000policy}\nSutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).\n\\newblock Policy gradient methods for reinforcement learning with function\n  approximation.\n\\newblock In {\\em Advances in neural information processing systems\\/}, pages\n  1057--1063.\n\n\\bibitem[Tampuu {\\em et~al.}(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus,\n  Aru, Aru, and Vicente]{tampuu2017multiagent}\nTampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,\n  J., and Vicente, R. (2017).\n\\newblock Multiagent cooperation and competition with deep reinforcement\n  learning.\n\\newblock {\\em PloS one\\/}, {\\bf 12}(4), e0172395.\n\n\\bibitem[Tan(1993)Tan]{tan1993multi}\nTan, M. (1993).\n\\newblock Multi-agent reinforcement learning: Independent vs. cooperative\n  agents.\n\\newblock In {\\em Proceedings of the tenth international conference on machine\n  learning\\/}, pages 330--337.\n\n\\bibitem[Taylor and Stone(2009)Taylor and Stone]{taylor2009transfer}\nTaylor, M.~E. and Stone, P. (2009).\n\\newblock Transfer learning for reinforcement learning domains: A survey.\n\\newblock {\\em Journal of Machine Learning Research\\/}, {\\bf 10}(Jul),\n  1633--1685.\n\n\\bibitem[Thrun(1992)Thrun]{Thrun:1992:EER:865072}\nThrun, S.~B. (1992).\n\\newblock Efficient exploration in reinforcement learning.\n\\newblock Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.\n\n\\bibitem[Van~Lange {\\em et~al.}(2013)Van~Lange, Joireman, Parks, and\n  Van~Dijk]{van2013psychology}\nVan~Lange, P.~A., Joireman, J., Parks, C.~D., and Van~Dijk, E. (2013).\n\\newblock The psychology of social dilemmas: A review.\n\\newblock {\\em Organizational Behavior and Human Decision Processes\\/}, {\\bf\n  120}(2), 125--141.\n\n\\bibitem[Vezhnevets {\\em et~al.}(2017)Vezhnevets, Osindero, Schaul, Heess,\n  Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}\nVezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,\n  D., and Kavukcuoglu, K. (2017).\n\\newblock Feudal networks for hierarchical reinforcement learning.\n\\newblock In {\\em Proceedings of the 34th International Conference on Machine\n  Learning-Volume 70\\/}, pages 3540--3549. JMLR. org.\n\n\\bibitem[Wu {\\em et~al.}(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,\n  Mordatch, and Abbeel]{wu2018variance}\nWu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,\n  Mordatch, I., and Abbeel, P. (2018).\n\\newblock Variance reduction for policy gradient with action-dependent\n  factorized baselines.\n\\newblock In {\\em International Conference on Learning Representations\\/}.\n\n\\bibitem[Zhang {\\em et~al.}(2018)Zhang, Yang, Liu, Zhang, and\n  Basar]{zhang2018fully}\nZhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018).\n\\newblock Fully decentralized multi-agent reinforcement learning with networked\n  agents.\n\\newblock In {\\em Proceedings of the 35th International Conference on Machine\n  Learning\\/}, pages 5872--5881.\n\n\\bibitem[Zhang {\\em et~al.}(2019)Zhang, Yang, and Zha]{zhang2019integrating}\nZhang, Z., Yang, J., and Zha, H. (2019).\n\\newblock Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization.\n\\newblock {\\em arXiv preprint arXiv:1909.10651\\/}.\n\n\\end{thebibliography}\n"
    }
}