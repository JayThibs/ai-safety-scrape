"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"XBZAPQFK","blogPost","2020","Kokotajlo, Daniel","Three kinds of competitiveness","AI Impacts","","","","https://aiimpacts.org/three-kinds-of-competitiveness/","By Daniel Kokotajlo In this post, I distinguish between three different kinds of competitiveness -- Performance, Cost, and Date -- and explain why I think these distinctions are worth the brainspace they occupy. For example, they help me introduce and discuss a problem for AI safety proposals having to do with aligned AIs being outcompeted...","2020-03-30","2022-01-30 01:53:10","2022-01-30 01:53:10","2021-11-20 18:55:39","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/PU9A2KS8/three-kinds-of-competitiveness.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HX9UZ5JP","journalArticle","2020","Cihon, Peter; Maas, Matthijs M.; Kemp, Luke","Fragmentation and the Future: Investigating Architectures for International AI Governance","Global Policy","","1758-5899","10.1111/1758-5899.12890","https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12890","The international governance of artificial intelligence (AI) is at a crossroads: should it remain fragmented or be centralised? We draw on the history of environment, trade, and security regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak for centralisation. The risk of creating a slow and brittle institution, and the difficulty of pairing deep rules with adequate participation, speak against it. Other considerations depend on the specific design. A centralised body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial, and fragmented institutions could self-organise. In sum, these trade-offs should inform development of the AI governance architecture, which is only now emerging. We apply the trade-offs to the case of the potential development of high-level machine intelligence. We conclude with two recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, fragmentation will likely persist for now. The developing landscape should be monitored to see if it is self-organising or simply inadequate.","2020","2022-01-30 04:47:43","2022-01-30 04:47:43","2021-11-13 15:58:24","545-556","","5","11","","","Fragmentation and the Future","","","","","","","en","","","","","Wiley Online Library","","ZSCC: 0000010  _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1758-5899.12890","","/Users/jacquesthibodeau/Zotero/storage/2TZBI3FR/Cihon et al. - 2020 - Fragmentation and the Future Investigating Archit.pdf","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQCZM53S","blogPost","2021","Clarke, Sam; Martin, Samuel Dylan","Distinguishing AI takeover scenarios","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios","Epistemic status: lots of this involves interpreting/categorising other people’s scenarios, and could be wrong. We’d really appreciate being corrected if so. [ETA: so far, no corrections.] TLDR: see the summary table. In the last few years, people have proposed various AI takeover scenarios. We think this type of scenario building is great, since there are now more concrete ideas of what AI takeover could realistically look like. That said, we have been confused for a while about how the different scenarios relate to each other and what different assumptions they make. This post might be helpful for anyone who has similar confusions. We focus on explaining the differences between seven prominent scenarios: the  ‘Brain-in-a-box’ scenario, ‘What failure looks like’ part 1 (WFLL 1), ‘What failure looks like’ part 2 (WFLL 2), ‘Another (outer) alignment failure story’  (AAFS), ‘Production Web’, ‘Flash economy’ and ‘Soft takeoff leading to decisive strategic advantage’. While these scenarios do not capture alI of the risks from transformative AI, participants in a recent survey aimed at leading AI safety/governance researchers estimated the first three of these scenarios to cover 50% of existential catastrophes from AI.[1] We plan to follow up with a subsequent post, which discusses some of the issues raised here in greater depth. VARIABLES RELATING TO AI TAKEOVER SCENARIOS We define AI takeover to be a scenario where the most consequential decisions about the future get made by AI systems with goals that aren’t desirable by human standards. There are three variables which are sufficient to distinguish the takeover scenarios discussed in this post. We will briefly introduce these three variables, and a number of others that are generally useful for thinking about takeover scenarios. Key variables for distinguishing the AI takeover scenarios in this post:  * Speed. Is there a sudden jump in AI capabilities over a very short period    (i.e. much faster than what we","2021-09-08","2022-01-30 04:47:42","2022-01-30 04:47:42","2021-11-18 23:45:23","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/ENAMQXCU/distinguishing-ai-takeover-scenarios.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JVMJ4RMM","journalArticle","2020","Stray, Jonathan","Aligning AI Optimization to Community Well-Being","International Journal of Community Well-Being","","2524-5295, 2524-5309","10.1007/s42413-020-00086-3","http://link.springer.com/10.1007/s42413-020-00086-3","","2020-12","2022-01-30 04:47:36","2022-01-30 04:47:36","2021-11-13 22:47:54","443-463","","4","3","","Int. Journal of Com. WB","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000010","","/Users/jacquesthibodeau/Zotero/storage/V3BEV7X4/Stray - 2020 - Aligning AI Optimization to Community Well-Being.pdf","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W8F6VI9I","thesis","2020","Shah, Rohin Monish","Extracting and Using Preference Information from the State of the World","","","","","https://www.proquest.com/openview/da8bf63ef343781a5bb552122be1bd6a/1?pq-origsite=gscholar&cbl=18750&diss=y","Typically when learning about what people want and don’t want, we look to human action as evidence: what reward they specify, how they perform a task, or what preferences they express can all provide useful information about what an agent should do. This is essential in order to build AI systems that do what we intend them to do. However, existing methods require a lot of expensive human feedback in order to learn even simple tasks. This dissertation argues that there is an additional source of information that is rather helpful: the state of the world. The key insight of this dissertation is that when a robot is deployed in an envi- ronment that humans have been acting in, the state of the environment is already optimized for what humans want, and is thus informative about human preferences. We formalize this setting by assuming that a human H has been acting in an environment for some time, and a robot R observes the final state produced. From this final state, R must infer as much as possible about H’s reward function. We analyze this problem formulation theoretically and show that it is particularly well suited to inferring aspects of the state that should not be changed – exactly the aspects of the reward that H is likely to forget to specify. We develop an algorithm using dynamic programming for tabular environments, analogously to value iteration, and demonstrate its behavior on several simple environments. To scale to high-dimensional environments, we use function approximators judiciously to allow the various parts of our algorithm to be trained without needing to enumerate all possible states. Of course, there is no point in learning about H’s reward function unless we use it to guide R’s decision-making. While we could have R simply optimize the inferred reward, this suffers from a “status quo bias”: the inferred reward is likely to strongly prefer the observed state, since by assumption it is already optimized for H’s preferences. To get R to make changes to 2 the environment, we will usually need to integrate the inferred reward with other sources of preference information. In order to support such reward combination, we use a model in which R must maximize an unknown reward function known only to H. Learning from the state of the world arises as an instrumentally useful behavior in such a setting, and can serve to form a prior belief over the reward function that can then be updated after further interaction with H","2020-12-17","2022-01-30 04:47:35","2022-01-30 04:47:35","","","24","","","","","","","","","","University of California, Berkeley","Berkeley, CA","en","","","","","Zotero","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/S96M3KTK/Shah - Extracting and Using Preference Information from t.pdf","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IWUNUHTA","conferencePaper","2020","Yang, Zitong; Yu, Yaodong; You, Chong; Steinhardt, Jacob; Ma, Yi","Rethinking Bias-Variance Trade-off for Generalization of Neural Networks","Proceedings of the 37th International Conference on Machine Learning","","","","http://arxiv.org/abs/2002.11328","The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.","2020-12-07","2022-01-30 04:47:35","2022-01-30 04:47:35","2021-11-13 14:19:28","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000061  arXiv: 2002.11328","","/Users/jacquesthibodeau/Zotero/storage/7J95438X/Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML 2020","","","","","","","","","","","","","","",""
"TIBNQTSX","conferencePaper","2020","Matheos, George; Lew, Alexander K.; Ghavamizadeh, Matin; Russell, Stuart; Cusumano-Towner, Marco; Mansinghka, Vikash","Transforming Worlds: Automated Involutive MCMC for Open-Universe Probabilistic Models","","","","","https://openreview.net/forum?id=8Itm8dQnJRc","Inference in open-universe probabilistic models can be challenging.  We show how to automate a broad class of MCMC kernels for them, facilitating the development of domain-specific algorithms for...","2020-11-23","2022-01-30 04:47:35","2022-01-30 04:47:35","2021-10-30 21:42:12","","","","","","","Transforming Worlds","","","","","","","en","","","","","openreview.net","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/4QSFBCAS/forum.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Third Symposium on Advances in Approximate Bayesian Inference","","","","","","","","","","","","","","",""
"RQWBFZ9W","conferencePaper","2020","Dathathri, Sumanth; Dvijotham, Krishnamurthy; Kurakin, Alexey; Raghunathan, Aditi; Uesato, Jonathan; Bunel, Rudy; Shankar, Shreya; Steinhardt, Jacob; Goodfellow, Ian; Liang, Percy; Kohli, Pushmeet","Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming","arXiv:2010.11645 [cs]","","","","http://arxiv.org/abs/2010.11645","Convex relaxations have emerged as a promising approach for verifying desirable properties of neural networks like robustness to adversarial perturbations. Widely used Linear Programming (LP) relaxations only work well when networks are trained to facilitate verification. This precludes applications that involve verification-agnostic networks, i.e., networks not specially trained for verification. On the other hand, semidefinite programming (SDP) relaxations have successfully be applied to verification-agnostic networks, but do not currently scale beyond small networks due to poor time and space asymptotics. In this work, we propose a first-order dual SDP algorithm that (1) requires memory only linear in the total number of network activations, (2) only requires a fixed number of forward/backward passes through the network per iteration. By exploiting iterative eigenvector methods, we express all solver operations in terms of forward and backward passes through the network, enabling efficient use of hardware like GPUs/TPUs. For two verification-agnostic networks on MNIST and CIFAR-10, we significantly improve L-inf verified robust accuracy from 1% to 88% and 6% to 40% respectively. We also demonstrate tight verification of a quadratic stability specification for the decoder of a variational autoencoder.","2020-11-03","2022-01-30 04:47:35","2022-01-30 04:47:35","2021-10-31 19:04:24","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 24  arXiv: 2010.11645","","/Users/jacquesthibodeau/Zotero/storage/TFVBZFJ6/Dathathri et al. - 2020 - Enabling certification of verification-agnostic ne.pdf; /Users/jacquesthibodeau/Zotero/storage/SQ7TS9GM/2010.html; /Users/jacquesthibodeau/Zotero/storage/VMH5N4E5/2010.html","","UnsortedSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","34th Conference on Neural Information Processing Systems (NeurIPS 2020),","","","","","","","","","","","","","","",""
"IK3Q65C6","conferencePaper","2020","Li, Alexander C.; Pinto, Lerrel; Abbeel, Pieter","Generalized Hindsight for Reinforcement Learning","34th Conference on Neural Information Processing Systems (NeurIPS 2020),","","","","http://arxiv.org/abs/2002.11708","One of the key reasons for the high sample complexity in reinforcement learning (RL) is the inability to transfer knowledge from one task to another. In standard multi-task RL settings, low-reward data collected while trying to solve one task provides little to no signal for solving that particular task and is hence effectively wasted. However, we argue that this data, which is uninformative for one task, is likely a rich source of information for other tasks. To leverage this insight and efficiently reuse data, we present Generalized Hindsight: an approximate inverse reinforcement learning technique for relabeling behaviors with the right tasks. Intuitively, given a behavior generated under one task, Generalized Hindsight returns a different task that the behavior is better suited for. Then, the behavior is relabeled with this new task before being used by an off-policy RL optimizer. Compared to standard relabeling techniques, Generalized Hindsight provides a substantially more efficient reuse of samples, which we empirically demonstrate on a suite of multi-task navigation and manipulation tasks. Videos and code can be accessed here: https://sites.google.com/view/generalized-hindsight.","2020-02-26","2022-01-30 04:47:35","2022-01-30 04:47:35","2021-11-07 18:21:08","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000016  arXiv: 2002.11708","","/Users/jacquesthibodeau/Zotero/storage/9T8PK9IJ/Li et al. - 2020 - Generalized Hindsight for Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/KRXF5ZMV/2002.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","34th Conference on Neural Information Processing Systems (NeurIPS 2020),","","","","","","","","","","","","","","",""
"GZB84KMR","conferencePaper","2020","Richardson, Oliver; Halpern, Joseph Y.","Probabilistic Dependency Graphs","arXiv:2012.10800 [cs, math]","","","","http://arxiv.org/abs/2012.10800","We introduce Probabilistic Dependency Graphs (PDGs), a new class of directed graphical models. PDGs can capture inconsistent beliefs in a natural way and are more modular than Bayesian Networks (BNs), in that they make it easier to incorporate new information and restructure the representation. We show by example how PDGs are an especially natural modeling tool. We provide three semantics for PDGs, each of which can be derived from a scoring function (on joint distributions over the variables in the network) that can be viewed as representing a distribution's incompatibility with the PDG. For the PDG corresponding to a BN, this function is uniquely minimized by the distribution the BN represents, showing that PDG semantics extend BN semantics. We show further that factor graphs and their exponential families can also be faithfully represented as PDGs, while there are significant barriers to modeling a PDG with a factor graph.","2020-12-19","2022-01-30 04:47:35","2022-01-30 04:47:35","2021-10-30 21:44:21","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s0]  ACC: 0  arXiv: 2012.10800","","/Users/jacquesthibodeau/Zotero/storage/CBQAK6J6/Richardson and Halpern - 2020 - Probabilistic Dependency Graphs.pdf; /Users/jacquesthibodeau/Zotero/storage/MPAXVG7Q/2012.html","","UnsortedSafety","Computer Science - Artificial Intelligence; Computer Science - Information Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI 2020","","","","","","","","","","","","","","",""
"D95GDF2T","blogPost","2020","Wentworth, John","Toy Problem: Detective Story Alignment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/4kYkYSKSALH4JaQ99/toy-problem-detective-story-alignment","Suppose I train some simple unsupervised topic model (e.g. LDA) on a bunch of books. I look through the topics it learns, and find one corresponding to detective stories. The problem: I would like to use the identified detective-story cluster to generate detective stories from GPT. The hard part: I would like to do this in such a way that the precision of the notion of detective-stories used by the final system is not limited by the original simple model. Here’s what that means, visually. The space of real-world books has some clusters in it: One of those clusters is the detective-story cluster. The simple model approximates those clusters using something simple - for the sake of visualization, ellipses: The more complex model (e.g. GPT) presumably has a much more precise approximation of the shape of the clusters: So, we’d like to use the simple model to identify one of the clusters, but then still use the full power of the complex model to sample from that cluster. Of course, GPT may not contain a single variable corresponding to a cluster-id, which is largely what makes the problem interesting. GPT may not internally use a notion of “cluster” at all. However, the GPT model should still contain something (approximately) isomorphic to the original cluster, since that real pattern is still in the data/environment: since there is a real cluster of ""detective stories"" in the data/environment itself, the GPT model should also contain that cluster, to the extent that the GPT model matches the data/environment. In particular, the “precision not limited by original model” requirement rules out the obvious strategy of generating random samples from GPT and selecting those which the simple model labels as detective-stories. If we do that, then we’ll end up with some non-detective-stories in the output, because of shortcomings in the simple model’s notion of detective-stories. Visually, we’d be filtering based on the ellipse approximation of the cluster, which is exac","2020-10-13","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-08 23:30:38","","","","","","","Toy Problem","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/MWR5EK7V/toy-problem-detective-story-alignment.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TK5F29IU","journalArticle","2021","Hayden, Benjamin; Niv, Yael","The case against economic values in the orbitofrontal cortex (or anywhere else in the brain)","Behavioral Neuroscience","","","10.1037/bne0000448","https://osf.io/7hgup","Much of traditional neuroeconomics proceeds from the hypothesis that value is reified in the brain, that is, that there are neurons or brain regions whose responses serve the discrete purpose of encoding value. This hypothesis is supported by the finding that the activity of many neurons covaries with subjective value as estimated in specific tasks and has led to the idea that the primary function of the orbitofrontal cortex is to compute and signal economic value. Here we consider an alternative: that economic value, in the cardinal, common-currency sense, is not represented in the brain and used for choice by default. This idea is motivated by consideration of the economic concept of value, which places important epistemic constraints on our ability to identify its neural basis. It is also motivated by the behavioral economics literature, especially work on heuristics, which proposes value-free process models for much if not all of choice. Finally, it is buoyed by recent neural and behavioral findings regarding how animals and humans learn to choose between options. In light of our hypothesis, we critically reevaluate putative neural evidence for the representation of value and explore an alternative: direct learning of action policies. We delineate how this alternative can provide a robust account of behavior that concords with existing empirical data.","2021","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-08 23:41:47","192-201","","2","135","","","","","","","","","","","","","","","DOI.org (Crossref)","","ZSCC: 0000026  DOI: 10.31234/osf.io/7hgup","","","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IF3G8A6B","blogPost","2020","Riedel, Jess; Deibel, Angelica","TAI Safety Bibliographic Database","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database","Authors: Jess Riedel and Angelica Deibel Cross-posted to EA Forum In this post we present the first public version of our bibliographic database of research on the safety of transformative artificial intelligence (TAI). The primary motivations for assembling this database were to:  1. Aid potential donors in assessing organizations focusing on TAI safety by     collecting and analyzing their research output.  2. Assemble a comprehensive bibliographic database that can be used as a base     for future projects, such as a living review of the field. The database contains research works motivated by, and substantively informing, the challenge of ensuring the safety of TAI, including both technical and meta topics. This initial version of the database has attempted comprehensive coverage only for traditionally formatted research produced in 2016-2020 by organizations with a significant safety focus (~360 items). The database also has significant but non-comprehensive coverage (~570 items) of earlier years, less traditional formats (e.g., blog posts), and non-safety-focused organizations. Usefully, we also have citation counts for essentially all the items for which that is applicable. The core database takes the form of a Zotero library. Snapshots are also available as Google Sheet, CSV, and Zotero RDF. (Compact version for easier human reading: Google Sheet, CSV.) The rest of this post describes the composition of the database in more detail and presents some high-level quantitative analysis of the contents. In particular, our analysis includes:  * Lists of the most cited TAI safety research for each of the past few years    (Tables 2 and 3)  * A chart showing how written TAI safety research output has changed since 2016    (Figure 1).  * A visualization of the degree of collaboration on TAI safety between    different research organizations (Table 4).  * A chart showing how the format of written research varied between    organizations, e.g., manuscripts vs. jou","2020-12-22","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-13 14:31:43","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZ3NUMGN","conferencePaper","2020","Buçinca, Zana; Lin, Phoebe; Gajos, Krzysztof Z.; Glassman, Elena L.","Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems","Proceedings of the 25th International Conference on Intelligent User Interfaces","978-1-4503-7118-6","","10.1145/3377325.3377498","https://doi.org/10.1145/3377325.3377498","Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone.","2020-03-17","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-13","454–464","","","","","","","IUI '20","","","","Association for Computing Machinery","Cagliari, Italy","","","","","","ACM Digital Library","","ZSCC: 0000052","","/Users/jacquesthibodeau/Zotero/storage/AXTMBBWM/Buçinca et al. - 2020 - Proxy tasks and subjective measures can be mislead.pdf","","UnsortedSafety","artificial intelligence; explanations; trust","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4QR28FMI","conferencePaper","2020","McGregor, Sean","Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database","arXiv:2011.08512 [cs]","","","","http://arxiv.org/abs/2011.08512","Mature industrial sectors (e.g., aviation) collect their real world failures in incident databases to inform safety improvements. Intelligent systems currently cause real world harms without a collective memory of their failings. As a result, companies repeatedly make the same mistakes in the design, development, and deployment of intelligent systems. A collection of intelligent system failures experienced in the real world (i.e., incidents) is needed to ensure intelligent systems benefit people and society. The AI Incident Database is an incident collection initiated by an industrial/non-profit cooperative to enable AI incident avoidance and mitigation. The database supports a variety of research and development use cases with faceted and full text search on more than 1,000 incident reports archived to date.","2020-11-17","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-13 14:28:31","","","","","","","Preventing Repeated Real World AI Failures by Cataloging Incidents","","","","","","","","","","","","arXiv.org","","ZSCC: 0000008  arXiv: 2011.08512","","/Users/jacquesthibodeau/Zotero/storage/T3W45ATE/McGregor - 2020 - Preventing Repeated Real World AI Failures by Cata.pdf","","UnsortedSafety","Computer Science - Computers and Society; Computer Science - Software Engineering; I.2.0; K.4.0; K.4.3","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Innovative Applications of Artificial Intelligence (IAAI-21)","","","","","","","","","","","","","","",""
"AICDKECS","blogPost","2020","Muehlhauser, Luke","Our AI governance grantmaking so far","Open Philanthropy","","","","https://www.openphilanthropy.org/blog/ai-governance-grantmaking","When the Soviet Union began to fracture in 1991, the world was forced to reckon with the first collapse of a nuclear superpower in history.My thanks to Nathan Calvin for his help researching and drafting these opening paragraphs about the Nunn-Lugar Act. The USSR was home to more than 27,000","2020-12-16","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-13 14:33:13","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K3XMEA86","blogPost","2020","Xu, Mark","The Solomonoff Prior is Malign","","","","","https://www.alignmentforum.org/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign","This argument came to my attention from this post by Paul Christiano. I also found this clarification helpful. I found these counter-arguments stimulating and have included some discussion of them. Very little of this content is original. My contributions consist of fleshing out arguments and constructing examples. Thank you to Beth Barnes and Thomas Kwa for helpful discussion and comments. WHAT IS THE SOLOMONOFF PRIOR? The Solomonoff prior is intended to answer the question ""what is the probability of X?"" for any X, where X is a finite string over some finite alphabet. The Solomonoff prior is defined by taking the set of all Turing machines (TMs) which output strings when run with no input and weighting them proportional to2−K, whereKis the description length of the TM (informally its size in bits). The Solomonoff prior says the probability of a string is the sum over all the weights of all TMs that print that string. One reason to care about the Solomonoff prior is that we can use it to do a form of idealized induction. If you have seen 0101 and want to predict the next bit, you can use the Solomonoff prior to get the probability of 01010 and 01011. Normalizing gives you the chances of seeing 1 versus 0, conditioned on seeing 0101. In general, any process that assigns probabilities to all strings in a consistent way can be used to do induction in this way. This post provides more information about Solomonoff Induction. WHY IS IT MALIGN? Imagine that you wrote a programming language called python^10 that works as follows: First, it takes all alpha-numeric chars that are not in literals and checks if they're repeated 10 times sequentially. If they're not, they get deleted. If they are, they get replaced by a single copy. Second, it runs this new program through a python interpreter. Hello world in python^10: ppppppppprrrrrrrrrriiiiiiiiiinnnnnnnnnntttttttttt('Hello, world!') Luckily, python has an exec function that executes literals as code. This lets us w","2020-10-13","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-08 23:25:30","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/JSBHRC49/the-solomonoff-prior-is-malign.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZHAC26JP","blogPost","2020","Branwen, Gwern","The Scaling Hypothesis","","","","","https://www.gwern.net/Scaling-hypothesis","On GPT-3: meta-learning, scaling, implications, and deep theory. The scaling hypothesis: neural nets absorb data & compute, generalizing and becoming more Bayesian as problems get harder, manifesting new abilities even at trivial-by-global-standards-scale. The deep learning revolution has begun as foretold.","2020-05-28","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-14 18:58:38","","","","","","","","","","","","","","en-us","https://creativecommons.org/publicdomain/zero/1.0/","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6N626N6Q/Scaling-hypothesis.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5Q2FQC3T","blogPost","2020","Shimi, Adam","The ""Backchaining to Local Search"" Technique in AI Alignment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/qEjh8rpxjG4qGtfuK/the-backchaining-to-local-search-technique-in-ai-alignment","In the spirit of this post by John S. Wentworth, this is a reference for a technique I learned from Evan Hubinger. He's probably not the first to use it, but he introduced it to me, so he gets the credit. In a single sentence, backchaining to local search is the idea of looking at how a problem of alignment could appear through local search (think gradient descent). So it starts with a certain problem (say reward tampering), and then tries to create a context where the usual training process in ML (local search) could create a system suffering from this problem. It’s an instance of backchaining in general, which just looks for how a problem could appear in practice. Backchaining to local search has two main benefits:  * It helps decide whether this specific problem is something we should worry    about.  * It forces you to consider your problem from a local search perspective,    instead of the more intuitive human/adversarial perspective (how would I mess    this up?). Let's look at a concrete example: reward gaming (also called specification gaming). To be even more concrete, we have a system with a camera and other sensors, and its goal is to maximize the amount of time when my friend Tom smiles, as measured through a loss function that captures whether the camera sees Tom smiling. The obvious (for us) way to do reward gaming here is to put a picture of Tom’s smiling face in front of the camera -- then the loss function is minimized. The backchaining to local search technique applied to this example asks ""How can I get this reward gaming behavior by local search?"" Well this reward gaming strategy is probably a local minima for the loss function (as changing just a little the behavior would increase the loss significantly), so local search could find it and stay in there. It's also better than most simple strategies, as ensuring that someone smiles (not necessarily a good goal, mind you) requires rather complex actions in the world (like going full ""Joker"" on","2020-09-18","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-07 22:33:47","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/74AE9R4X/the-backchaining-to-local-search-technique-in-ai-alignment.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AQ9D7WK8","blogPost","2020","Ngo, Richard","Shaping safer goals","AI Alignment Forum","","","","https://www.alignmentforum.org/s/boLPsyNwd6teK5key","A community blog devoted to technical AI alignment research","2020-07-01","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-07 22:49:04","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VCMH6DTH/boLPsyNwd6teK5key.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AXN2R6VV","conferencePaper","2020","Eysenbach, Benjamin; Geng, Xinyang; Levine, Sergey; Salakhutdinov, Ruslan","Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement","arXiv:2002.11089 [cs, stat]","","","","http://arxiv.org/abs/2002.11089","Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward functions can improve sample efficiency. Relabeling methods typically ask: if, in hindsight, we assume that our experience was optimal for some task, for what task was it optimal? In this paper, we show that hindsight relabeling is inverse RL, an observation that suggests that we can use inverse RL in tandem for RL algorithms to efficiently solve many tasks. We use this idea to generalize goal-relabeling techniques from prior work to arbitrary classes of tasks. Our experiments confirm that relabeling data using inverse RL accelerates learning in general multi-task settings, including goal-reaching, domains with discrete sets of rewards, and those with linear reward functions.","2020-02-25","2022-01-30 04:48:47","2022-01-30 04:48:47","2021-11-07 18:36:46","","","","","","","Rewriting History with Inverse RL","","","","","","","","","","","","arXiv.org","","ZSCC: 0000022  arXiv: 2002.11089","","/Users/jacquesthibodeau/Zotero/storage/BZXT2G8S/Eysenbach et al. - 2020 - Rewriting History with Inverse RL Hindsight Infer.pdf; /Users/jacquesthibodeau/Zotero/storage/WGP4ZA47/2002.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020","","","","","","","","","","","","","","",""
"79Z4M7BF","blogPost","2020","Turner, Alex","Non-Obstruction: A Simple Concept Motivating Corrigibility","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/Xts5wm3akbemk4pDa/non-obstruction-a-simple-concept-motivating-corrigibility","Thanks to Mathias Bonde, Tiffany Cai, Ryan Carey, Michael Cohen, Joe Collman, Andrew Critch, Abram Demski, Michael Dennis, Thomas Gilbert, Matthew Graves, Koen Holtman, Evan Hubinger, Victoria Krakovna, Amanda Ngo, Rohin Shah, Adam Shimi, Logan Smith, and Mark Xu for their thoughts. Main claim: corrigibility’s benefits can be mathematically represented as a counterfactual form of alignment. Overview: I’m going to talk about a unified mathematical frame I have for understanding corrigibility’s benefits, what it “is”, and what it isn’t. This frame is precisely understood by graphing the human overseer’s ability to achieve various goals (their attainable utility (AU) landscape). I argue that corrigibility’s benefits are secretly a form of counterfactual alignment (alignment with a set of goals the human may want to pursue). A counterfactually aligned agent doesn't have to let us literally correct it. Rather, this frame theoretically motivates why we might want corrigibility anyways. This frame also motivates other AI alignment subproblems, such as intent alignment, mild optimization, and low impact. NOMENCLATURE Corrigibility goes by a lot of concepts: “not incentivized to stop us from shutting it off”, “wants to account for its own flaws”, “doesn’t take away much power from us”, etc. Named by Robert Miles, the word ‘corrigibility’ means “able to be corrected [by humans]."" I’m going to argue that these are correlates of a key thing we plausibly actually want from the agent design, which seems conceptually simple. In this post, I take the following common-language definitions:  * Corrigibility: the AI literally lets us correct it (modify its policy), and    it doesn't manipulate us either. * Without both of these conditions, the AI's       behavior isn't sufficiently constrained for the concept to be useful.       Being able to correct it is small comfort if it manipulates us into making       the modifications it wants. An AI which is only non-manipulative doesn'","2020","2022-01-30 04:48:46","2022-01-30 04:48:46","2021-11-13 15:28:56","","","","","","","Non-Obstruction","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/8IPMHT9D/non-obstruction-a-simple-concept-motivating-corrigibility.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2C8U9GC5","manuscript","2020","Garcez, Artur d'Avila; Lamb, Luis C.","Neurosymbolic AI: The 3rd Wave","","","","","http://arxiv.org/abs/2012.05876","Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.","2020-12-16","2022-01-30 04:48:46","2022-01-30 04:48:46","2021-11-13 21:56:59","","","","","","","Neurosymbolic AI","","","","","","","","","","","","arXiv.org","","ZSCC: 0000029  arXiv: 2012.05876","","/Users/jacquesthibodeau/Zotero/storage/4EQI3ZIW/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf; /Users/jacquesthibodeau/Zotero/storage/JISGMD7C/2012.html","","UnsortedSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.4; I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8XXTW2U4","manuscript","2020","Srinivasan, Krishnan; Eysenbach, Benjamin; Ha, Sehoon; Tan, Jie; Finn, Chelsea","Learning to be Safe: Deep RL with a Safety Critic","","","","","http://arxiv.org/abs/2010.14603","Safety is an essential component for deploying reinforcement learning (RL) algorithms in real-world scenarios, and is critical during the learning process itself. A natural first approach toward safe RL is to manually specify constraints on the policy's behavior. However, just as learning has enabled progress in large-scale development of AI systems, learning safety specifications may also be necessary to ensure safety in messy open-world environments where manual safety specifications cannot scale. Akin to how humans learn incrementally starting in child-safe environments, we propose to learn how to be safe in one set of tasks and environments, and then use that learned intuition to constrain future behaviors when learning new, modified tasks. We empirically study this form of safety-constrained transfer learning in three challenging domains: simulated navigation, quadruped locomotion, and dexterous in-hand manipulation. In comparison to standard deep RL techniques and prior approaches to safe RL, we find that our method enables the learning of new tasks and in new environments with both substantially fewer safety incidents, such as falling or dropping an object, and faster, more stable learning. This suggests a path forward not only for safer RL systems, but also for more effective RL systems.","2020-10-27","2022-01-30 04:48:46","2022-01-30 04:48:46","2021-11-13 14:06:18","","","","","","","Learning to be Safe","","","","","","","","","","","","arXiv.org","","ZSCC: 0000022  arXiv: 2010.14603","","/Users/jacquesthibodeau/Zotero/storage/GDXUUMCR/Srinivasan et al. - 2020 - Learning to be Safe Deep RL with a Safety Critic.pdf","","UnsortedSafety","Computer Science - Machine Learning; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZFZET66K","conferencePaper","2020","Levine, Sergey; Kumar, Aviral; Tucker, George; Fu, Justin","Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems","arXiv:2005.01643 [cs, stat]","","","","http://arxiv.org/abs/2005.01643","In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.","2020-11-01","2022-01-30 04:48:46","2022-01-30 04:48:46","2021-11-07 23:28:45","","","","","","","Offline Reinforcement Learning","","","","","","","","","","","","arXiv.org","","ZSCC: 0000299  arXiv: 2005.01643","","/Users/jacquesthibodeau/Zotero/storage/Q4HR9G8B/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf; /Users/jacquesthibodeau/Zotero/storage/KA6FVASR/2005.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020","","","","","","","","","","","","","","",""
"NHWZIKZ2","journalArticle","2020","Fernandes, Pedro; Santos, Francisco C.; Lopes, Manuel","Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem","AI Communications","","18758452, 09217126","10.3233/AIC-201502","http://arxiv.org/abs/1907.03843","The rise of artificial intelligence (A.I.) based systems is already offering substantial benefits to the society as a whole. However, these systems may also enclose potential conflicts and unintended consequences. Notably, people will tend to adopt an A.I. system if it confers them an advantage, at which point non-adopters might push for a strong regulation if that advantage for adopters is at a cost for them. Here we propose an agent-based game-theoretical model for these conflicts, where agents may decide to resort to A.I. to use and acquire additional information on the payoffs of a stochastic game, striving to bring insights from simulation to what has been, hitherto, a mostly philosophical discussion. We frame our results under the current discussion on ethical A.I. and the conflict between individual and societal gains: the societal value alignment problem. We test the arising equilibria in the adoption of A.I. technology under different norms followed by artificial agents, their ensuing benefits, and the emergent levels of wealth inequality. We show that without any regulation, purely selfish A.I. systems will have the strongest advantage, even when a utilitarian A.I. provides significant benefits for the individual and the society. Nevertheless, we show that it is possible to develop A.I. systems following human conscious policies that, when introduced in society, lead to an equilibrium where the gains for the adopters are not at a cost for non-adopters, thus increasing the overall wealth of the population and lowering inequality. However, as shown, a self-organised adoption of such policies would require external regulation.","2020-12-18","2022-01-30 04:48:46","2022-01-30 04:48:46","2021-11-13 22:40:37","155-171","","3-6","33","","AIC","Norms for Beneficial A.I.","","","","","","","","","","","","arXiv.org","","ZSCC: 0000004  arXiv: 1907.03843","","/Users/jacquesthibodeau/Zotero/storage/JAVXSVNK/Fernandes et al. - 2020 - Norms for Beneficial A.I. A Computational Analysi.pdf; /Users/jacquesthibodeau/Zotero/storage/A9VEVGPV/1907.html","","UnsortedSafety","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"56TFSGHP","conferencePaper","2020","Anderson, Greg; Verma, Abhinav; Dillig, Isil; Chaudhuri, Swarat","Neurosymbolic Reinforcement Learning with Formally Verified Exploration","arXiv:2009.12612 [cs, stat]","","","","http://arxiv.org/abs/2009.12612","We present Revel, a partially neural reinforcement learning (RL) framework for provably safe exploration in continuous state and action spaces. A key challenge for provably safe deep RL is that repeatedly verifying neural networks within a learning loop is computationally infeasible. We address this challenge using two policy classes: a general, neurosymbolic class with approximate gradients and a more restricted class of symbolic policies that allows efficient verification. Our learning algorithm is a mirror descent over policies: in each iteration, it safely lifts a symbolic policy into the neurosymbolic space, performs safe gradient updates to the resulting policy, and projects the updated policy into the safe symbolic subset, all without requiring explicit verification of neural networks. Our empirical results show that Revel enforces safe exploration in many scenarios in which Constrained Policy Optimization does not, and that it can discover policies that outperform those learned through prior approaches to verified exploration.","2020-10-26","2022-01-30 04:48:46","2022-01-30 04:48:46","2021-11-09 00:01:45","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000017  arXiv: 2009.12612","","/Users/jacquesthibodeau/Zotero/storage/3ENESDMT/Anderson et al. - 2020 - Neurosymbolic Reinforcement Learning with Formally.pdf; /Users/jacquesthibodeau/Zotero/storage/75R2UPKX/2009.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","34th Conference on Neural Information Processing Systems (NeurIPS 2020),","","","","","","","","","","","","","","",""
"SN2KMUEK","blogPost","2020","Aird, Michael; Shovelain, Justin; algekalipso","Memetic downside risks: How ideas can evolve and cause harm","LessWrong","","","","https://www.lesswrong.com/posts/EdAHNdbkGR6ndAPJD/memetic-downside-risks-how-ideas-can-evolve-and-cause-harm","This post was written for Convergence Analysis. OVERVIEW We introduce the concept of memetic downside risks (MDR): risks of unintended negative effects that arise from how ideas “evolve” over time (as a result of  replication, mutation, and selection). We discuss how this concept relates to the existing concepts of memetics, downside risks, and information hazards. We then outline four “directions” in which ideas may evolve: towards simplicity, salience, usefulness, and perceived usefulness. For each “direction”, we give an example to illustrate how an idea mutating in that direction could have negative effects. We then discuss some implications of these ideas for people and organisations trying to improve the world, who wish to achieve their altruistic objectives and minimise the unintended harms they cause. For example, we argue that the possibility of memetic downside risks increases the value of caution about what and how to communicate, and of “high-fidelity” methods of communication. BACKGROUND AND CONCEPT MEMETICS Wikipedia describes a meme as: an idea, behavior, or style that spreads by means of imitation from person to person within a culture—often with the aim of conveying a particular phenomenon, theme, or meaning represented by the meme. A meme acts as a unit for carrying cultural ideas, symbols, or practices, that can be transmitted from one mind to another through writing, speech, gestures, rituals, or other imitable phenomena with a mimicked theme. The same article goes on to say: Proponents [of the concept of memes] theorize that memes are a viral phenomenon that may evolve by natural selection in a manner analogous to that of biological evolution. Memes do this through the processes of variation, mutation, competition, and inheritance, each of which influences a meme's reproductive success. Memes spread through the behavior that they generate in their hosts. Memes that propagate less prolifically may become extinct, while others may survive,","2020-02-25","2022-01-30 04:48:46","2022-01-30 04:48:46","2021-11-20 19:01:44","","","","","","","Memetic downside risks","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/X65UHRG3/memetic-downside-risks-how-ideas-can-evolve-and-cause-harm.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"98XXRJPB","blogPost","2020","Koch, Jack","Mapping the Conceptual Territory in AI Existential Safety and Alignment","Jack Koch","","","","https://jbkjr.github.io/posts/2020/12/mapping_conceptual_territory_AI_safety_alignment/","Throughout my studies in alignment and AI-related existential risks, I’ve found it helpful to build a mental map of the field and how its various questions and considerations interrelate, so that when I read a new paper, a post on the Alignment Forum, or similar material, I have some idea of how it might contribute to the overall goal of making our deployment of AI technology go as well as possible for humanity. I’m writing this post to communicate what I’ve learned through this process, in order to help others trying to build their own mental maps and provide them with links to relevant resources for further, more detailed information. This post was largely inspired by (and would not be possible without) two talks by Paul Christiano and Rohin Shah, respectively, that give very similar overviews of the field,1 as well as a few posts on the Alignment Forum that will be discussed below. This post is not intended to replace these talks but is instead an attempt to coherently integrate their ideas with ideas from other sources attempting to clarify various aspects of the field. You should nonetheless watch these presentations and read some of the resources provided below if you’re trying to build your mental map as completely as possible. Rohin also did a two part podcast with the Future of Life Institute discussing the contents of his presentation in more depth, both of which are worth listening to. ↩","2020-12-17","2022-01-30 04:48:46","2022-01-30 04:48:46","2021-11-13 15:45:40","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/78JC5UMH/mapping_conceptual_territory_AI_safety_alignment.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T3ITWCM7","conferencePaper","2020","Liu, Jian; Cui, Leyang; Liu, Hanmeng; Huang, Dandan; Wang, Yile; Zhang, Yue","LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning","arXiv:2007.08124 [cs]","","","","http://arxiv.org/abs/2007.08124","Machine reading is a fundamental task for testing the capability of natural language understanding, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human performances on simple QA, and thus increasingly challenging machine reading datasets have been proposed. Though various challenges such as evidence integration and commonsense knowledge have been integrated, one of the fundamental capabilities in human reading, namely logical reasoning, is not fully investigated. We build a comprehensive dataset, named LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. Our dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting. The dataset is freely available at https://github.com/lgw863/LogiQA-dataset","2020-07-16","2022-01-30 04:48:46","2022-01-30 04:48:46","2021-11-14 18:04:32","","","","","","","LogiQA","","","","","","","","","","","","arXiv.org","","ZSCC: 0000020  arXiv: 2007.08124","","/Users/jacquesthibodeau/Zotero/storage/G86XQK89/Liu et al. - 2020 - LogiQA A Challenge Dataset for Machine Reading Co.pdf; /Users/jacquesthibodeau/Zotero/storage/5KUSX66V/2007.html","","UnsortedSafety","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCAI 2020","","","","","","","","","","","","","","",""
"HDWGJGAP","journalArticle","2021","Mingard, Chris; Valle-Pérez, Guillermo; Skalse, Joar; Louis, Ard A.","Is SGD a Bayesian sampler? Well, almost","Journal of Machine Learning Research","","","","http://arxiv.org/abs/2006.15191","Overparameterised deep neural networks (DNNs) are highly expressive and so can, in principle, generate almost any function that fits a training dataset with zero error. The vast majority of these functions will perform poorly on unseen data, and yet in practice DNNs often generalise remarkably well. This success suggests that a trained DNN must have a strong inductive bias towards functions with low generalisation error. Here we empirically investigate this inductive bias by calculating, for a range of architectures and datasets, the probability $P_{SGD}(f\mid S)$ that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function $f$ consistent with a training set $S$. We also use Gaussian processes to estimate the Bayesian posterior probability $P_B(f\mid S)$ that the DNN expresses $f$ upon random sampling of its parameters, conditioned on $S$. Our main findings are that $P_{SGD}(f\mid S)$ correlates remarkably well with $P_B(f\mid S)$ and that $P_B(f\mid S)$ is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines $P_B(f\mid S)$), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime. While our results suggest that the Bayesian posterior $P_B(f\mid S)$ is the first order determinant of $P_{SGD}(f\mid S)$, there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on $P_{SGD}(f\mid S)$ and/or $P_B(f\mid S)$, can shed new light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.","2021-02","2022-01-30 04:48:46","2022-01-30 04:48:46","2021-11-13 22:56:31","","","","22","","","Is SGD a Bayesian sampler?","","","","","","","","","","","","arXiv.org","","ZSCC: 0000009  arXiv: 2006.15191","","/Users/jacquesthibodeau/Zotero/storage/ACV9IXEG/Mingard et al. - 2020 - Is SGD a Bayesian sampler Well, almost.pdf; /Users/jacquesthibodeau/Zotero/storage/EN2JTJZ8/2006.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SX36BZ9U","manuscript","2019","Chandra, Kartik; Meijer, Erik; Andow, Samantha; Arroyo-Fang, Emilio; Dea, Irene; George, Johann; Grueter, Melissa; Hosmer, Basil; Stumpos, Steffi; Tempest, Alanna; Yang, Shannon","Gradient Descent: The Ultimate Optimizer","","","","","http://arxiv.org/abs/1909.13371","Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer's hyperparameters, such as the learning rate. There exist many techniques for automated hyperparameter optimization, but they typically introduce even more hyperparameters to control the hyperparameter optimization process. We propose to instead learn the hyperparameters themselves by gradient descent, and furthermore to learn the hyper-hyperparameters by gradient descent as well, and so on ad infinitum. As these towers of gradient-based optimizers grow, they become significantly less sensitive to the choice of top-level hyperparameters, hence decreasing the burden on the user to search for optimal values.","2019-09-29","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-13 13:43:50","","","","","","","Gradient Descent","","","","","","","","","","","","arXiv.org","","ZSCC: 0000006  arXiv: 1909.13371","","/Users/jacquesthibodeau/Zotero/storage/TWKCH5RQ/Chandra et al. - 2019 - Gradient Descent The Ultimate Optimizer.pdf","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ICRWTVK7","blogPost","2020","Aird, Michael","Failures in technology forecasting? A reply to Ord and Yudkowsky","LessWrong","","","","https://www.lesswrong.com/posts/3qypPmmNHEmqegoFF/failures-in-technology-forecasting-a-reply-to-ord-and","In The Precipice, Toby Ord writes: we need to remember how quickly new technologies can be upon us, and to be wary of assertions that they are either impossible or so distant in time that we have no cause for concern. Confident denouncements by eminent scientists should certainly give us reason to be sceptical of a technology, but not to bet our lives against it - their track record just isn’t good enough for that. I strongly agree with those claims, think they’re very important in relation to  estimating existential risk,[1] and appreciate the nuanced way in which they’re stated. (There’s also a lot more nuance around this passage which I haven’t quoted.) I also largely agree with similar claims made in Eliezer Yudkowsky’s earlier essay There's No Fire Alarm for Artificial General Intelligence. But both Ord and Yudkowsky provide the same set of three specific historical cases as evidence of the poor track record of such “confident denouncements”. And I think those cases provide less clear evidence than those authors seem to suggest. So in this post, I’ll:  * Quote Ord and/or Yudkowsky’s descriptions of those three cases, as well as    one case mentioned by Yudkowsky but not Ord  * Highlight ways in which those cases may be murkier than Ord and Yudkowsky    suggest  * Discuss how much we could conclude about technology forecasting in general     from such a small and likely unrepresentative sample of cases, even if those    cases weren’t murky I should note that I don’t think that these historical cases are necessary to support claims like those Ord and Yudkowsky make. And I suspect there might be better evidence for those claims out there. But those cases were the main evidence Ord provided, and among the main evidence Yudkowsky provided. So those cases are being used as key planks supporting beliefs that are important to many EAs and longtermists. Thus, it seems healthy to prod at each suspicious plank on its own terms, and update incrementally. CASE: RUTHER","2020-05-08","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-20 19:08:07","","","","","","","Failures in technology forecasting?","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VI686GFK/failures-in-technology-forecasting-a-reply-to-ord-and.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JRKS8FN9","blogPost","2020","Aird, Michael","Existential risks are not just about humanity","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/EfCCgpvQX359xuZ4g/existential-risks-are-not-just-about-humanity","This post was written for Convergence Analysis. This post highlights and analyses existing ideas more than proposing new ones. In The Precipice, Toby Ord writes: An existential catastrophe is the destruction of humanity’s longterm potential. An existential risk is a risk that threatens the destruction of humanity’s longterm potential. I’ve previously discussed some distinctions and nuances relevant to these concepts. This post will focus on:  * The idea that these concepts are really about the destruction of the     potential of humanity or its “descendants”; they're not necessarily solely    about human wellbeing, nor just Homo sapiens’ potential.  * The implications of that, including for how “bad” an existential catastrophe    might be THE POTENTIAL OF HUMANITY AND ITS “DESCENDANTS” When explaining his definitions, Ord writes: my focus on humanity in the definitions is not supposed to exclude considerations of the value of the environment, other animals, successors to  Homo sapiens, or creatures elsewhere in the cosmos. It is not that I think only humans count. Instead, it is that humans are the only beings we know of that are responsive to moral reasons and moral argument - the beings who can examine the world and decide to do what is best. If we fail, that upwards force, that capacity to push towards what is best or what is just, will vanish from the world. Our potential is a matter of what humanity can achieve through the combined actions of each and every human. The value of our actions will stem in part from what we do to and for humans, but it will depend on the effects of our actions on non-humans too. If we somehow give rise to new kinds of moral agents in the future, the term ‘humanity’ in my definition should be taken to include them. This makes two points clear:  1. An existential catastrophe is not solely about the destruction of the     potential for human welfare, flourishing, achievement, etc. Instead, it’s     about humanity’s potential","2020-04-27","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-20 19:00:05","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/T2T8GF5K/are-existential-risks-just-about-humanity.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5PAQDW3C","manuscript","2020","Shuster, Kurt; Urbanek, Jack; Dinan, Emily; Szlam, Arthur; Weston, Jason","Deploying Lifelong Open-Domain Dialogue Learning","","","","","http://arxiv.org/abs/2008.08076","Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.","2020-08-19","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-07 14:31:04","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000009  arXiv: 2008.08076","","/Users/jacquesthibodeau/Zotero/storage/8R93AX2Q/Shuster et al. - 2020 - Deploying Lifelong Open-Domain Dialogue Learning.pdf","","UnsortedSafety","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBZPEDS4","blogPost","2020","Harth, Rafael","Factored Cognition","LessWrong","","","","https://www.lesswrong.com/s/xezt7HYfpWR6nwp7Z","A community blog devoted to refining the art of rationality","2020-08-30","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-13 21:59:30","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/SJKBM59W/xezt7HYfpWR6nwp7Z.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8RNGX2C6","blogPost","2020","Finnveden, Lukas","Extrapolating GPT-N performance","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance","Brown et al. (2020) (which describes the development of GPT-3) contains measurements of how 8 transformers of different sizes perform on several different benchmarks. In this post, I project how performance could improve for larger models, and give an overview of issues that may appear when scaling-up. Note that these benchmarks are for ‘downstream tasks’ that are different from the training task (which is to predict the next token); these extrapolations thus cannot be directly read off the scaling laws in OpenAI’s Scaling Laws for Neural Language Models (Kaplan et al., 2020) or Scaling Laws for Autoregressive Generative Modelling (Henighan et al., 2020). (If you don’t care about methodology or explanations, the final graphs are in  Comparisons and limits .) METHODOLOGY Brown et al. reports benchmark performance for 8 different model sizes. However, these models were not trained in a compute-optimal fashion. Instead, all models were trained on 300B tokens (one word is roughly 1.4 tokens), which is inefficiently much data. Since we’re interested in the best performance we can get for a given amount of compute, and these models weren’t compute-optimally trained, we cannot extrapolate these results on the basis of model-size. Instead, I fit a trend for how benchmark performance (measured in % accuracy) depends on the cross-entropy loss that the models get when predicting the next token on the validation set. I then use the scaling laws from Scaling Laws for Neural Language Models to extrapolate this loss. This is explained in the Appendix. PLOTTING AGAINST LOSS In order to get a sense of how GPT-3 performs on different types of tasks, I separately report few-shot progress on each of the 11 different categories discussed in Brown et al. For a fair comparison, I normalize the accuracy of each category between random performance and maximum performance; i.e., for each data point, I subtract the performance that a model would get if it responded randomly (or only respo","2020-12-18","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-13 22:24:33","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/2EXQWDPT/extrapolating-gpt-n-performance.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NPI7H29","manuscript","2020","Lohn, Andrew J.","Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance","","","","","http://arxiv.org/abs/2009.00802","Test, Evaluation, Verification, and Validation (TEVV) for Artificial Intelligence (AI) is a challenge that threatens to limit the economic and societal rewards that AI researchers have devoted themselves to producing. A central task of TEVV for AI is estimating brittleness, where brittleness implies that the system functions well within some bounds and poorly outside of those bounds. This paper argues that neither of those criteria are certain of Deep Neural Networks. First, highly touted AI successes (eg. image classification and speech recognition) are orders of magnitude more failure-prone than are typically certified in critical systems even within design bounds (perfectly in-distribution sampling). Second, performance falls off only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced emphasis is needed on designing systems that are resilient despite failure-prone AI components as well as on evaluating and improving OOD performance in order to get AI to where it can clear the challenging hurdles of TEVV and certification.","2020-09-01","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-07 17:00:41","","","","","","","Estimating the Brittleness of AI","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 2009.00802","","/Users/jacquesthibodeau/Zotero/storage/VU4UZTXJ/Lohn - 2020 - Estimating the Brittleness of AI Safety Integrity.pdf; /Users/jacquesthibodeau/Zotero/storage/MS2FRD92/2009.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Software Engineering; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8VJV98HV","conferencePaper","2020","Balakrishnan, Sreejith; Nguyen, Quoc Phong; Low, Bryan Kian Hsiang; Soh, Harold","Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization","arXiv:2011.08541 [cs]","","","","http://arxiv.org/abs/2011.08541","The problem of inverse reinforcement learning (IRL) is relevant to a variety of tasks including value alignment and robot learning from demonstration. Despite significant algorithmic contributions in recent years, IRL remains an ill-posed problem at its core; multiple reward functions coincide with the observed behavior and the actual reward function is not identifiable without prior knowledge or supplementary information. This paper presents an IRL framework called Bayesian optimization-IRL (BO-IRL) which identifies multiple solutions that are consistent with the expert demonstrations by efficiently exploring the reward function space. BO-IRL achieves this by utilizing Bayesian Optimization along with our newly proposed kernel that (a) projects the parameters of policy invariant reward functions to a single point in a latent space and (b) ensures nearby points in the latent space correspond to reward functions yielding similar likelihoods. This projection allows the use of standard stationary kernels in the latent space to capture the correlations present across the reward function space. Empirical results on synthetic and real-world environments (model-free and model-based) show that BO-IRL discovers multiple reward functions while minimizing the number of expensive exact policy optimizations.","2020-11-17","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-13 18:41:21","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000004  arXiv: 2011.08541","","/Users/jacquesthibodeau/Zotero/storage/9JPR5MCW/Balakrishnan et al. - 2020 - Efficient Exploration of Reward Functions in Inver.pdf; /Users/jacquesthibodeau/Zotero/storage/8SJT4KWC/2011.html","","UnsortedSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","34th Conference on Neural Information Processing Systems (NeurIPS 2020)","","","","","","","","","","","","","","",""
"ZADUIUIP","manuscript","2020","Nakkiran, Preetum; Bansal, Yamini","Distributional Generalization: A New Kind of Generalization","","","","","http://arxiv.org/abs/2009.08092","We introduce a new notion of generalization -- Distributional Generalization -- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. Our formal conjectures, which are much more general than this example, characterize the form of distributional generalization that can be expected in terms of problem parameters: model architecture, training procedure, number of samples, and data distribution. We give empirical evidence for these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. Our results thus advance our empirical understanding of interpolating classifiers.","2020-10-14","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-14 18:11:08","","","","","","","Distributional Generalization","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 2009.08092","","/Users/jacquesthibodeau/Zotero/storage/GUN2A4W9/Nakkiran and Bansal - 2020 - Distributional Generalization A New Kind of Gener.pdf; /Users/jacquesthibodeau/Zotero/storage/RR243E5K/2009.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing; Mathematics - Statistics Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CME59D5","conferencePaper","2020","Kumar, Aviral; Gupta, Abhishek; Levine, Sergey","DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction","34th Conference on Neural Information Processing Systems (NeurIPS 2020)","","","","http://arxiv.org/abs/2003.07305","Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difficult to use due to instability and sensitivity to hyperparameters. The reasons for this remain unclear. When using standard supervised methods (e.g., for bandits), on-policy data collection provides ""hard negatives"" that correct the model in precisely those states and actions that the policy is likely to visit. We call this phenomenon ""corrective feedback."" We show that bootstrapping-based Q-learning algorithms do not necessarily benefit from this corrective feedback, and training on the experience collected by the algorithm is not sufficient to correct errors in the Q-function. In fact, Q-learning and related methods can exhibit pathological interactions between the distribution of experience collected by the agent and the policy induced by training on that experience, leading to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. We demonstrate the existence of this problem, both theoretically and empirically. We then show that a specific correction to the data distribution can mitigate this issue. Based on these observations, we propose a new algorithm, DisCor, which computes an approximation to this optimal distribution and uses it to re-weight the transitions used for training, resulting in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals. Blog post presenting a summary of this work is available at: https://bair.berkeley.edu/blog/2020/03/16/discor/.","2020-03-16","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-13 13:40:43","","","","","","","DisCor","","","","","","","","","","","","arXiv.org","","ZSCC: 0000034  arXiv: 2003.07305","","/Users/jacquesthibodeau/Zotero/storage/ZWRH5U35/Kumar et al. - 2020 - DisCor Corrective Feedback in Reinforcement Learn.pdf","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","34th Conference on Neural Information Processing Systems (NeurIPS 2020)","","","","","","","","","","","","","","",""
"F4RGR6AR","blogPost","2020","Harris, Edouard","Defining capability and alignment in gradient descent","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/Xg2YycEfCnLYrCcjy/defining-capability-and-alignment-in-gradient-descent","This is the first post in a series where I'll explore AI alignment in a simplified setting: a neural network that's being trained by gradient descent. I'm choosing this setting because it involves a well-defined optimization process that has enough complexity to be interesting, but that's still understandable enough to make crisp mathematical statements about. As a result, it serves as a good starting point for rigorous thinking about alignment. DEFINING INNER ALIGNMENT First, I want to highlight a definitional issue. Right now there are two definitions of inner alignment circulating in the community. This issue was first pointed out to me by Evan Hubinger in a recent conversation. The first definition is the one from last year's Risks from Learned Optimization paper, which Evan co-authored and which introduced the term. This paper defined the inner alignment problem as ""the problem of eliminating the base-mesa objective gap"" (Section 1.2). The implication is that if we can eliminate the gap between the base objective of a base optimizer, and the mesa-objectives of any mesa-optimizers that base optimizer may give rise to, then we will have satisfied the necessary and sufficient conditions for the base optimizer to be inner-aligned. There's also a second definition that seems to be more commonly used. This definition says that ""inner alignment fails when your capabilities generalize but your objective does not"". This comes from an intuition (pointed out to me by  Rohin Shah) that the combination of inner alignment and outer alignment should be accident-proof with respect to an optimizer's intent: an optimizer that's both inner- and outer-aligned should be trying to do what we want. Since an outer-aligned optimizer is one whose base objective is something we want, this intuition suggests that the remaining part of the intent alignment problem — the problem of getting the optimizer to try to achieve the base objective we set — is what inner alignment refers to. Her","2020-11-05","2022-01-30 04:48:45","2022-01-30 04:48:45","2021-11-13 22:00:32","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/NKD85BQM/defining-capability-and-alignment-in-gradient-descent.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"63VS4I8K","blogPost","2020","DeepSpeed Team; Majumder, Rangan","DeepSpeed: Extreme-scale model training for everyone","Microsoft Research","","","","https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/","DeepSpeed continues to innovate, making its tools more powerful while broadening its reach. Learn how it now powers 10x bigger model training on one GPU, 10x longer input sequences, 5x less communication volume, & scales to train trillion-parameter models.","2020-09-10","2022-01-30 04:48:44","2022-01-30 04:48:44","2021-11-13 13:58:29","","","","","","","DeepSpeed","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4E97MC3E","journalArticle","2020","Zhu, Yixin; Gao, Tao; Fan, Lifeng; Huang, Siyuan; Edmonds, Mark; Liu, Hangxin; Gao, Feng; Zhang, Chi; Qi, Siyuan; Wu, Ying Nian; Tenenbaum, Joshua B.; Zhu, Song-Chun","Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense","Engineering","","20958099","10.1016/j.eng.2020.01.011","http://arxiv.org/abs/2004.09044","Recent progress in deep learning is essentially based on a ""big data for small tasks"" paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a ""small data for big tasks"" paradigm, wherein a single artificial intelligence (AI) system is challenged to develop ""common sense"", enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of ""why"" and ""how"", beyond the dominant ""what"" and ""where"" framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the ""dark matter"" of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace ""dark"" humanlike common sense for solving novel tasks.","2020-03","2022-01-30 04:48:44","2022-01-30 04:48:44","2021-11-07 23:18:37","310-345","","3","6","","Engineering","Dark, Beyond Deep","","","","","","","","","","","","arXiv.org","","ZSCC: 0000034  arXiv: 2004.09044","","/Users/jacquesthibodeau/Zotero/storage/G95A4VAE/Zhu et al. - 2020 - Dark, Beyond Deep A Paradigm Shift to Cognitive A.pdf; /Users/jacquesthibodeau/Zotero/storage/XMMDMVVS/2004.html","","UnsortedSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGIS6ZTT","blogPost","2020","Wentworth, John","Confucianism in AI Alignment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/3aDeaJzxinoGNWNpC/confucianism-in-ai-alignment","I hear there’s a thing where people write a lot in November, so I’m going to try writing a blog post every day. Disclaimer: this post is less polished than my median. And my median post isn’t very polished to begin with. Imagine a large corporation - we’ll call it BigCo. BigCo knows that quality management is high-value, so they have a special program to choose new managers. They run the candidates through a program involving lots of management exercises, simulations, and tests, and select those who perform best. Of course, the exercises and simulations and tests are not a perfect proxy for the would-be managers’ real skills and habits. The rules can be gamed. Within a few years of starting the program, BigCo notices a drastic disconnect between performance in the program and performance in practice. The candidates who perform best in the program are those who game the rules, not those who manage well, so of course many candidates devote all their effort to gaming the rules. How should this problem be solved? Ancient Chinese scholars had a few competing schools of thought on this question, most notably the Confucianists and the Legalists. The (stylized) Confucianists’ answer was: the candidates should be virtuous and not abuse the rules. BigCo should demonstrate virtue and benevolence in general, and in return their workers should show loyalty and obedience. I’m not an expert, but as far as I can tell this is not a straw man - though stylized and adapted to a modern context, it accurately captures the spirit of Confucian thought. The (stylized) Legalists instead took the position obvious to any student of modern economics: this is an incentive design problem, and BigCo leadership should design less abusable incentives. If you have decent intuition for economics, it probably seems like the Legalist position is basically right and the Confucian position is Just Wrong. I don't want to discourage this intuition, but I expect that many people who have this intuitio","2020-11-02","2022-01-30 04:48:44","2022-01-30 04:48:44","2021-11-13 13:49:51","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UPMVNDZV/confucianism-in-ai-alignment.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DV7P9734","journalArticle","2017","Lake, Brenden M.; Ullman, Tomer D.; Tenenbaum, Joshua B.; Gershman, Samuel J.","Building Machines That Learn and Think Like People","Behavioral and Brain Sciences","","","https://doi.org/10.1017/S0140525X16001837","http://arxiv.org/abs/1604.00289","Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.","2017","2022-01-30 04:48:44","2022-01-30 04:48:44","2021-11-18 22:58:21","","","","40","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0001764  arXiv: 1604.00289","","/Users/jacquesthibodeau/Zotero/storage/6UBFFSST/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WP3AREUF","conferencePaper","2020","Chang, Michael; Kaushik, Sidhant; Weinberg, S. Matthew; Griffiths, Thomas L.; Levine, Sergey","Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions","Proceedings of the 37th International Conference on Machine Learning","","","","http://arxiv.org/abs/2007.02382","This paper seeks to establish a framework for directing a society of simple, specialized, self-interested agents to solve what traditionally are posed as monolithic single-agent sequential decision problems. What makes it challenging to use a decentralized approach to collectively optimize a central objective is the difficulty in characterizing the equilibrium strategy profile of non-cooperative games. To overcome this challenge, we design a mechanism for defining the learning environment of each agent for which we know that the optimal solution for the global objective coincides with a Nash equilibrium strategy profile of the agents optimizing their own local objectives. The society functions as an economy of agents that learn the credit assignment process itself by buying and selling to each other the right to operate on the environment state. We derive a class of decentralized reinforcement learning algorithms that are broadly applicable not only to standard reinforcement learning but also for selecting options in semi-MDPs and dynamically composing computation graphs. Lastly, we demonstrate the potential advantages of a society's inherent modular structure for more efficient transfer learning.","2020-08-14","2022-01-30 04:48:44","2022-01-30 04:48:44","2021-11-07 16:27:41","","","","","","","Decentralized Reinforcement Learning","","","","","","","","","","","","arXiv.org","","ZSCC: 0000004  arXiv: 2007.02382","","/Users/jacquesthibodeau/Zotero/storage/FSTFPI4R/Chang et al. - 2020 - Decentralized Reinforcement Learning Global Decis.pdf","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML 2020","","","","","","","","","","","","","","",""
"76QR898M","journalArticle","2020","Sutrop, Margit","Challenges of Aligning Artificial Intelligence with Human Values","Acta Baltica Historiae et Philosophiae Scientiarum","","22282009, 22282017","10.11590/abhps.2020.2.04","https://www.ies.ee/bahps/acta-baltica/abhps-8-2/04_Sutrop-2020-2-04.pdf","As artificial intelligence (AI) systems are becoming increasingly autonomous and will soon be able to make decisions on their own about what to do, AI researchers have started to talk about the need to align AI with human values. The AI ‘value alignment problem’ faces two kinds of challenges—a technical and a normative one—which are interrelated. The technical challenge deals with the question of how to encode human values in artificial intelligence. The normative challenge is associated with two questions: “Which values or whose values should artificial intelligence align with?” My concern is that AI developers underestimate the difficulty of answering the normative question. They hope that we can easily identify the purposes we really desire and that they can focus on the design of those objectives. But how are we to decide which objectives or values to induce in AI, given that there is a plurality of values and moral principles and that our everyday life is full of moral disagreements? In my paper I will show that although it is not realistic to reach an agreement on what we, humans, really want as people value different things and seek different ends, it may be possible to agree on what we do not want to happen, considering the possibility that intelligence, equal to our own, or even exceeding it, can be created. I will argue for pluralism (and not for relativism!) which is compatible with objectivism. In spite of the fact that there is no uniquely best solution to every moral problem, it is still possible to identify which answers are wrong. And this is where we should begin the value alignment of AI.","2020-12-15","2022-01-30 04:48:44","2022-01-30 04:48:44","2021-11-13 22:38:41","54-72","","2","8","","ABHPS","","","","","","","","","","","","","DOI.org (Crossref)","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/VS3TSE5U/University of Tartu and Sutrop - 2020 - Challenges of Aligning Artificial Intelligence wit.pdf","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XRKE5R8P","report","2020","Flournoy, Michèle A; Haines, Avril; Chefitz, Gabrielle","Building Trust Through Testing","","","","","https://cset.georgetown.edu/wp-content/uploads/Building-Trust-Through-Testing.pdf","","2020","2022-01-30 04:48:44","2022-01-30 04:48:44","","","","","","","","","","","","","WestExec Advisors","","","","","","","","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/2SCQ6DTV/Flournoy et al. - 2020 - Building Trust Through Testing.pdf","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5WJ3NKZK","conferencePaper","2007","Ramachandran, Deepak","Bayesian Inverse Reinforcement Learning","","","","","https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf","Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert’s actions to derive a probability distribution over the space of reward functions. We present efﬁcient algorithms that ﬁnd solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.","2007","2022-01-30 04:48:44","2022-01-30 04:48:44","","6","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000660","","/Users/jacquesthibodeau/Zotero/storage/6MR8CJ2E/Ramachandran - 2007 - Bayesian Inverse Reinforcement Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCAI07","","","","","","","","","","","","","","",""
"3IHMRI74","conferencePaper","2019","Zhu, He; Xiong, Zikang; Magill, Stephen; Jagannathan, Suresh","An Inductive Synthesis Framework for Verifiable Reinforcement Learning","Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation","","","10.1145/3314221.3314638","http://arxiv.org/abs/1907.07273","Despite the tremendous advances that have been made in the last decade on developing useful machine-learning applications, their wider adoption has been hindered by the lack of strong assurance guarantees that can be made about their behavior. In this paper, we consider how formal verification techniques developed for traditional software systems can be repurposed for verification of reinforcement learning-enabled ones, a particularly important class of machine learning systems. Rather than enforcing safety by examining and altering the structure of a complex neural network implementation, our technique uses blackbox methods to synthesizes deterministic programs, simpler, more interpretable, approximations of the network that can nonetheless guarantee desired safety properties are preserved, even when the network is deployed in unanticipated or previously unobserved environments. Our methodology frames the problem of neural network verification in terms of a counterexample and syntax-guided inductive synthesis procedure over these programs. The synthesis procedure searches for both a deterministic program and an inductive invariant over an infinite state transition system that represents a specification of an application's control logic. Additional specifications defining environment-based constraints can also be provided to further refine the search space. Synthesized programs deployed in conjunction with a neural network implementation dynamically enforce safety conditions by monitoring and preventing potentially unsafe actions proposed by neural policies. Experimental results over a wide range of cyber-physical applications demonstrate that software-inspired formal verification techniques can be used to realize trustworthy reinforcement learning systems with low overhead.","2019-06-08","2022-01-30 04:48:44","2022-01-30 04:48:44","2021-11-09 00:07:06","686-701","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000051  arXiv: 1907.07273","","/Users/jacquesthibodeau/Zotero/storage/REZTXVMI/Zhu et al. - 2019 - An Inductive Synthesis Framework for Verifiable Re.pdf; /Users/jacquesthibodeau/Zotero/storage/ZH9U6EAJ/1907.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; 00-02","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3H57NUUP","conferencePaper","2020","Juric, Mislav; Sandic, Agneza; Brcic, Mario","AI safety: state of the field through quantitative lens","arXiv:2002.05671 [cs]","","","10.23919/MIPRO48935.2020.9245153","http://arxiv.org/abs/2002.05671","Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such mass-adoption has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability with its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes in society, AI safety is the field under which we need to decide the direction of humanity's future.","2020-07-09","2022-01-30 04:48:43","2022-01-30 04:48:43","2021-11-13 15:49:34","","","","","","","AI safety","","","","","","","","","","","","arXiv.org","","ZSCC: 0000006  arXiv: 2002.05671","","/Users/jacquesthibodeau/Zotero/storage/T6JFHRKD/Juric et al. - 2020 - AI safety state of the field through quantitative.pdf","","UnsortedSafety","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","MIPRO 2020","","","","","","","","","","","","","","",""
"8CU8JKMG","blogPost","2020","Ngo, Amanda; Pace, Ben","AGI Predictions","LessWrong","","","","https://www.lesswrong.com/posts/YMokuZdoY9tEDHjzv/agi-predictions","This post is a collection of key questions that feed into AI timelines and AI safety work where it seems like there is substantial interest or disagreement amongst the LessWrong community. You can make a prediction on a question by hovering over the widget and clicking. You can update your prediction by clicking at a new point, and remove your prediction by clicking on the same point. Try it out: Elicit Prediction (elicit.org/binary/questions/FIVfnQ_kJ) ADD QUESTIONS & OPERATIONALIZATIONS This is not intended to be a comprehensive list, so I’d love for people to add their own questions – here are instructions on making your own embedded question. If you have better operationalizations of the questions, you can make your own version in the comments. If there's general agreement on an alternative operationalization being better, I'll add it into the post. QUESTIONS AGI DEFINITION We’ll define AGI in this post as a unified system that, for almost all economically relevant cognitive tasks, at least matches any human's ability at the task. This is similar to Rohin Shah and Ben Cottier’s definition in this post. SAFETY QUESTIONS Elicit Prediction (elicit.org/binary/questions/_Sw39Z-kh)Elicit Prediction ( elicit.org/binary/questions/HqT9XSwfs)Elicit Prediction ( elicit.org/binary/questions/sTO9o3bLg)Elicit Prediction ( elicit.org/binary/questions/kua2HCDhi)Elicit Prediction ( elicit.org/binary/questions/KqSEIKayU)Elicit Prediction ( elicit.org/binary/questions/yoiBUdpgO)Elicit Prediction ( elicit.org/binary/questions/RcOt6wSs7)Elicit Prediction ( elicit.org/binary/questions/ZjN5qqVRz) TIMELINES QUESTIONS See Forecasting AI timelines, Ajeya Cotra’s OP AI timelines report, and Adam Gleave’s #AN80 comment, for more context on this breakdown. I haven’t tried to operationalize this too much, so feel free to be more specific in the comments. The first three questions in this section are mutually exclusive — that is, the probabilities you assign to them should not sum to m","2020-11-20","2022-01-30 04:48:43","2022-01-30 04:48:43","2021-11-13 14:15:06","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TEZ5G3WJ","manuscript","2020","Sharma, Utkarsh; Kaplan, Jared","A Neural Scaling Law from the Dimension of the Data Manifold","","","","","http://arxiv.org/abs/2004.10802","When data is plentiful, the loss achieved by well-trained neural networks scales as a power-law $L \propto N^{-\alpha}$ in the number of network parameters $N$. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension $d$. This simple theory predicts that the scaling exponents $\alpha \approx 4/d$ for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of $d$ and $\alpha$ by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.","2020-04-22","2022-01-30 04:48:43","2022-01-30 04:48:43","2021-11-13 23:00:10","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000012  arXiv: 2004.10802","","/Users/jacquesthibodeau/Zotero/storage/4IPPBR63/Sharma and Kaplan - 2020 - A Neural Scaling Law from the Dimension of the Dat.pdf; /Users/jacquesthibodeau/Zotero/storage/TEW3HG34/2004.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I8JC8ADC","conferencePaper","2019","Waytowich, Nicholas; Barton, Sean L.; Lawhern, Vernon; Warnell, Garrett","A Narration-based Reward Shaping Approach using Grounded Natural Language Commands","Proceedings of the 36th International Conference on Machine Learning","","","","http://arxiv.org/abs/1911.00497","While deep reinforcement learning techniques have led to agents that are successfully able to learn to perform a number of tasks that had been previously unlearnable, these techniques are still susceptible to the longstanding problem of reward sparsity. This is especially true for tasks such as training an agent to play StarCraft II, a real-time strategy game where reward is only given at the end of a game which is usually very long. While this problem can be addressed through reward shaping, such approaches typically require a human expert with specialized knowledge. Inspired by the vision of enabling reward shaping through the more-accessible paradigm of natural-language narration, we develop a technique that can provide the benefits of reward shaping using natural language commands. Our narration-guided RL agent projects sequences of natural-language commands into the same high-dimensional representation space as corresponding goal states. We show that we can get improved performance with our method compared to traditional reward-shaping approaches. Additionally, we demonstrate the ability of our method to generalize to unseen natural-language commands.","2019-10-31","2022-01-30 04:48:43","2022-01-30 04:48:43","2021-11-13 22:30:52","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 1911.00497","","/Users/jacquesthibodeau/Zotero/storage/7Q4ZC6TW/Waytowich et al. - 2019 - A Narration-based Reward Shaping Approach using Gr.pdf; /Users/jacquesthibodeau/Zotero/storage/C869HZX8/1911.html","","UnsortedSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GF7HGP5R","blogPost","2020","Tan, Xuan","AI Alignment, Philosophical Pluralism, and the Relevance of Non-Western Philosophy","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/jS2iiDPqMvZ2tnik2/ai-alignment-philosophical-pluralism-and-the-relevance-of","This is an extended transcript of the talk I gave at EAGxAsiaPacific 2020. In the talk, I present a somewhat critical take on how AI alignment has grown as a field, and how, from my perspective, it deserves considerably more philosophical and disciplinary diversity than it has enjoyed so far. I'm sharing it here in the hopes of generating discussion about the disciplinary and philosophical paradigms that (I understand) the AI alignment community to be rooted in, and whether or how we should move beyond them. Some sections cover introductory material that most people here are likely to be familiar with, so feel free to skip them. THE TALK Hey everyone, my name is Xuan (IPA: ɕɥɛn), and I’m doctoral student at MIT doing cognitive AI research. Specifically I work on how we can infer the hidden structure of human motivations by modeling humans using probabilistic programs. Today though I’ll be talking about something that’s more in the background that informs my work, and that’s about AI alignment, philosophical pluralism, and the relevance of non-Western philosophy. This talk will cover a lot of ground, so I want to give an overview to keep everyone oriented:  1. First, I’ll give a brief introduction to what AI alignment is, and why it     likely matters as an effective cause area.  2. I’ll then highlight some of the philosophical tendencies of current AI     alignment research, and argue that they reflect a relatively narrow set of     philosophical views.  3. Given that these philosophical views may miss crucial considerations, this     situation motivates the need for greater philosophical and disciplinary     pluralism.  4. And then as a kind of proof by example, I’ll aim to demonstrate how     non-Western philosophy might provide insight into several open problems in     AI alignment research. A BRIEF INTRODUCTION TO AI ALIGNMENT So what is AI alignment? One way to cache it out is the project of building intelligent systems that robustly act in our collective i","2020-12-31","2022-01-30 04:48:43","2022-01-30 04:48:43","2021-11-13 16:34:55","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6KXXKDVU/ai-alignment-philosophical-pluralism-and-the-relevance-of.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTDRV7MR","manuscript","2020","Yuan, Li; Xiao, Will; Kreiman, Gabriel; Tay, Francis E. H.; Feng, Jiashi; Livingstone, Margaret S.","Adversarial images for the primate brain","","","","","http://arxiv.org/abs/2011.05623","Deep artificial neural networks have been proposed as a model of primate vision. However, these networks are vulnerable to adversarial attacks, whereby introducing minimal noise can fool networks into misclassifying images. Primate vision is thought to be robust to such adversarial images. We evaluated this assumption by designing adversarial images to fool primate vision. To do so, we first trained a model to predict responses of face-selective neurons in macaque inferior temporal cortex. Next, we modified images, such as human faces, to match their model-predicted neuronal responses to a target category, such as monkey faces. These adversarial images elicited neuronal responses similar to the target category. Remarkably, the same images fooled monkeys and humans at the behavioral level. These results challenge fundamental assumptions about the similarity between computer and primate vision and show that a model of neuronal activity can selectively direct primate visual behavior.","2020-11-11","2022-01-30 04:48:43","2022-01-30 04:48:43","2021-11-13 22:51:54","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 2011.05623","","/Users/jacquesthibodeau/Zotero/storage/SG5SCXZA/Yuan et al. - 2020 - Adversarial images for the primate brain.pdf; /Users/jacquesthibodeau/Zotero/storage/CVAPB22W/2011.html","","UnsortedSafety","Computer Science - Neural and Evolutionary Computing; Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing; Quantitative Biology - Neurons and Cognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5Q25A4W2","conferencePaper","2021","Bousquet, Olivier; Hanneke, Steve; Moran, Shay; van Handel, Ramon; Yehudayoff, Amir","A Theory of Universal Learning","STOC 2021: Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing","","","10.1145/3406325.3451087","http://arxiv.org/abs/2011.04483","How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its ""learning curve"", that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy. In this paper, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this paper is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case. For concreteness, we consider in this paper only the realizable case, though analogous results are expected to extend to more general learning scenarios.","2021-06","2022-01-30 04:48:43","2022-01-30 04:48:43","2021-11-13 23:03:40","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000007  arXiv: 2011.04483","","/Users/jacquesthibodeau/Zotero/storage/QPAPTBUB/Bousquet et al. - 2020 - A Theory of Universal Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/5C38AGW9/2011.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Mathematics - Statistics Theory; Computer Science - Data Structures and Algorithms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","STOC 2021","","","","","","","","","","","","","","",""
"X3VIZNQR","manuscript","2020","Klinger, Joel; Mateos-Garcia, Juan; Stathoulopoulos, Konstantinos","A narrowing of AI research?","","","","","http://arxiv.org/abs/2009.10385","Artificial Intelligence (AI) is being hailed as the latest example of a General Purpose Technology that could transform productivity and help tackle important societal challenges. This outcome is however not guaranteed: a myopic focus on short-term benefits could lock AI into technologies that turn out to be sub-optimal in the longer-run. Recent controversies about the dominance of deep learning methods and private labs in AI research suggest that the field may be getting narrower, but the evidence base is lacking. We seek to address this gap with an analysis of the thematic diversity of AI research in arXiv, a widely used pre-prints site. Having identified 110,000 AI papers in this corpus, we use hierarchical topic modelling to estimate the thematic composition of AI research, and this composition to calculate various metrics of research diversity. Our analysis suggests that diversity in AI research has stagnated in recent years, and that AI research involving private sector organisations tends to be less diverse than research in academia. This appears to be driven by a small number of prolific and narrowly-focused technology companies. Diversity in academia is bolstered by smaller institutions and research groups that may have less incentives to race and lower levels of collaboration with the private sector. We also find that private sector AI researchers tend to specialise in data and computationally intensive deep learning methods at the expense of research involving other (symbolic and statistical) AI methods, and of research that considers the societal and ethical implications of AI or applies it in domains like health. Our results suggest that there may be a rationale for policy action to prevent a premature narrowing of AI research that could reduce its societal benefits, but we note the incentive, information and scale hurdles standing in the way of such interventions.","2020-11-17","2022-01-30 04:48:43","2022-01-30 04:48:43","2021-11-13 21:54:52","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000017  arXiv: 2009.10385","","/Users/jacquesthibodeau/Zotero/storage/R7VVT4NZ/Klinger et al. - 2020 - A narrowing of AI research.pdf; /Users/jacquesthibodeau/Zotero/storage/KZJD9H3Z/2009.html","","UnsortedSafety","Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8JRIA7DF","blogPost","2020","Larks","2020 AI Alignment Literature Review and Charity Comparison","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/pTYDdcag9pTzFQ7vw/2020-ai-alignment-literature-review-and-charity-comparison","cross-posted to the EA forum here. INTRODUCTION As in 2016, 2017, 2018, and 2019, I have attempted to review the research that has been produced by various organisations working on AI safety, to help potential donors gain a better understanding of the landscape. This is a similar role to that which GiveWell performs for global health charities, and somewhat similar to a securities analyst with regards to possible investments. My aim is basically to judge the output of each organisation in 2020 and compare it to their budget. This should give a sense of the organisations' average cost-effectiveness. We can also compare their financial reserves to their 2020 budgets to get a sense of urgency. I’d like to apologize in advance to everyone doing useful AI Safety work whose contributions I have overlooked or misconstrued. As ever I am painfully aware of the various corners I have had to cut due to time constraints from my job, as well as being distracted by 1) other projects, 2) the miracle of life and 3) computer games. This article focuses on AI risk work. If you think other causes are important too, your priorities might differ. This particularly affects GCRI, FHI and CSER, who both do a lot of work on other issues which I attempt to cover but only very cursorily. HOW TO READ THIS DOCUMENT This document is fairly extensive, and some parts (particularly the methodology section) are largely the same as last year, so I don’t recommend reading from start to finish. Instead, I recommend navigating to the sections of most interest to you. If you are interested in a specific research organisation, you can use the table of contents to navigate to the appropriate section. You might then also want to Ctrl+F for the organisation acronym in case they are mentioned elsewhere as well. Papers listed as ‘X researchers contributed to the following research lead by other organisations’ are included in the section corresponding to their first author and you can Cntrl+F to find them","2020-12-21","2022-01-30 04:48:43","2022-01-30 04:48:43","2021-11-13 14:30:11","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QHKZ9UMR","conferencePaper","2021","Power, Alethea; Burda, Yuri; Edwards, Harri; Babuschkin, Igor; Misra, Vedant","GROKKING: GENERALIZATION BEYOND OVERFIT- TING ON SMALL ALGORITHMIC DATASETS","","","","","","In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efﬁciency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of “grokking” a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overﬁtting. We also study generalization as a function of dataset size and ﬁnd that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the ﬁnite training dataset.","2021","2022-01-30 04:48:28","2022-01-30 04:48:28","","9","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s0]  ACC: 4","","/Users/jacquesthibodeau/Zotero/storage/MPKP826T/Power et al. - GROKKING GENERALIZATION BEYOND OVERFIT- TING ON S.pdf","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","1stMathematical Reasoning in General Artificial Intelligence Workshop, ICLR 2021","","","","","","","","","","","","","","",""
"D5VWMR6B","blogPost","2020","Barnes, Beth; Christiano, Paul","Debate update: Obfuscated arguments problem","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem","This is an update on the work on AI Safety via Debate that we previously wrote about here. Authors and Acknowledgements The researchers on this project were Elizabeth Barnes and Paul Christiano, with substantial help from William Saunders (who built the current web interface as well as other help), Joe Collman (who helped develop the structured debate mechanisms), and Mark Xu, Chris Painter, Mihnea Maftei and Ronny Fernandez (who took part in many debates as well as helping think through problems). We're also grateful to Geoffrey Irving and Evan Hubinger for feedback on drafts, and for helpful conversations, along with Richard Ngo, Daniel Ziegler, John Schulman, Amanda Askell and Jeff Wu. Finally, we're grateful to our contractors who participated in experiments, including Adam Scherlis, Kevin Liu, Rohan Kapoor and Kunal Sharda. WHAT WE DID We tested the debate protocol introduced in AI Safety via Debate with human judges and debaters. We found various problems and improved the mechanism to fix these issues (details of these are in the appendix). However, we discovered that a dishonest debater can often create arguments that have a fatal error, but where it is very hard to locate the error. We don’t have a fix for this “obfuscated argument” problem, and believe it might be an important quantitative limitation for both IDA and Debate. KEY TAKEAWAYS AND RELEVANCE FOR ALIGNMENT Our ultimate goal is to find a mechanism that allows us to learn anything that a machine learning model knows: if the model can efficiently find the correct answer to some problem, our mechanism should favor the correct answer while only requiring a tractable number of human judgements and a reasonable number of computation steps for the model.[1] We’re working under a hypothesis that there are broadly two ways to know things: via step-by-step reasoning about implications (logic, computation…), and by learning and generalizing from data (pattern matching, bayesian updating…). Debate focuse","2020-12-22","2022-01-30 04:48:28","2022-01-30 04:48:28","2021-11-13 16:27:28","","","","","","","Debate update","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/M6NPQG3H/debate-update-obfuscated-arguments-problem.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VXW84MIC","journalArticle","2020","Cammarata, Nick; Goh, Gabriel; Carter, Shan; Schubert, Ludwig; Petrov, Michael; Olah, Chris","Curve Detectors","Distill","","2476-0757","10.23915/distill.00024.003","https://distill.pub/2020/circuits/curve-detectors","Part one of a three part deep dive into the curve neuron family.","2020-06-17","2022-01-30 04:48:28","2022-01-30 04:48:28","2021-11-14 16:21:07","e00024.003","","6","5","","Distill","","","","","","","","en","","","","","distill.pub","","ZSCC: 0000005","","","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BZTGZTHQ","journalArticle","2020","Olah, Chris; Cammarata, Nick; Voss, Chelsea; Schubert, Ludwig; Goh, Gabriel","Naturally Occurring Equivariance in Neural Networks","Distill","","2476-0757","10.23915/distill.00024.004","https://distill.pub/2020/circuits/equivariance","Neural networks naturally learn many transformed copies of the same feature, connected by symmetric weights.","2020-12-08","2022-01-30 04:48:28","2022-01-30 04:48:28","2021-11-14 16:21:59","e00024.004","","12","5","","Distill","","","","","","","","en","","","","","distill.pub","","ZSCC: 0000001","","","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9ZJEXJS","blogPost","2020","Diffractor; Kosoy, Vanessa","Infra-Bayesianism","AI Alignment Forum","","","","https://www.alignmentforum.org/s/CmrW8fCmSLK7E25sa","A community blog devoted to technical AI alignment research","2020","2022-01-30 04:48:21","2022-01-30 04:48:21","2021-11-14 16:24:35","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/GASHJBIH/CmrW8fCmSLK7E25sa.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V295HB6Q","blogPost","2020","Demski, Abram","Recursive Quantilizers II","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/YNuJjRuxsWWzfvder/recursive-quantilizers-ii","I originally introduced the recursive quantilizers idea here, but didn't provide a formal model until my recent Learning Normativity post. That formal model had some problems. I'll correct some of those problems here. My new model is closer to HCH+IDA, and so, is even closer to Paul Christiano style systems than my previous. However, I'm also beginning to suspect that quantilizers aren't the right starting point. I'll state several problems with quantilizers at the end of this post. First, let's reiterate the design criteria, and why the model in Learning Normativity wasn't great. CRITERIA Here are the criteria from Learning Normativity, with slight revisions. See the earlier post for further justifications/intuitions behind these criteria.  1. No Perfect Feedback: we want to be able to learn with the possibility that     any one piece of data is corrupt. 1. Uncertain Feedback: data can be given in an uncertain form, allowing         100% certain feedback to be given (if there ever is such a thing), but         also allowing the system to learn significant things in the absence of         any certainty.      2. Reinterpretable Feedback: ideally, we want rich hypotheses about the         meaning of feedback, which help the system to identify corrupt feedback,         and interpret the information in imperfect feedback. To this criterion,         I add two clarifying criteria: 1. Robust Listening: in some sense, we don't want the system to be able             to ""entirely ignore"" humans. If the system goes off-course, we want             to be able to correct that.          2. Arbitrary Reinterpretation: at the same time, we want the AI to be             able to entirely reinterpret feedback based on a rich model of what             humans mean. This criterion stands in tension with Robust Listening.             However, the proposal in the present post is, I think, a plausible             way to achieve both.                              2. No Perfect Loss Functi","2020","2022-01-30 04:48:21","2022-01-30 04:48:21","2021-11-13 19:41:28","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/Q5MIGHSB/recursive-quantilizers-ii.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V9AHZ7MX","blogPost","2020","Demski, Abram","Learning Normativity: A Research Agenda","","","","","https://www.alignmentforum.org/posts/2JGu9yxiJkoGdQR4s/learning-normativity-a-research-agenda","(Related to Inaccessible Information, Learning the Prior, and Better Priors as a Safety Problem. Builds on several of my alternate alignment ideas.) I want to talk about something which I'll call learning normativity. What is normativity? Normativity is correct behavior. I mean something related to the fuzzy concept humans convey with the word ""should"". I think it has several interesting features:  * Norms are the result of a complex negotiation between humans, so they    shouldn't necessarily be thought of as the result of maximizing some set of    values. This distinguishes learning normativity from value learning.  * A lot of information about norms is present in the empirical distribution of    what people actually do, but you can't learn norms just by learning human    behavior. This distinguishes it from imitation learning.  * It's often possible to provide a lot of information in the form of ""good/bad""    feedback. This feedback should be interpreted more like approval-directed    learning rather than RL. However, approval should not be treated as a gold    standard.  * Similarly, it's often possible to provide a lot of information in the form of    rules, but rules are not necessarily 100% true; they are just very likely to    apply in typical cases.  * In general, it's possible to get very rich types of feedback, but very sparse    : humans get all sorts of feedback, including not only instruction on how to    act, but also how to think.  * Any one piece of feedback is suspect. Teachers can make mistakes,    instructions can be wrong, demonstrations can be imperfect, dictionaries can    contain spelling errors, reward signals can be corrupt, and so on. EXAMPLE: LANGUAGE LEARNING A major motivating example for me is how language learning works in humans. There is clearly, to some degree, a ""right way"" and a ""wrong way"" to use a language. I'll call this correct usage. One notable feature of language learning is that we don't always speak, or write, in c","2020","2022-01-30 04:48:21","2022-01-30 04:48:21","2021-11-13 19:38:49","","","","","","","Learning Normativity","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/ZWEX5GR8/learning-normativity-a-research-agenda.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"USEM8RBA","blogPost","2020","Demski, Abram","Comparing Utilities","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/cYsGrWEzjb324Zpjx/comparing-utilities","(This is a basic point about utility theory which many will already be familiar with. I draw some non-obvious conclusions which may be of interest to you even if you think you know this from the title -- but the main point is to communicate the basics. I'm posting it to the alignment forum because I've heard misunderstandings of this from some in the AI alignment research community.) I will first give the basic argument that the utility quantities of different agents aren't directly comparable, and a few important consequences of this. I'll then spend the rest of the post discussing what to do when you need to compare utility functions. UTILITIES AREN'T COMPARABLE. Utility isn't an ordinary quantity. A utility function is a device for expressing the preferences of an agent. Suppose we have a notion of outcome.* We could try to represent the agent's preferences between outcomes as an ordering relation: if we have outcomes A, B, and C, then one possible preference would be A<B<C. However, a mere ordering does not tell us how the agent would decide between  gambles, ie, situations giving A, B, and C with some probability. With just three outcomes, there is only one thing we need to know: is B closer to A or C, and by how much? We want to construct a utility function U() which represents the preferences. Let's say we set U(A)=0 and U(C)=1. Then we can represent B=G as U(B)=1/2. If not, we would look for a different gamble which does equal B, and then set B's utility to the expected value of that gamble. By assigning real-numbered values to each outcome, we can fully represent an agent's preferences over gambles. (Assuming the VNM axioms hold, that is.) But the initial choices U(A)=0 and U(C)=1 were arbitrary! We could have chosen any numbers so long as U(A)<U(C), reflecting the preference A<C. In general, a valid representation of our preferences U() can be modified into an equally valid U'() by adding/subtracting arbitrary numbers, or multiplying/dividing by pos","2020","2022-01-30 04:48:21","2022-01-30 04:48:21","2021-11-07 22:08:47","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/THUBZR6E/comparing-utilities.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7W9CCJS","blogPost","2020","Hubinger, Evan","Clarifying inner alignment terminology","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology","I have seen a lot of confusion recently surrounding exactly how outer and inner alignment should be defined and I want to try and provide my attempt at a clarification. Here's my diagram of how I think the various concepts should fit together: The idea of this diagram is that the arrows are implications—that is, for any problem in the diagram, if its direct subproblems are solved, then it should be solved as well (though not necessarily vice versa). Thus, we get: inner alignment→objective robustnessouter alignment∧objective robustness→intent alignmentintent alignment∧capability robustness→alignment -------------------------------------------------------------------------------- And here are all my definitions of the relevant terms which I think produce those implications: (Impact) Alignment: An agent is impact aligned (with humans) if it doesn't take actions that we would judge to be bad/problematic/dangerous/catastrophic. Intent Alignment: An agent is intent aligned if the optimal policy for its  behavioral objective[1] is impact aligned with humans. Outer Alignment: An objective function r is outer aligned if all models that perform optimally on r in the limit of perfect training and infinite data are intent aligned.[2] Robustness: An agent is robust if it performs well on the base objective it was trained under even in deployment/off-distribution.[3] Objective Robustness: An agent is objective robust if the optimal policy for its  behavioral objective is impact aligned with the base objective it was trained under. Capability Robustness: An agent is capability robust if it performs well on its  behavioral objective even in deployment/off-distribution. Inner Alignment: A mesa-optimizer is inner aligned if the optimal policy for its  mesa-objective is impact aligned with the base objective it was trained under. -------------------------------------------------------------------------------- And an explanation of each of the diagram's implications:","2020-11-09","2022-01-30 04:48:20","2022-01-30 04:48:20","2021-11-13 13:52:21","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSX2XFIQ","report","2020","Fitzgerald, McKenna; Boddy, Aaron; Baum, Seth","2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy","","","","","https://www.ssrn.com/abstract=3070741","","2020","2022-01-30 04:48:11","2022-01-30 04:48:11","2021-10-31 19:23:44","","","","","","","","","","","","Global Catastrophic Risk Institute","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/DQQQHZAB/Baum - 2017 - A Survey of Artificial General Intelligence Projec.pdf","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2KRZ2X5K","blogPost","2020","Armstrong, Stuart","Knowledge, manipulation, and free will","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/2dKvTYYN4PTT7g4of/knowledge-manipulation-and-free-will","Thanks to Rebecca Gorman for co-developing this idea On the 26th of September 1983, Stanislav Petrov observed the early warning satellites reporting the launch of five nuclear missiles towards the Soviet Union. He decided to disobey orders and not pass on the message to higher command, which could easily have resulted in a nuclear war (since the soviet nuclear position was ""launch on warning""). Now, did Petrov have free will when he decided to save the world? MAINTAINING FREE WILL WHEN KNOWLEDGE INCREASES I don't intend to go into the subtle philosophical debate on the nature of free will. See this post for a good reductionist account. Instead, consider the following scenarios:  1. The standard Petrov incident.  2. The standard Petrov incident, except that it is still ongoing and Petrov     hasn't reached a decision yet.  3. The standard Petrov incident, after it was over, except that we don't yet     know what his final decision was.  4. The standard Petrov incident, except that we know that, if Petrov had had     eggs that morning (instead of porridge[1]), he would have made a different     decision.  5. The same as scenario 4., except that some entity deliberately gave Petrov     porridge that morning, aiming to determine his decision.  6. The standard Petrov incident, except that a guy with a gun held Petrov     hostage and forced him not to pass on the report. There is an interesting contrast between scenarios 1, 2, and 3. Clearly, 1 and 3 only differ in our knowledge of the incident. It does not seem that Petrov's free will should depend on the degree of knowledge of some other person. Scenarios 1 and 2 only differ in time: in one case the decision is made, in the second it is yet to be made. If we say that Petrov has free will, whatever that is, in scenario 2, then it seems that in scenario 1, we have to say that he ""had"" free will. So whatever our feeling on free will, it seems that knowing the outcome doesn't change whether there was free will or not.","2020-10-13","2022-01-30 04:48:04","2022-01-30 04:48:04","2021-11-08 23:34:20","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/9VD67FAD/knowledge-manipulation-and-free-will.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZQ49PGTV","conferencePaper","2020","Dafoe, Allan; Hughes, Edward; Bachrach, Yoram; Collins, Tantum; McKee, Kevin R.; Leibo, Joel Z.; Larson, Kate; Graepel, Thore","Open Problems in Cooperative AI","arXiv:2012.08630 [cs]","","","","http://arxiv.org/abs/2012.08630","Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.","2020-12-15","2022-01-30 04:48:04","2022-01-30 04:48:04","2021-11-13 19:00:19","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000025  arXiv: 2012.08630","","/Users/jacquesthibodeau/Zotero/storage/J98ABBH2/Dafoe et al. - 2020 - Open Problems in Cooperative AI.pdf; /Users/jacquesthibodeau/Zotero/storage/E5XXCHBD/2012.html","","UnsortedSafety","Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020 Cooperative AI Workshop","","","","","","","","","","","","","","",""
"V5PJ9SVN","blogPost","2020","Dafoe, Allan","AI Governance: Opportunity and Theory of Impact","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact","Note: We have recently opened up roles for researchers and a project manager at the Centre for the Governance of Artificial Intelligence, part of the Future of Humanity Institute, University of Oxford AI governance concerns how humanity can best navigate the transition to a world with advanced AI systems.[1] It relates to how decisions are made about AI,[2]  and what institutions and arrangements would help those decisions to be made well. I believe advances in AI are likely to be among the most impactful global developments in the coming decades, and that AI governance will become among the most important global issue areas. AI governance is a new field and is relatively neglected. I’ll explain here how I think about this as a cause area and my perspective on how best to pursue positive impact in this space. The value of investing in this field can be appreciated whether one is primarily concerned with contemporary policy challenges or long-term risks and opportunities (“longtermism”); this piece is primarily aimed at a longtermist  perspective. Differing from some other longtermist work on AI, I emphasize the importance of also preparing for more conventional scenarios of AI development. CONTEMPORARY POLICY CHALLENGES AI systems are increasingly being deployed in important domains: for many kinds of surveillance; by authoritarian governments to shape online discourse; for autonomous weapons systems; for cyber tools and autonomous cyber capabilities; to aid and make consequential decisions such as for employment, loans, and criminal sentencing; in advertising; in education and testing; in self-driving cars and navigation; in social media. Society and policy makers are rapidly trying to catch up, to adapt, to create norms and policies to guide these new areas. We see this scramble in contemporary international tax law, competition/antitrust policy, innovation policy, and national security motivated controls on trade and investment. To understand and advise conte","2020-09-17","2022-01-30 04:48:03","2022-01-30 04:48:03","2021-11-07 19:52:53","","","","","","","AI Governance","","","","","","","","","","","","","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/CE75WCFR/ai-governance-opportunity-and-theory-of-impact.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QG8SCUFF","conferencePaper","2020","Dafoe, Allan; Hughes, Edward; Bachrach, Yoram; Collins, Tantum; McKee, Kevin R.; Leibo, Joel Z.; Larson, Kate; Graepel, Thore","Open Problems in Cooperative AI","arXiv:2012.08630 [cs]","","","","http://arxiv.org/abs/2012.08630","Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.","2020-12-15","2022-01-30 04:47:57","2022-01-30 04:47:57","2021-11-13 19:00:19","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000025  arXiv: 2012.08630","","/Users/jacquesthibodeau/Zotero/storage/W24KA5HE/Dafoe et al. - 2020 - Open Problems in Cooperative AI.pdf; /Users/jacquesthibodeau/Zotero/storage/QJWWRSPI/2012.html","","UnsortedSafety","Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020 Cooperative AI Workshop","","","","","","","","","","","","","","",""
"EFKBAFK8","conferencePaper","2020","Dathathri, Sumanth; Dvijotham, Krishnamurthy; Kurakin, Alexey; Raghunathan, Aditi; Uesato, Jonathan; Bunel, Rudy; Shankar, Shreya; Steinhardt, Jacob; Goodfellow, Ian; Liang, Percy; Kohli, Pushmeet","Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming","arXiv:2010.11645 [cs]","","","","http://arxiv.org/abs/2010.11645","Convex relaxations have emerged as a promising approach for verifying desirable properties of neural networks like robustness to adversarial perturbations. Widely used Linear Programming (LP) relaxations only work well when networks are trained to facilitate verification. This precludes applications that involve verification-agnostic networks, i.e., networks not specially trained for verification. On the other hand, semidefinite programming (SDP) relaxations have successfully be applied to verification-agnostic networks, but do not currently scale beyond small networks due to poor time and space asymptotics. In this work, we propose a first-order dual SDP algorithm that (1) requires memory only linear in the total number of network activations, (2) only requires a fixed number of forward/backward passes through the network per iteration. By exploiting iterative eigenvector methods, we express all solver operations in terms of forward and backward passes through the network, enabling efficient use of hardware like GPUs/TPUs. For two verification-agnostic networks on MNIST and CIFAR-10, we significantly improve L-inf verified robust accuracy from 1% to 88% and 6% to 40% respectively. We also demonstrate tight verification of a quadratic stability specification for the decoder of a variational autoencoder.","2020-11-03","2022-01-30 04:47:57","2022-01-30 04:47:57","2021-10-31 19:04:24","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 24  arXiv: 2010.11645","","/Users/jacquesthibodeau/Zotero/storage/7ZPP6KFD/Dathathri et al. - 2020 - Enabling certification of verification-agnostic ne.pdf; /Users/jacquesthibodeau/Zotero/storage/WGFX7DCQ/2010.html; /Users/jacquesthibodeau/Zotero/storage/UCH7ESQD/2010.html","","UnsortedSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","34th Conference on Neural Information Processing Systems (NeurIPS 2020),","","","","","","","","","","","","","","",""
"8R5CM8V5","blogPost","2020","The AlphaFold Team","AlphaFold: a solution to a 50-year-old grand challenge in biology","Deepmind","","","","https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology","In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP) assessment. This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world.","2020-11-30","2022-01-30 04:47:57","2022-01-30 04:47:57","2021-11-13 14:16:32","","","","","","","AlphaFold","","","","","","","ALL","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UD8JD4XD","journalArticle","2020","Barreto, André; Hou, Shaobo; Borsa, Diana; Silver, David; Precup, Doina","Fast reinforcement learning with generalized policy updates","Proceedings of the National Academy of Sciences","","0027-8424, 1091-6490","10.1073/pnas.1907370117","http://www.pnas.org/lookup/doi/10.1073/pnas.1907370117","The combination of reinforcement learning with deep learning is a promising approach to tackle important sequential decision-making problems that are currently intractable. One obstacle to overcome is the amount of data needed by learning systems of this type. In this article, we propose to address this issue through a divide-and-conquer approach. We argue that complex decision problems can be naturally decomposed into multiple tasks that unfold in sequence or in parallel. By associating each task with a reward function, this problem decomposition can be seamlessly accommodated within the standard reinforcement-learning formalism. The specific way we do so is through a generalization of two fundamental operations in reinforcement learning: policy improvement and policy evaluation. The generalized version of these operations allow one to leverage the solution of some tasks to speed up the solution of others. If the reward function of a task can be well approximated as a linear combination of the reward functions of tasks previously solved, we can reduce a reinforcement-learning problem to a simpler linear regression. When this is not the case, the agent can still exploit the task solutions by using them to interact with and learn about the environment. Both strategies considerably reduce the amount of data needed to solve a reinforcement-learning problem.","2020-12-01","2022-01-30 04:47:57","2022-01-30 04:47:57","2021-11-13 13:59:52","30079-30087","","48","117","","Proc Natl Acad Sci USA","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000026","","/Users/jacquesthibodeau/Zotero/storage/WGDHG555/Barreto et al. - 2020 - Fast reinforcement learning with generalized polic.pdf","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9US7JVWW","report","2020","Lohn, Andrew","Hacking AI","","","","","https://cset.georgetown.edu/publication/hacking-ai/","Machine learning systems’ vulnerabilities are pervasive. Hackers and adversaries can easily exploit them. As such, managing the risks is too large a task for the technology community to handle alone. In this primer, Andrew Lohn writes that policymakers must understand the threats well enough to assess the dangers that the United States, its military and intelligence services, and its civilians face when they use machine learning.","2020-12","2022-01-30 04:47:49","2022-01-30 04:47:49","2021-10-31 18:59:26","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/5H3AC6WR/hacking-ai.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HUDHI962","report","2020","Page, Michael; Aiken, Catherine; Murdick, Dewey","Future Indices","","","","","https://cset.georgetown.edu/publication/future-indices/","Foretell is CSET's crowd forecasting pilot project focused on technology and security policy. It connects historical and forecast data on near-term events with the big-picture questions that are most relevant to policymakers. This issue brief uses recent forecast data to illustrate Foretell’s methodology.","2020-10-19","2022-01-30 04:47:49","2022-01-30 04:47:49","2021-11-08 23:47:33","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/SEWFFFFP/future-indices.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KUTR98J7","bookSection","2017","Sotala, Kaj; Yampolskiy, Roman","Responses to the Journey to the Singularity","The Technological Singularity","978-3-662-54031-2 978-3-662-54033-6","","","http://link.springer.com/10.1007/978-3-662-54033-6_3","","2017","2022-01-30 04:51:36","2022-01-30 04:51:36","2020-11-22 05:26:17","25-83","","","","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 5  Series Title: The Frontiers Collection DOI: 10.1007/978-3-662-54033-6_3","","","","CLR; MetaSafety","","Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZBWCFTFF","bookSection","2017","Sotala, Kaj; Yampolskiy, Roman","Risks of the Journey to the Singularity","The Technological Singularity: Managing the Journey","978-3-662-54033-6","","","https://doi.org/10.1007/978-3-662-54033-6_2","SummaryMany researchers have argued that humanity will create artificial general intelligence (AGI) within the next twenty to one hundred years. Unlike current AI systems, individual AGIs would be capable of learning to operate in a wide variety of domains, including ones they had not been specifically designed for. It has been proposed that AGIs might eventually pose a significant risk to humanity, for they could accumulate significant amounts of power and influence in society while being indifferent to what humans valued. The accumulation of power might either happen gradually over time, or it might happen very rapidly (a so-called “hard takeoff”). Gradual accumulation would happen through normal economic mechanisms, as AGIs came to carry out an increasing share of economic tasks. A hard takeoff could be possible if AGIs required significantly less hardware to run than was available, or if they could redesign themselves to run at ever faster speeds, or if they could repeatedly redesign themselves into more intelligent versions of themselves.","2017","2022-01-30 04:51:36","2022-01-30 04:51:36","2020-11-24 02:59:39","11-23","","","","","","","The Frontiers Collection","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","ZSCC: NoCitationData[s1]  ACC: 6  DOI: 10.1007/978-3-662-54033-6_2","","","","CLR; MetaSafety","Automate Trading; Catastrophic Risk; Flash Crash; Machine Ethic; Virtual Assistant","Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"32WKIH7F","manuscript","2011","Tomasik, Brian","Risks of Astronomical Future Suﬀering","","","","","","It’s far from clear that human values will shape an Earth-based space-colonization wave, but even if they do, it seems more likely that space colonization will increase total suﬀering rather than decrease it. That said, other people care a lot about humanity’s survival and spread into the cosmos, so I think suﬀering reducers should let others pursue their spacefaring dreams in exchange for stronger safety measures against future suﬀering. In general, I encourage people to focus on making an intergalactic future more humane if it happens rather than making sure there will be an intergalactic future.","2011","2022-01-30 04:51:36","2022-01-30 04:51:36","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000018","","/Users/jacquesthibodeau/Zotero/storage/RENG9486/Tomasik - Risks of Astronomical Future Suﬀering.pdf","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75Q2IHR5","report","2020","Althaus, David; Baumann, Tobias","Reducing long-term risks from malevolent actors","","","","","https://longtermrisk.org/reducing-long-term-risks-from-malevolent-actors/","Summary Dictators who exhibited highly narcissistic, psychopathic, or sadistic traits were involved in some of the greatest catastrophes in human history.  Malevolent individuals in positions of power could negatively affect humanity’s long-term trajectory by, for example, exacerbating international conflict or other broad risk factors. Malevolent humans with access to advanced technology—such as whole brain emulation […]","2020-07-07","2022-01-30 04:51:36","2022-01-30 04:51:36","2020-08-20 20:10:59","","","","","","","","","","","","Center on Long-Term Risk","","en-US","","","","","","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/8TP4B2UC/Althaus and Baumann - 2020 - Reducing long-term risks from malevolent actors.pdf; /Users/jacquesthibodeau/Zotero/storage/7NBW623R/reducing-long-term-risks-from-malevolent-actors.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"822KXARD","blogPost","2019","Gloor, Lukas","Rebuttal of Christiano and AI Impacts on takeoff speeds?","LessWrong","","","","https://www.lesswrong.com/posts/PzAnWgqvfESgQEvdg/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds#zFEhTxNqEp3eZbjLZ","14 months ago, Paul Christiano and AI Impacts both published forceful and well-received take-downs of many arguments for fast (discontinuous) takeoff. I haven’t seen any rebuttals that are written by established researchers, longer than comments, or otherwise convincing. The longer there is no response, the less weight I put on the outside view that proponents of fast takeoff may be right. Where are the rebuttals? Did I miss them? Is the debate decided? Did nobody have time or motivation to write something? Is the topic too hard to explain? Why rebuttals would be useful: -Give the community a sense of the extent of expert disagreement to form outside views. -Prioritization in AI policy, and to a lesser extent safety, depends on the likelihood of discontinuous progress. We may have more leverage in such cases, but this could be overwhelmed if the probability is low. -Motivate more people to work on MIRI’s research which seems more important to solve early if there is fast takeoff.","2019-04-25","2022-01-30 04:51:36","2022-01-30 04:51:36","2020-11-23 00:36:12","","","","","","","Any rebuttals of Christiano and AI Impacts on takeoff speeds?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/3QVB2E9R/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9GQXT7XM","blogPost","2019","Shah, Rohin","What is narrow value learning?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/vX7KirQwHsBaSEdfK/what-is-narrow-value-learning","Ambitious value learning aims to achieve superhuman performance by figuring out the underlying latent ""values"" that humans have, and evaluating new situations according to these values. In other words, it is trying to infer the criteria by which we judge situations to be good. This is particularly hard because in novel situations that humans haven't seen yet, we haven't even developed the criteria by which we would evaluate. (This is one of the reasons why we need to model humans as suboptimal, which causes problems.) Instead of this, we can use narrow value learning, which produces behavior that we want in some narrow domain, without expecting generalization to novel circumstances. The simplest form of this is imitation learning, where the AI system simply tries to imitate the supervisor's behavior. This limits the AI’s performance to that of its supervisor. We could also learn from preferences over behavior, which can scale to superhuman performance, since the supervisor can often evaluate whether a particular behavior meets our preferences even if she can’t perform it herself. We could also teach our AI systems to perform tasks that we would not want to do ourselves, such as handling hot objects. Nearly all of the work on preference learning, including most work on inverse reinforcement learning (IRL), is aimed at narrow value learning. IRL is often explicitly stated to be a technique for imitation learning, and early algorithms phrase the problem as matching the features in the demonstration, not exceeding them. The few algorithms that try to generalize to different test distributions, such as AIRL, are only aiming for relatively small amounts of generalization. (Why use IRL instead of behavioral cloning, where you mimic the actions that the demonstrator took? The hope is that IRL gives you a good inductive bias for imitation, allowing you to be more sample efficient and to generalize a little bit.) You might have noticed that I talk about narrow value learn","2019","2022-01-30 04:51:12","2022-01-30 04:51:12","2020-12-17 04:37:01","","","","","","","What is narrow value learning?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WZFPW9XN/vX7KirQwHsBaSEdfK.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5IKAMSXK","blogPost","2018","Shah, Rohin","What is ambitious value learning?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning","I think of ambitious value learning as a proposed solution to the specification problem, which I define as the problem of defining the behavior that we would want to see from our AI system. I italicize “defining” to emphasize that this is  not the problem of actually computing behavior that we want to see -- that’s the full AI safety problem. Here we are allowed to use hopelessly impractical schemes, as long as the resulting definition would allow us to in theory compute the behavior that an AI system would take, perhaps with assumptions like infinite computing power or arbitrarily many queries to a human. (Although we do prefer specifications that seem like they could admit an efficient implementation.) In terms of DeepMind’s classification, we are looking for a design specification that exactly matches the ideal specification. HCH and  indirect normativity are examples of attempts at such specifications. We will consider a model in which our AI system is maximizing the expected utility of some explicitly represented utility function that can depend on history. (It does not matter materially whether we consider utility functions or reward functions, as long as they can depend on history.) The utility function may be learned from data, or designed by hand, but it must be an explicit part of the AI that is then maximized. I will not justify this model for now, but simply assume it by fiat and see where it takes us. I’ll note briefly that this model is often justified by the  VNM utility theorem and AIXI, and as the natural idealization of reinforcement learning, which aims to maximize the expected sum of rewards, although typically rewards in RL depend only on states. A lot of conceptual arguments, as well as experiences with specification gaming, suggest that we are unlikely to be able to simply think hard and write down a good specification, since even small errors in specifications can lead to bad results. However, machine learning is particularly good at narro","2018","2022-01-30 04:51:12","2022-01-30 04:51:12","2020-12-17 04:36:27","","","","","","","What is ambitious value learning?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WV7FNFRK/5eX8ko7GCxwR5N9mN.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZFP7FN3X","blogPost","2020","Turner, Alex","What counts as defection?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/8LEPDY36jBYpijrSw/what-counts-as-defection","Thanks to Michael Dennis for proposing the formal definition; to Andrew Critch for pointing me in this direction; to Abram Demski for proposing non-negative weighting; and to Alex Appel, Scott Emmons, Evan Hubinger, philh, Rohin Shah, and Carroll Wainwright for their feedback and ideas. There's a good chance I'd like to publish this at some point as part of a larger work. However, I wanted to make the work available now, in case that doesn't happen soon. They can't prove the conspiracy... But they could, if Steve runs his mouth. The police chief stares at you. You stare at the table. You'd agreed (sworn!) to stay quiet. You'd even studied game theory together. But, you hadn't understood what an extra year of jail meant. The police chief stares at you. Let Steve be the gullible idealist. You have a family waiting for you. Sunlight stretches across the valley, dappling the grass and warming your bow. Your hand anxiously runs along the bowstring. A distant figure darts between trees, and your stomach rumbles. The day is near spent. The stags run strong and free in this land. Carla should meet you there. Shouldn't she? Who wants to live like a beggar, subsisting on scraps of lean rabbit meat? In your mind's eye, you reach the stags, alone. You find one, and your arrow pierces its barrow. The beast bucks and bursts away; the rest of the herd follows. You slump against the tree, exhausted, and never open your eyes again. You can't risk it. People talk about 'defection' in social dilemma games, from the prisoner's dilemma to stag hunt to chicken. In the tragedy of the commons, we talk about defection. The concept has become a regular part of LessWrong discourse. Informal definition. A player defects when they increase their personal payoff at the expense of the group. This informal definition is no secret, being echoed from the ancient Formal Models of Dilemmas in Social Decision-Making to the recent Classifying games like the Prisoner's Dilemma: you can mo","2020-07-12","2022-01-30 04:51:11","2022-01-30 04:51:11","2020-08-28 17:44:13","","","","","","","What counts as defection?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/2H5VW8FH/formalizing-game-theoretic-defection.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3FMN9R8M","bookSection","2014","Russell, Stuart","Unifying Logic and Probability: A New Dawn for AI?","Information Processing and Management of Uncertainty in Knowledge-Based Systems","978-3-319-08794-8 978-3-319-08795-5","","","http://link.springer.com/10.1007/978-3-319-08795-5_2","","2014","2022-01-30 04:51:11","2022-01-30 04:51:11","2020-11-22 05:26:08","10-14","","","442","","","Unifying Logic and Probability","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 11  Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-319-08795-5_2","","/Users/jacquesthibodeau/Zotero/storage/46AICRPP/Russell - 2014 - Unifying Logic and Probability A New Dawn for AI.pdf","","CHAI; TechSafety","","Laurent, Anne; Strauss, Olivier; Bouchon-Meunier, Bernadette; Yager, Ronald R.","Junqueira Barbosa, Simone Diniz; Chen, Phoebe; Cuzzocrea, Alfredo; Du, Xiaoyong; Filipe, Joaquim; Kara, Orhun; Kotenko, Igor; Sivalingam, Krishna M.; Ślęzak, Dominik; Washio, Takashi; Yang, Xiaokang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IIW23ZE3","manuscript","2020","Michaud, Eric J.; Gleave, Adam; Russell, Stuart","Understanding Learned Reward Functions","","","","","http://arxiv.org/abs/2012.05862","In many real-world tasks, it is not possible to procedurally specify an RL agent's reward function. In such cases, a reward function must instead be learned from interacting with and observing humans. However, current techniques for reward learning may fail to produce reward functions which accurately reflect user preferences. Absent significant advances in reward learning, it is thus important to be able to audit learned reward functions to verify whether they truly capture user preferences. In this paper, we investigate techniques for interpreting learned reward functions. In particular, we apply saliency methods to identify failure modes and predict the robustness of reward functions. We find that learned reward functions often implement surprising algorithms that rely on contingent aspects of the environment. We also discover that existing interpretability techniques often attend to irrelevant changes in reward output, suggesting that reward interpretability may need significantly different methods from policy interpretability.","2020-12-10","2022-01-30 04:51:11","2022-01-30 04:51:11","2020-12-18 00:37:06","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000005  arXiv: 2012.05862","","/Users/jacquesthibodeau/Zotero/storage/7RS4HDDE/Michaud et al. - 2020 - Understanding Learned Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/58H3Z5GF/2012.html","","CHAI; TechSafety","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R9QCI7FK","manuscript","2020","Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew; Bluemke, Emma; Lebensold, Jonathan; O'Keefe, Cullen; Koren, Mark; Ryffel, Théo; Rubinovitz, J. B.; Besiroglu, Tamay; Carugati, Federica; Clark, Jack; Eckersley, Peter; de Haas, Sarah; Johnson, Maritza; Laurie, Ben; Ingerman, Alex; Krawczuk, Igor; Askell, Amanda; Cammarota, Rosario; Lohn, Andrew; Krueger, David; Stix, Charlotte; Henderson, Peter; Graham, Logan; Prunkl, Carina; Martin, Bianca; Seger, Elizabeth; Zilberman, Noa; hÉigeartaigh, Seán Ó; Kroeger, Frens; Sastry, Girish; Kagan, Rebecca; Weller, Adrian; Tse, Brian; Barnes, Elizabeth; Dafoe, Allan; Scharre, Paul; Herbert-Voss, Ariel; Rasser, Martijn; Sodhani, Shagun; Flynn, Carrick; Gilbert, Thomas Krendl; Dyer, Lisa; Khan, Saif; Bengio, Yoshua; Anderljung, Markus","Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims","","","","","http://arxiv.org/abs/2004.07213","With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.","2020-04-20","2022-01-30 04:51:11","2022-01-30 04:51:11","2020-08-18 21:36:21","","","","","","","Toward Trustworthy AI Development","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 92  arXiv: 2004.07213","","/Users/jacquesthibodeau/Zotero/storage/7V57AEI3/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf; /Users/jacquesthibodeau/Zotero/storage/JWN7NA8T/2004.html; /Users/jacquesthibodeau/Zotero/storage/J44EXJ2E/2004.html","","MetaSafety; CHAI; CFI; CSER; CSET; FHI; Open-AI","Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NVDI9SGV","conferencePaper","2020","Stray, Jonathan; Adler, Steven; Hadfield-Menell, Dylan","What are you optimizing for? Aligning Recommender Systems with Human Values","","","","","http://arxiv.org/abs/2107.10939","We describe cases where real recommender systems were modiﬁed in the service of various human values such as diversity, fairness, well-being, time well spent, and factual accuracy. From this we identify the current practice of values engineering: the creation of classiﬁers from humancreated data with value-based labels. This has worked in practice for a variety of issues, but problems are addressed one at a time, and users and other stakeholders have seldom been involved. Instead, we look to AI alignment work for approaches that could learn complex values directly from stakeholders, and identify four major directions: useful measures of alignment, participatory design and operation, interactive value learning, and informed deliberative judgments.","2020","2022-01-30 04:51:11","2022-01-30 04:51:11","","7","","","","","","What are you optimizing for?","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000003[s0]","","/Users/jacquesthibodeau/Zotero/storage/VX5CHWKD/Stray et al. - 2021 - What are you optimizing for Aligning Recommender .pdf; /Users/jacquesthibodeau/Zotero/storage/XIEIB3NT/2107.html; /Users/jacquesthibodeau/Zotero/storage/PQ7MPFGP/Stray et al. - What are you optimizing for Aligning Recommender .pdf","","CHAI; TechSafety","Computer Science - Machine Learning; Computer Science - Computers and Society; Computer Science - Information Retrieval","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Participatory Approaches to Machine Learning Workshop, ICML 2020","","","","","","","","","","","","","","",""
"FGCH7XG7","blogPost","2021","Steinhardt, Jacob","Updates and Lessons from AI Forecasting","Bounded Regret","","","","https://bounded-regret.ghost.io/ai-forecasting/","Earlier this year, my research group commissioned 6 questions  for professional forecasters to predict about AI. Broadly speaking, 2 were on geopolitical aspects of AI and 4 were on future capabilities:  Geopolitical: How much larger or smaller will the largest Chinese ML experiment be compared to the largest U.S.","2021-08-18","2022-01-30 04:51:11","2022-01-30 04:51:11","2021-11-18 23:38:06","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/EJIEH689/ai-forecasting.html","","MetaSafety; AmbiguousSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2EZUTH82","conferencePaper","2021","Srivastava, Siddharth","Unifying Principles and Metrics for Safe and Assistive AI","Proceedings of the AAAI Conference on Artificial Intelligence","","","","https://ojs.aaai.org/index.php/AAAI/article/view/17769","The prevalence and success of AI applications have been tempered by concerns about the controllability of AI systems about AI's impact on the future of work.  These concerns reflect two aspects of a central question: how  would  humans work with AI systems? While research on AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems, research on AI and the future of work focuses on the impact of AI on humans who may be unable to do so. This Blue Sky Ideas paper proposes a unifying set of declarative principles that enable a more uniform evaluation of arbitrary AI systems along multiple dimensions of the extent to which they are suitable for use by specific classes of human operators. It leverages recent AI research and the unique strengths of the field to develop human-centric principles for AI systems that address the concerns noted above.","2021-05-18","2022-01-30 04:51:11","2022-01-30 04:51:11","2021-10-30 20:54:11","15064-15068","","","35","","","","","","","","","","en","Copyright (c) 2021 Association for the Advancement of Artificial Intelligence","","","","ojs.aaai.org","","ZSCC: 0000004  Number: 17","","/Users/jacquesthibodeau/Zotero/storage/KSRRJQU3/Srivastava - 2021 - Unifying Principles and Metrics for Safe and Assis.pdf","","MetaSafety; AmbiguousSafety","Metrics For Safe And Beneficial AI Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2JA5F8T","conferencePaper","2021","Laidlaw, Cassidy; Russell, Stuart","Uncertain Decisions Facilitate Better Preference Learning","35th Conference on Neural Information Processing Systems (NeurIPS 2021)","","","","http://arxiv.org/abs/2106.10394","Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human's environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human's preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the first statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity -- the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difficult. It also provides a first step towards understanding and improving preference learning methods for uncertain and suboptimal humans.","2021-10-28","2022-01-30 04:51:11","2022-01-30 04:51:11","2021-11-18 23:19:35","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2106.10394","","/Users/jacquesthibodeau/Zotero/storage/9KSDNPMR/Laidlaw and Russell - 2021 - Uncertain Decisions Facilitate Better Preference L.pdf; /Users/jacquesthibodeau/Zotero/storage/PRVT74B5/2106.html","","TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","35th Conference on Neural Information Processing Systems (NeurIPS 2021)","","","","","","","","","","","","","","",""
"R8H8NHUI","blogPost","2018","Alex Turner","Towards a New Impact Measure","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure","In which I propose a closed-form solution to low impact, increasing  corrigibility and seemingly taking major steps to neutralize basic AI drives 1 (self-improvement), 5 (self-protectiveness), and 6 (acquisition of resources). Previously: Worrying about the Vase: Whitelisting, Overcoming Clinginess in Impact Measures, Impact Measure Desiderata To be used inside an advanced agent, an impact measure... must capture so much variance that there is no clever strategy whereby an advanced agent can produce some special type of variance that evades the measure. ~ Safe Impact MeasureIf we have a safe impact measure, we may have arbitrarily-intelligent unaligned agents which do small (bad) things instead of big (bad) things.  For the abridged experience, read up to ""Notation"", skip to ""Experimental Results"", and then to ""Desiderata"". WHAT IS ""IMPACT""? One lazy Sunday afternoon, I worried that I had written myself out of a job. After all, Overcoming Clinginess in Impact Measures basically said, ""Suppose an impact measure extracts 'effects on the world'. If the agent penalizes itself for these effects, it's incentivized to stop the environment (and any agents in it) from producing them. On the other hand, if it can somehow model other agents and avoid penalizing their effects, the agent is now incentivized to get the other agents to do its dirty work."" This seemed to be strong evidence against the possibility of a simple conceptual core underlying ""impact"", and I didn't know what to do. At this point, it sometimes makes sense to step back and try to say exactly what you don't know how to solve – try to crisply state what it is that you want an unbounded solution for. Sometimes you can't even do that much, and then you may actually have to spend some time thinking 'philosophically' – the sort of stage where you talk to yourself about some mysterious ideal quantity of [chess] move-goodness and you try to pin down what its properties might be. ~  Methodology of Unbounded Anal","2018","2022-01-30 04:51:11","2022-01-30 04:51:11","2020-12-13 23:51:04","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"59SRKKCH","conferencePaper","2018","Milli, Smitha; Miller, John; Dragan, Anca D.; Hardt, Moritz","The Social Cost of Strategic Classification","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency","","","","http://arxiv.org/abs/1808.08460","Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift. We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population. Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.","2018-11-22","2022-01-30 04:51:10","2022-01-30 04:51:10","2019-12-18 02:40:02","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000016[s2]  arXiv: 1808.08460","","/Users/jacquesthibodeau/Zotero/storage/HX3R88QW/Milli et al. - 2018 - The Social Cost of Strategic Classification.pdf; /Users/jacquesthibodeau/Zotero/storage/522JRDXP/1808.html","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Conference on Fairness, Accountability, and Transparency","","","","","","","","","","","","","","",""
"ADB4QQ4U","conferencePaper","2021","Shah, Rohin; Wild, Cody; Wang, Steven H.; Alex, Neel; Houghton, Brandon; Guss, William; Mohanty, Sharada; Kanervisto, Anssi; Milani, Stephanie; Topin, Nicholay; Abbeel, Pieter; Russell, Stuart; Dragan, Anca","The MineRL BASALT Competition on Learning from Human Feedback","arXiv:2107.01969 [cs]","","","","http://arxiv.org/abs/2107.01969","The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve. The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations. Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.","2021-07-05","2022-01-30 04:51:10","2022-01-30 04:51:10","2021-11-14 18:53:31","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 2107.01969","","/Users/jacquesthibodeau/Zotero/storage/PNMP8N4E/Shah et al. - 2021 - The MineRL BASALT Competition on Learning from Hum.pdf; /Users/jacquesthibodeau/Zotero/storage/ASZTCQCH/2107.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2021","","","","","","","","","","","","","","",""
"ANADKJFM","conferencePaper","2019","Chan, Lawrence; Hadfield-Menell, Dylan; Srinivasa, Siddhartha; Dragan, Anca","The Assistive Multi-Armed Bandit","2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","","","","http://arxiv.org/abs/1901.08654","Learning preferences implicit in the choices humans make is a well studied problem in both economics and computer science. However, most work makes the assumption that humans are acting (noisily) optimally with respect to their preferences. Such approaches can fail when people are themselves learning about what they want. In this work, we introduce the assistive multi-armed bandit, where a robot assists a human playing a bandit task to maximize cumulative reward. In this problem, the human does not know the reward function but can learn it through the rewards received from arm pulls; the robot only observes which arms the human pulls but not the reward associated with each pull. We offer sufficient and necessary conditions for successfully assisting the human in this framework. Surprisingly, better human performance in isolation does not necessarily lead to better performance when assisted by the robot: a human policy can do better by effectively communicating its observed rewards to the robot. We conduct proof-of-concept experiments that support these results. We see this work as contributing towards a theory behind algorithms for human-robot interaction.","2019-01-24","2022-01-30 04:51:10","2022-01-30 04:51:10","2019-07-08 15:45:03","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000024  arXiv: 1901.08654","","/Users/jacquesthibodeau/Zotero/storage/RJHMMH5U/Chan et al. - 2019 - The Assistive Multi-Armed Bandit.pdf; /Users/jacquesthibodeau/Zotero/storage/SP2MCRP5/1901.html","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","","","","","","","","","","","","","","",""
"GZZWAE9K","manuscript","2017","Critch, Andrew","Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making","","","","","http://arxiv.org/abs/1701.01302","Existing multi-objective reinforcement learning (MORL) algorithms do not account for objectives that arise from players with differing beliefs. Concretely, consider two players with different beliefs and utility functions who may cooperate to build a machine that takes actions on their behalf. A representation is needed for how much the machine's policy will prioritize each player's interests over time. Assuming the players have reached common knowledge of their situation, this paper derives a recursion that any Pareto optimal policy must satisfy. Two qualitative observations can be made from the recursion: the machine must (1) use each player's own beliefs in evaluating how well an action will serve that player's utility function, and (2) shift the relative priority it assigns to each player's expected utilities over time, by a factor proportional to how well that player's beliefs predict the machine's inputs. Observation (2) represents a substantial divergence from na\""{i}ve linear utility aggregation (as in Harsanyi's utilitarian theorem, and existing MORL algorithms), which is shown here to be inadequate for Pareto optimal sequential decision-making on behalf of players with different beliefs.","2017-01-05","2022-01-30 04:51:10","2022-01-30 04:51:10","2018-12-09 18:04:21","","","","","","","Toward negotiable reinforcement learning","","","","","","","","","","","","arXiv.org","","ZSCC: 0000010  arXiv: 1701.01302","","/Users/jacquesthibodeau/Zotero/storage/G65SGP7V/Critch - 2017 - Toward negotiable reinforcement learning shifting.pdf; /Users/jacquesthibodeau/Zotero/storage/X9ZHKTCD/Critch - 2017 - Toward negotiable reinforcement learning shifting.pdf; /Users/jacquesthibodeau/Zotero/storage/ARS283R8/1701.html; /Users/jacquesthibodeau/Zotero/storage/XSPJ5PMX/1701.html; /Users/jacquesthibodeau/Zotero/storage/K885DNUJ/1701.html","","CHAI; TechSafety; MIRI","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JUZZK322","conferencePaper","2017","Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart","The Off-Switch Game","Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence","978-0-9992411-0-3","","10.24963/ijcai.2017/32","https://www.ijcai.org/proceedings/2017/32","It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R’s off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H’s actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.","2017","2022-01-30 04:51:10","2022-01-30 04:51:10","2019-07-08 15:30:25","220-227","","","","","","","","","","","International Joint Conferences on Artificial Intelligence Organization","Melbourne, Australia","en","","","","","DOI.org (Crossref)","","ZSCC: 0000086","","","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Twenty-Sixth International Joint Conference on Artificial Intelligence","","","","","","","","","","","","","","",""
"GHMMCFVM","magazineArticle","2018","Arkin, Ronald; Russell, Stuart; Min-Seok, Kim","The new weapons of mass destruction?","The Security Times","","","","","","2018","2022-01-30 04:51:10","2022-01-30 04:51:10","","1","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s3]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/XXVH9ATC/Arkin et al. - The new weapons of mass destruction.pdf","","MetaSafety; CHAI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZHP842W","conferencePaper","2020","Toyer, Sam; Shah, Rohin; Critch, Andrew; Russell, Stuart","The MAGICAL Benchmark for Robust Imitation","Advances in Neural Information Processing Systems 33 Pre-proceedings","","","","https://papers.nips.cc/paper/2020/hash/d464b5ac99e74462f321c06ccacc4bff-Abstract.html","Imitation Learning (IL) algorithms are typically evaluated in the same environment that was used to create demonstrations. This rewards precise reproduction of demonstrations in one particular environment, but provides little information about how robustly an algorithm can generalise the demonstrator’s intent to substantially different deployment settings. This paper presents the MAGICAL benchmark suite, which permits systematic evaluation of generalisation by quantifying robustness to different kinds of distribution shift that an IL algorithm is likely to encounter in practice. Using the MAGICAL suite, we conﬁrm that existing IL algorithms overﬁt signiﬁcantly to the context in which demonstrations are provided. We also show that standard methods for reducing overﬁtting are effective at creating narrow perceptual invariances, but are not sufﬁcient to enable transfer to contexts that require substantially different behaviour, which suggests that new approaches will be needed in order to robustly generalise demonstrator intent. Code and data for the MAGICAL suite is available at https://github.com/qxcv/magical/.","2020","2022-01-30 04:51:10","2022-01-30 04:51:10","2020-12-18","25","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000007","","/Users/jacquesthibodeau/Zotero/storage/68R9Q97E/Toyer et al. - The MAGICAL Benchmark for Robust Imitation.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020","","","","","","","","","","","","","","",""
"PMHJK9F7","blogPost","2019","Shah, Rohin","The human side of interaction","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/eD9T4kiwB6MHpySGE/the-human-side-of-interaction","The last few posts have motivated an analysis of the human-AI system rather than an AI system in isolation. So far we’ve looked at the notion that the AI system should get feedback from the user and that it could use reward uncertainty for corrigibility. These are focused on the AI system, but what about the human? If we build a system that explicitly solicits feedback from the human, what do we have to say about the human policy, and how the human should provide feedback? INTERPRETING HUMAN ACTIONS One major free variable in any explicit interaction or feedback mechanism is what semantics the AI system should attach to the human feedback. The classic examples of AI risk are usually described in a way where this is the problem: when we provide a reward function that rewards paperclips, the AI system interprets it literally and maximizes paperclips, rather than interpreting it pragmatically as another human would. (Aside: I suspect this was not the original point of the paperclip maximizer, but it has become a very popular retelling, so I’m using it anyway.) Modeling this classic example as a human-AI system, we can see that the problem is that the human is offering a form of “feedback”, the reward function, and the AI system is not ascribing the correct semantics to it. The way it uses the reward function implies that the reward function encodes the optimal behavior of the AI system in all possible environments -- a moment’s thought is sufficient to see that this is not actually the case. There will definitely be many cases and environments that the human did not consider when designing the reward function, and we should not expect that the reward function incentivizes the right behavior in those cases. So what can the AI system assume if the human provides it a reward function?  Inverse Reward Design (IRD) offers one answer: the human is likely to provide a particular reward function if it leads to high true utility behavior in the training environment. So, in","2019","2022-01-30 04:51:10","2022-01-30 04:51:10","2020-12-17 04:37:16","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WHZXKZ8H/eD9T4kiwB6MHpySGE.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4K6R3T4K","conferencePaper","2019","Dragan, Anca","Specifying AI Objectives As a Human-AI Collaboration Problem","Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","978-1-4503-6324-2","","10.1145/3306618.3314227","http://doi.acm.org/10.1145/3306618.3314227","Estimation, planning, control, and learning are giving us robots that can generate good behavior given a specified objective and set of constraints. What I care about is how humans enter this behavior generation picture, and study two complementary challenges: 1) how to optimize behavior when the robot is not acting in isolation, but needs to coordinate or collaborate with people; and 2) what to optimize in order to get the behavior we want. My work has traditionally focused on the former, but more recently I have been casting the latter as a human-robot collaboration problem as well (where the human is the end-user, or even the robotics engineer building the system). Treating it as such has enabled us to use robot actions to gain information; to account for human pedagogic behavior; and to exchange information between the human and the robot via a plethora of communication channels, from external forces that the person physically applies to the robot, to comparison queries, to defining a proxy objective function.","2019","2022-01-30 04:51:09","2022-01-30 04:51:09","2019-12-18 02:38:17","329–329","","","","","","","AIES '19","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","ZSCC: 0000000  event-place: Honolulu, HI, USA","","","","CHAI; TechSafety","inverse reinforcement learning; reward design; value alignment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6ZGCJNUC","journalArticle","2018","Rozo, Leonel; Amor, Heni Ben; Calinon, Sylvain; Dragan, Anca; Lee, Dongheui","Special issue on learning for human–robot collaboration","Autonomous Robots","","1573-7527","10.1007/s10514-018-9756-z","https://doi.org/10.1007/s10514-018-9756-z","","2018-06-01","2022-01-30 04:51:09","2022-01-30 04:51:09","2019-12-18 02:40:04","953-956","","5","42","","Auton Robot","","","","","","","","en","","","","","Springer Link","","ZSCC: 0000010","","/Users/jacquesthibodeau/Zotero/storage/G3JHNZVF/Rozo et al. - 2018 - Special issue on learning for human–robot collabor.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QGGZQCD2","conferencePaper","2017","Milli, Smitha; Hadfield-Menell, Dylan; Dragan, Anca; Russell, Stuart","Should Robots be Obedient?","IJCAI'17: Proceedings of the 26th International Joint Conference on Artificial Intelligence","","","","http://arxiv.org/abs/1705.09990","Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.","2017-05-28","2022-01-30 04:51:09","2022-01-30 04:51:09","2019-05-07 20:04:43","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000050  arXiv: 1705.09990","","/Users/jacquesthibodeau/Zotero/storage/3A9TS8IF/Milli et al. - 2017 - Should Robots be Obedient.pdf; /Users/jacquesthibodeau/Zotero/storage/U6MHTWRU/1705.html; /Users/jacquesthibodeau/Zotero/storage/VKNMUE9J/1705.html","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCAI 2017","","","","","","","","","","","","","","",""
"S5KR49MS","manuscript","2017","Critch, Andrew; Russell, Stuart","Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making","","","","","http://arxiv.org/abs/1711.00363","It is often argued that an agent making decisions on behalf of two or more principals who have different utility functions should adopt a {\em Pareto-optimal} policy, i.e., a policy that cannot be improved upon for one agent without making sacrifices for another. A famous theorem of Harsanyi shows that, when the principals have a common prior on the outcome distributions of all policies, a Pareto-optimal policy for the agent is one that maximizes a fixed, weighted linear combination of the principals' utilities. In this paper, we show that Harsanyi's theorem does not hold for principals with different priors, and derive a more precise generalization which does hold, which constitutes our main result. In this more general case, the relative weight given to each principal's utility should evolve over time according to how well the agent's observations conform with that principal's prior. The result has implications for the design of contracts, treaties, joint ventures, and robots.","2017-10-31","2022-01-30 04:51:09","2022-01-30 04:51:09","2018-12-09 18:04:24","","","","","","","Servant of Many Masters","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 1711.00363","","/Users/jacquesthibodeau/Zotero/storage/P2ISQ5FB/Critch and Russell - 2017 - Servant of Many Masters Shifting priorities in Pa.pdf; /Users/jacquesthibodeau/Zotero/storage/ZJ7FA6BB/1711.html; /Users/jacquesthibodeau/Zotero/storage/SNVKFDTA/1711.html","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJ8K6HTE","conferencePaper","2019","Swamy, Gokul; Reddy, Siddharth; Levine, Sergey; Dragan, Anca D.","Scaled Autonomy: Enabling Human Operators to Control Robot Fleets","2020 IEEE International Conference on Robotics and Automation (ICRA)","","","","http://arxiv.org/abs/1910.02910","Autonomous robots often encounter challenging situations where their control policies fail and an expert human operator must briefly intervene, e.g., through teleoperation. In settings where multiple robots act in separate environments, a single human operator can manage a fleet of robots by identifying and teleoperating one robot at any given time. The key challenge is that users have limited attention: as the number of robots increases, users lose the ability to decide which robot requires teleoperation the most. Our goal is to automate this decision, thereby enabling users to supervise more robots than their attention would normally allow for. Our insight is that we can model the user's choice of which robot to control as an approximately optimal decision that maximizes the user's utility function. We learn a model of the user's preferences from observations of the user's choices in easy settings with a few robots, and use it in challenging settings with more robots to automatically identify which robot the user would most likely choose to control, if they were able to evaluate the states of all robots at all times. We run simulation experiments and a user study with twelve participants that show our method can be used to assist users in performing a navigation task and manipulator reaching task.","2019-09-21","2022-01-30 04:51:09","2022-01-30 04:51:09","2019-12-18 02:35:11","","","","","","","Scaled Autonomy","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003[s0]  arXiv: 1910.02910","","/Users/jacquesthibodeau/Zotero/storage/VPEQJUNS/Swamy et al. - 2019 - Scaled Autonomy Enabling Human Operators to Contr.pdf; /Users/jacquesthibodeau/Zotero/storage/EVFPGQ5H/1910.html","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 IEEE International Conference on Robotics and Automation (ICRA)","","","","","","","","","","","","","","",""
"JVDEZZWU","conferencePaper","2018","Fridovich-Keil, David; Fisac, Jaime F.; Tomlin, Claire J.","Safely Probabilistically Complete Real-Time Planning and Exploration in Unknown Environments","2019 International Conference on Robotics and Automation (ICRA)","","","","http://arxiv.org/abs/1811.07834","We present a new framework for motion planning that wraps around existing kinodynamic planners and guarantees recursive feasibility when operating in a priori unknown, static environments. Our approach makes strong guarantees about overall safety and collision avoidance by utilizing a robust controller derived from reachability analysis. We ensure that motion plans never exit the safe backward reachable set of the initial state, while safely exploring the space. This preserves the safety of the initial state, and guarantees that that we will eventually ﬁnd the goal if it is possible to do so while exploring safely. We implement our framework in the Robot Operating System (ROS) software environment and demonstrate it in a real-time simulation.","2018-11-19","2022-01-30 04:51:09","2022-01-30 04:51:09","2019-07-08 16:10:47","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 24  J: 6 arXiv: 1811.07834","","","","CHAI; TechSafety","Computer Science - Robotics; Electrical Engineering and Systems Science - Systems and Control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 International Conference on Robotics and Automation (ICRA)","","","","","","","","","","","","","","",""
"QKVQDMCH","conferencePaper","2021","Emmons, Scott; Oesterheld, Caspar; Critch, Andrew; Conitzer, Vince; Russell, Stuart","Symmetry, Equilibria, and Robustness in Common-Payoff Games","","","","","https://preflib.github.io/gaiw2021/papers/GAIW_2021_paper_32.pdf","Although it has been known since the 1970s that a globally optimal strategy profile in a common-payoff game is a Nash equilibrium, global optimality is a strict requirement that limits the result’s applicability. In this work, we show that any locally optimal sym- metric strategy profile is also a (global) Nash equilibrium. Applied to machine learning, our result provides a global guarantee for any gradient method that finds a local optimum in symmetric strategy space. Furthermore, we show that this result is robust to pertur- bations to the common payoff and to the local optimum. While these results indicate stability to unilateral deviation, we neverthe- less identify broad classes of games where mixed local optima are unstable under joint, asymmetric deviations. We analyze the preva- lence of instability by running learning algorithms in a suite of symmetric games, and we conclude with results on the complexity of computing game symmetries.","2021-05","2022-01-30 04:51:09","2022-01-30 04:51:09","2021-10-30 21:01:14","17","","","","","","","","","","","","London, UK","en","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/7B72JZQB/GAIW_2021_paper_32.pdf","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","3rd Games, Agents, and Incentives Workshop (GAIW 2021)","","","","","","","","","","","","","","",""
"KRGHV4EA","blogPost","2020","Critch, Andrew","Some AI research areas and their relevance to existential safety","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1","INTRODUCTION This post is an overview of a variety of AI research areas in terms of how much I think contributing to and/or learning from those areas might help reduce AI x-risk. By research areas I mean “AI research topics that already have groups of people working on them and writing up their results”, as opposed to research “directions” in which I’d like to see these areas “move”. I formed these views mostly pursuant to writing AI Research Considerations for Human Existential Safety (ARCHES). My hope is that my assessments in this post can be helpful to students and established AI researchers who are thinking about shifting into new research areas specifically with the goal of contributing to existential safety somehow. In these assessments, I find it important to distinguish between the following types of value:  * The helpfulness of the area to existential safety, which I think of as a    function of what services are likely to be provided as a result of research    contributions to the area, and whether those services will be helpful to    existential safety, versus  * The educational value of the area for thinking about existential safety,    which I think of as a function of how much a researcher motivated by    existential safety might become more effective through the process of    familiarizing with or contributing to that area, usually by focusing on ways    the area could be used in service of existential safety.  * The neglect of the area at various times, which is a function of how much    technical progress has been made in the area relative to how much I think is    needed. Importantly:  * The helpfulness to existential safety scores do not assume that your    contributions to this area would be used only for projects with existential    safety as their mission. This can negatively impact the helpfulness of    contributing to areas that are more likely to be used in ways that harm    existential safety.  * The educational value scores are not ab","2020-11-18","2022-01-30 04:51:09","2022-01-30 04:51:09","2020-12-19 02:13:03","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/U3PJ2KXP/some-ai-research-areas-and-their-relevance-to-existential-1.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NWHFR69Q","conferencePaper","2018","Ratner, Ellis; Hadfield-Menell, Dylan; Dragan, Anca D.","Simplifying Reward Design through Divide-and-Conquer","Robotics: Science and Systems XIV","","","","http://arxiv.org/abs/1806.02501","Designing a good reward function is essential to robot planning and reinforcement learning, but it can also be challenging and frustrating. The reward needs to work across multiple different environments, and that often requires many iterations of tuning. We introduce a novel divide-andconquer approach that enables the designer to specify a reward separately for each environment. By treating these separate reward functions as observations about the underlying true reward, we derive an approach to infer a common reward across all environments. We conduct user studies in an abstract grid world domain and in a motion planning domain for a 7-DOF manipulator that measure user effort and solution quality. We show that our method is faster, easier to use, and produces a higher quality solution than the typical method of designing a reward jointly across all environments. We additionally conduct a series of experiments that measure the sensitivity of these results to different properties of the reward design task, such as the number of environments, the number of feasible solutions per environment, and the fraction of the total features that vary within each environment. We ﬁnd that independent reward design outperforms the standard, joint, reward design process but works best when the design problem can be divided into simpler subproblems.","2018-06-06","2022-01-30 04:51:09","2022-01-30 04:51:09","2019-07-12 00:11:37","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000020  arXiv: 1806.02501","","/Users/jacquesthibodeau/Zotero/storage/3W9MFI95/Ratner et al. - 2018 - Simplifying Reward Design through Divide-and-Conqu.pdf; /Users/jacquesthibodeau/Zotero/storage/M3RN4N6G/1806.html; /Users/jacquesthibodeau/Zotero/storage/CTPZ3Q7F/1806.html; /Users/jacquesthibodeau/Zotero/storage/3HUP68HH/Ratner et al. - 2018 - Simplifying Reward Design through Divide-and-Conqu.pdf","","CHAI; TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Robotics: Science and Systems XIV","","","","","","","","","","","","","","",""
"AXU8XRXT","conferencePaper","2020","Köster, Raphael; Hadfield-Menell, Dylan; Hadfield, Gillian K.; Leibo, Joel Z.","Silly rules improve the capacity of agents to learn stable enforcement and compliance behaviors","Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020),","","","","http://arxiv.org/abs/2001.09318","How can societies learn to enforce and comply with social norms? Here we investigate the learning dynamics and emergence of compliance and enforcement of social norms in a foraging game, implemented in a multi-agent reinforcement learning setting. In this spatiotemporally extended game, individuals are incentivized to implement complex berry-foraging policies and punish transgressions against social taboos covering specific berry types. We show that agents benefit when eating poisonous berries is taboo, meaning the behavior is punished by other agents, as this helps overcome a credit-assignment problem in discovering delayed health effects. Critically, however, we also show that introducing an additional taboo, which results in punishment for eating a harmless berry, improves the rate and stability with which agents learn to punish taboo violations and comply with taboos. Counterintuitively, our results show that an arbitrary taboo (a ""silly rule"") can enhance social learning dynamics and achieve better outcomes in the middle stages of learning. We discuss the results in the context of studying normativity as a group-level emergent phenomenon.","2020-01-25","2022-01-30 04:51:09","2022-01-30 04:51:09","2020-11-21 18:30:39","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000006  arXiv: 2001.09318","","/Users/jacquesthibodeau/Zotero/storage/G24AAEA3/Köster et al. - 2020 - Silly rules improve the capacity of agents to lear.pdf; /Users/jacquesthibodeau/Zotero/storage/TS9NAKX8/2001.html; /Users/jacquesthibodeau/Zotero/storage/W6K5KE25/2001.html","","CHAI; TechSafety; DeepMind","Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GRWKGUDI","conferencePaper","2018","Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey","Shared Autonomy via Deep Reinforcement Learning","Robotics: Science and Systems XIV","","","","http://arxiv.org/abs/1802.01744","In shared autonomy, user input is combined with semi-autonomous control to achieve a common goal. The goal is often unknown ex-ante, so prior work enables agents to infer the goal from user input and assist with the task. Such methods tend to assume some combination of knowledge of the dynamics of the environment, the user’s policy given their goal, and the set of possible goals the user might target, which limits their application to real-world scenarios. We propose a deep reinforcement learning framework for model-free shared autonomy that lifts these assumptions. We use human-in-the-loop reinforcement learning with neural network function approximation to learn an end-to-end mapping from environmental observation and user input to agent action values, with task reward as the only form of supervision. This approach poses the challenge of following user commands closely enough to provide the user with real-time action feedback and thereby ensure high-quality user input, but also deviating from the user’s actions when they are suboptimal. We balance these two needs by discarding actions whose values fall below some threshold, then selecting the remaining action closest to the user’s input. Controlled studies with users (n = 12) and synthetic pilots playing a video game, and a pilot study with users (n = 4) ﬂying a real quadrotor, demonstrate the ability of our algorithm to assist users with real-time control tasks in which the agent cannot directly access the user’s private information through observations, but receives a reward signal and user input that both depend on the user’s intent. The agent learns to assist the user without access to this private information, implicitly inferring it from the user’s input. This enables the assisted user to complete the task more effectively than the user or an autonomous agent could on their own. This paper is a proof of concept that illustrates the potential for deep reinforcement learning to enable ﬂexible and practical assistive systems.","2018-02-05","2022-01-30 04:51:09","2022-01-30 04:51:09","2019-07-12 00:10:49","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000087  arXiv: 1802.01744","","/Users/jacquesthibodeau/Zotero/storage/6HMCX5IQ/Reddy et al. - 2018 - Shared Autonomy via Deep Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/KS26B5X8/1802.html; /Users/jacquesthibodeau/Zotero/storage/FWIRNIMR/1802.html; /Users/jacquesthibodeau/Zotero/storage/P49DC639/Reddy et al. - 2018 - Shared Autonomy via Deep Reinforcement Learning.pdf","","CHAI; TechSafety","Computer Science - Machine Learning; Computer Science - Robotics; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Robotics: Science and Systems XIV","","","","","","","","","","","","","","",""
"EU74EWFE","blogPost","2015","Tomasik, Brian","Reasons to Be Nice to Other Value Systems","Center on Long-Term Risk","","","","https://longtermrisk.org/reasons-to-be-nice-to-other-value-systems/","Several arguments support the heuristic that we should help groups holding different value systems from our own when doing so is cheap, unless those groups prove uncooperative to our values. This is true even if we don't directly care at all about other groups' value systems. Exactly how nice to be depends on the particulars of the situation.","2015-08-29","2022-01-30 04:51:09","2022-01-30 04:51:09","2020-11-23 20:07:23","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/ZCKXSPBP/reasons-to-be-nice-to-other-value-systems.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MKUM9MAZ","blogPost","2020","Kokotajlo, Daniel","Persuasion Tools: AI takeover without AGI or agency?","LessWrong","","","","https://www.lesswrong.com/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency","[epistemic status: speculation] I'm envisioning that in the future there will also be systems where you can input any conclusion that you want to argue (including moral conclusions) and the target audience, and the system will give you the most convincing arguments for it. At that point people won't be able to participate in any online (or offline for that matter) discussions without risking their object-level values being hijacked.--Wei Dai What if most people already live in that world? A world in which taking arguments at face value is not a capacity-enhancing tool, but a security vulnerability? Without trusted filters, would they not dismiss highfalutin arguments out of hand, and focus on whether the person making the argument seems friendly, or unfriendly, using hard to fake group-affiliation signals?--Benquo 1. AI-powered memetic warfare makes all humans effectively insane.--Wei Dai, listing nonstandard AI doom scenarios This post speculates about persuasion tools—how likely they are to get better in the future relative to countermeasures, what the effects of this might be, and what implications there are for what we should do now. To avert eye-rolls, let me say up front that I don’t think the world is likely to be driven insane by AI-powered memetic warfare. I think progress in persuasion tools will probably be gradual and slow, and defenses will improve too, resulting in an overall shift in the balance that isn’t huge: a deterioration of collective epistemology, but not a massive one. However, (a) I haven’t yet ruled out more extreme scenarios, especially during a slow takeoff, and (b) even small, gradual deteriorations are important to know about. Such a deterioration would make it harder for society to notice and solve AI safety and governance problems, because it is worse at noticing and solving problems in general. Such a deterioration could also be a risk factor for world war three, revolutions, sectarian conflict, terrorism, and the like. Moreover","2020","2022-01-30 04:51:09","2022-01-30 04:51:09","2020-12-12 15:02:20","","","","","","","Persuasion Tools","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BPNW2WW3/persuasion-tools-ai-takeover-without-agi-or-agency.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R95SH2RD","conferencePaper","2019","Li, Shihui; Wu, Yi; Cui, Xinyue; Dong, Honghua; Fang, Fei; Russell, Stuart","Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient","Proceedings of the AAAI Conference on Artificial Intelligence","","","10.1609/aaai.v33i01.33014213","http://www.aaai.org/ojs/index.php/AAAI/article/view/4327","Despite the recent advances of deep reinforcement learning (DRL), agents trained by DRL tend to be brittle and sensitive to the training environment, especially in the multi-agent scenarios. In the multi-agent setting, a DRL agent’s policy can easily get stuck in a poor local optima w.r.t. its training partners – the learned policy may be only locally optimal to other agents’ current policies. In this paper, we focus on the problem of training robust DRL agents with continuous actions in the multi-agent learning setting so that the trained agents can still generalize when its opponents’ policies alter. To tackle this problem, we proposed a new algorithm, MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG) with the following contributions: (1) we introduce a minimax extension of the popular multi-agent deep deterministic policy gradient algorithm (MADDPG), for robust policy learning; (2) since the continuous action space leads to computational intractability in our minimax learning objective, we propose Multi-Agent Adversarial Learning (MAAL) to efﬁciently solve our proposed formulation. We empirically evaluate our M3DDPG algorithm in four mixed cooperative and competitive multi-agent environments and the agents trained by our method signiﬁcantly outperforms existing baselines.","2019-07-17","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-12-17 22:55:06","4213-4220","","","33","","","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000108","","/Users/jacquesthibodeau/Zotero/storage/TAT4QW33/Li et al. - 2019 - Robust Multi-Agent Reinforcement Learning via Mini.pdf","","CHAI; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4W6H9NQQ","manuscript","2017","Dragan, Anca D.","Robot Planning with Mathematical Models of Human State and Action","","","","","http://arxiv.org/abs/1705.04226","Robots interacting with the physical world plan with models of physics. We advocate that robots interacting with people need to plan with models of cognition. This writeup summarizes the insights we have gained in integrating computational cognitive models of people into robotics planning and control. It starts from a general game-theoretic formulation of interaction, and analyzes how different approximations result in different useful coordination behaviors for the robot during its interaction with people.","2017-05-11","2022-01-30 04:51:08","2022-01-30 04:51:08","2019-07-22 21:49:18","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000027  arXiv: 1705.04226","","","","CHAI; TechSafety","Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XH9E33N4","blogPost","2019","Shah, Rohin","Reward uncertainty","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/ZiLLxaLB5CCofrzPp/reward-uncertainty","In my last post, I argued that interaction between the human and the AI system was necessary in order for the AI system to “stay on track” as we encounter new and unforeseen changes to the environment. The most obvious implementation of this would be to have an AI system that keeps an estimate of the reward function. It acts to maximize its current estimate of the reward function, while simultaneously updating the reward through human feedback. However, this approach has significant problems. Looking at the description of this approach, one thing that stands out is that the actions are chosen according to a reward that we know is going to change. (This is what leads to the incentive to disable the narrow value learning system.) This seems clearly wrong: surely our plans should account for the fact that our rewards will change, without treating such a change as adversarial? This suggests that we need to have our action selection mechanism take the future rewards into account as well. While we don’t know what the future reward will be, we can certainly have a  probability distribution over it. So what if we had uncertainty over reward functions, and took that uncertainty into account while choosing actions? SETUP We’ve drilled down on the problem sufficiently far that we can create a formal model and see what happens. So, let’s consider the following setup:  * The human, Alice, knows the “true” reward function that she would like to    have optimized.  * The AI system maintains a probability distribution over reward functions, and    acts to maximize the expected sum of rewards under this distribution.  * Alice and the AI system take turns acting. Alice knows that the AI learns    from her actions, and chooses actions accordingly.  * Alice’s action space is such that she cannot take the action “tell the AI    system the true reward function” (otherwise the problem would become    trivial).  * Given these assumptions, Alice and the AI system act optimally. This is","2019","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-12-17 04:37:10","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/S52J9GZH/ZiLLxaLB5CCofrzPp.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CJVETACN","journalArticle","2015","Russell, Stuart; Dewey, Daniel; Tegmark, Max","Research priorities for robust and beneficial artificial intelligence: an open letter","AI Magazine","","","","","","2015","2022-01-30 04:51:08","2022-01-30 04:51:08","","","","4","36","","","Research priorities for robust and beneficial artificial intelligence","","","","","","","","","","","","Google Scholar","","ZSCC: 0000017","","/Users/jacquesthibodeau/Zotero/storage/Q8Q62AV8/Russell et al. - 2015 - Research priorities for robust and beneficial arti.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IRTZ78N7","journalArticle","2015","Russell, Stuart","Recent developments in unifying logic and probability","Communications of the ACM","","00010782","10.1145/2699411","http://dl.acm.org/citation.cfm?doid=2797100.2699411","","2015-06-25","2022-01-30 04:51:08","2022-01-30 04:51:08","2018-12-09 19:18:27","88-97","","7","58","","","","","","","","","","en","","","","","Crossref","","ZSCC: NoCitationData[s2]  ACC: 74","","","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTQZCG32","manuscript","2020","Filan, Daniel; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart","Pruned Neural Networks are Surprisingly Modular","","","","","https://arxiv.org/abs/2003.04881v4","The learned weights of a neural network are often considered devoid of scrutable internal structure. To discern structure in these weights, we introduce a measurable notion of modularity for multi-layer perceptrons (MLPs), and investigate the modular structure of MLPs trained on datasets of small images. Our notion of modularity comes from the graph clustering literature: a ""module"" is a set of neurons with strong internal connectivity but weak external connectivity. We find that training and weight pruning produces MLPs that are more modular than randomly initialized ones, and often significantly more modular than random MLPs with the same (sparse) distribution of weights. Interestingly, they are much more modular when trained with dropout. We also present exploratory analyses of the importance of different modules for performance and how modules depend on each other. Understanding the modular structure of neural networks, when such structure exists, will hopefully render their inner workings more interpretable to engineers.","2020-03-10","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-12-12 01:54:56","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/MWESESHU/Filan et al. - 2020 - Pruned Neural Networks are Surprisingly Modular.pdf; /Users/jacquesthibodeau/Zotero/storage/MN3V4CEA/2003.html; /Users/jacquesthibodeau/Zotero/storage/IDPDA9W6/2003.html","","CHAI; TechSafety; AmbiguosSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4HM6IEQ3","magazineArticle","2016","Russell, Stuart","Robots in war: the next weapons of mass destruction?","World Economic Forum","","","","https://www.weforum.org/agenda/2016/01/robots-in-war-the-next-weapons-of-mass-destruction/","Davos 2016: There is no doubt that as the technology improves, autonomous weapons will be highly effective. But does that necessarily mean they’re a good idea?","2016","2022-01-30 04:51:08","2022-01-30 04:51:08","2019-12-18 01:17:33","","","","","","","Robots in war","","","","","","","","","","","","","","ZSCC: 0000005","","/Users/jacquesthibodeau/Zotero/storage/UCHEJG2V/robots-in-war-the-next-weapons-of-mass-destruction.html","","MetaSafety; CHAI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNU8DV83","journalArticle","2015","Russell, Stuart; Dewey, Daniel; Tegmark, Max","Research Priorities for Robust and Beneficial Artificial Intelligence","AI Magazine","","0738-4602, 0738-4602","10.1609/aimag.v36i4.2577","https://aaai.org/ojs/index.php/aimagazine/article/view/2577","Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.","2015-12-31","2022-01-30 04:51:08","2022-01-30 04:51:08","2019-12-18 01:27:03","105","","4","36","","AIMag","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000575","","/Users/jacquesthibodeau/Zotero/storage/W2CF629Z/Russell et al. - 2015 - Research Priorities for Robust and Beneficial Arti.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AE7K4ISJ","bookSection","2016","Russell, Stuart","Rationality and Intelligence: A Brief Update","Fundamental Issues of Artificial Intelligence","978-3-319-26483-7 978-3-319-26485-1","","","http://link.springer.com/10.1007/978-3-319-26485-1_2","The long-term goal of AI is the creation and understanding of intelligence. This requires a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. The concept of rational agency has long been considered a leading candidate to fulﬁll this role. This paper, which updates a much earlier version (Russell, 1997), reviews the sequence of conceptual shifts leading to a different candidate, bounded optimality, that is closer to our informal conception of intelligence and reduces the gap between theory and practice. Some promising recent developments are also described.","2016","2022-01-30 04:51:08","2022-01-30 04:51:08","2019-12-18 01:41:16","7-28","","","","","","Rationality and Intelligence","Synthese Library","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 40  DOI: 10.1007/978-3-319-26485-1_2","","/Users/jacquesthibodeau/Zotero/storage/MC8IBHE9/Russell - 2016 - Rationality and Intelligence A Brief Update.pdf","","CHAI; TechSafety","Bounded rationality; Intelligence; Metareasoning; Rationality","Müller, Vincent C.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9QDMNDKS","conferencePaper","2016","Halpern, Joseph Y.; Vilaca, Xavier","Rational Consensus","Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing","","","","http://arxiv.org/abs/2005.10141","We provide a game-theoretic analysis of consensus, assuming that processes are controlled by rational agents and may fail by crashing. We consider agents that \emph{care only about consensus}: that is, (a) an agent's utility depends only on the consensus value achieved (and not, for example, on the number of messages the agent sends) and (b) agents strictly prefer reaching consensus to not reaching consensus. We show that, under these assumptions, there is no \emph{ex post Nash Equilibrium}, even with only one failure. Roughly speaking, this means that there must always exist a \emph{failure pattern} (a description of who fails, when they fail, and which agents they do not send messages to in the round that they fail) and initial preferences for which an agent can gain by deviating. On the other hand, if we assume that there is a distribution $\pi$ on the failure patterns and initial preferences, then under minimal assumptions on $\pi$, there is a Nash equilibrium that tolerates $f$ failures (i.e., $\pi$ puts probability 1 on there being at most $f$ failures) if $f+1 < n$ (where $n$ is the total number of agents). Moreover, we show that a slight extension of the Nash equilibrium strategy is also a \emph{sequential} equilibrium (under the same assumptions about the distribution $\pi$).","2016","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-12-17 22:22:59","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000022  arXiv: 2005.10141","","/Users/jacquesthibodeau/Zotero/storage/5MCJDWZ2/Halpern and Vilaca - 2020 - Rational Consensus.pdf; /Users/jacquesthibodeau/Zotero/storage/MAM8FX3J/2005.html","","CHAI; TechSafety; AmbiguosSafety","Computer Science - Computer Science and Game Theory; Computer Science - Distributed, Parallel, and Cluster Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2016 ACM Symposium on Principles of Distributed Computing","","","","","","","","","","","","","","",""
"NFZZSNRF","conferencePaper","2020","Gleave, Adam; Dennis, Michael; Legg, Shane; Russell, Stuart; Leike, Jan","Quantifying Differences in Reward Functions","","","","","http://arxiv.org/abs/2006.13900","For many tasks, the reward function is too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by examining rollouts from a policy optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences, and the reinforcement learning algorithm failing to optimize the learned reward. Moreover, the rollout method is highly sensitive to details of the environment the learned reward is evaluated in, which often differ in the deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without training a policy. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be precisely approximated and is more robust than baselines to the choice of visitation distribution. Finally, we find that the EPIC distance of learned reward functions to the ground-truth reward is predictive of the success of training a policy, even in different transition dynamics.","2020-06-24","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-08-31 17:51:23","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000007  arXiv: 2006.13900","","/Users/jacquesthibodeau/Zotero/storage/QZKBAKKJ/Gleave et al. - 2020 - Quantifying Differences in Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/SRRD2J53/2006.html","","CHAI; TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2021","","","","","","","","","","","","","","",""
"7G9QKQZ8","blogPost","2021","Kumar, Ramana; Kokotajlo, Daniel","P₂B: Plan to P₂B Better","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/CAwwFpbteYBQw2Gkp/p-b-plan-to-p-b-better","tl;dr: Most good plans involve taking steps to make better plans. Making better plans is the convergent instrumental goal, of which all familiar convergent instrumental goals are an instance. This is key to understanding what agency is and why it is powerful. Planning means using a world model to predict the consequences of various courses of actions one could take, and taking actions that have good predicted consequences. (We think of this with the handle “doing things for reasons,” though we acknowledge this may be an idiosyncratic use of “reasons.”) We take “planning” to include things that are relevantly similar to this procedure, such as following a bag of heuristics that approximates it. We’re also including actually following the plans, in what might more clunkily be called “planning-acting.” Planning, in this broad sense, seems essential to the kind of goal-directed, consequential, agent-like intelligence that we expect to be highly impactful. This sequence explains why. ONE CONVERGENT INSTRUMENTAL GOAL TO RULE THEM ALL Consider the maxim “make there be more and/or better planning towards your goal.” This section argues that all the classic convergent instrumental goals are special cases of this maxim. To flesh this out a little, here are some categories of ways to follow the maxim. Remember that a planner is typically close (in terms of what it might affect via action) to at least one planner – itself – so these directions can typically be applied in the first case to the planner itself.  * Make the planners with your goal better at planning. For example, get them    new relevant data to work with¹, get them to run faster or more effective    algorithms, build protections against value drift, etc.  * Make the planners with your goal have better options. For example, move them    to better locations, get them more resources, get them more power or a    greater number of options to select from, have them take steps in an    object-level plan towards t","2021-10-24","2022-01-30 04:51:08","2022-01-30 04:51:08","2021-12-11 14:17:29","","","","","","","P₂B","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QRHZCPVT/p-b-plan-to-p-b-better.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R9GDT9M8","blogPost","2017","Oesterheld, Caspar","Naturalized induction – a challenge for evidential and causal decision theory","LessWrong","","","","https://www.lesswrong.com/posts/kgsaSbJqWLtJfiCcz/naturalized-induction-a-challenge-for-evidential-and-causal","As some of you may know, I disagree with many of the criticisms leveled against  evidential decision theory (EDT). Most notably, I believe that Smoking lesion-type problems don't refute EDT. I also don't think that EDT's non-updatelessness leaves a lot of room for disagreement, given that EDT  recommends immediate self-modification to updatelessness. However, I do believe there are some issues with run-of-the-mill EDT. One of them is naturalized induction. It is in fact not only a problem for EDT but also for causal decision theory (CDT) and most other decision theories that have been proposed in- and outside of academia. It does not affect logical decision theories, however. THE ROLE OF NATURALIZED INDUCTION IN DECISION THEORY Recall that EDT prescribes taking the action that maximizes expected utility, i.e. where is the set of available actions, is the agent's utility function, is a set of possible world models, represents the agent's past observations (which may include information the agent has collected about itself). CDT works in a – for the purpose of this article – similar way, except that instead of conditioning on in the usual way, it calculates some causal counterfactual, such as Pearl's do-calculus: . The problem of naturalized induction is that of assigning posterior probabilities to world models (or or whatever) when the agent is  naturalized, i.e., embedded into its environment. Consider the following example. Let's say there are 5 world models , each of which has equal prior probability. These world models may be cellular automata. Now, the agent makes the observation . It turns out that worlds and don't contain any agents at all, and contains no agent making the observation . The other two world models, on the other hand, are consistent with . Thus, for and  for . Let's assume that the agent has only two actions and that in world model  the only agent making observation takes action and in the only agent making observation takes action , then a","2017-09-22","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-11-23 00:47:30","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/PFXHQPWU/naturalized-induction-a-challenge-for-evidential-and-causal.html","","CLR; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GG3RHP5E","manuscript","2021","Stastny, Julian; Treutlein, Johannes; Riché, Maxime; Clifton, Jesse","Multi-agent learning in mixed-motive coordination problems","","","","","https://longtermrisk.org/files/stastny_et_al_implicit_bargaining.pdf","Cooperation in settings where agents have diﬀerent but overlapping preferences (mixed-motive settings) has recently received considerable attention in multi-agent learning. However, the mixed-motive environments typically studied are simplistic in that they have a single cooperative outcome on which all agents can agree. Multi-agent systems in general may exhibit many payoﬀ proﬁles which might be called cooperative, but which agents have diﬀerent preferences over. This causes problems for independently trained agents that do not arise in the case of that there is a unique cooperative payoﬀ proﬁle. In this note, we illustrate this problem with a class of games called mixed-motive coordination problems (MCPs). We demonstrate the failure of several methods for achieving cooperation in sequential social dilemmas when used to independently train policies in a simple MCP. We discuss some possible directions for ameliorating MCPs.","2021-03-08","2022-01-30 04:51:08","2022-01-30 04:51:08","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/K84ENAGC/Stastny et al. - 2021 - Multi-agent learning in mixed-motive coordination .pdf","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQHJJJJI","journalArticle","2019","Baum, Seth D.; Armstrong, Stuart; Ekenstedt, Timoteus; Häggström, Olle; Hanson, Robin; Kuhlemann, Karin; Maas, Matthijs M.; Miller, James D.; Salmela, Markus; Sandberg, Anders","Long-term trajectories of human civilization","Foresight","","","","","","2019","2022-01-30 04:51:08","2022-01-30 04:51:08","","53–83","","1","21","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000048","","/Users/jacquesthibodeau/Zotero/storage/EFQQZHCW/Baum et al. - 2019 - Long-term trajectories of human civilization.pdf; /Users/jacquesthibodeau/Zotero/storage/RB58JW9E/html.html; /Users/jacquesthibodeau/Zotero/storage/T9WH9T4G/html.html; /Users/jacquesthibodeau/Zotero/storage/HDZZWUJ2/html.html; /Users/jacquesthibodeau/Zotero/storage/2WQX97PA/Baum et al. - 2019 - Long-term trajectories of human civilization.pdf","","CLR; MetaSafety; FHI; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XKENFS2F","blogPost","2017","Oesterheld, Caspar","Is it a bias or just a preference? An interesting issue in preference idealization","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2017/01/18/is-it-a-bias-or-just-a-preference-an-interesting-issue-in-preference-idealization/","When taking others’ preferences into account, we will often want to idealize them rather than taking them too literally. Consider the following example. You hold a glass of transparent liquid…","2017-01-18","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-11-23 00:56:11","","","","","","","Is it a bias or just a preference?","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UQ3WP8PZ/is-it-a-bias-or-just-a-preference-an-interesting-issue-in-preference-idealization.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFFRRWRE","blogPost","2015","Tomasik, Brian","International Cooperation vs. AI Arms Race","Center on Long-Term Risk","","","","https://longtermrisk.org/international-cooperation-vs-ai-arms-race/","There's a decent chance that governments will be the first to build artificial general intelligence (AI). International hostility, especially an AI arms race, could exacerbate risk-taking, hostile motivations, and errors of judgment when creating AI. If so, then international cooperation could be an important factor to consider when evaluating the flow-through effects of charities.","2015-04-08","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-11-23 01:06:39","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/D464WPF3/international-cooperation-vs-ai-arms-race.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BUB3ZWDG","blogPost","2015","Tomasik, Brian","Gains from Trade through Compromise","Center on Long-Term Risk","","","","https://longtermrisk.org/gains-from-trade-through-compromise/","When agents of differing values compete, they may often find it mutually advantageous to compromise rather than continuing to engage in zero-sum conflicts. Potential ways of encouraging cooperation include promoting democracy, tolerance and (moral) trade. Because a future without compromise could be many times worse than a future with it, advancing compromise seems an important undertaking.","2015-04-10","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-11-23 20:08:18","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VHTBJW4Q/gains-from-trade-through-compromise.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RG8HBUB7","conferencePaper","2021","Stastny, Julian; Riché, Maxime; Lyzhov, Alexander; Treutlein, Johannes; Dafoe, Allan; Clifton, Jesse","Normative Disagreement as a Challenge for Cooperative AI","Cooperative AI workshop and the Strategic ML workshop at NeurIPS 2021","","","","http://arxiv.org/abs/2111.13872","Cooperation in settings where agents have both common and conflicting interests (mixed-motive environments) has recently received considerable attention in multi-agent learning. However, the mixed-motive environments typically studied have a single cooperative outcome on which all agents can agree. Many real-world multi-agent environments are instead bargaining problems (BPs): they have several Pareto-optimal payoff profiles over which agents have conflicting preferences. We argue that typical cooperation-inducing learning algorithms fail to cooperate in BPs when there is room for normative disagreement resulting in the existence of multiple competing cooperative equilibria, and illustrate this problem empirically. To remedy the issue, we introduce the notion of norm-adaptive policies. Norm-adaptive policies are capable of behaving according to different norms in different circumstances, creating opportunities for resolving normative disagreement. We develop a class of norm-adaptive policies and show in experiments that these significantly increase cooperation. However, norm-adaptiveness cannot address residual bargaining failure arising from a fundamental tradeoff between exploitability and cooperative robustness.","2021-11-27","2022-01-30 04:51:08","2022-01-30 04:51:08","2021-12-11 14:19:23","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2111.13872","","/Users/jacquesthibodeau/Zotero/storage/57K6T8XI/Stastny et al. - 2021 - Normative Disagreement as a Challenge for Cooperat.pdf","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2021","","","","","","","","","","","","","","",""
"D4HJK6A2","manuscript","2017","Oesterheld, Caspar","Multiverse-wide Cooperation via Correlated Decision Making","","","","","","Some decision theorists argue that when playing a prisoner’s dilemma-type game against a suﬃciently similar opponent, we should cooperate to make it more likely that our opponent also cooperates. This idea, which Hofstadter calls superrationality, has strong implications when combined with the insight from modern physics that we probably live in a large universe or multiverse of some sort. If we care about what happens in civilizations located elsewhere in the multiverse, we can superrationally cooperate with some of their inhabitants. That is, if we take their values into account, this makes it more likely that they do the same for us. In this paper, I attempt to assess the practical implications of this idea. I argue that to reap the full gains from trade, everyone should maximize the same impartially weighted sum of the utility functions of all collaborators. I also argue that we can obtain at least weak evidence about the content of these utility functions. In practice, the application of superrationality implies that we should promote causal cooperation, moral pluralism, moral reﬂection, and ensure that our descendants, who will be smarter and thus better at ﬁnding out how to beneﬁt other superrationalists in the universe, engage in superrational cooperation.","2017-08-10","2022-01-30 04:51:08","2022-01-30 04:51:08","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/WSAM3743/Oesterheld - Multiverse-wide Cooperation via Correlated Decisio.pdf","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R9625IHB","blogPost","2017","Gloor, Lukas","Multiverse-wide cooperation in a nutshell","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/7MdLurJGhGmqRv25c/multiverse-wide-cooperation-in-a-nutshell","(Crossposted from the FRI blog.)  This is a post I wrote about Caspar Oesterheld’s long paper Multiverse-wide cooperation via correlated decision-making. Because I have found the idea tricky to explain – which unfortunately makes it difficult to get feedback from others on whether the thinking behind it makes sense – I decided to write a shorter summary. While I am hoping that my text can serve as a standalone piece, for additional introductory content I also recommend reading the beginning of Caspar’s paper, or watching the short video introduction here (requires basic knowledge of the “CDT, EDT or something else” debate in decision theory). 0. ELEVATOR PITCH (Disclaimer: Especially for the elevator pitch section here, I am sacrificing accuracy and precision for brevity. References can be found in Caspar’s paper.)  It would be an uncanny coincidence if the observable universe made up everything that exists. The reason we cannot find any evidence for there being stuff beyond the edges of our universe is not because it is likely that there is nothingness, but because photons from further away simply would not have had sufficient time after the big bang to reach us. This means that the universe we find ourselves in may well be vastly larger than what we can observe, in fact even infinitely  larger. The theory of inflationary cosmology in addition hints at the existence of other universe bubbles with different fundamental constants forming or disappearing under certain conditions, somehow co-existing with our universe in parallel. The umbrella term multiverse captures the idea that the observable universe is just a tiny portion of everything that exists. The multiverse may contain myriads of worlds like ours, including other worlds with intelligent life and civilization. An infinite multiverse (of one sort or another) is actually amongst the most popular cosmological hypotheses, arguably even favored by the majority of experts.  Many ethical theories (in particular","2017-11-02","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-11-23 00:46:11","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/65NR427Z/multiverse-wide-cooperation-in-a-nutshell.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HCKKDM49","blogPost","2018","Oesterheld, Caspar","Moral realism and AI alignment","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2018/08/06/moral-realism-and-ai-alignment/","“Abstract”: Some have claimed that moral realism – roughly, the claim that moral claims can be true or false – would, if true, have implications for AI alignment research, such that moral realists …","2018-08-06","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-11-23 00:38:57","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/R98Z34VR/moral-realism-and-ai-alignment.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8M48CF7J","manuscript","2020","Hutter, Adrian","Learning in two-player games between transparent opponents","","","","","http://arxiv.org/abs/2012.02671","We consider a scenario in which two reinforcement learning agents repeatedly play a matrix game against each other and update their parameters after each round. The agents' decision-making is transparent to each other, which allows each agent to predict how their opponent will play against them. To prevent an infinite regress of both agents recursively predicting each other indefinitely, each agent is required to give an opponent-independent response with some probability at least epsilon. Transparency also allows each agent to anticipate and shape the other agent's gradient step, i.e. to move to regions of parameter space in which the opponent's gradient points in a direction favourable to them. We study the resulting dynamics experimentally, using two algorithms from previous literature (LOLA and SOS) for opponent-aware learning. We find that the combination of mutually transparent decision-making and opponent-aware learning robustly leads to mutual cooperation in a single-shot prisoner's dilemma. In a game of chicken, in which both agents try to manoeuvre their opponent towards their preferred equilibrium, converging to a mutually beneficial outcome turns out to be much harder, and opponent-aware learning can even lead to worst-case outcomes for both agents. This highlights the need to develop opponent-aware learning algorithms that achieve acceptable outcomes in social dilemmas involving an equilibrium selection problem.","2020-12-04","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-12-12 15:01:23","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2012.02671","","/Users/jacquesthibodeau/Zotero/storage/EFFXSWJA/Hutter - 2020 - Learning in two-player games between transparent o.pdf; /Users/jacquesthibodeau/Zotero/storage/U9M27NJU/2012.html","","CLR; TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9EK5VIJC","blogPost","2015","Tomasik, Brian","How Would Catastrophic Risks Affect Prospects for Compromise?","Center on Long-Term Risk","","","","https://longtermrisk.org/how-would-catastrophic-risks-affect-prospects-for-compromise/","Global catastrophic risks – such as biotech disasters or nuclear war – would cause major damage in the short run, but their effects on the long-run trajectory that humanity takes are also significant. In particular, to the extent these disasters increase risks of war, they seem likely to precipitate AI arms races between nations and worsen prospects for compromise.","2015-08-29","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-11-23 01:11:09","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4P4VEEUM","manuscript","2016","Tomasik, Brian","How the Simulation Argument Dampens Future Fanaticism","","","","","","Some eﬀective altruists assume that most of the expected impact of our actions comes from how we inﬂuence the very long-term future of Earthoriginating intelligence over the coming ∼billions of years. According to this view, helping humans and animals in the short term matters, but it mainly only matters via eﬀects on far-future outcomes.","2016","2022-01-30 04:51:08","2022-01-30 04:51:08","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/CEX9FT56/Tomasik - How the Simulation Argument Dampens Future Fanatic.pdf","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24MAX7RT","blogPost","2020","Kokotajlo, Daniel","How Roodman's GWP model translates to TAI timelines","LessWrong","","","","https://www.lesswrong.com/posts/L23FgmpjsTebqcSZb/how-roodman-s-gwp-model-translates-to-tai-timelines","How does David Roodman’s world GDP model translate to TAI timelines? Now, before I go any further, let me be the first to say that I don’t think we should use this model to predict TAI. This model takes a very broad outside view and is thus inferior to models like Ajeya Cotra’s which make use of more relevant information. (However, it is still useful for rebutting claims that TAI is unprecedented, inconsistent with historical trends, low-prior, etc.) Nevertheless, out of curiosity I thought I’d calculate what the model implies for TAI timelines. Here is the projection made by Roodman’s model. The red line is real historic GWP data; the splay of grey shades that continues it is the splay of possible futures calculated by the model. The median trajectory is the black line. I messed around with a ruler to make some rough calculations, marking up the image with blue lines as I went. The big blue line indicates the point on the median trajectory where GWP is 10x what is was in 2019. Eyeballing it, it looks like it happens around 2040, give or take a year. The small vertical blue line indicates the year 2037. The small horizontal blue line indicates GWP in 2037 on the median trajectory. Thus, it seems that between 2037 and 2040 on the median trajectory, GWP doubles. (One-ninth the distance between 1,000 and 1,000,000 is crossed, which is one-third of an order of magnitude, which is about one doubling). This means that TAI happens around 2037 on the median trajectory according to this model, at least according to Ajeya Cotra’s definition of transformative AI  as “software which causes a tenfold acceleration in the rate of growth of the world economy (assuming that it is used everywhere that it would be economically profitable to use it)... This means that if TAI is developed in year Y, the entire world economy would more than double by year Y + 4.” What about the non-median trajectories? Each shade of grey represents 5 percent of the simulated future trajectories, so","2020","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-12-12 15:02:13","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/G7VN452V/how-roodman-s-gwp-model-translates-to-tai-timelines.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9H4IQEWN","journalArticle","2017","Sotala, Kaj","How feasible is the rapid development of artificial superintelligence?","Physica Scripta","","1402-4896","10.1088/1402-4896/aa90e8","https://doi.org/10.1088%2F1402-4896%2Faa90e8","What kinds of fundamental limits are there in how capable artificial intelligence (AI) systems might become? Two questions in particular are of interest: (1) How much more capable could AI become relative to humans, and (2) how easily could superhuman capability be acquired? To answer these questions, we will consider the literature on human expertise and intelligence, discuss its relevance for AI, and consider how AI could improve on humans in two major aspects of thought and expertise, namely simulation and pattern recognition. We find that although there are very real limits to prediction, it seems like AI could still substantially improve on human intelligence.","2017-10","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-11-23 00:17:29","113001","","11","92","","Phys. Scr.","","","","","","","","en","","","","","Institute of Physics","","ZSCC: 0000016  Publisher: IOP Publishing","","/Users/jacquesthibodeau/Zotero/storage/EUNUQQPC/Sotala - 2017 - How feasible is the rapid development of artificia.pdf","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JZ2JGFE","blogPost","2018","Oesterheld, Caspar","Goertzel’s GOLEM implements evidential decision theory applied to policy choice","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2018/04/26/goertzels-golem-implements-evidential-decision-theory-applied-to-policy-choice/","I’ve written about the question of which decision theories describe the behavior of approaches to AI like the “Law of Effect”. In this post, I would like to discuss GOLEM, an arch…","2018-04-26","2022-01-30 04:51:08","2022-01-30 04:51:08","2020-11-23 00:41:22","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/GK7JQRH4/goertzels-golem-implements-evidential-decision-theory-applied-to-policy-choice.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79KF8UHI","journalArticle","1995","Russell, S. J.; Subramanian, D.","Provably Bounded-Optimal Agents","Journal of Artificial Intelligence Research","","1076-9757","10.1613/jair.133","https://www.jair.org/index.php/jair/article/view/10134","Since its inception, artificial intelligence has relied  upon a theoretical foundation centered around  perfect rationality  as   the desired property of intelligent systems. We argue, as others have   done, that this foundation is inadequate because it imposes   fundamentally unsatisfiable requirements. As a result, there has   arisen a wide gap between theory and practice in AI, hindering   progress in the field. We propose instead a property called  bounded   optimality. Roughly speaking, an agent is bounded-optimal if its   program is a solution to the constrained optimization problem   presented by its architecture and the task environment. We show how to   construct agents with this property for a simple class of machine   architectures in a broad class of real-time environments. We   illustrate these results using a simple model of an automated mail   sorting facility.  We also define a weaker property,  asymptotic   bounded optimality (ABO), that generalizes the notion of optimality in   classical complexity theory.  We then construct  universal  ABO   programs, i.e., programs that are ABO no matter what real-time   constraints are applied.  Universal ABO programs can be used as   building blocks for more complex systems. We conclude with a   discussion of the prospects for bounded optimality as a theoretical   basis for AI, and relate it to similar trends in philosophy,   economics, and game theory.","1995-05-01","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-22 02:23:36","575-609","","","2","","jair","","","","","","","","","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 437","","/Users/jacquesthibodeau/Zotero/storage/E5SU8UNI/Russell and Subramanian - 1995 - Provably Bounded-Optimal Agents.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7IV657SA","manuscript","2019","Plonsky, Ori; Apel, Reut; Ert, Eyal; Tennenholtz, Moshe; Bourgin, David; Peterson, Joshua C.; Reichman, Daniel; Griffiths, Thomas L.; Russell, Stuart J.; Carter, Evan C.; Cavanagh, James F.; Erev, Ido","Predicting human decisions with behavioral theories and machine learning","","","","","http://arxiv.org/abs/1904.06866","Behavioral decision theories aim to explain human behavior. Can they help predict it? An open tournament for prediction of human choices in fundamental economic decision tasks is presented. The results suggest that integration of certain behavioral theories as features in machine learning systems provides the best predictions. Surprisingly, the most useful theories for prediction build on basic properties of human and animal learning and are very different from mainstream decision theories that focus on deviations from rational choice. Moreover, we find that theoretical features should be based not only on qualitative behavioral insights (e.g. loss aversion), but also on quantitative behavioral foresights generated by functional descriptive models (e.g. Prospect Theory). Our analysis prescribes a recipe for derivation of explainable, useful predictions of human decisions.","2019-04-15","2022-01-30 04:51:07","2022-01-30 04:51:07","2019-12-18 02:16:33","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 34  J: 10 arXiv: 1904.06866","","/Users/jacquesthibodeau/Zotero/storage/ZJE7ABQH/Plonsky et al. - 2019 - Predicting human decisions with behavioral theorie.pdf; /Users/jacquesthibodeau/Zotero/storage/BFKM6MGZ/1904.html","","CHAI; TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NCWIBN2Q","journalArticle","2018","Sadigh, Dorsa; Landolfi, Nick; Sastry, Shankar S.; Seshia, Sanjit A.; Dragan, Anca D.","Planning for cars that coordinate with people: leveraging effects on human actions for planning and active information gathering over human internal state","Autonomous Robots","","0929-5593, 1573-7527","10.1007/s10514-018-9746-1","http://link.springer.com/10.1007/s10514-018-9746-1","Traditionally, autonomous cars treat human-driven vehicles like moving obstacles. They predict their future trajectories and plan to stay out of their way. While physically safe, this results in defensive and opaque behaviors. In reality, an autonomous car’s actions will actually affect what other cars will do in response, creating an opportunity for coordination. Our thesis is that we can leverage these responses to plan more efﬁcient and communicative behaviors. We introduce a formulation of interaction with human-driven vehicles as an underactuated dynamical system, in which the robot’s actions have consequences on the state of the autonomous car, but also on the human actions and thus the state of the human-driven car. We model these consequences by approximating the human’s actions as (noisily) optimal with respect to some utility function. The robot uses the human actions as observations of her underlying utility function parameters. We ﬁrst explore learning these parameters ofﬂine, and show that a robot planning in the resulting underactuated system is more efﬁcient than when treating the person as a moving obstacle. We also show that the robot can target speciﬁc desired effects, like getting the person to switch lanes or to proceed ﬁrst through an intersection. We then explore estimating these parameters online, and enable the robot to perform active information gathering: generating actions that purposefully probe the human in order to clarify their underlying utility parameters, like driving style or attention level. We show that this signiﬁcantly outperforms passive estimation and improves efﬁciency. Planning in our model results in coordination behaviors: the robot inches forward at an intersection to see if can go through, or it reverses to make the other car proceed ﬁrst. These behaviors result from the optimization, without relying on hand-coded signaling strategies. Our user studies support the utility of our model when interacting with real users.","2018-10","2022-01-30 04:51:07","2022-01-30 04:51:07","2019-12-18 02:39:40","1405-1426","","7","42","","Auton Robot","Planning for cars that coordinate with people","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000086","","/Users/jacquesthibodeau/Zotero/storage/IZXDWBKE/Sadigh et al. - 2018 - Planning for cars that coordinate with people lev.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FCTCJRGU","conferencePaper","2018","Fisac, Jaime F.; Bajcsy, Andrea; Herbert, Sylvia L.; Fridovich-Keil, David; Wang, Steven; Tomlin, Claire J.; Dragan, Anca D.","Probabilistically Safe Robot Planning with Confidence-Based Human Predictions","arXiv:1806.00109 [cs]","","","","https://arxiv.org/abs/1806.00109v1","In order to safely operate around humans, robots can employ predictive models of human motion. Unfortunately, these models cannot capture the full complexity of human behavior and necessarily introduce simplifying assumptions. As a result, predictions may degrade whenever the observed human behavior departs from the assumed structure, which can have negative implications for safety. In this paper, we observe that how ""rational"" human actions appear under a particular model can be viewed as an indicator of that model's ability to describe the human's current motion. By reasoning about this model confidence in a real-time Bayesian framework, we show that the robot can very quickly modulate its predictions to become more uncertain when the model performs poorly. Building on recent work in provably-safe trajectory planning, we leverage these confidence-aware human motion predictions to generate assured autonomous robot motion. Our new analysis combines worst-case tracking error guarantees for the physical robot with probabilistic time-varying human predictions, yielding a quantitative, probabilistic safety certificate. We demonstrate our approach with a quadcopter navigating around a human.","2018-05-31","2022-01-30 04:51:07","2022-01-30 04:51:07","2019-12-18 01:36:19","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000076","","/Users/jacquesthibodeau/Zotero/storage/5ESRXS33/Fisac et al. - 2018 - Probabilistically Safe Robot Planning with Confide.pdf; /Users/jacquesthibodeau/Zotero/storage/ATI9G9AD/1806.html; /Users/jacquesthibodeau/Zotero/storage/EFE4XMVX/Fisac et al. - 2018 - Probabilistically Safe Robot Planning with Confide.pdf; /Users/jacquesthibodeau/Zotero/storage/4UZA833Q/1806.html","","CHAI; TechSafety","Computer Science - Machine Learning; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Robotics: Science and Systems 2018","","","","","","","","","","","","","","",""
"NXDBDRKX","manuscript","2020","Zhan, Albert; Tiomkin, Stas; Abbeel, Pieter","Preventing Imitation Learning with Adversarial Policy Ensembles","","","","","http://arxiv.org/abs/2002.01059","Imitation learning can reproduce policies by observing experts, which poses a problem regarding policy privacy. Policies, such as human, or policies on deployed robots, can all be cloned without consent from the owners. How can we protect against external observers cloning our proprietary policies? To answer this question we introduce a new reinforcement learning framework, where we train an ensemble of near-optimal policies, whose demonstrations are guaranteed to be useless for an external observer. We formulate this idea by a constrained optimization problem, where the objective is to improve proprietary policies, and at the same time deteriorate the virtual policy of an eventual external observer. We design a tractable algorithm to solve this new optimization problem by modifying the standard policy gradient algorithm. Our formulation can be interpreted in lenses of confidentiality and adversarial behaviour, which enables a broader perspective of this work. We demonstrate the existence of ""non-clonable"" ensembles, providing a solution to the above optimization problem, which is calculated by our modified policy gradient algorithm. To our knowledge, this is the first work regarding the protection of policies in Reinforcement Learning.","2020-08-02","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-21 18:40:40","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000004  arXiv: 2002.01059","","/Users/jacquesthibodeau/Zotero/storage/EUFJR46P/Zhan et al. - 2020 - Preventing Imitation Learning with Adversarial Pol.pdf; /Users/jacquesthibodeau/Zotero/storage/THDFQB7N/Zhan et al. - 2020 - Preventing Imitation Learning with Adversarial Pol.pdf; /Users/jacquesthibodeau/Zotero/storage/6WBXIJZN/2002.html; /Users/jacquesthibodeau/Zotero/storage/RR6ER6D9/2002.html","","CHAI; TechSafety; AmbiguosSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PEQIA7QQ","conferencePaper","2019","Shah, Rohin; Krasheninnikov, Dmitrii; Alexander, Jordan; Abbeel, Pieter; Dragan, Anca","Preferences Implicit in the State of the World","","","","","http://arxiv.org/abs/1902.04198","Reinforcement learning (RL) agents optimize only the features speciﬁed in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisﬁed in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to ﬁll in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We ﬁnd that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.","2019-02-11","2022-01-30 04:51:07","2022-01-30 04:51:07","2019-07-11 18:35:33","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000031  arXiv: 1902.04198","","/Users/jacquesthibodeau/Zotero/storage/W5WZD9IE/Shah et al. - 2019 - Preferences Implicit in the State of the World.pdf; /Users/jacquesthibodeau/Zotero/storage/H6E44KI8/Shah et al. - 2019 - Preferences Implicit in the State of the World.pdf; /Users/jacquesthibodeau/Zotero/storage/6ZGSF3I5/1902.html; /Users/jacquesthibodeau/Zotero/storage/WQ6W7MGD/1902.html","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2019","","","","","","","","","","","","","","",""
"QIVPQ6DS","blogPost","2018","Shah, Rohin","Preface to the sequence on value learning","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/oH8KMnXHnw964QyS6/preface-to-the-sequence-on-value-learning","This is a meta-post about the upcoming sequence on Value Learning that will start to be published this Thursday. This preface will also be revised significantly once the second half of the sequence is fully written. PURPOSE OF THE SEQUENCE The first part of this sequence will be about the tractability of ambitious value learning, which is the idea of inferring a utility function for an AI system to optimize based on observing human behavior. After a short break, we will (hopefully) continue with the second part, which will be about why we might want to think about techniques that infer human preferences, even if we assume we won’t do ambitious value learning with such techniques. The aim of this part of the sequence is to gather the current best public writings on the topic, and provide a unifying narrative that ties them into a cohesive whole. This makes the key ideas more discoverable and discussable, and provides a quick reference for existing researchers. It is meant to teach the ideas surrounding one specific approach to aligning advanced AI systems. We’ll explore the specification problem, in which we would like to define the behavior we want to see from an AI system. Ambitious value learning is one potential avenue of attack on the specification problem, that assumes a particular model of an AI system (maximizing expected utility) and a particular source of data (human behavior). We will then delve into conceptual work on ambitious value learning that has revealed obstructions to this approach. There will be pointers to current research that aims to circumvent these obstructions. The second part of this sequence is currently being assembled, and this preface will be updated with details once it is ready. The first half of this sequence takes you near the cutting edge of conceptual  work on the ambitious value learning problem, with some pointers to work being done at this frontier. Based on the arguments in the sequence, I am confident that the obvious f","2018","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-12-17 04:36:13","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/2HGSWMFH/oH8KMnXHnw964QyS6.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZAMXRUV6","conferencePaper","2020","Fisac, Jaime F.; Gates, Monica A.; Hamrick, Jessica B.; Liu, Chang; Hadfield-Menell, Dylan; Palaniappan, Malayandi; Malik, Dhruv; Sastry, S. Shankar; Griffiths, Thomas L.; Dragan, Anca D.","Pragmatic-Pedagogic Value Alignment","Robotics Research","978-3-030-28618-7 978-3-030-28619-4","","10.1007/978-3-030-28619-4_7","http://link.springer.com/10.1007/978-3-030-28619-4_7","As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workﬂows, successfully inferring and adapting to their users’ objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people’s natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human’s actions pragmatically. To our knowledge, this work constitutes the ﬁrst formal analysis of value alignment grounded in empirically validated cognitive models.","2020","2022-01-30 04:51:07","2022-01-30 04:51:07","2019-12-18 01:15:26","49-57","","","10","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 47","","/Users/jacquesthibodeau/Zotero/storage/ZADVEM2M/Fisac et al. - 2018 - Pragmatic-Pedagogic Value Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/IJB43GBG/1707.html; /Users/jacquesthibodeau/Zotero/storage/NAHBPPDH/1707.html","","CHAI; TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; I.2.0; I.2.6; Computer Science - Human-Computer Interaction; 68T05; I.2.8; I.2.9","Amato, Nancy M.; Hager, Greg; Thomas, Shawna; Torres-Torriti, Miguel","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XWGR86QB","blogPost","2021","Kokotajlo, Daniel","Fun with +12 OOMs of Compute","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute","OR: BIG TIMELINES CRUX OPERATIONALIZED What fun things could one build with +12 orders of magnitude of compute? By ‘fun’ I mean ‘powerful.’ This hypothetical is highly relevant to AI timelines, for reasons I’ll explain later. Summary (Spoilers): I describe a hypothetical scenario that concretizes the question “what could be built with 2020’s algorithms/ideas/etc. but a trillion times more compute?”Then I give some answers to that question. Then I ask: How likely is it that some sort of TAI would happen in this scenario? This second question is a useful operationalization of the (IMO) most important, most-commonly-discussed timelines crux: “Can we get TAI just by throwing more compute at the problem?” I consider this operationalization to be the main contribution of this post; it directly plugs into Ajeya’s timelines model and is quantitatively more cruxy than anything else I know of. The secondary contribution of this post is my set of answers to the first question: They serve as intuition pumps for my answer to the second, which strongly supports my views on timelines. THE HYPOTHETICAL In 2016 the Compute Fairy visits Earth and bestows a blessing: Computers are magically 12 orders of magnitude faster! Over the next five years, what happens? The Deep Learning AI Boom still happens, only much crazier: Instead of making AlphaStar for 10^23 floating point operations, DeepMind makes something for 10^35. Instead of making GPT-3 for 10^23 FLOPs, OpenAI makes something for 10^35. Instead of industry and academia making a cornucopia of things for 10^20 FLOPs or so, they make a cornucopia of things for 10^32 FLOPs or so. When random grad students and hackers spin up neural nets on their laptops, they have a trillion times more compute to work with. [EDIT: Also assume magic +12 OOMs of memory, bandwidth, etc. All the ingredients of compute.] For context on how big a deal +12 OOMs is, consider the graph below, from ARK. It’s measuring petaflop-days, which are about 10^20 F","2021-03-01","2022-01-30 04:51:07","2022-01-30 04:51:07","2021-12-11 14:12:30","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WWZQP7ZC/fun-with-12-ooms-of-compute.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7BCH8MC8","blogPost","2015","Tomasik, Brian","Flavors of Computation Are Flavors of Consciousness","Center on Long-Term Risk","","","","https://longtermrisk.org/flavors-of-computation-are-flavors-of-consciousness/","If we don't understand why we're conscious, how come we're so sure that extremely simple minds are not? I propose to think of consciousness as intrinsic to computation, although different types of computation may have very different types of consciousness – some so alien that we can't imagine them. Since all physical processes are computations, […]","2015-04-10","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-23 20:03:32","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","CLR; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PCHKUWWW","conferencePaper","2019","Oesterheld, Caspar; Conitzer, Vincent","Extracting Money from Causal Decision Theorists","Proceedings of the Workshop on Artificial Intelligence Safety 2020","","","","http://ceur-ws.org/Vol-2640/paper_21.pdf","Newcomb’s problem has spawned a debate about which variant of expected utility maximization (if any) should guide rational choice. In this paper, we provide a new argument against what is probably the most popular variant: causal decision theory (CDT). In particular, we provide two scenarios in which CDT voluntarily loses money. In the ﬁrst, an agent faces a single choice and following CDT’s recommendation yields a loss of money in expectation. The second scenario extends the ﬁrst to a diachronic Dutch book against CDT.","2019-08-30","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-12-18","19","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000[s0]","","/Users/jacquesthibodeau/Zotero/storage/ANCD7C23/Oesterheld and Conitzer - Extracting Money from Causal Decision Theorists.pdf","","CLR; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"93Q953KS","blogPost","2020","Clifton, Jesse","Equilibrium and prior selection problems in multipolar deployment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/Tdu3tGT4i24qcLESh/equilibrium-and-prior-selection-problems-in-multipolar-1","To avoid catastrophic conflict in multipolar AI scenarios, we would like to design AI systems such that AI-enabled actors will tend to cooperate. This post is about some problems facing this effort and some possible solutions. To explain these problems, I'll take the view that the agents deployed by AI developers (the ''principals'') in a multipolar scenario are moves in a game. The payoffs to a principal in this game depend on how the agents behave over time. We can talk about the equilibria of this game, and so on. Ideally, we would be able to make guarantees like this:  1. The payoffs resulting from the deployed agents' actions are optimal with     respect to some appropriate ""welfare function''. This welfare function would     encode some combination of total utility, fairness, and other social     desiderata;  2. The agents are in equilibrium --- that is, no principal has an incentive to     deploy an agent with a different design, given the agents deployed by the     other principals. The motivation for item 1 is clear: we want outcomes which are fair by each of the principals' lights. In particular, we want an outcome that the principals will all agree to. And item 2 is desirable because an equilibrium constitutes a self-enforcing contract; each agent wants to play their equilibrium strategy, if they believe that the other agents are playing the same equilibrium. Thus, given that the principals all say that they will deploy agents that satisfy 1 and 2, we could have some confidence that a welfare-optimal outcome will in fact obtain. Two simple but critical problems need to be addressed in order to make such guarantees: the equilibrium and prior selection problems. The equilibrium selection problem is that this deployment game will have many equilibria. Even if the principals agree on a welfare function, it is possible that many different profiles of agents optimize the same welfare function. So the principals need to coordinate on the profile of agents dep","2020","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-12-12 14:54:27","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/HV7EIWIV/equilibrium-and-prior-selection-problems-in-multipolar-1.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8QQEAZVI","blogPost","2018","Althaus, David","Descriptive Population Ethics and Its Relevance for Cause Prioritization","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/CmNBmSf6xtMyYhvcs/descriptive-population-ethics-and-its-relevance-for-cause","SUMMARY Descriptive ethics is the empirical study of people's values and ethical views, e.g. via a survey or questionnaire. This overview focuses on beliefs about population ethics and exchange rates between goods (e.g. happiness) and bads  (e.g. suffering). Two variables seem particularly important and action-guiding in this context, especially when trying to make informed choices about how to best shape the long-term future: 1) One’s normative goods-to-bads ratio  (N-ratio) and 2) one’s expected bads-to-goods ratio (E-ratio). I elaborate on how a framework consisting of these two variables could inform our decision-making with respect to shaping the long-term future, as well as facilitate cooperation among differing value systems and further moral reflection. I then present concrete ideas for further research in this area and investigate associated challenges. The last section lists resources which discuss further methodological and theoretical issues which were beyond the scope of the present text. DESCRIPTIVE ETHICS AND LONG-TERM FUTURE PRIORITIZATION Recently, some debate has emerged on whether reducing extinction risk is the ideal course of action for shaping the long-term future. For instance, in the  Global Priorities Institute (GPI) research agenda, Greaves & MacAskill (2017, p.13) ask “[...] whether it might be more important to ensure that future civilisation is good, assuming we don’t go extinct, than to ensure that future civilisation happens at all.” We could further ask to what extent we should focus our efforts on reducingrisks of astronomical suffering (s-risks). Again, Greaves & MacAskill: “Should we be more concerned about avoiding the worst possible outcomes for the future than we are for ensuring the very best outcomes occur [...]?” Given the enormous stakes, these are arguably some of the most important questions facing those who prioritize shaping the long-term future.1 Some interventions increase both the quality of future civilization as w","2018-04-03","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-23 00:40:27","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/K9FMSZDC/descriptive-population-ethics-and-its-relevance-for-cause.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DC33WP3T","blogPost","2017","Oesterheld, Caspar","Decision Theory and the Irrelevance of Impossible Outcomes","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2017/01/17/decision-theory-and-the-irrelevance-of-impossible-outcomes/","(This post assumes some knowledge of the decision theory of Newcomb-like scenarios.) One problem in the decision theory of Newcomb-like scenarios (i.e. the study of whether causal, evidential or so…","2017-01-17","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-23 00:56:44","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/F7GPMPMH/decision-theory-and-the-irrelevance-of-impossible-outcomes.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MTT7H2E","blogPost","2021","Torges, Stefan","Coordination challenges for preventing AI conflict","Center on Long-Term Risk","","","","https://longtermrisk.org/coordination-challenges-for-preventing-ai-conflict/","Summary In this article, I will sketch arguments for the following claims: Transformative AI scenarios involving multiple systems pose a unique existential risk: catastrophic bargaining failure between multiple AI systems (or joint AI-human systems). This risk is not sufficiently addressed by successfully aligning those systems, and we cannot safely delegate its solution to the AI systems themselves. Developers are better positioned than more far-sighted successor agents to coordinate in a way that solves this problem, but a solution also does not seem guaranteed. Developers intent on solving this problem can choose between developing separate but compatible systems that do not engage in costly conflict or building a single joint system. While the second option seems preferable from an altruistic perspective, […]","2021-03-09","2022-01-30 04:51:07","2022-01-30 04:51:07","2021-12-11 14:20:34","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/9645PS8Z/coordination-challenges-for-preventing-ai-conflict.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3JEMBWP9","report","2020","Clifton, Jesse","Cooperation, Conflict, and Transformative Artificial Intelligence - A Research Agenda","","","","","https://longtermrisk.org/files/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf","","2020-03","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-22 07:42:34","","","","","","","","","","","","Center on Long-Term Risk","","","","","","","","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/8FM6BFXK/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I4AA6S2X","blogPost","2017","Oesterheld, Caspar","Complications in evaluating neglectedness","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/","Neglectedness (or crowdedness) is a heuristic that effective altruists use to assess how much impact they could have in a specific cause area. It is usually combined with scale (a.k.a. importance) …","2017-06-25","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-23 00:51:16","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/737I3Z2G/complications-in-evaluating-neglectedness.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CDIWGCQQ","blogPost","2021","Clifton, Jesse","CLR's recent work on multi-agent systems","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/EzoCZjTdWTMgacKGS/clr-s-recent-work-on-multi-agent-systems","INTRODUCTION We at the Center on Long-Term Risk (CLR) are focused on reducing risks of cosmically significant amounts of suffering[1], or s-risks, from transformative artificial intelligence (TAI). We currently believe that:  * Several agential s-risks — in which agents deliberately cause great amounts    of suffering — are among the largest s-risks in expectation.  * One of the most promising ways of addressing these risks involves intervening    in the design of TAI systems to make them more cooperative.  * Increasing the cooperativeness of powerful systems is robustly valuable for    everybody interested in shaping the long-run future. (See the recent     Cooperative AI and AI Research Considerations for Existential Safety (ARCHES)     research agendas for examples of work motivated by considerations other than    s-risks.) This was the subject of our research agenda on cooperative, conflict, and TAI. In this post, I’d like to give an overview of some of the research that CLR has been doing on multi-agent systems. I’ll then briefly remark on the importance, tractability, and neglectedness of this work (both from a downside-focused and more mainstream longtermist perspective), though a thorough discussion of prioritization is beyond the scope of this post. The main goal is to inform the community about what we’ve been up to recently in this space. We’re interested in supporting people who want to contribute to this work. If you might benefit from financial support to work on topics related to the research described here, you can fill out this form. If you’re interested in working with us as a temporary summer research fellow, full-time researcher, or research assistant, apply here. POTENTIAL CAUSES OF CONFLICT BETWEEN INTELLIGENT ACTORS It is possible that TAI systems will find themselves in conflict, despite the fact that conflict is often seemingly Pareto-inefficient. Factors that might lead intelligent agents to become engaged in conflict include:  1. Unce","2021-03-08","2022-01-30 04:51:07","2022-01-30 04:51:07","2021-11-14 18:22:51","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/5WVXDMTD/clr-s-recent-work-on-multi-agent-systems.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7T52V2ZJ","journalArticle","2016","Oesterheld, Caspar","Formalizing preference utilitarianism in physical world models","Synthese","","1573-0964","10.1007/s11229-015-0883-1","https://doi.org/10.1007/s11229-015-0883-1","Most ethical work is done at a low level of formality. This makes practical moral questions inaccessible to formal and natural sciences and can lead to misunderstandings in ethical discussion. In this paper, we use Bayesian inference to introduce a formalization of preference utilitarianism in physical world models, specifically cellular automata. Even though our formalization is not immediately applicable, it is a first step in providing ethics and ultimately the question of how to “make the world better” with a formal basis.","2016-09-01","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-23 00:22:50","2747-2759","","9","193","","Synthese","","","","","","","","en","","","","","Springer Link","","ZSCC: 0000010","","/Users/jacquesthibodeau/Zotero/storage/WH4KR7BA/Oesterheld - 2016 - Formalizing preference utilitarianism in physical .pdf","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NWMXW7B8","blogPost","2016","Tomasik, Brian","Do Artificial Reinforcement-Learning Agents Matter Morally?","Center on Long-Term Risk","","","","https://longtermrisk.org/do-artificial-reinforcement-learning-agents-matter-morally/","Artificial reinforcement learning (RL), a widely used training method in computer science, has striking parallels to reward and punishment learning in biological brains. Plausible theories of consciousness imply a non-zero probability that RL agents qualify as sentient and deserve our moral consideration, especially as AI research advances and RL agents become more sophisticated.","2016-07-28","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-23 01:03:14","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/KD663JCF/do-artificial-reinforcement-learning-agents-matter-morally.html","","CLR; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"639338X6","blogPost","2015","Tomasik, Brian","Differential Intellectual Progress as a Positive-Sum Project","Center on Long-Term Risk","","","","https://longtermrisk.org/differential-intellectual-progress-as-a-positive-sum-project/","Fast technological development carries a risk of creating extremely powerful tools, especially AI, before society has a chance to figure out how best to use those tools in positive ways for many value systems. Suffering reducers may want to help mitigate the arms race for AI so that AI developers take fewer risks and have […]","2015-08-29","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-23 01:07:52","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WT7HXT5J/differential-intellectual-progress-as-a-positive-sum-project.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQ7VHBIZ","blogPost","2017","Treutlein, Johannes","Did EDT get it right all along? Introducing yet another medical Newcomb problem","LessWrong","","","","https://www.lesswrong.com/posts/iqpizeN4hkbTjkugo/did-edt-get-it-right-all-along-introducing-yet-another","One of the main arguments given against Evidential Decision Theory (EDT) is that it would “one-box” in medical Newcomb problems. Whether this is the winning action has been a hotly debated issue on LessWrong. A majority, including experts in the area such as Eliezer Yudkowsky and Wei Dai, seem to think that one should two-box (See e.g. Yudkowsky 2010, p.67). Others have tried to argue  in favor of EDT by claiming that the winning action would be to one-box, or by offering reasons why EDT would in some cases two-box after all. In this blog post, I want to argue that EDT gets it right: one-boxing is the correct action in medical Newcomb problems. I introduce a new thought experiment, the Coin Flip Creation problem, in which I believe the winning move is to one-box. This new problem is structurally similar to other medical Newcomb problems such as the  Smoking Lesion, though it might elicit the intuition to one-box even in people who would two-box in some of the other problems. I discuss both how EDT and other decision theories would reason in the problem and why people’s intuitions might diverge in different formulations of medical Newcomb problems. TWO KINDS OF NEWCOMBLIKE PROBLEMS There are two different kinds of Newcomblike problems. In Newcomb’s original paradox, both EDT and Logical Decision Theories (LDT), such as Timeless Decision Theory (TDT) would one-box and therefore, unlike CDT, win $1 million. In medical Newcomb problems, EDT’s and LDT’s decisions diverge. This is because in the latter, a (physical) causal node that isn’t itself a decision algorithm influences both the current world state and our decisions – resulting in a correlation between action and environment but, unlike the original Newcomb, no “logical” causation. It’s often unclear exactly how a causal node can exert influence on our decisions. Does it change our decision theory, utility function, or the information available to us? In the case of the Smoking Lesion problem, it seems plausible","2017-01-24","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-23 00:55:10","","","","","","","Did EDT get it right all along?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/EJ9I6KHN/did-edt-get-it-right-all-along-introducing-yet-another.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7ITNVNC","blogPost","2020","Leskela, Anni","Commitment and credibility in multipolar AI scenarios","LessWrong","","","","https://www.lesswrong.com/posts/LvtsFKxg2t3nWhKRq/commitment-and-credibility-in-multipolar-ai-scenarios","The ability to make credible commitments is a key factor in many bargaining situations ranging from trade to international conflict. This post builds a taxonomy of the commitment mechanisms that transformative AI (TAI) systems could use in future multipolar scenarios, describes various issues they have in practice, and draws some tentative conclusions about the landscape of commitments we might expect in the future. INTRODUCTION A better understanding of the commitments that future AI systems could make is helpful for predicting and influencing the dynamics of multipolar scenarios. The option to credibly bind oneself to certain actions or strategies fundamentally changes the game theory behind bargaining, cooperation, and conflict. Credible commitments and general transparency can work to stabilize positive-sum agreements, and to increase the efficiency of threats (Schelling 1960), both of which could be relevant to how well TAI trajectories will reflect our values. Because human goals can be contradictory, and even broadly aligned AI systems could come to prioritize different outcomes depending on their domains and histories, these systems could end up in competitive situations and bargaining failures where a lot of value is lost. Similarly, if some systems in a multipolar scenario are well aligned and others less so, some worst cases might be avoidable if stable peaceful agreements can be reached. As an example of the practical significance of commitment ability in stabilizing peaceful strategies, standard theories in international relations hold that conflicts between nations are difficult to avoid indefinitely primarily because there are no reliable commitment mechanisms for peaceful agreements (e.g. Powell 2004, Lake 1999, Rosato 2015), even when nations would overall prefer them. In addition to the direct costs of conflict, the lack of enforceable commitments leads to continuous resource loss from arms races, monitoring, and other preparations for possible","2020","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-12-12 15:04:18","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WFUHVWSK/commitment-and-credibility-in-multipolar-ai-scenarios.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N6AANRRU","blogPost","2021","Clifton, Jesse","Collaborative game specification: arriving at common models in bargaining","Center on Long-Term Risk","","","","https://longtermrisk.org/collaborative-game-specification/","Conflict is often an inefficient outcome to a bargaining problem. This is true in the sense that, for a given game-theoretic model of a strategic interaction, there is often some equilibrium in which all agents are better off than the conflict outcome. But real-world agents may not make decisions according to game-theoretic models, and when they do, they may use different models. This makes it more difficult to guarantee that real-world agents will avoid bargaining failure than is suggested by the observation that conflict is often inefficient.   In another post, I described the ""prior selection problem"", on which different agents having different models of their situation can lead to bargaining failure. Moreover, techniques for addressing bargaining problems like coordination on […]","2021-03-06","2022-01-30 04:51:07","2022-01-30 04:51:07","2021-10-31 16:55:52","","","","","","","Collaborative game specification","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/HJ3TRAF5/collaborative-game-specification.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UTZBKAUW","blogPost","2018","Gloor, Lukas","Cause prioritization for downside-focused value systems","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/225Aq4P4jFPoWBrb5/cause-prioritization-for-downside-focused-value-systems","Last edited: August 27th 2019.  This post outlines my thinking on cause prioritization from the perspective of value systems whose primary concern is reducing disvalue. I’m mainly thinking of  suffering-focused ethics (SFE), but I also want to include moral views that attribute substantial disvalue to things other than suffering, such as inequality or preference violation. I will limit the discussion to interventions targeted at improving the long-term future (see the reasons in section II). I hope my post will also be informative for people who do not share a downside-focused outlook, as thinking about cause prioritization from different perspectives, with emphasis on considerations other than those one is used to, can be illuminating. Moreover, understanding the strategic considerations for plausible moral views is essential for acting under moral uncertainty and cooperating with people with other values. I will talk about the following topics:  * Which views qualify as downside-focused (given our empirical situation)  * Why downside-focused views prioritize s-risk reduction over utopia creation  * Why extinction risk reduction is unlikely to be a promising intervention    according to downside-focused views  * Why AI alignment is probably positive for downside-focused views, and    especially positive if done with certain precautions  * What to include in an EA portfolio that incorporates population ethical    uncertainty and cooperation between value systems WHICH VIEWS QUALIFY AS DOWNSIDE-FOCUSED? I’m using the term downside-focused to refer to value systems that in practice (given what we know about the world) primarily recommend working on interventions that make bad things less likely.[1] For example, if one holds that what is most important is how things turn out for individuals (welfarist consequentialism), and that it is comparatively unimportant to add well-off beings to the world, then one should likely focus on preventing suffering.[2] That would b","2018-01-31","2022-01-30 04:51:07","2022-01-30 04:51:07","2020-11-23 00:44:01","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/DI58FZJA/cause-prioritization-for-downside-focused-value-systems.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5HPEHZIR","conferencePaper","2016","Sadigh, Dorsa; Sastry, Shankar; A. Seshia, Sanjit; D. Dragan, Anca","Planning for Autonomous Cars that Leverage Effects on Human Actions","Robotics: Science and Systems XII","978-0-9923747-2-3","","10.15607/RSS.2016.XII.029","http://www.roboticsproceedings.org/rss12/p29.pdf","Traditionally, autonomous cars make predictions about other drivers’ future trajectories, and plan to stay out of their way. This tends to result in defensive and opaque behaviors. Our key insight is that an autonomous car’s actions will actually affect what other cars will do in response, whether the car is aware of it or not. Our thesis is that we can leverage these responses to plan more efﬁcient and communicative behaviors. We model the interaction between an autonomous car and a human driver as a dynamical system, in which the robot’s actions have immediate consequences on the state of the car, but also on human actions. We model these consequences by approximating the human as an optimal planner, with a reward function that we acquire through Inverse Reinforcement Learning. When the robot plans with this reward function in this dynamical system, it comes up with actions that purposefully change human state: it merges in front of a human to get them to slow down or to reach its own goal faster; it blocks two lanes to get them to switch to a third lane; or it backs up slightly at an intersection to get them to proceed ﬁrst. Such behaviors arise from the optimization, without relying on hand-coded signaling strategies and without ever explicitly modeling communication. Our user study results suggest that the robot is indeed capable of eliciting desired changes in human state by planning using this dynamical system.","2016","2022-01-30 04:51:06","2022-01-30 04:51:06","2020-12-13 23:37:50","","","","","","","","","","","","Robotics: Science and Systems Foundation","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000363","","/Users/jacquesthibodeau/Zotero/storage/KQ9WS3I7/SadighPlanning2016.pdf","","CHAI; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Robotics: Science and Systems 2016","","","","","","","","","","","","","","",""
"2EIBJKHU","blogPost","2021","Jia","Case studies of self-governance to reduce technology risk","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/Xf6QE6txgvfCGvZpk/case-studies-of-self-governance-to-reduce-technology-risk","I worked on this research project during my summer fellowship at the Center on Long-Term Risk. Though the findings aren't particularly insightful, I’m posting this unpolished version to:  * Hopefully help other people attempting similar projects save time and effort  * Demonstrate one approach to case study selection  * Give others a sense of what one type of AI governance summer research project    might look like. SUMMARY  * Self-governance occurs when private actors coordinate to address issues that    are not obviously related to profit, with minimal involvement from    governments and standards bodies.  * Historical cases of self-governance to reduce technology risk are rare. I    find 6 cases that seem somewhat similar to AI development, including the    actions of Leo Szilard and other physicists in 1939 and the 1975 Asilomar    conference.  * The following factors seem to make self-governance efforts more likely to    occur: * Risks are salient     * The government looks like it might step in if private actors do       nothing     * The field or industry is small     * Support from gatekeepers (like journals and large consumer-facing       firms)     * Support from credentialed scientists.          * After the initial self-governance effort, governments usually step in to    develop and codify rules.  * My biggest takeaway is probably that self-governance efforts seem more likely    to occur when risks are somewhat prominent. As a result, we could do more to     connect “near-term” issues like data privacy and algorithmic bias with    “long-term” concerns. We could try to preemptively identify “fire alarms” for    TAI, and be ready to take advantage of these warning signals if they occur. INTRODUCTION Private actors play an important role in AI governance. Several companies have released their own guidelines, and the Partnership on AI is a notable actor in the space.[1] In other words, we are beginning to see elements of self-governance in the AI industry","2021-04-06","2022-01-30 04:51:06","2022-01-30 04:51:06","2021-11-14 19:00:58","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/SHI8RDJ9/case-studies-of-self-governance-to-reduce-technology-risk.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H57NI5W3","blogPost","2021","Kokotajlo, Daniel","Birds, Brains, Planes, and AI: Against Appeals to the Complexity/Mysteriousness/Efficiency of the Brain","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/HhWhaSzQr6xmBki8F/birds-brains-planes-and-ai-against-appeals-to-the-complexity","[Epistemic status: Strong opinions lightly held, this time with a cool graph.] I argue that an entire class of common arguments against short timelines is bogus, and provide weak evidence that anchoring to the human-brain-human-lifetime milestone is reasonable. In a sentence, my argument is that the complexity and mysteriousness and efficiency of the human brain (compared to artificial neural nets) is almost zero evidence that building TAI will be difficult, because evolution typically makes things complex and mysterious and efficient, even when there are simple, easily understood, inefficient designs that work almost as well (or even better!) for human purposes. In slogan form: If all we had to do to get TAI was make a simple neural net 10x the size of my brain, my brain would still look the way it does. The case of birds & planes illustrates this point nicely. Moreover, it is also a precedent for several other short-timelines talking points, such as the human-brain-human-lifetime (HBHL) anchor. PLAN:  1. Illustrative Analogy  2. Exciting Graph  3. Analysis 1. Extra brute force can make the problem a lot easier      2. Evolution produces complex mysterious efficient designs by         default, even when simple inefficient designs work just fine for human         purposes.      3. What’s bogus and what’s not      4. Example: Data-efficiency            4. Conclusion  5. Appendix 1909 French military plane, the Antionette VII. By Deep silence (Mikaël Restoux) - Own work (Bourget museum, in France), CC BY 2.5, https://commons.wikimedia.org/w/index.php?curid=1615429 ILLUSTRATIVE ANALOGY AI timelines, from our current perspectiveFlying machine timelines, from the perspective of the late 1800’s:Shorty: Human brains are giant neural nets. This is reason to think we can make human-level AGI (or at least AI with  strategically relevant skills, like politics and science) by making giant neural nets.Shorty: Birds are winged creatures that paddle through the air. This i","2021","2022-01-30 04:51:06","2022-01-30 04:51:06","2021-11-13 21:52:57","","","","","","","Birds, Brains, Planes, and AI","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/GNFCNKHK/birds-planes-brains-and-ai-against-appeals-to-the-complexity.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5XCN8E2T","blogPost","2017","Treutlein, Johannes","“Betting on the Past” by Arif Ahmed","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2017/02/06/betting-on-the-past-by-arif-ahmed/","[This post assumes knowledge of decision theory, as discussed in Eliezer Yudkowsky’s Timeless Decision Theory and in Arbital’s Introduction to Logical Decision Theory.] I recently discovered an int…","2017-02-06","2022-01-30 04:51:06","2022-01-30 04:51:06","2020-11-23 00:54:29","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UBVFK75X/betting-on-the-past-by-arif-ahmed.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BD2HXECW","manuscript","2016","Oesterheld, Caspar","Backup utility functions as a fail-safe AI technique","","","","","https://longtermrisk.org/files/backup-utility-functions.pdf","Many experts believe that AIs will, within the not-too-distant future, become powerful enough for their decisions to have tremendous impact. Unfortunately, setting up AI goal systems in a way that results in benevolent behavior is expected to be diﬃcult, and we cannot be certain to get it completely right on the ﬁrst attempt. We should therefore account for the possibility that the goal systems fail to implement our values the intended way. In this paper, we propose the idea of backup utility functions: Secondary utility functions that are used in case the primary ones “fail”. We also describe how this approach can be generalized to the use of multi-layered utility functions, some of which can fail without aﬀecting the ﬁnal outcome as badly as without the backup mechanism.","2016-10","2022-01-30 04:51:06","2022-01-30 04:51:06","2020-12-18","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/FWH7VQ85/Oesterheld - Backup utility functions as a fail-safe AI techniq.pdf","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5GGXJ663","blogPost","2017","Oesterheld, Caspar","Are causal decision theorists trying to outsmart conditional probabilities?","LessWrong","","","","https://www.lesswrong.com/posts/cyJgdhgYaM2CbZ7tP/are-causal-decision-theorists-trying-to-outsmart-conditional","Presumably, this has been discussed somewhere in the past, but I wonder to which extent causal decision theorists (and many other non-evidential decision theorists, too) are trying to make better predictions than (what they think to be) their own conditional probabilities. To state this question more clearly, let’s look at the generic Newcomb-like problem with two actions a1 and a2 (e.g., one-boxing and two-boxing, cooperating or defecting, not smoking or smoking) and two states s1 and s2 (specifying, e.g., whether there is money in both boxes, whether the other agent cooperates, whether one has cancer). The Newcomb-ness is the result of two properties:  * No matter the state, it is better to take action a2, i.e. u(a2,s1)>u(a1,s1)    and u(a2,s2)>u(a1,s2). (There are also problems without dominance where CDT    and EDT nonetheless disagree. For simplicity I will assume dominance, here.)          * The action cannot causally affect the state, but somehow taking a1 gives us    evidence that we’re in the preferable state s1. That is, P(s1|a1)>P(s1|a2)    and u(a1,s1)>u(a2,s2).         Then, if the latter two differences are large enough, it may be that E[u|a1] > E[u|a2]. I.e. P(s1|a1) * u(s1,a1) + P(s2|a1) * u(s2,a1) > P(s1|a2) * u(s1,a2) + P(s2|a2) * u(s2,a2), despite the dominance. Now, my question is: After having taken one of the two actions, say a1, but before having observed the state, do causal decision theorists really assign the probability P(s1|a1) (specified in the problem description) to being in state s1? I used to think that this was the case. E.g., the way I learned about Newcomb’s problem is that causal decision theorists understand that, once they have said the words “both boxes for me, please”, they assign very low probability to getting the million. So, if there were a period between saying those words and receiving the payoff, they would bet at odds that reveal that they assign a low probability (namely P(s1,a2)) to money being under","2017-05-16","2022-01-30 04:51:06","2022-01-30 04:51:06","2020-11-23 00:52:29","","","","","","","Are causal decision theorists trying to outsmart conditional probabilities?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/R7Z7P6H5/are-causal-decision-theorists-trying-to-outsmart-conditional.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2PTAEDI","journalArticle","2019","Oesterheld, Caspar","Approval-directed agency and the decision theory of Newcomb-like problems","Synthese","","","","","","2019","2022-01-30 04:51:06","2022-01-30 04:51:06","","1–14","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000003  Publisher: Springer","","/Users/jacquesthibodeau/Zotero/storage/9XXQZD4D/Oesterheld - 2019 - Approval-directed agency and the decision theory o.pdf; /Users/jacquesthibodeau/Zotero/storage/45AXUP2J/Oesterheld - 2019 - Approval-directed agency and the decision theory o.pdf","","CLR; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PRZR88VR","blogPost","2017","Treutlein, Johannes","Anthropic uncertainty in the Evidential Blackmail","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2017/05/12/anthropic-uncertainty-in-the-evidential-blackmail/","I’m currently writing a piece on anthropic uncertainty in Newcomb problems. The idea is that whenever someone simulates us to predict our actions, this leads us to have anthropic uncertainty about …","2017-05-12","2022-01-30 04:51:06","2022-01-30 04:51:06","2020-11-23 00:51:55","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/X3JHA4D9/anthropic-uncertainty-in-the-evidential-blackmail.html","","CLR; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6WFI4PND","blogPost","2021","Lyzhov, Alex","""AI and Compute"" trend isn't predictive of what is happening","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening","(open in a new tab to view at higher resolution) In May 2018 (almost 3 years ago) OpenAI published their ""AI and Compute""  blogpost where they highlighted the trend of increasing compute spending on training the largest AI models and speculated that the trend might continue into the future. This note is aimed to show that the trend has ended right around the moment of OpenAI publishing their post and doesn't hold up anymore. On the above image, I superimposed the scatter plot from OpenAI blogpost and my estimates of compute required for some recent large and ambitious ML experiments. To the best of my knowledge (and I have tried to check for this), there haven't been any experiments that required more compute than those shown on the plot. The main thing shown here is that less than one doubling of computational resources for the largest training occured in the 3-year period between 2018 and 2021, compared to around 10 doublings in the 3-year period between 2015 and 2018. This seems to correspond to a severe slowdown of computational scaling. To stay on the trend line, we currently would need an experiment requiring roughly around 100 times more compute than GPT-3. Considering that GPT-3 may have costed between $5M and $12M and accelerators haven't vastly improved since then, such an experiment would now likely cost $0.2B - $1.5B.","2021-04-01","2022-01-30 04:51:06","2022-01-30 04:51:06","2021-12-11 14:10:12","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/3RKNJGBW/ai-and-compute-trend-isn-t-predictive-of-what-is-happening.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78R2ACU4","blogPost","2020","Kokotajlo, Daniel","Against GDP as a metric for timelines and takeoff speeds","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/aFaKhG86tTrKvtAnT/against-gdp-as-a-metric-for-timelines-and-takeoff-speeds","OR: WHY AI TAKEOVER MIGHT HAPPEN BEFORE GDP ACCELERATES, AND OTHER THOUGHTS ON WHAT MATTERS FOR TIMELINES AND TAKEOFF SPEEDS [Epistemic status: Strong opinion, lightly held] I think world GDP (and economic growth more generally) is overrated as a metric for AI timelines and takeoff speeds. Here are some uses of GDP that I disagree with, or at least think should be accompanied by cautionary notes:  * Timelines: Ajeya Cotra thinks of transformative AI as “software which causes    a tenfold acceleration in the rate of growth of the world economy (assuming    that it is used everywhere that it would be economically profitable to use    it).” I don’t mean to single her out in particular; this seems like the    standard definition now. And I think it's much better than one prominent    alternative, which is to date your AI timelines to the first time world GDP    (GWP) doubles in a year!  * Takeoff Speeds: Paul Christiano argues for Slow Takeoff. He thinks we can use    GDP growth rates as a proxy for takeoff speeds. In particular, he thinks Slow    Takeoff ~= GWP doubles in 4 years before the start of the first 1-year GWP    doubling. This proxy/definition has received a lot of uptake.  * Timelines: David Roodman’s excellent model projects GWP hitting infinity in    median 2047, which I calculate means TAI in median 2037. To be clear, he    would probably agree that we shouldn’t use these projections to forecast TAI,    but I wish to add additional reasons for caution.  * Timelines: I’ve sometimes heard things like this: “GWP growth is stagnating    over the past century or so; hyperbolic progress has ended; therefore TAI is    very unlikely.”  * Takeoff Speeds: Various people have said things like this to me: “If you    think there’s a 50% chance of TAI by 2032, then surely you must think there’s    close to a 50% chance of GWP growing by 8% per year by 2025, since TAI is    going to make growth rates go much higher than that, and progress is    typically continuous.","2020-12-29","2022-01-30 04:51:06","2022-01-30 04:51:06","2021-12-11 14:14:38","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/NUWWJRAQ/against-gdp-as-a-metric-for-timelines-and-takeoff-speeds.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R8SVD9CH","conferencePaper","2021","Koch, Jack; Langosco, Lauro; Pfau, Jacob; Le, James; Sharkey, Lee","Objective Robustness in Deep Reinforcement Learning","arXiv:2105.14111 [cs]","","","","http://arxiv.org/abs/2105.14111","We study objective robustness failures, a type of out-of-distribution robustness failure in reinforcement learning (RL). Objective robustness failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong objective. This kind of failure presents different risks than the robustness problems usually considered in the literature, since it involves agents that leverage their capabilities to pursue the wrong objective rather than simply failing to do anything useful. We provide the first explicit empirical demonstrations of objective robustness failures and present a partial characterization of its causes.","2021-06-08","2022-01-30 04:49:30","2022-01-30 04:49:30","2021-10-30 17:55:11","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 2105.14111","","/Users/jacquesthibodeau/Zotero/storage/HBRPM964/Koch et al. - 2021 - Objective Robustness in Deep Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/HQIR73N6/2105.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML 2021","","","","","","","","","","","","","","",""
"E9GVD3BD","blogPost","2020","Leong, Chris","Embedded vs. External Decision Problems","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/br7KRSeNymwSvZnf5/embedded-vs-external-decision-problems","","2020-03-04","2022-01-30 04:49:30","2022-01-30 04:49:30","2020-08-18 20:51:01","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/E9UCKBGB/embedded-vs-external-decision-problems.html","","TechSafety; AI-Safety-Camp; AISRP2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VM3QWB8S","blogPost","2021","Koch, Jack; Langosco, Lauro","Discussion: Objective Robustness and Inner Alignment Terminology","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/pDaxobbB9FG5Dvqyv/discussion-objective-robustness-and-inner-alignment","In the alignment community, there seem to be two main ways to frame and define objective robustness and inner alignment. They are quite similar, mainly differing in the manner in which they focus on the same basic underlying problem. We’ll call these the objective-focused approach and the generalization-focused approach. We don’t delve into these issues of framing the problem in Empirical Observations of Objective Robustness Failures, where we present empirical observations of objective robustness failures. Instead, we think it is worth having a separate discussion of the matter. These issues have been mentioned only infrequently in a few comments on the Alignment Forum, so it seemed worthwhile to write a post describing the framings and their differences in an effort to promote further discussion in the community. TL;DR This post compares two different paradigmatic approaches to objective robustness/inner alignment: Objective-focused approach  * Emphasis: “How do we ensure our models/agents have the right    (mesa-)objectives?”  * Outer alignment: “an objective function r is outer aligned if all models that    perform optimally on r in the limit of perfect training and infinite data are    intent aligned.” * Outer alignment is a property of the training objective.         Generalization-focused approach  * Emphasis: “How will this model/agent generalize out-of-distribution?” *        Considering a model’s “objectives” or “goals,” whether behavioral or       internal, is instrumentally useful for predicting OOD behavior, but what       you ultimately care about is whether it generalizes “acceptably.”          * Outer alignment: a model is outer aligned if it performs desirably on the    training distribution. * Outer alignment is a property of the tuple (training       objective, training data, training setup, model).         Special thanks to Rohin Shah, Evan Hubinger, Edouard Harris, Adam Shimi, and Adam Gleave for their helpful feedback on drafts of this po","2021-06-23","2022-01-30 04:49:30","2022-01-30 04:49:30","2021-10-30 18:03:29","","","","","","","Discussion","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/X74XW5JR/discussion-objective-robustness-and-inner-alignment.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5HWDDTTB","conferencePaper","2019","Mancuso, Jason; Kisielewski, Tomasz; Lindner, David; Singh, Alok","Detecting Spiky Corruption in Markov Decision Processes","","","","","http://arxiv.org/abs/1907.00452","Current reinforcement learning methods fail if the reward function is imperfect, i.e. if the agent observes reward different from what it actually receives. We study this problem within the formalism of Corrupt Reward Markov Decision Processes (CRMDPs). We show that if the reward corruption in a CRMDP is sufficiently ""spiky"", the environment is solvable. We fully characterize the regret bound of a Spiky CRMDP, and introduce an algorithm that is able to detect its corrupt states. We show that this algorithm can be used to learn the optimal policy with any common reinforcement learning algorithm. Finally, we investigate our algorithm in a pair of simple gridworld environments, finding that our algorithm can detect the corrupt states and learn the optimal policy despite the corruption.","2019-06-30","2022-01-30 04:49:30","2022-01-30 04:49:30","2019-12-16 03:27:42","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000002  arXiv: 1907.00452","","/Users/jacquesthibodeau/Zotero/storage/7UWHJF9I/Mancuso et al. - 2019 - Detecting Spiky Corruption in Markov Decision Proc.pdf; /Users/jacquesthibodeau/Zotero/storage/ZJH6BKAF/Mancuso et al. - 2019 - Detecting Spiky Corruption in Markov Decision Proc.pdf; /Users/jacquesthibodeau/Zotero/storage/ZW3BBNWK/1907.html","","TechSafety; AI-Safety-Camp","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AI Safety Workshop in IJCAI 2019","","","","","","","","","","","","","","",""
"7QA8U489","conferencePaper","2019","Majha, Arushi; Sarkar, Sayan; Zagami, Davide","Categorizing Wireheading in Partially Embedded Agents","","","","","http://arxiv.org/abs/1906.09136","$\textit{Embedded agents}$ are not explicitly separated from their environment, lacking clear I/O channels. Such agents can reason about and modify their internal parts, which they are incentivized to shortcut or $\textit{wirehead}$ in order to achieve the maximal reward. In this paper, we provide a taxonomy of ways by which wireheading can occur, followed by a definition of wirehead-vulnerable agents. Starting from the fully dualistic universal agent AIXI, we introduce a spectrum of partially embedded agents and identify wireheading opportunities that such agents can exploit, experimentally demonstrating the results with the GRL simulation platform AIXIjs. We contextualize wireheading in the broader class of all misalignment problems - where the goals of the agent conflict with the goals of the human designer - and conjecture that the only other possible type of misalignment is specification gaming. Motivated by this taxonomy, we define wirehead-vulnerable agents as embedded agents that choose to behave differently from fully dualistic agents lacking access to their internal parts.","2019-06-21","2022-01-30 04:49:30","2022-01-30 04:49:30","2019-12-16 03:26:47","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/2PGIU7BA/Majha et al. - 2019 - Categorizing Wireheading in Partially Embedded Age.pdf; /Users/jacquesthibodeau/Zotero/storage/UGH6MA5M/Majha et al. - 2019 - Categorizing Wireheading in Partially Embedded Age.pdf; /Users/jacquesthibodeau/Zotero/storage/429IP46K/1906.html; /Users/jacquesthibodeau/Zotero/storage/MMNA4BT2/1906.html","","TechSafety; AI-Safety-Camp","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AI Safety Workshop in IJCAI 2019","","","","","","","","","","","","","","",""
"NWMVFW46","blogPost","2020","Chiswick, Max; Makiievskyi, Anton; Zhou, Liang","Assessing Generalization in Reward Learning: Intro and Background","Towards Data Science (Medium)","","","","https://towardsdatascience.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48","An overview of reinforcement learning, generalization, and reward learning","2020-11-20","2022-01-30 04:49:30","2022-01-30 04:49:30","2020-11-21 18:11:23","","","","","","","Assessing Generalization in Reward Learning","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WCTVNGGW/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48.html","","TechSafety; AI-Safety-Camp","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XMHWUMQC","blogPost","2020","Kosch, Sebastian","AISC4: Research Summaries","AI Safety Camp","","","","https://aisafety.camp/2020/05/30/aisc4-research-summaries/","The fourth AI Safety Camp took place in May 2020 in Toronto. Due to COVID-19, the camp was held virtually. Six teams participated and worked on the following topics: Survey on AI risk scenarios Opt…","2020-05-30","2022-01-30 04:49:30","2022-01-30 04:49:30","2020-11-21 19:14:38","","","","","","","AISC4","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/N39D7ZWI/aisc4-research-summaries.html","","TechSafety; AI-Safety-Camp","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F4NQ9ZES","blogPost","2019","Kovarik, Vojta","AI Safety Debate and Its Applications","LessWrong","","","","https://www.lesswrong.com/posts/5Kv2qNfRyXXihNrx2/ai-safety-debate-and-its-applications","All of the experimental work and some of the theoretical work has been done jointly with Anna Gajdova, David Lindner, Lukas Finnveden, and Rajashree Agrawal as part of the third AI Safety Camp. We are grateful to Ryan Carey and Geoffrey Irving for the advice regarding this project. The remainder of the theoretical part relates to my stay at FHI, and I would like to thank the above people, Owain Evans, Michael Dennis, Ethan Perez, Stuart Armstrong, and Max Daniel for comments/discussions. -------------------------------------------------------------------------------- Debate is a recent proposal for AI alignment, which naturally incorporates elicitation of human preferences and has the potential to offload the costly search for flaws in an AI’s suggestions onto the AI. After briefly recalling the intuition behind debate, we list the main open problems surrounding it and summarize how the existing work on debate addresses them. Afterward, we describe, and distinguish between, Debate games and their different applications in more detail. We also formalize what it means for a debate to be truth-promoting. Finally, we present results of our experiments on Debate games and Training via Debate on MNIST and fashion MNIST. DEBATE GAMES AND WHY THEY ARE USEFUL Consider an answer A to some question Q --- for example, ""Where should I go for a vacation?"" and ""Alaska"". Rather than directly verifying whether A is an accurate answer to Q, it might be easier to first decompose A into lower-level components (How far/expensive is it? Do they have nice beaches? What is the average temperature? What language do they speak?). Moreover, it isn't completely clear what to do even if we know the relevant facts --- indeed, how does Alaska's cold weather translate to a preference for Alaska from 0 to 10? And how does this preference compare to English being spoken in Alaska? As an alternative, we can hold a debate between two competing answers A and A′=""Bali"" to Q. This allows strategic de","2019-07-23","2022-01-30 04:49:29","2022-01-30 04:49:29","2019-12-16 03:27:22","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/R59PFVJB/ai-safety-debate-and-its-applications.html","","TechSafety; AI-Safety-Camp","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MKCF7837","journalArticle","2019","Perry, Brandon; Uuk, Risto","AI Governance and the Policymaking Process: Key Considerations for Reducing AI Risk","Big Data and Cognitive Computing","","","10.3390/bdcc3020026","https://www.mdpi.com/2504-2289/3/2/26","This essay argues that a new subfield of AI governance should be explored that examines the policy-making process and its implications for AI governance. A growing number of researchers have begun working on the question of how to mitigate the catastrophic risks of transformative artificial intelligence, including what policies states should adopt. However, this essay identifies a preceding, meta-level problem of how the space of possible policies is affected by the politics and administrative mechanisms of how those policies are created and implemented. This creates a new set of key considerations for the field of AI governance and should influence the action of future policymakers. This essay examines some of the theories of the policymaking process, how they compare to current work in AI governance, and their implications for the field at large and ends by identifying areas of future research.","2019-06","2022-01-30 04:49:29","2022-01-30 04:49:29","2020-12-14 23:48:16","26","","2","3","","","AI Governance and the Policymaking Process","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","ZSCC: 0000017  Number: 2 Publisher: Multidisciplinary Digital Publishing Institute","","/Users/jacquesthibodeau/Zotero/storage/2R7BPXBH/Perry and Uuk - 2019 - AI Governance and the Policymaking Process Key Co.pdf","","MetaSafety; Other-org","AI governance; AI risk; policymaking process; typologies of AI policy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMQECS38","blogPost","2021","Smith, Ben; Pihlakas, Roland; Klassert, Robert","A brief review of the reasons multi-objective RL could be important in AI Safety Research","LessWrong","","","","https://www.lesswrong.com/posts/i5dLfi6m6FCexReK9/a-brief-review-of-the-reasons-multi-objective-rl-could-be","By Ben Smith, Roland Pihlakas, and Robert Klassert Thanks to Linda Linsefors, Alex Turner, Richard Ngo, Peter Vamplew, JJ Hepburn, Tan Zhi-Xuan, Remmelt Ellen, Kaj Sotala, Koen Holtman, and Søren Elverlin for their time and kind remarks in reviewing this essay. Thanks to the organisers of the AI Safety Camp for incubating this project from its inception and for connecting our team. For the last 9 months, we have been investigating the case for a multi-objective approach to reinforcement learning in AI Safety. Based on our work so far, we’re moderately convinced that multi-objective reinforcement learning should be explored as a useful way to help us understand ways in which we can achieve safe superintelligence. We’re writing this post to explain why, to inform readers of the work we and our colleagues are doing in this area, and invite critical feedback about our approach and about multi-objective RL in general. We were first attracted to the multi-objective space because human values are inherently multi-objective--in any number of frames: deontological, utilitarian, and virtue ethics; egotistical vs. moral objectives; maximizing life values including hedonistic pleasure, eudaemonic meaning, or the enjoyment of power and status. AGI systems aiming to solve for human values are likely to be multi-objective themselves, if not by explicit design, then multi-objective systems would emerge from learning about human preferences. As a first pass at technical research in this area, we took a commonly-used example, the “BreakableBottles” problem, and showed that for low-impact AI, an agent could more quickly solve this toy problem if it uses a conservative but flexible trade-off between alignment and performance values, compared to using a thresholded alignment system to maximize a certain amount of alignment and only then maximizing on performance. Such tradeoffs will be critical for understanding the conflicts between more abstract human objectives a human-preferen","2021-09-29","2022-01-30 04:49:29","2022-01-30 04:49:29","2021-10-30 18:07:11","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VPK7QUUN/a-brief-review-of-the-reasons-multi-objective-rl-could-be.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BP895WND","blogPost","2016","AI Impacts","What if you turned the world’s hardware into AI minds?","AI Impacts","","","","https://aiimpacts.org/what-if-you-turned-the-worlds-hardware-into-ai-minds/","In a classic 'AI takes over the world' scenario, one of the first things an emerging superintelligence wants to do is steal most of the world's computing hardware and repurpose it to running the AI's own software. This step takes one from 'super-proficient hacker' levels of smart to 'my brain is one of the main things happening on Planet Earth' levels of smart. There is quite a...","2016-09-04","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-12-13 19:54:15","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/BMDFBIQX/what-if-you-turned-the-worlds-hardware-into-ai-minds.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZBUX2G49","blogPost","2021","Kokotajlo, Daniel","What 2026 looks like","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like","This was written for the Vignettes Workshop.[1] The goal is to write out a  detailed future history (“trajectory”) that is as realistic (to me) as I can currently manage, i.e. I’m not aware of any alternative trajectory that is similarly detailed and clearly more plausible to me. The methodology is roughly: Write a future history of 2022. Condition on it, and write a future history of 2023. Repeat for 2024, 2025, etc. (I'm posting 2022-2026 now so I can get feedback that will help me write 2027+. I intend to keep writing until the story reaches singularity/extinction/utopia/etc.) What’s the point of doing this? Well, there are a couple of reasons:  * Sometimes attempting to write down a concrete example causes you to learn    things, e.g. that a possibility is more or less plausible than you thought.  * Most serious conversation about the future takes place at a high level of    abstraction, talking about e.g. GDP acceleration, timelines until TAI is    affordable, multipolar vs. unipolar takeoff… vignettes are a neglected    complementary approach worth exploring.  * Most stories are written backwards. The author begins with some idea of how    it will end, and arranges the story to achieve that ending. Reality, by    contrast, proceeds from past to future. It isn’t trying to entertain anyone    or prove a point in an argument.  * Anecdotally, various people seem to have found Paul Christiano’s “tales of    doom” stories helpful, and relative to typical discussions those stories are    quite close to what we want. (I still think a bit more detail would be good —    e.g. Paul’s stories don’t give dates, or durations, or any numbers at all    really.)[2]  * “I want someone to ... write a trajectory for how AI goes down, that is    really specific about what the world GDP is in every one of the years from    now until insane intelligence explosion. And just write down what the world    is like in each of those years because I don't know how to write an    internally","2021-08-06","2022-01-30 04:49:21","2022-01-30 04:49:21","2021-11-18 23:05:44","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/3DAS3TNK/what-2026-looks-like-daniel-s-median-future.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WFC7D3BN","blogPost","2019","Walsh, Toby","Walsh 2017 survey","AI Impacts","","","","https://aiimpacts.org/walsh-2017-survey/","Toby Walsh surveyed hundreds of experts and non-experts in 2016 and found their median estimates for ‘when a computer might be able to carry out most human professions at least as well as a typical human’ were as follows: Probability of HLMIGroup of survey respondentsAI expertsRobotics expertsNon-experts10%20352033202650%20612065203990%210921182060 Details Toby Walsh, professor of AI at the...","2019-12-24","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-11-14 03:21:35","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timeline Surveys","","/Users/jacquesthibodeau/Zotero/storage/77WDNJVZ/walsh-2017-survey.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KSMW4U3T","blogPost","2020","AI Impacts","Time for AI to cross the human range in English draughts","AI Impacts","","","","https://aiimpacts.org/time-for-ai-to-cross-the-human-range-in-english-draughts/","AI took over 30 years to go from beginner level to superhuman level","2020-10-26","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-11-21 20:34:50","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance","","/Users/jacquesthibodeau/Zotero/storage/367ECIGF/time-for-ai-to-cross-the-human-range-in-english-draughts.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"794VTKRN","blogPost","2020","AI Impacts","Time for AI to cross the human performance range in Go","AI Impacts","","","","https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-go/","Computer Go took over 30 years to go from beginner level to superhuman.","2020-10-15","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-11-21 20:37:18","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance","","/Users/jacquesthibodeau/Zotero/storage/ETACUV59/time-for-ai-to-cross-the-human-performance-range-in-go.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BR5ED5D2","blogPost","2020","AI Impacts","Time for AI to cross the human performance range in chess","AI Impacts","","","","https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-chess/","Computer chess took around 50 years to go from beginner level to superhuman level.","2020-10-15","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-11-21 20:37:56","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance","","/Users/jacquesthibodeau/Zotero/storage/4CIAV85I/time-for-ai-to-cross-the-human-performance-range-in-chess.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EDIFICXQ","blogPost","2020","Bergal, Asya","Takeaways from safety by default interviews","AI Impacts","","","","https://aiimpacts.org/takeaways-from-safety-by-default-interviews/","Last year, several researchers at AI Impacts (primarily Robert Long and I) interviewed prominent researchers inside and outside of the AI safety field who are relatively optimistic about advanced AI being developed safely. These interviews were originally intended to focus narrowly on reasons for optimism, but we ended up covering a variety of topics, including AGI timelines, the likelihood of current techniques leading to AGI, and what the right things to do in AI safety are right now. (...)","2020-04-03","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-09-05 18:07:33","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/H79BNVC8/takeaways-from-safety-by-default-interviews.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NJQ2C7UT","blogPost","2020","Kokotajlo, Daniel","Relevant pre-AGI possibilities","AI Impacts","","","","https://aiimpacts.org/relevant-pre-agi-possibilities/","Brainstorm of ways the world could be relevantly different by the time advanced AGI arrives","2020-06-19","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-08-31 18:08:43","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/XG932AG6/relevant-pre-agi-possibilities.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZGB57MGS","blogPost","2018","Garfinkel, Ben","Reinterpreting “AI and Compute”","AI Impacts","","","","https://aiimpacts.org/reinterpreting-ai-and-compute/","This is a guest post by Ben Garfinkel. We revised it slightly, at his request, on February 9, 2019. A recent OpenAI blog post, “AI and Compute,” showed that the amount of computing power consumed by the most computationally intensive machine learning projects has been doubling every three months. The post presents this trend as...","2018-12-18","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-11-14 03:34:13","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/BXV9445T/reinterpreting-ai-and-compute.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G3BNDJ2G","journalArticle","2018","Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain","When Will AI Exceed Human Performance? Evidence from AI Experts","Journal of Artificial Intelligence Research","","","","http://arxiv.org/abs/1705.08807","Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.","2018","2022-01-30 04:49:21","2022-01-30 04:49:21","2019-12-16 02:29:10","729–754","","","62","","","When Will AI Exceed Human Performance?","","","","","","","","","","","","arXiv.org","","ZSCC: 0000539  arXiv: 1705.08807","","/Users/jacquesthibodeau/Zotero/storage/MXS9Q4IA/Grace et al. - 2018 - When Will AI Exceed Human Performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/XZIMBZVK/1705.html; /Users/jacquesthibodeau/Zotero/storage/9HN4IS26/1705.html; /Users/jacquesthibodeau/Zotero/storage/MM69SWUB/Grace et al. - 2018 - When will AI exceed human performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/AENSKWZK/11222.html; /Users/jacquesthibodeau/Zotero/storage/RVNTGCXF/11222.html; /Users/jacquesthibodeau/Zotero/storage/H2NGMISH/11222.html","","MetaSafety; FHI; AI-Impacts-NotFeatured","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I2IG8DRB","blogPost","2020","Bergal, Asya","Trends in DRAM price per gigabyte","AI Impacts","","","","https://aiimpacts.org/trends-in-dram-price-per-gigabyte/","The price of a gigabyte of DRAM has fallen by about a factor of ten every 5 years from 1957 to 2020. Since 2010, the price has fallen much more slowly, at a rate that would yield an order of magnitude over roughly 14 years. Details Background DRAM, “dynamic random-access memory”, is a type of...","2020-04-14","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-09-05 17:11:39","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Hardware and AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/PI98VB6J/trends-in-dram-price-per-gigabyte.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BU7J7CAX","blogPost","2020","AI Impacts","Time for AI to cross the human range in StarCraft","AI Impacts","","","","https://aiimpacts.org/time-for-ai-to-cross-the-human-range-in-starcraft/","AI took about twenty years to go from beginner level to high professional level.","2020-10-20","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-11-21 20:36:03","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance","","/Users/jacquesthibodeau/Zotero/storage/EFWDD55K/time-for-ai-to-cross-the-human-range-in-starcraft.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"22TF52ND","blogPost","2018","Johnson, Aysja","Time for AI to cross the human performance range in diabetic retinopathy","AI Impacts","","","","https://aiimpacts.org/diabetic-retinopathy-as-a-case-study-in-time-for-ai-to-cross-the-range-of-human-performance/","In diabetic retinopathy, automated systems started out just below expert human level performance, and took around ten years to reach expert human level performance. Details Diabetic retinopathy is a complication of diabetes in which the back of the eye is damaged by high blood sugar levels. It is the most common cause of blindness among working-age...","2018-11-21","2022-01-30 04:49:21","2022-01-30 04:49:21","2020-11-14 03:21:47","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance","","/Users/jacquesthibodeau/Zotero/storage/M8IRAET5/diabetic-retinopathy-as-a-case-study-in-time-for-ai-to-cross-the-range-of-human-performance.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QX5FAHWI","blogPost","2019","McCaslin, Tegan","Primates vs birds: Is one brain architecture better than the other?","AI Impacts","","","","https://aiimpacts.org/primates-vs-birds-is-one-brain-architecture-better-than-the-other/","The boring answer to that question is, “Yes, birds.” But that’s only because birds can pack more neurons into a walnut-sized brain than a monkey with a brain four times that size. So let’s forget about brain volume for a second and ask the really interesting question: neuron per neuron, who’s coming out ahead? You...","2019-02-28","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-11-14 03:21:40","","","","","","","Primates vs birds","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/FZQSAPHU/primates-vs-birds-is-one-brain-architecture-better-than-the-other.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MTAD8Z45","blogPost","2020","Grace, Katja","Misalignment and misuse: whose values are manifest?","AI Impacts","","","","https://aiimpacts.org/misalignment-and-misuse-whose-values-are-manifest/","Are misalignment and misuse helpful catastrophe categories?","2020-11-18","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-11-21 20:30:10","","","","","","","Misalignment and misuse","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/ZHKEVBHX/misalignment-and-misuse-whose-values-are-manifest.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXFNQ8B3","blogPost","2018","Carey, Ryan","Interpreting AI compute trends","AI Impacts","","","","https://aiimpacts.org/interpreting-ai-compute-trends/","This is a guest post by Ryan Carey. Over the last few years, we know that AI experiments have used much more computation than previously. But just last month, an investigation by OpenAI made some initial estimates of just how fast this growth has been. Comparing AlphaGo Zero to AlexNet, they found that the largest...","2018-07-10","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-11-14 03:21:51","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: 0000007  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/ZSPZMGTB/interpreting-ai-compute-trends.html; /Users/jacquesthibodeau/Zotero/storage/9RR92JIT/interpreting-ai-compute-trends.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EAXRN4ZU","blogPost","2019","Etzioni, Oren","Etzioni 2016 survey","AI Impacts","","","","https://aiimpacts.org/etzioni-2016-survey/","Oren Etzioni surveyed 193 AAAI fellows in 2016 and found that 67% of them expected that 'we will achieve Superintelligence' someday, but in more than 25 years. Details Oren Etzioni, CEO of the Allen Institute for AI, reported on a survey in an MIT Tech Review article published on 20 Sep 2016. The rest of...","2019-11-06","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-11-14 03:22:04","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timeline Surveys","","/Users/jacquesthibodeau/Zotero/storage/K8BQJN6K/etzioni-2016-survey.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HTHTXZPA","blogPost","2016","AI Impacts","Error in Armstrong and Sotala 2012","AI Impacts","","","","https://aiimpacts.org/error-in-armstrong-and-sotala-2012/","Can AI researchers say anything useful about when strong AI will arrive? Back in 2012, Stuart Armstrong and Kaj Sotala weighed in on this question in a paper called 'How We're Predicting AI—or Failing To'. They looked at a dataset of predictions about AI timelines, and concluded that predictions made by AI experts were indistinguishable from those of non-experts. (Which might suggest...","2016-05-17","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-12-13 19:54:50","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/V9RNPU92/error-in-armstrong-and-sotala-2012.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U8ZTEBVU","blogPost","2020","Kokotajlo, Daniel","Cortés, Pizarro, and Afonso as precedents for takeover","AI Impacts","","","","https://aiimpacts.org/cortes-pizarro-and-afonso-as-precedents-for-ai-takeover/","Epistemic status: I am not a historian, nor have I investigated these case studies in detail. I admit I am still uncertain about how the conquistadors were able to colonize so much of the world so quickly. I think my ignorance is excusable because this is just a blog post; I welcome corrections from people...","2020-02-29","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-09-05 19:00:45","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/46NH99PN/cortes-pizarro-and-afonso-as-precedents-for-ai-takeover.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8MBI682B","blogPost","2019","Shah, Rohin; Bergal, Asya; Long, Robert; Haxhia, Sara","Conversation with Rohin Shah","AI Impacts","","","","https://aiimpacts.org/conversation-with-rohin-shah/","AI Impacts talked to AI safety researcher Rohin Shah about his views on AI risk. With his permission, we have transcribed this interview. Participants Rohin Shah -- PhD student at the Center for Human-Compatible AI, UC BerkeleyAsya Bergal - AI ImpactsRobert Long – AI ImpactsSara Haxhia -- Independent researcher Summary We spoke with Rohin Shah...","2019-10-31","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-11-14 03:34:17","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes","","/Users/jacquesthibodeau/Zotero/storage/UHCQGXPU/conversation-with-rohin-shah.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UX6HJZUV","blogPost","2019","Christiano, Paul; Bergal, Asya","Conversation with Paul Christiano","AI Impacts","","","","https://aiimpacts.org/conversation-with-paul-christiano/","AI Impacts talked to AI safety researcher Paul Christiano about his views on AI risk. With his permission, we have transcribed this interview. Participants Paul Christiano -- OpenAI safety teamAsya Bergal - AI ImpactsRonny Fernandez - AI ImpactsRobert Long – AI Impacts Summary We spoke with Paul Christiano on August 13, 2019. Here is a...","2019-09-11","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-11-14 03:34:15","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes","","/Users/jacquesthibodeau/Zotero/storage/FCCIUWG4/conversation-with-paul-christiano.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQT2S8TU","blogPost","2020","Kokotajlo, Daniel","Precedents for economic n-year doubling before 4n-year doubling","AI Impacts","","","","https://aiimpacts.org/precedents-for-economic-n-year-doubling-before-4n-year-doubling/","Does the economy ever double without having first doubled four times slower? Yes, but not since 3000BC.","2020-04-14","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-09-05 17:09:17","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/MJQA2RMI/precedents-for-economic-n-year-doubling-before-4n-year-doubling.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGQZA34B","blogPost","2018","O’Keefe, Cullen","On the (in)applicability of corporate rights cases to digital minds","AI Impacts","","","","https://aiimpacts.org/on-the-inapplicability-of-corporate-rights-cases-to-digital-minds/","This is a guest cross-post by Cullen O'Keefe. High-Level Takeaway The extension of rights to corporations likely does not provide useful analogy to potential extension of rights to digital minds. Introduction Examining how law can protect the welfare of possible future digital minds is part of my research agenda. I expect that study of historical...","2018-09-28","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-11-14 03:21:08","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/FTES6UQT/on-the-inapplicability-of-corporate-rights-cases-to-digital-minds.html","","MetaSafety; AmbiguosSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E6AH852C","blogPost","2018","Grace, Katja","Likelihood of discontinuous progress around the development of AGI","AI Impacts","","","","https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/","We aren’t convinced by any of the arguments we’ve seen to expect large discontinuity in AI progress above the extremely low base rate for all technologies. However this topic is controversial, and many thinkers on the topic disagree with us, so we consider this an open question. Details Definitions We say a technological discontinuity has...","2018-02-23","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-12-13 23:06:58","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: 0000002[s0]  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/26QUMN55/likelihood-of-discontinuous-progress-around-the-development-of-agi.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M9JQA2EK","blogPost","2017","Grace, Katja","Human-level hardware timeline","AI Impacts","","","","https://aiimpacts.org/human-level-hardware-timeline/","We estimate that 'human-level hardware'— hardware able to perform as many computations per second as a human brain, at a similar cost to a human brain—has a 30% chance of having already occurred, a 45% third chance of occurring by 2040, and a 25% chance of occurring later. We are not confident about these estimates....","2017-12-22","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-12-13 23:05:56","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/5QQHA88X/human-level-hardware-timeline.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7572XHJN","blogPost","2020","Grace, Katja","Discontinuous progress in history: an update","AI Impacts","","","","https://aiimpacts.org/discontinuous-progress-in-history-an-update/","We’ve been looking for historic cases of discontinuously fast technological progress, to help with reasoning about the likelihood and consequences of abrupt progress in AI capabilities. We recently finished expanding this investigation to 37 technological trends. This blog post is a quick update on our findings. See the main page on the research and its outgoing links for more details.","2020-04-13","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-09-05 16:57:42","","","","","","","Discontinuous progress in history","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/PUBJABIW/discontinuous-progress-in-history-an-update.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5T66WNU","blogPost","2020","Korzekwa, Rick","Description vs simulated prediction","AI Impacts","","","","https://aiimpacts.org/description-vs-simulated-prediction/","What are we trying to do when we look at history to inform forecasting?","2020-04-22","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-09-05 16:59:15","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/XG7CX3FD/description-vs-simulated-prediction.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EN3HHGSU","blogPost","2019","Hanson, Robin; Bergal, Asya; Long, Robert","Conversation with Robin Hanson","AI Impacts","","","","https://aiimpacts.org/conversation-with-robin-hanson/","AI Impacts talked to economist Robin Hanson about his views on AI risk and timelines. With his permission, we have posted and transcribed this interview. Participants Robin Hanson -- Associate Professor of Economics, George Mason UniversityAsya Bergal - AI ImpactsRobert Long – AI Impacts Summary We spoke with Robin Hanson on September 5, 2019. Here...","2019-11-13","2022-01-30 04:49:20","2022-01-30 04:49:20","2020-11-14 03:34:19","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes","","/Users/jacquesthibodeau/Zotero/storage/RIDWXW85/conversation-with-robin-hanson.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NNU453Z3","blogPost","2019","Gleave, Adam; Bergal, Asya; Long, Robert","Conversation with Adam Gleave","AI Impacts","","","","https://aiimpacts.org/conversation-with-adam-gleave/","AI Impacts talked to AI safety researcher Adam Gleave about his views on AI risk. With his permission, we have transcribed this interview. Participants Adam Gleave -- PhD student at the Center for Human-Compatible AI, UC BerkeleyAsya Bergal - AI ImpactsRobert Long – AI Impacts Summary We spoke with Adam Gleave on August 27, 2019....","2019-12-23","2022-01-30 04:49:19","2022-01-30 04:49:19","2020-11-14 03:34:21","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes","","/Users/jacquesthibodeau/Zotero/storage/699C2QIP/conversation-with-adam-gleave.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6WT3U7VF","blogPost","2019","AI Impacts","AI conference attendance","AI Impacts","","","","https://aiimpacts.org/ai-conference-attendance/","Six of the largest seven AI conferences hosted a total of 27,396 attendees in 2018. Attendance at these conferences has grown by an average of 21% per year over 2011-2018. These six conferences host around six times as many attendees as six smaller AI conferences. Details Artificial Intelligence Index reports on this, from data they collected...","2019-03-06","2022-01-30 04:49:19","2022-01-30 04:49:19","2020-12-13 23:57:21","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Inputs","","/Users/jacquesthibodeau/Zotero/storage/PPUFMS4K/ai-conference-attendance.html","","MetaSafety; AmbiguosSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BKDUHIDV","blogPost","2019","Davis, Ernie; Long, Robert","Conversation with Ernie Davis","AI Impacts","","","","https://aiimpacts.org/conversation-with-ernie-davis/","AI Impacts spoke with computer scientist Ernie Davis about his views of AI risk. With his permission, we have transcribed this interview. Participants Ernest Davis – professor of computer science at the Courant Institute of Mathematical Science, New York University Robert Long – AI Impacts Summary We spoke over the phone with Ernie Davis on...","2019-08-23","2022-01-30 04:49:19","2022-01-30 04:49:19","2020-11-14 03:21:12","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes","","/Users/jacquesthibodeau/Zotero/storage/XIGWJRHU/conversation-with-ernie-davis.html; /Users/jacquesthibodeau/Zotero/storage/5757QWMU/conversation-with-ernie-davis.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MH3BZG9F","blogPost","2021","Grace, Katja","Beyond fire alarms: freeing the groupstruck","AI Impacts","","","","https://aiimpacts.org/beyond-fire-alarms-freeing-the-groupstruck/","Fire alarms are the wrong way to think about the public AGI conversation.","2021-09-26","2022-01-30 04:49:19","2022-01-30 04:49:19","2021-11-18 23:46:19","","","","","","","Beyond fire alarms","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/PD5APM7U/beyond-fire-alarms-freeing-the-groupstruck.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HRMA7X54","blogPost","2020","Grace, Katja","Automated intelligence is not AI","AI Impacts","","","","https://aiimpacts.org/automated-intelligence-is-not-ai/","Katja Grace Sometimes we think of ‘artificial intelligence’ as whatever technology ultimately automates human cognitive labor...","2020-11-01","2022-01-30 04:49:19","2022-01-30 04:49:19","2020-11-21 20:33:46","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/NMHPZQPX/automated-intelligence-is-not-ai.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F5B4F2NG","blogPost","2020","Grace, Katja","Atari early","AI Impacts","","","","https://aiimpacts.org/atari-early/","Deepmind announced that their Agent57 beats the ‘human baseline’ at all 57 Atari games usually used as a benchmark. I think this is probably enough to resolve one of the predictions we had respondents make in our 2016 survey. Our question was when it would be feasible to ‘outperform professional game testers...","2020-04-01","2022-01-30 04:49:19","2022-01-30 04:49:19","2020-09-05 17:39:14","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog","","/Users/jacquesthibodeau/Zotero/storage/AWPUP66U/atari-early.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ISXUCTN","blogPost","2018","Mills, Justis","AGI-11 survey","AI Impacts","","","","https://aiimpacts.org/agi-11-survey/","The AGI-11 survey was a survey of 60 participants at the AGI-11 conference. In it: Nearly half of respondents believed that AGI would appear before 2030. Nearly 90% of respondents believed that AGI would appear before 2100. About 85% of respondents believed that AGI would be beneficial for humankind. Details James Barrat and Ben Goertzel...","2018-11-10","2022-01-30 04:49:19","2022-01-30 04:49:19","2020-11-14 03:22:14","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timeline Surveys","","/Users/jacquesthibodeau/Zotero/storage/UIJU2KS5/agi-11-survey.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SDJRTUC7","blogPost","2020","Bergal, Asya","2019 recent trends in Geekbench score per CPU price","AI Impacts","","","","https://aiimpacts.org/2019-recent-trends-in-geekbench-score-per-cpu-price/","From 2006 - 2020, Geekbench score per CPU price has grown by around 16% a year, for rates that would yield an order of magnitude over roughly 16 years. Details We looked at Geekbench 5, a benchmark for CPU performance. We combined Geekbench’s multi-core scores on its 'Processor Benchmarks' page with release dates and prices...","2020-04-14","2022-01-30 04:49:19","2022-01-30 04:49:19","2020-09-05 17:12:28","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Hardware and AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/88Z7BP58/2019-recent-trends-in-geekbench-score-per-cpu-price.html","","MetaSafety; AI-Impacts-NotFeatured","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J6GC98TM","blogPost","2020","AI Impacts","Time for AI to cross the human performance range in ImageNet image classification","AI Impacts","","","","https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification/","Computer image classification performance took 3 years to go from untrained human level to trained human level","2020-10-19","2022-01-30 04:49:04","2022-01-30 04:49:04","2020-11-21 20:36:48","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/PHMU6GH8/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification.html; /Users/jacquesthibodeau/Zotero/storage/T8WDT98T/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FD3UFI3X","blogPost","2015","AI Impacts","Research topic: Hardware, software and AI","AI Impacts","","","","https://aiimpacts.org/research-topic-hardware-software-and-ai/","This is the first in a sequence of articles outlining research which could help forecast AI development. Interpretation Concrete research projects are in boxes. ∑5 ∆8  means we guess the project will take (very) roughly five hours, and we rate its value (very) roughly 8/10. Most projects could be done to very different degrees of depth, or at very different scales. Our time cost...","2015-02-19","2022-01-30 04:49:04","2022-01-30 04:49:04","2020-12-18 19:06:28","","","","","","","Research topic","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/6VMRDPXR/research-topic-hardware-software-and-ai.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZDGQ8324","blogPost","2015","AI Impacts","Trends in the cost of computing","AI Impacts","","","","https://aiimpacts.org/trends-in-the-cost-of-computing/","Computing power available per dollar has probably increased by a factor of ten roughly every four years over the last quarter of a century (measured in FLOPS or MIPS). Over the past 6-8 years, the rate has been slower: around an order of magnitude every 10-16 years, measured in single precision theoretical peak FLOPS or Passmark's benchmark scores. Since...","2015-03-10","2022-01-30 04:49:04","2022-01-30 04:49:04","2020-12-18 19:05:42","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: 0000009  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/FTJZTW93/trends-in-the-cost-of-computing.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5MPTINM8","blogPost","2018","McCaslin, Tegan","Transmitting fibers in the brain: Total length and distribution of lengths","AI Impacts","","","","https://aiimpacts.org/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths/","The human brain’s approximately 86 billion neurons are probably connected by something like 850,000 km of axons and dendrites. Of this total, roughly 80% is short-range, local connections (averaging 680 microns in length), and approximately 20% is long-range, global connections in the form of myelinated fibers (likely averaging several centimeters in length). Background The brain’s...","2018-03-29","2022-01-30 04:49:04","2022-01-30 04:49:04","2020-12-13 23:22:39","","","","","","","Transmitting fibers in the brain","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/DWXT75KD/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths.html; /Users/jacquesthibodeau/Zotero/storage/K8XGXG6Q/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths.html","","MetaSafety; AmbiguosSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6RGUKGG5","blogPost","2015","AI Impacts","The cost of TEPS","AI Impacts","","","","https://aiimpacts.org/cost-of-teps/","A billion Traversed Edges Per Second (a GTEPS) can be bought for around $0.26/hour via a powerful supercomputer, including hardware and energy costs only. We do not know if GTEPS can be bought more cheaply elsewhere. We estimate that available TEPS/$ grows by a factor of ten every four years, based the relationship between TEPS and FLOPS. TEPS have not been...","2015-03-21","2022-01-30 04:49:04","2022-01-30 04:49:04","2020-12-18 19:04:55","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KFB3JVAP","blogPost","2020","Bergal, Asya","Surveys on fractional progress towards HLAI","AI Impacts","","","","https://aiimpacts.org/surveys-on-fractional-progress-towards-hlai/","How long until human-level performance, if we naively extrapolate progress since researchers joined their subfields?","2020-04-14","2022-01-30 04:49:04","2022-01-30 04:49:04","2020-09-05 17:08:05","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys","","/Users/jacquesthibodeau/Zotero/storage/AZVQBRB7/surveys-on-fractional-progress-towards-hlai.html; /Users/jacquesthibodeau/Zotero/storage/H8GGTM77/surveys-on-fractional-progress-towards-hlai.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C2DQQ6BM","blogPost","2016","AI Impacts","Returns to scale in research","AI Impacts","","","","https://aiimpacts.org/returns-to-scale-in-research/","When universities or university departments produce research outputs—such as published papers—they sometimes experience increasing returns to scale, sometimes constant returns to scale, and sometimes decreasing returns to scale. At the level of nations however, R&D tends to see increasing returns to scale. These results are preliminary. Background “Returns to scale” refers to the responsiveness of...","2016-07-06","2022-01-30 04:49:04","2022-01-30 04:49:04","2020-12-18 18:56:48","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/TR933CQ5/returns-to-scale-in-research.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZH6RHC67","blogPost","2020","Bergal, Asya","Resolutions of mathematical conjectures over time","AI Impacts","","","","https://aiimpacts.org/resolutions-of-mathematical-conjectures-over-time/","Conditioned on being remembered as a notable conjecture, the time-to-proof for a mathematical problem appears to be exponentially distributed with a half-life of about 100 years. However, these observations are likely to be distorted by various biases. Support In 2014, we found conjectures referenced on Wikipedia, and recorded the dates that they were proposed and...","2020-04-14","2022-01-30 04:49:04","2022-01-30 04:49:04","2020-09-05 17:10:04","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/6QIT4QFT/resolutions-of-mathematical-conjectures-over-time.html; /Users/jacquesthibodeau/Zotero/storage/HJKW6MVS/resolutions-of-mathematical-conjectures-over-time.html","","MetaSafety; AmbiguosSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U6J7QJB4","blogPost","2017","AI Impacts","Progress in general purpose factoring","AI Impacts","","","","https://aiimpacts.org/progress-in-general-purpose-factoring/","The largest number factored to date grew by about 4.5 decimal digits per year over the past roughly half-century. Between 1988, when we first have good records, and 2009, when the largest number to date was factored, progress was roughly 6 decimal digits per year. Progress was relatively smooth during the two decades for which we have good records, with half of...","2017-03-16","2022-01-30 04:49:04","2022-01-30 04:49:04","2020-12-18 18:51:25","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Inputs","","/Users/jacquesthibodeau/Zotero/storage/29ZRZ757/progress-in-general-purpose-factoring.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZV6ZEP7K","blogPost","2020","Korzekwa, Rick","Preliminary survey of prescient actions","AI Impacts","","","","https://aiimpacts.org/survey-of-prescient-actions/","In a 10-20 hour exploration, we did not find clear examples of 'prescient actions'—specific efforts to address severe and complex problems decades ahead of time and in the absence of broader scientific concern, experience with analogous problems, or feedback on the success of the effort—though we found six cases that may turn out to be...","2020-04-03","2022-01-30 04:49:04","2022-01-30 04:49:04","2020-09-05 17:13:16","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/2QCZB5HZ/survey-of-prescient-actions.html; /Users/jacquesthibodeau/Zotero/storage/3FEHV8RT/survey-of-prescient-actions.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKDHRC3V","blogPost","2015","AI Impacts","List of Analyses of Time to Human-Level AI","AI Impacts","","","","https://aiimpacts.org/list-of-analyses-of-time-to-human-level-ai/","This is a list of most of the substantial analyses of AI timelines that we know of. It also covers most of the arguments and opinions of which we are aware. Details The list below contains substantial publically available analyses of when human-level AI will appear. To qualify for the list, an item must provide both a claim...","2015-01-22","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:07:58","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/JMI38J5D/list-of-analyses-of-time-to-human-level-ai.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WEQ27ARZ","blogPost","2019","McCaslin, Tegan","Investigation into the relationship between neuron count and intelligence across differing cortical architectures","AI Impacts","","","","https://aiimpacts.org/investigation-into-the-relationship-between-neuron-count-and-intelligence-across-differing-cortical-architectures/","","2019","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-14","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/FUXE4PED/investigation-into-the-relationship-between-neuron-count-and-intelligence-across-differing-cort.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JK5CBDEJ","blogPost","2019","AI Impacts","Historical economic growth trends","AI Impacts","","","","https://aiimpacts.org/historical-growth-trends/","An analysis of historical growth supports the possibility of radical increases in growth rate. Naive extrapolation of long-term trends would suggest massive increases in growth rate over the coming century, although growth over the last half-century has lagged very significantly behind these long-term trends. Support Bradford DeLong has published estimates for historical world GDP, piecing together...","2019-03-06","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-13 23:57:45","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/295DBKF8/historical-growth-trends.html; /Users/jacquesthibodeau/Zotero/storage/6QQFGPWX/historical-growth-trends.html","","MetaSafety; AmbiguosSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X3DQBHIS","blogPost","2014","AI Impacts","Effect of nuclear weapons on historic trends in explosives","AI Impacts","","","","https://aiimpacts.org/discontinuity-from-nuclear-weapons/","Nuclear weapons constituted a ~7 thousand year discontinuity in relative effectiveness factor (TNT equivalent per kg of explosive). Nuclear weapons do not appear to have clearly represented progress in the cost-effectiveness of explosives, though the evidence there is weak. Details This case study is part of AI Impacts’ discontinuous progress investigation. Background The development of nuclear...","2014-12-31","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:09:55","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Continuity of progress","","/Users/jacquesthibodeau/Zotero/storage/BXQBM3XU/discontinuity-from-nuclear-weapons.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ID37HU49","blogPost","2016","Wulfsohn, Michael","Costs of extinction risk mitigation","AI Impacts","","","","https://aiimpacts.org/costs-of-extinction-risk-mitigation/","We very roughly estimate that the annual cost of reducing the probability of human extinction by 0.01% is within the range of $1.1 billion to $3.5 trillion. Introduction This article is intended to be usable in a Cost-Benefit Analysis (CBA) analysis of extinction risk mitigation. It explores the costs of such efforts. A corresponding article...","2016-08-04","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 18:55:48","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Evaluation","","/Users/jacquesthibodeau/Zotero/storage/B7NXCJJP/costs-of-extinction-risk-mitigation.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R88INF7A","blogPost","2014","AI Impacts","Cases of Discontinuous Technological Progress","AI Impacts","","","","https://aiimpacts.org/cases-of-discontinuous-technological-progress/","We know of ten events which produced a robust discontinuity in progress equivalent to more than one hundred years at previous rates in some interesting metric. We know of 53 other events which produced smaller or less robust discontinuities. Background These cases were researched as part of our discontinuous progress investigation. List of cases Events...","2014-12-31","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:09:14","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/2I97PIUR/cases-of-discontinuous-technological-progress.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6BMGVAKN","blogPost","2015","AI Impacts","AI Timeline Surveys","AI Impacts","","","","https://aiimpacts.org/ai-timeline-surveys/","[This page is out of date and will be updated soon. It does not reflect all surveys known and documented by AI Impacts.] We know of thirteen surveys on the predicted timing of human-level AI. If we collapse a few slightly different meanings of 'human-level AI', then: Median estimates for when there will be a 10% chance of human-level AI are...","2015-01-10","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:08:31","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys","","/Users/jacquesthibodeau/Zotero/storage/RNANISVC/ai-timeline-surveys.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R53RXIT7","blogPost","2015","AI Impacts","MIRI AI Predictions Dataset","AI Impacts","","","","https://aiimpacts.org/miri-ai-predictions-dataset/","The MIRI AI predictions dataset is a collection of public predictions about human-level AI timelines. We edited the original dataset, as described below. Our dataset is available here, and the original here. Interesting features of the dataset include: The median dates at which people's predictions suggest AI is less likely than not and more likely than not are...","2015-05-20","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:03:59","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/HXM7IC2U/miri-ai-predictions-dataset.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DXPWI4DP","blogPost","2015","AI Impacts","List of multipolar research projects","AI Impacts","","","","https://aiimpacts.org/multipolar-research-projects/","This list currently consists of research projects suggested at the Multipolar AI workshop we held on January 26 2015. Relatively concrete projects are marked [concrete]. These are more likely to already include specific questions to answer and feasible methods to answer them with. Other 'projects' are more like open questions, or broad directions for inquiry. Projects are divided into three sections: Paths to multipolar scenarios What...","2015-02-11","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:06:54","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/2FV59JK6/multipolar-research-projects.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"557XUX37","blogPost","2014","AI Impacts","Human-Level AI","AI Impacts","","","","https://aiimpacts.org/human-level-ai/","'Human-level AI' refers to AI which can reproduce everything a human can do, approximately. Several variants of this concept are worth distinguishing. Details Variations in the meaning of 'human-level AI' Considerations in specifying 'human-level AI' more precisely: Do we mean to imply anything about running costs? Is an AI that reproduces human behavior for ten billion dollars per year 'human-level',...","2014-01-23","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:10:54","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Clarifying concepts","","/Users/jacquesthibodeau/Zotero/storage/V3S9A9CP/human-level-ai.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NUG5E483","blogPost","2020","Fernandez, Ronny","How energy efficient are human-engineered flight designs relative to natural ones?","AI Impacts","","","","https://aiimpacts.org/are-human-engineered-flight-designs-better-or-worse-than-natural-ones/","Nature is responsible for the most energy efficient flight, according to an investigation of albatrosses, butterflies and nine different human-engineered flying machines.","2020-12-10","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 18:30:29","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Evolution engineering comparison","","","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XS42FABG","blogPost","2014","AI Impacts","Hanson AI Expert Survey","AI Impacts","","","","https://aiimpacts.org/hanson-ai-expert-survey/","In a small informal survey running since 2012, AI researchers generally estimated that their subfields have moved less than ten percent of the way to human-level intelligence. Only one (in the slowest moving subfield) observed acceleration. This suggests on a simple extrapolation that reaching human-level capability across subfields will take over a century (in contrast with many other...","2014-12-29","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:10:27","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys","","/Users/jacquesthibodeau/Zotero/storage/JSVEBI48/hanson-ai-expert-survey.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ZPZ6KTC","blogPost","2021","AI Impacts","Fiction relevant to AI futurism","AI Impacts","","","","https://aiimpacts.org/partially-plausible-fictional-ai-futures/","A list of stories potentially relevant to thinking about the development of advanced AI, including both those intended as futurism and those intended as entertainment.","2021-04-12","2022-01-30 04:49:03","2022-01-30 04:49:03","2021-10-30 16:38:18","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/IKURBDT5/partially-plausible-fictional-ai-futures.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z83BXIK7","blogPost","2016","AI Impacts","Examples of early action on risks","AI Impacts","","","","https://aiimpacts.org/examples-of-early-action-on-a-risk/","Details Discussion There are many current efforts to mitigate risks from artificial intelligence. We might learn something about the likelihood of these efforts influencing AI risk by looking at similar past efforts. To this end, we are interested here in past risk mitigation efforts that have the following characteristics (taken from this paper contributing to the same project (p5) and...","2016-08-16","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 18:55:26","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Control","","/Users/jacquesthibodeau/Zotero/storage/C6A4Q39T/examples-of-early-action-on-a-risk.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QKDDT8NV","blogPost","2019","Kokotajlo, Daniel","Evidence on good forecasting practices from the Good Judgment Project","AI Impacts","","","","https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project/","According to experience and data from the Good Judgment Project, the following are associated with successful forecasting, in rough decreasing order of combined importance and confidence: Past performance in the same broad domain Making more predictions on the same question Deliberation time Collaboration on teams Intelligence Domain expertise Having taken a one-hour training module on...","2019-02-07","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-11-14 03:21:44","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/AKFKIG5V/evidence-on-good-forecasting-practices-from-the-good-judgment-project.html; /Users/jacquesthibodeau/Zotero/storage/44WBB3QC/evidence-on-good-forecasting-practices-from-the-good-judgment-project.html","","MetaSafety; AmbiguosSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"798WSB6H","blogPost","2019","Long, Robert; Bergal, Asya","Evidence against current methods leading to human level artificial intelligence","AI Impacts","","","","https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/","This is a list of published arguments that we know of that current methods in artificial intelligence will not lead to human-level AI. Details Clarifications We take 'current methods' to mean techniques for engineering artificial intelligence that are already known, involving no “qualitatively new ideas”. We have not precisely defined 'current methods'. Many of the...","2019-08-12","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-14 23:33:42","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/WI59RSHC/evidence-against-current-methods-leading-to-human-level-artificial-intelligence.html; /Users/jacquesthibodeau/Zotero/storage/I2J57E3V/evidence-against-current-methods-leading-to-human-level-artificial-intelligence.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9RGZGX6U","blogPost","2015","AI Impacts","Discontinuous progress investigation","AI Impacts","","","","https://aiimpacts.org/discontinuous-progress-investigation/","Published Feb 2, 2015; last updated April 12 2020 We have collected cases of discontinuous technological progress to inform our understanding of whether artificial intelligence performance is likely to undergo such a discontinuity. This page details our investigation. We know of ten events that produced a robust discontinuity in progress equivalent to more than a century...","2015-02-02","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:07:26","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/BGNFTNE2/discontinuous-progress-investigation.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9Q4KHV7Q","blogPost","2015","AI Impacts","Costs of human-level hardware","AI Impacts","","","","https://aiimpacts.org/costs-of-human-level-hardware/","Computing hardware which is equivalent to the brain - in terms of FLOPS probably costs between $1 x 105 and $3 x 1016, or $2/hour-$700bn/hour. in terms of TEPS probably costs $200M - $7B, or or $4,700 – $170,000/hour (including energy costs in the hourly rate). in terms of secondary memory probably costs $300-3,000, or $0.007-$0.07/hour. Details Partial costs...","2015-07-26","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 18:59:32","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/64B2PHMG/costs-of-human-level-hardware.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZAUDQANV","blogPost","2016","AI Impacts","Coordinated human action as example of superhuman intelligence","AI Impacts","","","","https://aiimpacts.org/coordinated-human-action-example-superhuman-intelligence/","Collections of humans organized into groups and institutions provide many historical examples of the creation and attempted control of intelligences that routinely outperform individual humans. A preliminary look at the available evidence suggests that individuals are often cognitively outperformed in head-to-head competition with groups of similar average intelligence. This article surveys considerations relevant to the...","2016-01-21","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 18:57:29","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Control","","/Users/jacquesthibodeau/Zotero/storage/9Q4E9ZW2/coordinated-human-action-example-superhuman-intelligence.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N387UVHE","blogPost","2016","Griffiths, Tom; Adamson, Finan","Conversation with Tom Griffiths","AI Impacts","","","","https://aiimpacts.org/conversation-with-tom-griffiths/","Participants Professor Tom Griffiths, ­ Director of the Computational Cognitive Science Lab and the Institute of Cognitive and Brain Sciences at the University of California, Berkeley. Finan Adamson, ­ AI Impacts. Note: These notes were compiled by AI impacts and give an overview of the major points made by Professor Tom Griffiths. They are available...","2016-09-08","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 18:54:17","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/9BKW84NI/conversation-with-tom-griffiths.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VXNKZP2R","blogPost","2015","Potter, Steve; Grace, Katja","Conversation with Steve Potter","AI Impacts","","","","https://aiimpacts.org/conversation-with-steve-potter/","Posted 13 July 2015 Participants Professor Steve Potter – Associate Professor, Laboratory of NeuroEngineering, Coulter Department of Biomedical Engineering, Georgia Institute of Technology Katja Grace – Machine Intelligence Research Institute (MIRI) Note: These notes were compiled by MIRI and give an overview of the major points made by Professor Steve Potter. Summary Katja Grace spoke...","2015-07-13","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:00:09","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/VW3X2BWF/conversation-with-steve-potter.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UW2DZD9A","blogPost","2015","AI Impacts","Brain performance in TEPS","AI Impacts","","","","https://aiimpacts.org/brain-performance-in-teps/","Traversed Edges Per Second (TEPS) is a benchmark for measuring a computer's ability to communicate information internally. Given several assumptions, we can also estimate the human brain's communication performance in terms of TEPS, and use this to meaningfully compare brains to computers. We estimate that (given these assumptions) the human brain performs around  0.18 - 6.4 *...","2015-05-06","2022-01-30 04:49:03","2022-01-30 04:49:03","2020-12-18 19:04:30","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/SDDXHFH4/brain-performance-in-teps.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RZZWAGZ9","blogPost","2021","AI Impacts","AI Vignettes Project","AI Impacts","","","","https://aiimpacts.org/ai-vignettes-project/","The AI Vignettes Project is an ongoing effort to write concrete plausible future histories of AI development and its social impacts. Details Purposes We hope to: Check that abstract views about the future of AI have plausible concrete instantiations. (Especially, hypothesized extinction scenarios, and proposed safe scenarios.)Develop better intuitions about possible scenarios by thinking through...","2021-10-12","2022-01-30 04:49:03","2022-01-30 04:49:03","2021-10-30 16:33:40","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/WSPITJEN/ai-vignettes-project.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KII8U9NV","blogPost","2015","AI Impacts","AI Impacts research bounties","AI Impacts","","","","https://aiimpacts.org/ai-impacts-research-bounties/","We are offering rewards for several inputs to our research, described below. These offers have no specific deadline except where noted. We may modify them or take them down, but will give at least one week's notice here unless there is strong reason not to. To submit an entry, email katja@intelligence.org. There is currently a large backlog of entries to...","2015-08-06","2022-01-30 04:49:02","2022-01-30 04:49:02","2020-12-18 18:58:51","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/4UIJK9K6/ai-impacts-research-bounties.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VZKUDNZD","blogPost","2020","Bergal, Asya","2019 recent trends in GPU price per FLOPS","AI Impacts","","","","https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/","We estimate that in recent years, GPU prices have fallen at rates that would yield an order of magnitude over roughly: 17 years for single-precision FLOPS10 years for half-precision FLOPS5 years for half-precision fused multiply-add FLOPS Details GPUs (graphics processing units) are specialized electronic circuits originally used for computer graphics. In recent years, they have...","2020-03-25","2022-01-30 04:49:02","2022-01-30 04:49:02","2020-09-05 18:37:02","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/WBT5HR2S/2019-recent-trends-in-gpu-price-per-flops.html; /Users/jacquesthibodeau/Zotero/storage/NC8ARCAU/2019-recent-trends-in-gpu-price-per-flops.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BF7984KG","blogPost","2016","Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain","2016 Expert Survey on Progress in AI","AI Impacts","","","","https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/","Published June 2016; last substantial update before Oct 2017 The 2016 Expert Survey on Progress in AI is a survey of machine learning researchers that Katja Grace and John Salvatier of AI Impacts ran in collaboration with Allan Dafoe, Baobao Zhang, and Owain Evans in 2016. Details Some survey results are reported in When Will...","2016-12-14","2022-01-30 04:49:02","2022-01-30 04:49:02","2020-12-18 18:52:20","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys","","/Users/jacquesthibodeau/Zotero/storage/624ZXBKR/2016-expert-survey-on-progress-in-ai.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUDJM3MC","blogPost","2015","AI Impacts","AI Risk Terminology","AI Impacts","","","","https://aiimpacts.org/ai-risk-terminology/","AI timeline - an expectation about how much time will lapse before important AI events, especially the advent of human-level AI or a similar milestone. The term can also refer to the actual periods of time (which are not yet known), rather than an expectation about them. Artificial General Intelligence (also, AGI) - the intelligence of a machine that could successfully...","2015-10-30","2022-01-30 04:49:02","2022-01-30 04:49:02","2020-12-18 18:58:20","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles","","/Users/jacquesthibodeau/Zotero/storage/VNQR5F2J/ai-risk-terminology.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RAXPGCP3","blogPost","2015","AI Impacts","Accuracy of AI Predictions","AI Impacts","","","","https://aiimpacts.org/accuracy-of-ai-predictions/","Updated 4 June 2015 It is unclear how informative we should expect expert predictions about AI timelines to be. Individual predictions are undoubtedly often off by many decades, since they disagree with each other. However their aggregate may still be quite informative. The main potential reason we know of to doubt the accuracy of expert predictions is that experts are generally poor predictors in many areas, and...","2015-06-04","2022-01-30 04:49:02","2022-01-30 04:49:02","2020-12-18 19:02:01","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: Accuracy of AI Predictions","","/Users/jacquesthibodeau/Zotero/storage/EVEQ3P8B/accuracy-of-ai-predictions.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G95WAZZB","blogPost","2017","AI Impacts","2017 trend in the cost of computing","AI Impacts","","","","https://aiimpacts.org/recent-trend-in-the-cost-of-computing/","The cheapest hardware prices (for single precision FLOPS/$) appear to be falling by around an order of magnitude every 10-16 years. This rate is slower than the trend of FLOPS/$ observed over the past quarter century, which was an order of magnitude every 4 years. There is no particular sign of slowing between 2011 and 2017....","2017-11-11","2022-01-30 04:49:02","2022-01-30 04:49:02","2020-12-18 18:50:38","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines","","/Users/jacquesthibodeau/Zotero/storage/3WDJKPFI/recent-trend-in-the-cost-of-computing.html","","MetaSafety; AI-Impacts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D2PUDC8S","journalArticle","2017","Lin, Henry W.; Tegmark, Max; Rolnick, David","Why Does Deep and Cheap Learning Work So Well?","Journal of Statistical Physics","","0022-4715, 1572-9613","10.1007/s10955-017-1836-5","http://link.springer.com/10.1007/s10955-017-1836-5","","2017-09","2022-01-30 04:48:55","2022-01-30 04:48:55","2021-11-13 22:54:48","1223-1247","","6","168","","J Stat Phys","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000533","","/Users/jacquesthibodeau/Zotero/storage/4PBUWPQI/Lin et al. - 2017 - Why Does Deep and Cheap Learning Work So Well.pdf","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"768PN8FI","blogPost","2020","Partnership on AI","What the AI Community Can Learn From Sneezing Ferrets and a Mutant Virus Debate","AI&.","","","","https://medium.com/partnership-on-ai/lessons-for-the-ai-community-from-the-h5n1-controversy-32432438a82e","Lessons on publication norms for the AI community from biosecurity","2020-12-09","2022-01-30 04:48:55","2022-01-30 04:48:55","2021-11-18 23:28:05","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QWKPVW7U/lessons-for-the-ai-community-from-the-h5n1-controversy-32432438a82e.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6C9TQWFX","conferencePaper","2019","Tangkaratt, Voot; Han, Bo; Khan, Mohammad Emtiyaz; Sugiyama, Masashi","VILD: Variational Imitation Learning with Diverse-quality Demonstrations","Proceedings of the 37th International Conference on Machine Learning","","","","http://arxiv.org/abs/1909.06769","The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations. However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. IL in such situations can be challenging, especially when the level of demonstrators' expertise is unknown. We propose a new IL method called \underline{v}ariational \underline{i}mitation \underline{l}earning with \underline{d}iverse-quality demonstrations (VILD), where we explicitly model the level of demonstrators' expertise with a probabilistic graphical model and estimate it along with a reward function. We show that a naive approach to estimation is not suitable to large state and action spaces, and fix its issues by using a variational approach which can be easily implemented using existing reinforcement learning methods. Experiments on continuous-control benchmarks demonstrate that VILD outperforms state-of-the-art methods. Our work enables scalable and data-efficient IL under more realistic settings than before.","2019-09-15","2022-01-30 04:48:55","2022-01-30 04:48:55","2021-11-18 23:17:53","","","","","","","VILD","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 1909.06769","","/Users/jacquesthibodeau/Zotero/storage/KR4JBV4S/Tangkaratt et al. - 2019 - VILD Variational Imitation Learning with Diverse-.pdf; /Users/jacquesthibodeau/Zotero/storage/KZEUIWTZ/1909.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML 2020","","","","","","","","","","","","","","",""
"GPKVQEG4","conferencePaper","2021","Hunt, Nathan; Fulton, Nathan; Magliacane, Sara; Hoang, Nghia; Das, Subhro; Solar-Lezama, Armando","Verifiably Safe Exploration for End-to-End Reinforcement Learning","HSCC '21: Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control","","","10.1145/3447928.3456653","http://arxiv.org/abs/2007.01223","Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We also prove that our method of enforcing the safety constraints preserves all safe policies from the original environment.","2021-05","2022-01-30 04:48:55","2022-01-30 04:48:55","2021-11-09 12:10:05","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000002  arXiv: 2007.01223","","/Users/jacquesthibodeau/Zotero/storage/NHP43TJ9/Hunt et al. - 2020 - Verifiably Safe Exploration for End-to-End Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/VK2PPGIX/2007.html","","UnsortedSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.8; Computer Science - Logic in Computer Science; F.3.1","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","HSCC 2021","","","","","","","","","","","","","","",""
"4AMXU6SC","manuscript","2020","Andrychowicz, Marcin; Raichuk, Anton; Stańczyk, Piotr; Orsini, Manu; Girgin, Sertan; Marinier, Raphael; Hussenot, Léonard; Geist, Matthieu; Pietquin, Olivier; Michalski, Marcin; Gelly, Sylvain; Bachem, Olivier","What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study","","","","","http://arxiv.org/abs/2006.05990","In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.","2020-06-10","2022-01-30 04:48:55","2022-01-30 04:48:55","2021-11-07 22:59:39","","","","","","","What Matters In On-Policy Reinforcement Learning?","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 44  arXiv: 2006.05990","","/Users/jacquesthibodeau/Zotero/storage/PMZ4SVSJ/Andrychowicz et al. - 2020 - What Matters In On-Policy Reinforcement Learning .pdf","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FN5EFZJ8","manuscript","2020","D'Amour, Alexander; Heller, Katherine; Moldovan, Dan; Adlam, Ben; Alipanahi, Babak; Beutel, Alex; Chen, Christina; Deaton, Jonathan; Eisenstein, Jacob; Hoffman, Matthew D.; Hormozdiari, Farhad; Houlsby, Neil; Hou, Shaobo; Jerfel, Ghassen; Karthikesalingam, Alan; Lucic, Mario; Ma, Yian; McLean, Cory; Mincu, Diana; Mitani, Akinori; Montanari, Andrea; Nado, Zachary; Natarajan, Vivek; Nielson, Christopher; Osborne, Thomas F.; Raman, Rajiv; Ramasamy, Kim; Sayres, Rory; Schrouff, Jessica; Seneviratne, Martin; Sequeira, Shannon; Suresh, Harini; Veitch, Victor; Vladymyrov, Max; Wang, Xuezhi; Webster, Kellie; Yadlowsky, Steve; Yun, Taedong; Zhai, Xiaohua; Sculley, D.","Underspecification Presents Challenges for Credibility in Modern Machine Learning","","","","","http://arxiv.org/abs/2011.03395","ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.","2020-11-24","2022-01-30 04:48:54","2022-01-30 04:48:54","2021-11-13 19:45:01","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s0]  ACC: 163  arXiv: 2011.03395","","/Users/jacquesthibodeau/Zotero/storage/X3D7X8IC/D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf; /Users/jacquesthibodeau/Zotero/storage/VP4HBEMV/2011.html","","UnsortedSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SFMIH4S","blogPost","2020","Tian, Yonglong","Understanding View Selection for Contrastive Learning","Google AI Blog","","","","http://ai.googleblog.com/2020/08/understanding-view-selection-for.html","Posted by Yonglong Tian, Student Researcher and Chen Sun, Staff Research Scientist, Google Research    Most people take for granted the abil...","2020-08-21","2022-01-30 04:48:54","2022-01-30 04:48:54","2021-11-07 16:20:33","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/RFIP9DCC/understanding-view-selection-for.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M4GP2KT9","conferencePaper","2021","Jia, Feiran; Mate, Aditya; Li, Zun; Jabbari, Shahin; Chakraborty, Mithun; Tambe, Milind; Wellman, Michael; Vorobeychik, Yevgeniy","A Game-Theoretic Approach for Hierarchical Policy-Making","arXiv:2102.10646 [cs]","","","","http://arxiv.org/abs/2102.10646","We present the design and analysis of a multi-level game-theoretic model of hierarchical policy-making, inspired by policy responses to the COVID-19 pandemic. Our model captures the potentially mismatched priorities among a hierarchy of policy-makers (e.g., federal, state, and local governments) with respect to two main cost components that have opposite dependence on the policy strength, such as post-intervention infection rates and the cost of policy implementation. Our model further includes a crucial third factor in decisions: a cost of non-compliance with the policy-maker immediately above in the hierarchy, such as non-compliance of state with federal policies. Our first contribution is a closed-form approximation of a recently published agent-based model to compute the number of infections for any implemented policy. Second, we present a novel equilibrium selection criterion that addresses common issues with equilibrium multiplicity in our setting. Third, we propose a hierarchical algorithm based on best response dynamics for computing an approximate equilibrium of the hierarchical policy-making game consistent with our solution concept. Finally, we present an empirical investigation of equilibrium policy strategies in this game in terms of the extent of free riding as well as fairness in the distribution of costs depending on game parameters such as the degree of centralization and disagreements about policy priorities among the agents.","2021-02-21","2022-01-30 04:50:42","2022-01-30 04:50:42","2021-10-30 21:50:53","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 2102.10646","","/Users/jacquesthibodeau/Zotero/storage/C28KUI7S/Jia et al. - 2021 - A Game-Theoretic Approach for Hierarchical Policy-.pdf; /Users/jacquesthibodeau/Zotero/storage/3CD6QZFT/2102.html","","MetaSafety; AmbiguousSafety","Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2nd International (Virtual) Workshop on Autonomous Agents for Social Good (AASG 2021)","","","","","","","","","","","","","","",""
"JXVPJ3PB","journalArticle","2017","Garrabrant, Scott; Benson-Tilsen, Tsvi; Critch, Andrew; Soares, Nate; Taylor, Jessica","A Formal Approach to the Problem of Logical Non-Omniscience","Electronic Proceedings in Theoretical Computer Science","","2075-2180","10.4204/EPTCS.251.16","http://arxiv.org/abs/1707.08747","We present the logical induction criterion for computable algorithms that assign probabilities to every logical statement in a given formal language, and refine those probabilities over time. The criterion is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence phi is associated with a stock that is worth $1 per share if phi is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where pt_N(phi)=50% means that on day N, shares of phi may be bought or sold from the reasoner for 50%. A market is then called a logical inductor if (very roughly) there is no polynomial-time computable trading strategy with finite risk tolerance that earns unbounded profits in that market over time. We then describe how this single criterion implies a number of desirable properties of bounded reasoners; for example, logical inductors outpace their underlying deductive process, perform universal empirical induction given enough time to think, and place strong trust in their own reasoning process.","2017-07-25","2022-01-30 04:50:42","2022-01-30 04:50:42","2019-05-05 21:13:06","221-235","","","251","","Electron. Proc. Theor. Comput. Sci.","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000008  arXiv: 1707.08747","","/Users/jacquesthibodeau/Zotero/storage/IBNV8E9I/Garrabrant et al. - 2017 - A Formal Approach to the Problem of Logical Non-Om.pdf; /Users/jacquesthibodeau/Zotero/storage/NIV483PR/1707.html; /Users/jacquesthibodeau/Zotero/storage/BA42R2SV/1707.html","","CHAI; TechSafety; MIRI","Computer Science - Logic in Computer Science; F.4.0; G.3","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MGJKQ2I","blogPost","2020","Shah, Rohin","AI Alignment 2018-19 Review","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review","PREAMBLE WHAT THIS POST IS This is a review post of public work in AI alignment over 2019, with some inclusions from 2018. It has this preamble (~700 words), a short version / summary (~1.6k words), and a long version (~8.3k words). It is available as a Google Doc here. There are many areas of work that are relevant to AI alignment that I have barely touched on, such as interpretability, uncertainty estimation, adversarial examples, and assured autonomy, primarily because I have not been following these fields and wouldn’t be able to write a good summary of what has happened in them. I have also mostly focused on articles that provide some conceptual insight, and excluded or briefly linked to papers that primarily make quantitative improvements on important metrics. While such papers are obviously important (ultimately, our techniques need to work well), there isn’t much to say about them in a yearly review other than that the quantitative metric was improved. Despite these exclusions, there was still a ton of work to select from, perhaps around ~500 articles, of which over 300 have been linked to in this post. There are many interesting articles that I really enjoyed that get only a sentence of description, in which I ignore many of the points that the article makes. Most have been summarized in the Alignment Newsletter, so if you’d like to learn more about any particular link, but don’t want to read the entire thing, just search for its title in the database. WHAT YOU SHOULD KNOW ABOUT THE STRUCTURE OF THIS POST I am not speaking for myself; by default I am trying to explain what has been said, in a way that the authors of the articles would agree with. Any extra opinion that I add will be in italics. As a post, this is meant to be read sequentially, but the underlying structure is a graph (nodes are posts, edges connect posts that are very related). I arranged it in a sequence that highlights the most salient-to-me connections. This means that the order in wh","2020","2022-01-30 04:50:42","2022-01-30 04:50:42","2020-12-18 00:14:21","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/SESAVJBI/ai-alignment-2018-19-review.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J95SGZ5U","conferencePaper","2018","Smitha Milli; Lieder, Falk; Griffiths, Thomas L","A Rational Reinterpretation of Dual-Process Theories","","","","10.13140/rg.2.2.14956.46722/1","http://rgdoi.net/10.13140/RG.2.2.14956.46722/1","Highly inﬂuential “dual-process"" accounts of human cognition postulate the coexistence of a slow accurate system with a fast error-prone system. But why would there be just two systems rather than, say, one or 93? Here, we argue that a dual-process architecture might be neither arbitrary nor irrational, but might instead reﬂect a rational tradeoff between the cognitive ﬂexibility afforded by multiple systems and the time and effort required to choose between them. We investigate what the optimal set and number of cognitive systems would be depending on the structure of the environment. We ﬁnd that the optimal number of systems depends on the variability of the environment and the difﬁculty of deciding when which system should be used. Furthermore, when having two systems is optimal, then the ﬁrst system is fast but error-prone and the second system is slow but accurate. Our ﬁndings thereby provide a rational reinterpretation of dual-process theories.","2018","2022-01-30 04:50:42","2022-01-30 04:50:42","2019-07-08 15:59:57","","","","","","","","","","","","","","en","","","","","DOI.org (Datacite)","","ZSCC: 0000003[s2]","","","","CHAI; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Thirty-First AAAI Conference on Artiﬁcial Intelligence","","","","","","","","","","","","","","",""
"JSDBTHXD","conferencePaper","2019","Bansal, Somil; Bajcsy, Andrea; Ratner, Ellis; Dragan, Anca D.; Tomlin, Claire J.","A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning","2020 IEEE International Conference on Robotics and Automation (ICRA)","","","","http://arxiv.org/abs/1910.13369","Real-world autonomous systems often employ probabilistic predictive models of human behavior during planning to reason about their future motion. Since accurately modeling the human behavior a priori is challenging, such models are often parameterized, enabling the robot to adapt predictions based on observations by maintaining a distribution over the model parameters. This leads to a probabilistic prediction problem, which even though attractive, can be computationally demanding. In this work, we formalize the prediction problem as a stochastic reachability problem in the joint state space of the human and the belief over the model parameters. We further introduce a Hamilton-Jacobi reachability framework which casts a deterministic approximation of this stochastic reachability problem by restricting the allowable actions to a set rather than a distribution, while still maintaining the belief as an explicit state. This leads to two advantages: our approach gives rise to a novel predictor wherein the predictions can be performed at a significantly lower computational expense, and to a general framework which also enables us to perform predictor analysis. We compare our approach to a fully stochastic predictor using Bayesian inference and the worst-case forward reachable set in simulation and in hardware, and demonstrate how it can enable robust planning while not being overly conservative, even when the human model is inaccurate.","2019-10-29","2022-01-30 04:50:42","2022-01-30 04:50:42","2019-12-18 02:35:08","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003[s0]  arXiv: 1910.13369","","/Users/jacquesthibodeau/Zotero/storage/XESDVH7I/Bansal et al. - 2019 - A Hamilton-Jacobi Reachability-Based Framework for.pdf; /Users/jacquesthibodeau/Zotero/storage/NAM8GA7X/1910.html","","CHAI; TechSafety; AmbiguosSafety","Computer Science - Machine Learning; Computer Science - Robotics; Electrical Engineering and Systems Science - Systems and Control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 IEEE International Conference on Robotics and Automation (ICRA)","","","","","","","","","","","","","","",""
"QAGKMBNQ","bookSection","2016","Ó hÉigeartaigh, Seán","Would You Hand Over a Decision to a Machine?","Philosophers Take On the World","","","","https://papers.ssrn.com/abstract=3446679","Artificial intelligence (AI) will be used in many decision-making contexts, both as a decision aide and to replace human decision-making. These include what might traditionally be considered moral decisions. This chapter explores risks and opportunities posed by the use of AI in moral decision-making.","2016-09-26","2022-01-30 04:50:26","2022-01-30 04:50:26","2020-12-12 17:56:19","","","","","","","","","","","","Oxford University Press","","en","","","","","papers.ssrn.com","","ZSCC: NoCitationData[s2]  ACC: 0","","/Users/jacquesthibodeau/Zotero/storage/MDPUV3RQ/Ó hÉigeartaigh - 2016 - Would You Hand Over a Decision to a Machine.pdf; /Users/jacquesthibodeau/Zotero/storage/QSSZ7NDS/papers.html","","MetaSafety; CFI; CSER; AmbiguosSafety","AI; Artificial intelligence; bias; decision-making; risk; uncertainty","Edmonds, D","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M6UT95U9","report","2019","Kemp, Luke; Cihon, Peter; Maas, Matthijs M; Belfield, Haydn; Ó hÉigeartaigh, Seán; Leung, Jade; Cremer, Zoe","UN High-level Panel on Digital Cooperation: A Proposal for International AI Governance","","","","","https://digitalcooperation.org/wp-content/uploads/2019/02/Luke_Kemp_Submission-to-the-UN-High-Level-Panel-on-Digital-Cooperation-2019-Kemp-et-al.pdf","","2019","2022-01-30 04:50:26","2022-01-30 04:50:26","2020-12-12","","","","","","","","","","","","Centre for the Study of Existential Risk and Leverhulme Centre for the Future of Intelligence","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: 3","","/Users/jacquesthibodeau/Zotero/storage/CP3XZSMU/Kemp et al. - 2019 - UN High-level Panel on Digital Cooperation A Prop.pdf","","MetaSafety; CFI; CSER; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H6CSHN6V","manuscript","2020","Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew; Bluemke, Emma; Lebensold, Jonathan; O'Keefe, Cullen; Koren, Mark; Ryffel, Théo; Rubinovitz, J. B.; Besiroglu, Tamay; Carugati, Federica; Clark, Jack; Eckersley, Peter; de Haas, Sarah; Johnson, Maritza; Laurie, Ben; Ingerman, Alex; Krawczuk, Igor; Askell, Amanda; Cammarota, Rosario; Lohn, Andrew; Krueger, David; Stix, Charlotte; Henderson, Peter; Graham, Logan; Prunkl, Carina; Martin, Bianca; Seger, Elizabeth; Zilberman, Noa; hÉigeartaigh, Seán Ó; Kroeger, Frens; Sastry, Girish; Kagan, Rebecca; Weller, Adrian; Tse, Brian; Barnes, Elizabeth; Dafoe, Allan; Scharre, Paul; Herbert-Voss, Ariel; Rasser, Martijn; Sodhani, Shagun; Flynn, Carrick; Gilbert, Thomas Krendl; Dyer, Lisa; Khan, Saif; Bengio, Yoshua; Anderljung, Markus","Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims","","","","","http://arxiv.org/abs/2004.07213","With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.","2020-04-20","2022-01-30 04:50:26","2022-01-30 04:50:26","2020-08-18 21:36:21","","","","","","","Toward Trustworthy AI Development","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 92  arXiv: 2004.07213","","/Users/jacquesthibodeau/Zotero/storage/FW8SRSKZ/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf; /Users/jacquesthibodeau/Zotero/storage/UMFHWWMX/2004.html; /Users/jacquesthibodeau/Zotero/storage/KFTMFAHE/2004.html","","MetaSafety; CHAI; CFI; CSER; CSET; FHI; Open-AI","Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62A83IME","journalArticle","2020","Cave, Stephen; Dihal, Kanta","The Whiteness of AI","Philosophy & Technology","","2210-5441","10.1007/s13347-020-00415-6","https://doi.org/10.1007/s13347-020-00415-6","This paper focuses on the fact that AI is predominantly portrayed as white—in colour, ethnicity, or both. We first illustrate the prevalent Whiteness of real and imagined intelligent machines in four categories: humanoid robots, chatbots and virtual assistants, stock images of AI, and portrayals of AI in film and television. We then offer three interpretations of the Whiteness of AI, drawing on critical race theory, particularly the idea of the White racial frame. First, we examine the extent to which this Whiteness might simply reflect the predominantly White milieus from which these artefacts arise. Second, we argue that to imagine machines that are intelligent, professional, or powerful is to imagine White machines because the White racial frame ascribes these attributes predominantly to White people. Third, we argue that AI racialised as White allows for a full erasure of people of colour from the White utopian imaginary. Finally, we examine potential consequences of the racialisation of AI, arguing it could exacerbate bias and misdirect concern.","2020-08-06","2022-01-30 04:50:26","2022-01-30 04:50:26","2020-08-21 19:30:56","","","","","","Philos. Technol.","","","","","","","","en","","","","","Springer Link","","ZSCC: 0000036","","/Users/jacquesthibodeau/Zotero/storage/PAGWG4H5/Cave and Dihal - 2020 - The Whiteness of AI.pdf","","MetaSafety; CFI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZSEVCJIG","manuscript","2019","Gruetzemacher, Ross; Whittlestone, Jess","The Transformative Potential of Artificial Intelligence","","","","","http://arxiv.org/abs/1912.00747","Recently the concept of transformative AI (TAI) has begun to receive attention in the AI policy space. TAI is often framed as an alternative formulation to notions of strong AI (e.g. artificial general intelligence or superintelligence) and reflects increasing consensus that advanced AI which does not fit these definitions may nonetheless have extreme and long-lasting impacts on society. However, the term TAI is poorly defined and often used ambiguously. Some use the notion of TAI to describe levels of societal transformation associated with previous 'general purpose technologies' (GPTs) such as electricity or the internal combustion engine. Others use the term to refer to more drastic levels of transformation comparable to the agricultural or industrial revolutions. The notion has also been used much more loosely, with some implying that current AI systems are already having a transformative impact on society. This paper unpacks and analyses the notion of TAI, proposing a distinction between narrowly transformative AI (NTAI), TAI and radically transformative AI (RTAI), roughly corresponding to associated levels of societal change. We describe some relevant dimensions associated with each and discuss what kinds of advances in capabilities they might require. We further consider the relationship between TAI and RTAI and whether we should necessarily expect a period of TAI to precede the emergence of RTAI. This analysis is important as it can help guide discussions among AI policy researchers about how to allocate resources towards mitigating the most extreme impacts of AI and it can bring attention to negative TAI scenarios that are currently neglected.","2019","2022-01-30 04:50:26","2022-01-30 04:50:26","2020-11-14 00:55:29","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 6  arXiv: 1912.00747","","/Users/jacquesthibodeau/Zotero/storage/SD4BQGGF/Gruetzemacher and Whittlestone - 2020 - The Transformative Potential of Artificial Intelli.pdf; /Users/jacquesthibodeau/Zotero/storage/F5KGIKR2/1912.html","","MetaSafety; CFI; CSER; AmbiguosSafety; BERI","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2CSBG92","conferencePaper","2018","Ó hÉigeartaigh, Seán","The State of Research in Existential Risk","Proceedings from the first Garrick Colloquium on Catastrophic and Existential Risk","","","","https://www.risksciences.ucla.edu/news-events/2018/1/2/proceedings-of-the-first-international-colloquium-on-catastrophic-and-existential-risk","","2018","2022-01-30 04:50:26","2022-01-30 04:50:26","2020-12-12","","","","","","","","","","","","B John Garrick Institute for the Risk Sciences, University of California Los Angeles","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: 2","","/Users/jacquesthibodeau/Zotero/storage/PTF948QW/Ó hÉigeartaigh - 2018 - The State of Research in Existential Risk.pdf","","MetaSafety; CFI; CSER","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TWPD2FHB","journalArticle","2021","Whittlestone, Jess; Arulkumaran, Kai; Crosby, Matthew","The Societal Implications of Deep Reinforcement Learning","Journal of Artificial Intelligence Research","","1076-9757","10.1613/jair.1.12360","https://doi.org/10.1613/jair.1.12360","Deep Reinforcement Learning (DRL) is an avenue of research in Artificial Intelligence (AI) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. DRL is one of the most promising routes towards developing more autonomous AI systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct ‘answer’. This could have substantial implications for people’s lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. In this paper, we review recent progress in DRL, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand DRL’s societal implications. This article appears in the special track on AI and Society.","2021-05-01","2022-01-30 04:50:26","2022-01-30 04:50:26","2021-10-30 19:47:53","1003–1030","","","70","","J. Artif. Int. Res.","","","","","","","","","","","","","May 2021","","ZSCC: 0000006","","/Users/jacquesthibodeau/Zotero/storage/C3HIICDA/Whittlestone et al. - 2021 - The Societal Implications of Deep Reinforcement Le.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IKHJ2UAC","conferencePaper","2019","Whittlestone, Jess; Nyrup, Rune; Alexandrova, Anna; Cave, Stephen","The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","","","","","The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.","2019","2022-01-30 04:50:26","2022-01-30 04:50:26","","7","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000087","","/Users/jacquesthibodeau/Zotero/storage/GUM2EHUB/Whittlestone et al. - The Role and Limits of Principles in AI Ethics To.pdf","","MetaSafety; CFI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"CG5RTG7G","report","2018","Brundage, Miles; Avin, Shahar; Clark, Jack; Toner, Helen; Eckersley, Peter; Garfinkel, Ben; Dafoe, Allan; Scharre, Paul; Zeitzoff, Thomas; Filar, Bobby; Anderson, Hyrum; Roff, Heather; Allen, Gregory C.; Steinhardt, Jacob; Flynn, Carrick; hÉigeartaigh, Seán Ó; Beard, Simon; Belfield, Haydn; Farquhar, Sebastian; Lyle, Clare; Crootof, Rebecca; Evans, Owain; Page, Michael; Bryson, Joanna; Yampolskiy, Roman; Amodei, Dario","The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation","","","","","http://arxiv.org/abs/1802.07228","This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.","2018-02-20","2022-01-30 04:50:26","2022-01-30 04:50:26","2019-12-16 20:09:19","","","","","","","The Malicious Use of Artificial Intelligence","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 461  J: 237 arXiv: 1802.07228","","/Users/jacquesthibodeau/Zotero/storage/TPDWWRCW/Brundage et al. - 2018 - The Malicious Use of Artificial Intelligence Fore.pdf; /Users/jacquesthibodeau/Zotero/storage/3WMW2XAM/1802.html; /Users/jacquesthibodeau/Zotero/storage/8DSHG3KJ/1802.html; /Users/jacquesthibodeau/Zotero/storage/VSTQKGMW/1802.html","","MetaSafety; CFI; CSER; FHI; Open-AI; BERI","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ADQ42JZJ","conferencePaper","2018","Martínez-Plumed, Fernando; Loe, Bao Sheng; Flach, Peter; Ó hÉigeartaigh, Seán; Vold, Karina; Hernández-Orallo, José","The Facets of Artificial Intelligence: A Framework to Track the Evolution of AI","Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence","978-0-9992411-2-7","","10.24963/ijcai.2018/718","https://www.ijcai.org/proceedings/2018/718","We present nine facets for the analysis of the past and future evolution of AI. Each facet has also a set of edges that can summarise different trends and contours in AI. With them, we ﬁrst conduct a quantitative analysis using the information from two decades of AAAI/IJCAI conferences and around 50 years of documents from AI topics, an ofﬁcial database from the AAAI, illustrated by several plots. We then perform a qualitative analysis using the facets and edges, locating AI systems in the intelligence landscape and the discipline as a whole. This analytical framework provides a more structured and systematic way of looking at the shape and boundaries of AI.","2018-07","2022-01-30 04:50:25","2022-01-30 04:50:25","2020-11-14 01:15:31","5180-5187","","","","","","The Facets of Artificial Intelligence","","","","","International Joint Conferences on Artificial Intelligence Organization","Stockholm, Sweden","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s1]  ACC: 17","","/Users/jacquesthibodeau/Zotero/storage/W37ZFTH7/Martínez-Plumed et al. - 2018 - The Facets of Artificial Intelligence A Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/IPMDVDXU/Martínez-Plumed et al. - 2018 - The Facets of Artificial Intelligence A Framework.pdf","","MetaSafety; CFI; CSER; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Twenty-Seventh International Joint Conference on Artificial Intelligence {IJCAI-18}","","","","","","","","","","","","","","",""
"TVJ2EKVB","conferencePaper","2019","Hernandez-Orallo, Jose; Martınez-Plumed, Fernando; Avin, Shahar","Surveying Safety-relevant AI Characteristics","1st AAAI's Workshop on Artificial Intelligence Safety (SafeAI)","","","","","The current analysis in the AI safety literature usually combines a risk or safety issue (e.g., interruptibility) with a particular paradigm for an AI agent (e.g., reinforcement learning). However, there is currently no survey of safety-relevant characteristics of AI systems that may reveal neglected areas of research or suggest to developers what design choices they could make to avoid or minimise certain safety concerns. In this paper, we take a first step towards delivering such a survey, from two angles. The first features AI system characteristics that are already known to be relevant to safety concerns, including internal system characteristics, characteristics relating to the effect of the external environment on the system, and characteristics relating to the effect of the system on the target environment. The second presents a brief survey of a broad range of AI system characteristics that could prove relevant to safety research, including types of interaction, computation, integration, anticipation, supervision, modification, motivation and achievement. This survey enables further work in exploring system characteristics and design choices that affect safety concerns.","2019","2022-01-30 04:50:25","2022-01-30 04:50:25","","9","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s7]  ACC: 10  J: 4","","/Users/jacquesthibodeau/Zotero/storage/X9P84E7X/Hernandez-Orallo et al. - Surveying Safety-relevant AI Characteristics.pdf","","TechSafety; CFI; CSER","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K4827FC7","conferencePaper","2020","Cihon, Peter; Maas, Matthijs M.; Kemp, Luke","Should Artificial Intelligence Governance be Centralised?: Design Lessons from History","Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","978-1-4503-7110-0","","10.1145/3375627.3375857","https://dl.acm.org/doi/10.1145/3375627.3375857","","2020-02-07","2022-01-30 04:50:25","2022-01-30 04:50:25","2020-12-12 16:10:16","228-234","","","","","","Should Artificial Intelligence Governance be Centralised?","","","","","ACM","New York NY USA","en","","","","","DOI.org (Crossref)","","ZSCC: 0000016","","","","MetaSafety; CFI; CSER; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AIES '20: AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"HFZ22SRN","conferencePaper","2021","Burden, John; Hernandez-Orallo, Jose","Negative Side Effects and AI Agent Indicators: Experiments in SafeLife","","","","","","The widespread adoption and ubiquity of AI systems will require them to be safe. The safety issues that can arise from AI are broad and varied. In this paper we consider the safety issue of negative side effects and the consequences they can have on an environment. In the safety benchmarking domain SafeLife, we discuss the way that side effects are measured, as well as presenting results showing the relation between the magnitude of side effects and other metrics for three agent types: Deep Q-Networks, Proximal Policy Optimisation, and a Uniform Random Agent. We observe that different metrics and agent types lead to both monotonic and non-monotonic interactions, with the ﬁnding that the size and complexity of the environment versus the capability of the agent plays a major role in negative side effects, sometimes in intricate ways.","2021-01-01","2022-01-30 04:50:25","2022-01-30 04:50:25","","9","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/92N83U65/Burden and Hernandez-Orallo - Negative Side Effects and AI Agent Indicators Exp.pdf","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","SafeAI@ AAAI","","","","","","","","","","","","","","",""
"NSJVHAQP","conferencePaper","2020","Bhatt, Umang; Andrus, McKane; Weller, Adrian; Xiang, Alice","Machine Learning Explainability for External Stakeholders","","","","","https://arxiv.org/abs/2007.05408v1","As machine learning is increasingly deployed in high-stakes contexts affecting people's livelihoods, there have been growing calls to open the black box and to make machine learning algorithms more explainable. Providing useful explanations requires careful consideration of the needs of stakeholders, including end-users, regulators, and domain experts. Despite this need, little work has been done to facilitate inter-stakeholder conversation around explainable machine learning. To help address this gap, we conducted a closed-door, day-long workshop between academics, industry experts, legal scholars, and policymakers to develop a shared language around explainability and to understand the current shortcomings of and potential solutions for deploying explainable machine learning in service of transparency goals. We also asked participants to share case studies in deploying explainable machine learning at scale. In this paper, we provide a short summary of various case studies of explainable machine learning, lessons from those studies, and discuss open challenges.","2020-07-10","2022-01-30 04:50:25","2022-01-30 04:50:25","2020-11-23 01:16:21","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000014","","/Users/jacquesthibodeau/Zotero/storage/2C2574GF/Bhatt et al. - 2020 - Machine Learning Explainability for External Stake.pdf; /Users/jacquesthibodeau/Zotero/storage/8EMZGHET/2007.html","","MetaSafety; CFI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML Workshop XXAI: Extending Explainable AI Beyond Deep Models and Classifiers","","","","","","","","","","","","","","",""
"CCRB9244","conferencePaper","2020","Burden, John; Hernandez-Orallo, Jose","Exploring AI Safety in Degrees: Generality, Capability and Control","Proceedings of the Workshop on Artificial Intelligence Safety (SafeAI 2020)","","","","","The landscape of AI safety is frequently explored differently by contrasting specialised AI versus general AI (or AGI), by analysing the short-term hazards of systems with limited capabilities against those more long-term risks posed by ‘superintelligence’, and by conceptualising sophisticated ways of bounding control an AI system has over its environment and itself (impact, harm to humans, self-harm, containment, etc.). In this position paper we reconsider these three aspects of AI safety as quantitative factors –generality, capability and control–, suggesting that by deﬁning metrics for these dimensions, AI risks can be characterised and analysed more precisely. As an example, we illustrate how to deﬁne these metrics and their values for some simple agents in a toy scenario within a reinforcement learning setting.","2020-08-10","2022-01-30 04:50:25","2022-01-30 04:50:25","","5","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000007","","/Users/jacquesthibodeau/Zotero/storage/Q7VVUNP9/Burden and Hernandez-Orallo - Exploring AI Safety in Degrees Generality, Capabi.pdf","","TechSafety; CFI; CSER; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D269DVPA","blogPost","2021","Clarke, Sam; Martin, Samuel Dylan","Distinguishing AI takeover scenarios","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios","Epistemic status: lots of this involves interpreting/categorising other people’s scenarios, and could be wrong. We’d really appreciate being corrected if so. [ETA: so far, no corrections.] TLDR: see the summary table. In the last few years, people have proposed various AI takeover scenarios. We think this type of scenario building is great, since there are now more concrete ideas of what AI takeover could realistically look like. That said, we have been confused for a while about how the different scenarios relate to each other and what different assumptions they make. This post might be helpful for anyone who has similar confusions. We focus on explaining the differences between seven prominent scenarios: the  ‘Brain-in-a-box’ scenario, ‘What failure looks like’ part 1 (WFLL 1), ‘What failure looks like’ part 2 (WFLL 2), ‘Another (outer) alignment failure story’  (AAFS), ‘Production Web’, ‘Flash economy’ and ‘Soft takeoff leading to decisive strategic advantage’. While these scenarios do not capture alI of the risks from transformative AI, participants in a recent survey aimed at leading AI safety/governance researchers estimated the first three of these scenarios to cover 50% of existential catastrophes from AI.[1] We plan to follow up with a subsequent post, which discusses some of the issues raised here in greater depth. VARIABLES RELATING TO AI TAKEOVER SCENARIOS We define AI takeover to be a scenario where the most consequential decisions about the future get made by AI systems with goals that aren’t desirable by human standards. There are three variables which are sufficient to distinguish the takeover scenarios discussed in this post. We will briefly introduce these three variables, and a number of others that are generally useful for thinking about takeover scenarios. Key variables for distinguishing the AI takeover scenarios in this post:  * Speed. Is there a sudden jump in AI capabilities over a very short period    (i.e. much faster than what we","2021-09-08","2022-01-30 04:50:25","2022-01-30 04:50:25","2021-11-18 23:45:23","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/S3I632SN/distinguishing-ai-takeover-scenarios.html","","UnsortedSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MCJ6I4DC","conferencePaper","2020","Cremer, Carla Zoe; Whittlestone, Jess","Canaries in Technology Mines: Warning Signs of Transformative Progress in AI","","","","","","","2020","2022-01-30 04:50:25","2022-01-30 04:50:25","","7","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/TQJZSWED/Cremer and Whittlestone - Canaries in Technology Mines Warning Signs of Tra.pdf","","MetaSafety; CFI; CSER; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","1st International Workshop on Evaluating Progress in Artificial Intelligence - EPAI 2020","","","","","","","","","","","","","","",""
"8TWH52ZI","journalArticle","2021","Stix, Charlotte; Maas, Matthijs M.","Bridging the gap: the case for an ‘Incompletely Theorized Agreement’ on AI policy","AI and Ethics","","2730-5961","10.1007/s43681-020-00037-w","https://doi.org/10.1007/s43681-020-00037-w","Recent progress in artificial intelligence (AI) raises a wide array of ethical and societal concerns. Accordingly, an appropriate policy approach is urgently needed. While there has been a wave of scholarship in this field, the research community at times appears divided amongst those who emphasize ‘near-term’ concerns and those focusing on ‘long-term’ concerns and corresponding policy measures. In this paper, we seek to examine this alleged ‘gap’, with a view to understanding the practical space for inter-community collaboration on AI policy. We propose to make use of the principle of an ‘incompletely theorized agreement’ to bridge some underlying disagreements, in the name of important cooperation on addressing AI’s urgent challenges. We propose that on certain issue areas, scholars working with near-term and long-term perspectives can converge and cooperate on selected mutually beneficial AI policy projects, while maintaining their distinct perspectives.","2021-08-01","2022-01-30 04:50:25","2022-01-30 04:50:25","2021-10-31 17:04:06","261-271","","3","1","","AI Ethics","Bridging the gap","","","","","","","en","","","","","Springer Link","","ZSCC: 0000008","","/Users/jacquesthibodeau/Zotero/storage/F7AQ9SK8/Stix and Maas - 2021 - Bridging the gap the case for an ‘Incompletely Th.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DZ9C9GKV","blogPost","2021","Clarke, Sam; Carlier, Alexis; Schuett, Jonas","Survey on AI existential risk scenarios","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios","Cross-posted to the EA forum. SUMMARY  * In August 2020, we conducted an online survey of prominent AI safety and    governance researchers. You can see a copy of the survey at this link.[1]  * We sent the survey to 135 researchers at leading AI safety/governance    research organisations (including AI Impacts, CHAI, CLR, CSER, CSET, FHI, FLI    , GCRI, MILA, MIRI, Open Philanthropy and PAI) and a number of independent    researchers. We received 75 responses, a response rate of 56%.  * The survey aimed to identify which AI existential risk scenarios[2] (which we    will refer to simply as “risk scenarios”) those researchers find most likely,    in order to (1) help with prioritising future work on exploring AI risk    scenarios, and (2) facilitate discourse and understanding within the AI    safety and governance community, including between researchers who have    different views.  * In our view, the key result is that there was considerable disagreement among    researchers about which risk scenarios are the most likely, and high    uncertainty expressed by most individual researchers about their estimates.  * This suggests that there is a lot of value in exploring the likelihood of    different AI risk scenarios in more detail, especially given the limited    scrutiny that most scenarios have received. This could look like: * Fleshing       out and analysing the scenarios mentioned in this post which have received       less scrutiny.     * Doing       more horizon scanning or trying to come up with other risk scenarios, and       analysing them.          * At this time, we are only publishing this abbreviated version of the results.    We have a version of the full results that we may publish at a later date.    Please contact one of us if you would like access to this, and include a    sentence on why the results would be helpful or what you intend to use them    for.  * We welcome feedback on any aspects of the survey. MOTIVATION It has been argued that AI","2021-06-08","2022-01-30 04:50:25","2022-01-30 04:50:25","2021-11-14 18:38:03","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/5WSJBIXI/survey-on-ai-existential-risk-scenarios.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GBC9PC5H","journalArticle","2021","Liu, Hin-Yan; Maas, Matthijs M.","‘Solving for X?’ Towards a problem-finding framework to ground long-term governance strategies for artificial intelligence","Futures","","0016-3287","10.1016/j.futures.2020.102672","https://www.sciencedirect.com/science/article/pii/S0016328720301634","Change is hardly a new feature in human affairs. Yet something has begun to change in change. In the face of a range of emerging, complex, and interconnected global challenges, society’s collective governance efforts may need to be put on a different footing. Many of these challenges derive from emerging technological developments – take Artificial Intelligence (AI), the focus of much contemporary governance scholarship and efforts. AI governance strategies have predominantly oriented themselves towards clear, discrete clusters of pre-defined problems. We argue that such ‘problem-solving’ approaches may be necessary, but are also insufficient in the face of many of the ‘wicked problems’ created or driven by AI. Accordingly, we propose in this paper a complementary framework for grounding long-term governance strategies for complex emerging issues such as AI into a ‘problem-finding’ orientation. We first provide a rationale by sketching the range of policy problems created by AI, and providing five reasons why problem-solving governance approaches to these challenges fail or fall short. We conversely argue that that creative, ‘problem-finding’ research into these governance challenges is not only warranted scientifically, but will also be critical in the formulation of governance strategies that are effective, meaningful, and resilient over the long-term. We accordingly illustrate the relation between and the complementarity of problem-solving and problem-finding research, by articulating a framework that distinguishes between four distinct ‘levels’ of governance: problem-solving research generally approaches AI (governance) issues from a perspective of (Level 0) ‘business-as-usual’ or as (Level 1) ‘governance puzzle-solving’. In contrast, problem-finding approaches emphasize (Level 2) ‘governance Disruptor-Finding’; or (Level 3) ‘Charting Macrostrategic Trajectories’. We apply this theoretical framework to contemporary governance debates around AI throughout our analysis to elaborate upon and to better illustrate our framework. We conclude with reflections on nuances, implications, and shortcomings of this long-term governance framework, offering a range of observations on intra-level failure modes, between-level complementarities, within-level path dependencies, and the categorical boundary conditions of governability (‘Governance Goldilocks Zone’). We suggest that this framework can help underpin more holistic approaches for long-term strategy-making across diverse policy domains and contexts, and help cross the bridge between concrete policies on local solutions, and longer-term considerations of path-dependent societal trajectories to avert, or joint visions towards which global communities can or should be rallied.","2021-02-01","2022-01-30 04:50:25","2022-01-30 04:50:25","2021-10-31 17:02:28","102672","","","126","","Futures","‘Solving for X?","","","","","","","en","","","","","ScienceDirect","","ZSCC: 0000005","","/Users/jacquesthibodeau/Zotero/storage/R74GTS83/S0016328720301634.html","","MetaSafety","Artificial intelligence; Governance disruptors; Governance goldilocks zone; Governance puzzles; Macrostrategic trajectories & destinations; Problem-finding","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"54ATHF4J","journalArticle","2020","Peters, Dorian; Vold, Karina; Robinson, Diana; Calvo, Rafael A.","Responsible AI—Two Frameworks for Ethical Design Practice","IEEE Transactions on Technology and Society","","2637-6415","10.1109/TTS.2020.2974991","https://ieeexplore.ieee.org/document/9001063/","In 2019, the IEEE launched the P7000 standards projects intended to address ethical issues in the design of autonomous and intelligent systems. This move came amidst a growing public concern over the unintended consequences of artiﬁcial intelligence (AI), compounded by the lack of an anticipatory process for attending to ethical impact within professional practice. However, the difﬁculty in moving from principles to practice presents a signiﬁcant challenge to the implementation of ethical guidelines. Herein, we describe two complementary frameworks for integrating ethical analysis into engineering practice to help address this challenge. We then provide the outcomes of an ethical analysis informed by these frameworks, conducted within the speciﬁc context of Internet-delivered therapy in digital mental health. We hope both the frameworks and analysis can provide tools and insights, not only for the context of digital healthcare but also for data-enabled and intelligent technology development more broadly.","2020-03","2022-01-30 04:50:25","2022-01-30 04:50:25","2020-08-21 19:54:52","34-47","","1","1","","IEEE Trans. Technol. Soc.","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000030","","/Users/jacquesthibodeau/Zotero/storage/RV2HP3AI/Peters et al. - 2020 - Responsible AI—Two Frameworks for Ethical Design P.pdf","","TechSafety; CFI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7CSE8NXS","manuscript","2019","Ovadya, Aviv; Whittlestone, Jess","Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning","","","","","http://arxiv.org/abs/1907.11274","The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the use of ML to create ""synthetic media"" (e.g. to generate or manipulate audio, video, images, and text), and the question of what publication and release processes around such research might look like, though many of the considerations discussed will apply to ML research more broadly. We are not arguing for any specific approach on when or how research should be distributed, but instead try to lay out some useful tools, analogies, and options for thinking about these issues. We begin with some background on the idea that ML research might be misused in harmful ways, and why advances in synthetic media, in particular, are raising concerns. We then outline in more detail some of the different paths to harm from ML research, before reviewing research risk mitigation strategies in other fields and identifying components that seem most worth emulating in the ML and synthetic media research communities. Next, we outline some important dimensions of disagreement on these issues which risk polarizing conversations. Finally, we conclude with recommendations, suggesting that the machine learning community might benefit from: working with subject matter experts to increase understanding of the risk landscape and possible mitigation strategies; building a community and norms around understanding the impacts of ML research, e.g. through regular workshops at major conferences; and establishing institutions and systems to support release practices that would otherwise be onerous and error-prone.","2019-07-28","2022-01-30 04:50:25","2022-01-30 04:50:25","2019-12-16 22:39:38","","","","","","","Reducing malicious use of synthetic media research","","","","","","","","","","","","arXiv.org","","ZSCC: 0000012  arXiv: 1907.11274","","/Users/jacquesthibodeau/Zotero/storage/9JS2GGPU/Ovadya and Whittlestone - 2019 - Reducing malicious use of synthetic media research.pdf; /Users/jacquesthibodeau/Zotero/storage/DX2WMWTT/Ovadya and Whittlestone - 2019 - Reducing malicious use of synthetic media research.pdf; /Users/jacquesthibodeau/Zotero/storage/R2ETWS25/1907.html; /Users/jacquesthibodeau/Zotero/storage/ADSVSN46/1907.html","","MetaSafety; CFI; CSER; AmbiguosSafety","Computer Science - Machine Learning; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CAZ95ZMD","journalArticle","2020","ÓhÉigeartaigh, Seán S.; Whittlestone, Jess; Liu, Yang; Zeng, Yi; Liu, Zhe","Overcoming Barriers to Cross-cultural Cooperation in AI Ethics and Governance","Philosophy & Technology","","2210-5433, 2210-5441","10.1007/s13347-020-00402-x","http://link.springer.com/10.1007/s13347-020-00402-x","Achieving the global benefits of artificial intelligence (AI) will require international cooperation on many areas of governance and ethical standards, while allowing for diverse cultural perspectives and priorities. There are many barriers to achieving this at present, including mistrust between cultures, and more practical challenges of coordinating across different locations. This paper focuses particularly on barriers to cooperation between Europe and North America on the one hand and East Asia on the other, as regions which currently have an outsized impact on the development of AI ethics and governance. We suggest that there is reason to be optimistic about achieving greater cross-cultural cooperation on AI ethics and governance. We argue that misunderstandings between cultures and regions play a more important role in undermining cross-cultural trust, relative to fundamental disagreements, than is often supposed. Even where fundamental differences exist, these may not necessarily prevent productive cross-cultural cooperation, for two reasons: (1) cooperation does not require achieving agreement on principles and standards for all areas of AI; and (2) it is sometimes possible to reach agreement on practical issues despite disagreement on more abstract values or principles. We believe that academia has a key role to play in promoting cross-cultural cooperation on AI ethics and governance, by building greater mutual understanding, and clarifying where different forms of agreement will be both necessary and possible. We make a number of recommendations for practical steps and initiatives, including translation and multilingual publication of key documents, researcher exchange programmes, and development of research agendas on cross-cultural topics.","2020-05-15","2022-01-30 04:50:25","2022-01-30 04:50:25","2020-08-21 19:45:24","","","","","","Philos. Technol.","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000026","","/Users/jacquesthibodeau/Zotero/storage/I6Z5EXPQ/ÓhÉigeartaigh et al. - 2020 - Overcoming Barriers to Cross-cultural Cooperation .pdf","","MetaSafety; CFI; CSER","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQ8SWGRG","journalArticle","2020","Coyle, Diane; Weller, Adrian","“Explaining” machine learning reveals policy challenges","Science","","0036-8075, 1095-9203","","https://www.sciencemag.org/lookup/doi/10.1126/science.aba9647","","2020-06-26","2022-01-30 04:50:25","2022-01-30 04:50:25","2020-08-21 19:38:44","1433-1434","","6498","368","","","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000021","","/Users/jacquesthibodeau/Zotero/storage/NDWSWJX4/Coyle and Weller - 2020 - “Explaining” machine learning reveals policy chall.pdf","","MetaSafety; CFI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJAXDNIF","conferencePaper","2020","Prunkl, Carina; Whittlestone, Jess","Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society","arXiv:2001.04335 [cs]","","","","http://arxiv.org/abs/2001.04335","One way of carving up the broad ‘AI ethics and society’ research space that has emerged in recent years is to distinguish between ‘near-term’ and ‘long-term’ research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.","2020-01-21","2022-01-30 04:50:24","2022-01-30 04:50:24","2020-08-21 20:00:24","","","","","","","Beyond Near- and Long-Term","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000014  arXiv: 2001.04335","","/Users/jacquesthibodeau/Zotero/storage/VCRACQA2/Prunkl and Whittlestone - 2020 - Beyond Near- and Long-Term Towards a Clearer Acco.pdf","","MetaSafety; CFI; CSER; FHI; AmbiguosSafety","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"TITT7C74","journalArticle","2020","Tzachor, Asaf; Whittlestone, Jess; Sundaram, Lalitha; hÉigeartaigh, Seán Ó","Artificial intelligence in a crisis needs ethics with urgency","Nature Machine Intelligence","","2522-5839","10.1038/s42256-020-0195-0","https://www.nature.com/articles/s42256-020-0195-0","Artificial intelligence tools can help save lives in a pandemic. However, the need to implement technological solutions rapidly raises challenging ethical issues. We need new approaches for ethics with urgency, to ensure AI can be safely and beneficially used in the COVID-19 response and beyond.","2020-07","2022-01-30 04:50:24","2022-01-30 04:50:24","2020-08-21 19:43:33","365-366","","7","2","","","","","","","","","","en","2020 Springer Nature Limited","","","","www.nature.com","","ZSCC: NoCitationData[s2]  ACC: 20  Number: 7 Publisher: Nature Publishing Group","","/Users/jacquesthibodeau/Zotero/storage/C2PV7XMN/Tzachor et al. - 2020 - Artificial intelligence in a crisis needs ethics w.pdf; /Users/jacquesthibodeau/Zotero/storage/N37E4MJK/s42256-020-0195-0.html","","MetaSafety; CFI; CSER; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IDS6VQUQ","conferencePaper","2018","Cave, Stephen; ÓhÉigeartaigh, Seán S.","An AI Race for Strategic Advantage: Rhetoric and Risks","Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society","978-1-4503-6012-8","","10.1145/3278721.3278780","https://dl.acm.org/doi/10.1145/3278721.3278780","","2018-12-27","2022-01-30 04:50:24","2022-01-30 04:50:24","2020-12-12 17:39:09","36-40","","","","","","An AI Race for Strategic Advantage","","","","","ACM","New Orleans LA USA","en","","","","","DOI.org (Crossref)","","ZSCC: 0000066","","/Users/jacquesthibodeau/Zotero/storage/5F7NG3ZZ/Cave and ÓhÉigeartaigh - 2018 - An AI Race for Strategic Advantage Rhetoric and R.pdf; /Users/jacquesthibodeau/Zotero/storage/IFPKRRTZ/Cave and ÓhÉigeartaigh - 2018 - An AI Race for Strategic Advantage Rhetoric and R.pdf","","MetaSafety; CFI; CSER","artificial intelligence; ai narratives; ai risks; ai safety; arms race; global governance; international cooperation; strategic competition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AIES '18: AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"46FKJWZG","conferencePaper","2021","Maas, Matthijs M.","AI, Governance Displacement, and the (De)Fragmentation of International Law","SSRN Electronic Journal","","","10.2139/ssrn.3806624","https://www.ssrn.com/abstract=3806624","The emergence, proliferation, and use of new general-purpose technologies can often produce significant political, redistributive, normative and legal effects on the world. Artificial intelligence (AI) has been identified as one such transformative technology. Many of its impacts may require global governance responses. However, what are the direct and indirect effects of AI technologies on the viability, form, or functioning of the international legal order itself? What, if any, are the prospects, peril or promise of AI-driven legal automation at the international level? This paper draws on an ‘AI Governance Disruption’ framework to understanding AI’s impacts on the global governance architecture. Focusing particularly on the potential for legal automation at the international law level, it explores three potential pathways of such ‘legal displacement’: (1) the automation of rule creation and arbitration; (2) the automation of monitoring & enforcement; or (3) the ‘replacement’ of international law with new architectural modes of (international) behaviour control. It then focuses on the effects of these trends on the architecture of international law. It distinguishes 10 different roles that AI applications could play, with distinct effects on the international legal order. That is, AI systems can serve as (1) legal ‘canary in the coal mine’, highlighting the need for greater cross-regime harmonization. However, it can also serve as (2) tough knot or (3) generator of regime fault lines. Under even modest scenarios of legal automation, AI systems may serve variably as a (4) shield, (5) patch, (6) cure, or (7) accelerator of international legal fragmentation. Finally, AI tools may serve as (8) differential enabler; (9) driver of value shifts, or (10) asymmetric weapon, potentially contributing to trends of contestation or erosion in the international legal order. The paper concludes with a brief review of the ways in which international lawyers or regime scholars might approach the risks and opportunities of increasing automation in international law, in order to leverage these trends and tools towards improved efficacy, resilience, and legitimacy of global governance.","2021-03","2022-01-30 04:50:24","2022-01-30 04:50:24","2021-10-31 16:58:43","","","","","","","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/EZ5UBQGN/Maas - 2021 - AI, Governance Displacement, and the (De)Fragmenta.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Studies Association 2021","","","","","","","","","","","","","","",""
"VJHUMTDA","conferencePaper","2020","Hernandez-Orallo, Jose; Martınez-Plumed, Fernando; Avin, Shahar; Whittlestone, Jess; Ó hÉigeartaigh, Seán","AI Paradigms and AI Safety: Mapping Artefacts and Techniques to Safety Issues","European Conference on Artificial Intelligence","","","","","AI safety often analyses a risk or safety issue, such as interruptibility, under a particular AI paradigm, such as reinforcement learning. But what is an AI paradigm and how does it affect the understanding and implications of the safety issue? Is AI safety research covering the most representative paradigms and the right combinations of paradigms with safety issues? Will current research directions in AI safety be able to anticipate more capable and powerful systems yet to come? In this paper we analyse these questions, introducing a distinction between two types of paradigms in AI: artefacts and techniques. We then use experimental data of research and media documents from AI Topics, an ofﬁcial publication of the AAAI, to examine how safety research is distributed across artefacts and techniques. We observe that AI safety research is not sufﬁciently anticipatory, and is heavily weighted towards certain research paradigms. We identify a need for AI safety to be more explicit about the artefacts and techniques for which a particular issue may be applicable, in order to identify gaps and cover a broader range of issues.","2020","2022-01-30 04:50:24","2022-01-30 04:50:24","","8","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000002[s1]","","/Users/jacquesthibodeau/Zotero/storage/8VXPTJN3/Hernandez-Orallo et al. - 2020 - AI Paradigms and AI Safety Mapping Artefacts and .pdf","","TechSafety; CFI; CSER","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FS66RI5N","journalArticle","2019","Cave, Stephen; Ó hÉigeartaigh, Seán S.","Bridging near- and long-term concerns about AI","Nature Machine Intelligence","","2522-5839","10.1038/s42256-018-0003-2","https://www.nature.com/articles/s42256-018-0003-2","Debate about the impacts of AI is often split into two camps, one associated with the near term and the other with the long term. This divide is a mistake — the connections between the two perspectives deserve more attention, say Stephen Cave and Seán S. ÓhÉigeartaigh.","2019-01","2022-01-30 04:50:24","2022-01-30 04:50:24","2019-12-16 22:26:28","5-6","","1","1","","","","","","","","","","en","2019 Springer Nature Limited","","","","www.nature.com","","ZSCC: 0000039[s0]","","/Users/jacquesthibodeau/Zotero/storage/Q8TE6ISU/Cave and ÓhÉigeartaigh - 2019 - Bridging near- and long-term concerns about AI.pdf; /Users/jacquesthibodeau/Zotero/storage/IBCVWHM9/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/SFDBAMP6/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/VI5J3DQP/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/PMXSKZ3S/s42256-018-0003-2.html","","MetaSafety; CFI; CSER; FHI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H9CGA9V7","thesis","2021","Maas, Matthijs M.","Artificial Intelligence Governance Under Change: Foundations, Facets, Frameworks","","","","","https://www.ssrn.com/abstract=3833395","This dissertation explores how we may govern a changing technology, in a changing world, using governance systems that may themselves be left changed. Artificial Intelligence (AI) has made remarkable progress in the past decade, and is anticipated to become an increasingly disruptive, even transformative technology. AI can be functionally understood as a diverse portfolio of computational techniques to improve the accuracy, speed, or scale of machine decision-making, producing capabilities that can support, substitute for-, or improve upon human task performance. The resulting breadth of application makes AI promising—and challenging — in so many domains of life. In recent years diverse AI applications — from facial recognition to automated legal decision-making, and from computational propaganda to Lethal Autonomous Weapons Systems — have raised deep ethical, political, legal and security concerns. With growing public and policymaker attention has come a wave of governance initiatives and proposals. Nonetheless, global governance for AI remains relatively fragmented and incipient. At this cross-roads, this dissertation takes up the research question, “How should global governance for artificial intelligence account for change?” To answer this question, this dissertation draws together scholarship on technology regulation, (international) law, and global governance, in order to unpack three facets of ‘change’ that will prove critical to the global governance of AI. These three facets of change are examined through the conceptual lenses of Sociotechnical Change, Governance Disruption, and Regime Complexity. Sociotechnical Change (Chapter 4) explores how and why technological change in AI produces societal changes that create a rationale for regulatory intervention, and how we can productively characterize the appropriate targets for AI governance. Along with material features, I distinguish six problem logics which highlight different governance solutions and conditions. Governance Disruption (Chapter 5) addresses when, where and why certain AI capabilities might drive or demand change in the substance (Development), tools or processes (Displacement) or political scaffolding (Destruction) of global governance itself, and what are the implications for global regime complexity. Regime Complexity (Chapter 6) helps focus attention on how prospective AI regimes are shaped by underlying changes in the broader global governance architecture. It provides insight into the (1) origins or foundations of AI regimes; the (2) topology of the AI ‘regime complex’; its (3) evolution towards integration or fragmentation; (4) the functional consequences of these paths; and (5) strategies for managing the AI regime complex. Through these three lenses, this dissertation explores key considerations, insights and tradeoffs for AI governance (Chapter 7). It argues that AI governance needs to shift or adopt novel strategies — in conceptual approach, instrument choice, and instrument design — to ensure the efficacy of AI regimes in tracking AI’s sociotechnical impacts, their resilience to future AI-driven disruption to the tools, norms or broader conditions of governance, and their coherence. In this way, AI governance regimes may remain fit for change.","2021","2022-01-30 04:50:24","2022-01-30 04:50:24","2021-12-11 14:45:47","","","","","","","Artificial Intelligence Governance Under Change","","","","","University of Copenhagen","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/2CR8NPGZ/Maas - 2021 - Artificial Intelligence Governance Under Change F.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UISZQD9X","bookSection","2019","Kunz, Martina; Ó hÉigeartaigh, Seán","Artificial Intelligence and Robotization","Oxford Handbook on the International Law of Global Security","","","","https://papers.ssrn.com/abstract=3310421","This chapter provides an overview of the international law governing applications of artificial intelligence and robotics which affect global security, highlighting challenges arising from technological developments and how international regulators are responding to them. Much of the international law literature thus far has focused on the implications of increasingly autonomous weapons systems. Our contribution instead seeks to cover a broader range of global security risks resulting from large-scale diffuse or concentrated, gradual or sudden, direct or indirect, intentional or unintentional, AI or robotics-caused harm. Applications of these technologies permeate almost every domain of human activity and thus unsurprisingly have an equally wide range of risk profiles, from a discriminatory algorithmic decision causing financial distress to an AI-sparked nuclear war collapsing global civilization. Hence, it is only natural that much of the international regulatory activity takes place in domain-specific fora. Many of these fora coordinate with each other, both within and beyond the UN system, spreading insights and best practices on how to deal with common concerns such as cybersecurity, monitoring, and reliability, so as to prevent accidents and misuse.","2019-01-15","2022-01-30 04:50:24","2022-01-30 04:50:24","2020-08-21 20:05:39","","","","","","","","","","","","Social Science Research Network","Rochester, NY","en","","","","","papers.ssrn.com","","ZSCC: NoCitationData[s3]  ACC: 6  DOI: 10.2139/ssrn.3310421","","/Users/jacquesthibodeau/Zotero/storage/BVDRNTDD/papers.html","","MetaSafety; CFI; CSER; AmbiguosSafety","artificial intelligence; global security; international law; robotics","Geiß, Robin; Melzer, Nils","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPZRS775","journalArticle","2021","Zoe Cremer, Carla; Whittlestone, Jess","Artificial Canaries: Early Warning Signs for Anticipatory and Democratic Governance of AI","International Journal of Interactive Multimedia and Artificial Intelligence","","1989-1660","10.9781/ijimai.2021.02.011","https://www.ijimai.org/journal/sites/default/files/2021-02/ijimai_6_5_10.pdf","","2021","2022-01-30 04:50:24","2022-01-30 04:50:24","2021-10-30 19:59:46","100","","5","6","","IJIMAI","Artificial Canaries","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/UJUII49M/Zoe Cremer and Whittlestone - 2021 - Artificial Canaries Early Warning Signs for Antic.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J4M8S57W","journalArticle","2019","Zerilli, John; Knott, Alistair; Maclaurin, James; Gavaghan, Colin","Algorithmic Decision-Making and the Control Problem","Minds and Machines","","0924-6495, 1572-8641","10.1007/s11023-019-09513-7","http://link.springer.com/10.1007/s11023-019-09513-7","Abstract                            The danger of human operators devolving responsibility to machines and failing to detect cases where they fail has been recognised for many years by industrial psychologists and engineers studying the human operators of complex machines. We call it “the control problem”, understood as the tendency of the human within a human–machine control loop to become complacent, over-reliant or unduly diffident when faced with the outputs of a reliable autonomous system. While the control problem has been investigated for some time, up to this point its manifestation in machine learning contexts has not received serious attention. This paper aims to fill that gap. We argue that, except in certain special circumstances, algorithmic decision tools should not be used in high-stakes or safety-critical decisions unless the systems concerned are significantly “better than human” in the relevant domain or subdomain of decision-making. More concretely, we recommend three strategies to address the control problem, the most promising of which involves a               complementary               (and potentially               dynamic               ) coupling between highly proficient algorithmic tools and human agents working alongside one another. We also identify six key principles which all such human–machine systems should reflect in their design. These can serve as a framework both for assessing the viability of any such human–machine system as well as guiding the design and implementation of such systems generally.","2019-12","2022-01-30 04:50:24","2022-01-30 04:50:24","2020-12-12 17:29:35","555-578","","4","29","","Minds & Machines","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000031","","/Users/jacquesthibodeau/Zotero/storage/EFMGD9CK/Zerilli et al. - 2019 - Algorithmic Decision-Making and the Control Proble.pdf","","TechSafety; CFI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TEXCA9UU","journalArticle","2020","Hollanek, Tomasz","AI transparency: a matter of reconciling design with critique","AI & Society","","1435-5655","10.1007/s00146-020-01110-y","https://doi.org/10.1007/s00146-020-01110-y","In the late 2010s, various international committees, expert groups, and national strategy boards have voiced the demand to ‘open’ the algorithmic black box, to audit, expound, and demystify artificial intelligence. The opening of the algorithmic black box, however, cannot be seen only as an engineering challenge. In this article, I argue that only the sort of transparency that arises from critique—a method of theoretical examination that, by revealing pre-existing power structures, aims to challenge them—can help us produce technological systems that are less deceptive and more just. I relate the question of AI transparency to the broader challenge of responsible making, contending that future action must aim to systematically reconcile design—as a way of concealing—with critique—as a manner of revealing.","2020-11-17","2022-01-30 04:50:24","2022-01-30 04:50:24","2020-11-23 01:14:10","","","","","","AI & Soc","AI transparency","","","","","","","en","","","","","Springer Link","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/SMRN47IQ/Hollanek - 2020 - AI transparency a matter of reconciling design wi.pdf","","TechSafety; CFI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CH7SXTSU","bookSection","2020","Ó hÉigeartaigh, Seán","AI Research with the Potential for Malicious Use: Publication Norms and Governance Considerations","AI Governance in 2019 - A Year In Review","","","","http://lcfi.ac.uk/resources/ai-research-potential-malicious-use-publication-no/","Chapter in AI Governance in 2019 - A Year in Review: Observations from 50 Global Experts. A report produced by the Shanghai Institute of Science for Science.","2020-04","2022-01-30 04:50:24","2022-01-30 04:50:24","2020-08-24 16:28:39","","","","","","","AI Research with the Potential for Malicious Use","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/3ERIHJMI/ai-research-potential-malicious-use-publication-no.html","","MetaSafety; CFI; CSER; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZAFKPQNH","manuscript","2018","Martínez-Plumed, Fernando; Avin, Shahar; Brundage, Miles; Dafoe, Allan; hÉigeartaigh, Sean Ó; Hernández-Orallo, José","Accounting for the neglected dimensions of AI progress","","","","","https://arxiv.org/abs/1806.00610","","2018","2022-01-30 04:50:23","2022-01-30 04:50:23","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: NoCitationData[s4]  ACC: 19","","/Users/jacquesthibodeau/Zotero/storage/RDSFDZK3/Martínez-Plumed et al. - 2018 - Accounting for the neglected dimensions of ai prog.pdf; /Users/jacquesthibodeau/Zotero/storage/XMI73HC7/1806.html","","MetaSafety; CFI; CSER; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DSDMH6GU","blogPost","2019","Shovelain, Justain; Emilsson, Andrés Gómez","Why Care About Meme Hazards and Thoughts on How to Handle Them","Qualia  Computing","","","","https://qualiacomputing.com/2019/08/30/why-care-about-meme-hazards-and-thoughts-on-how-to-handle-them/","By Justin Shovelain and Andrés Gómez Emilsson Definition Nick Bostrom defines an “Information Hazard” as: “A risk that arises from the dissemination or the potential dissemination of (true) informa…","2019-08-31","2022-01-30 04:50:08","2022-01-30 04:50:08","2020-12-12 02:32:24","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/P5U32AF7/why-care-about-meme-hazards-and-thoughts-on-how-to-handle-them.html","","MetaSafety; AmbiguosSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"65UKC9MC","blogPost","2019","Manheim, David","What does Optimization Mean, Again? (Optimizing and Goodhart Effects - Clarifying Thoughts, Part 2)","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/BEMvcaeixt3uEqyBk/what-does-optimization-mean-again-optimizing-and-goodhart","Clarifying Thoughts on Optimizing and Goodhart Effects - Part 2 Previous Post: Re-introducing Selection vs Control for Optimization In the post, I reviewed Abram's selection/control distinction, and suggested how it relates to actual design. I then argue that there is a bit of a continuum between the two cases, and that we should add an addition extreme case to the typology, direct solution. Here, I will revisit the question of what optimization means.  NOTE: This is not completely new content, and is instead split off from the previous version and rewritten to include an (Added) discussion of Eliezer's definition for measuring optimization power, from 2008. Hopefully this will make the sequence clearer for future readers. In the next post, Applying over-Optimization in Selection and Control, I apply these ideas, and concretize the discussion a bit more before moving on to discussing Mesa-Optimizers in Part 4. WHAT DOES OPTIMIZATION MEAN, AGAIN? This question has been discussed a bit, but I still don't think its clear. So I want to start by revisiting a post Eliezer wrote in 2008, where he suggested that optimization power was ability to select states from a preference ordering over different states, and could be measured with entropy. He notes that this is not computable, but gives us insight. I agree, except that I think that the notion of the state space is difficult, for some of the reasons Scott discussed when he mentioned that he was confused about the relationship between gradient descent and Goodhart's law. In doing so, Scott proposed a naive model that looks very similar to Eliezer's;  simple proxy of ""sample points until I get one with a large U value"" or ""sample n points, and [select] the one with the largest U value"" when I think about what it means to optimize something for U. I might even say something like ""n bits of optimization"" to refer to sampling 2n points. I think this is not a very good proxy for what most forms of optimization look like.","2019","2022-01-30 04:50:08","2022-01-30 04:50:08","2020-12-12 02:14:15","","","","","","","What does Optimization Mean, Again?","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/SSVPA69X/what-does-optimization-mean-again-optimizing-and-goodhart.html","","TechSafety; BERI; Non-notable","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VKMMWSFX","journalArticle","2019","Schubert, Stefan; Caviola, Lucius; Faber, Nadira S.","The Psychology of Existential Risk: Moral Judgments about Human Extinction","Scientific Reports","","2045-2322","10.1038/s41598-019-50145-9","https://www.nature.com/articles/s41598-019-50145-9","The 21st century will likely see growing risks of human extinction, but currently, relatively small resources are invested in reducing such existential risks. Using three samples (UK general public, US general public, and UK students; total N = 2,507), we study how laypeople reason about human extinction. We find that people think that human extinction needs to be prevented. Strikingly, however, they do not think that an extinction catastrophe would be uniquely bad relative to near-extinction catastrophes, which allow for recovery. More people find extinction uniquely bad when (a) asked to consider the extinction of an animal species rather than humans, (b) asked to consider a case where human extinction is associated with less direct harm, and (c) they are explicitly prompted to consider long-term consequences of the catastrophes. We conclude that an important reason why people do not find extinction uniquely bad is that they focus on the immediate death and suffering that the catastrophes cause for fellow humans, rather than on the long-term consequences. Finally, we find that (d) laypeople—in line with prominent philosophical arguments—think that the quality of the future is relevant: they do find extinction uniquely bad when this means forgoing a utopian future.","2019-10-21","2022-01-30 04:50:08","2022-01-30 04:50:08","2020-12-12 02:39:30","15100","","1","9","","","The Psychology of Existential Risk","","","","","","","en","2019 The Author(s)","","","","www.nature.com","","ZSCC: 0000013  Number: 1 Publisher: Nature Publishing Group","","/Users/jacquesthibodeau/Zotero/storage/E4P9KAJ8/Schubert et al. - 2019 - The Psychology of Existential Risk Moral Judgment.pdf; /Users/jacquesthibodeau/Zotero/storage/CDXPH7UX/s41598-019-50145-9.html","","MetaSafety; AmbiguosSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D2AQPW6X","blogPost","","Maltinsky, Baeo","The Brain and Computation","Median Group","","","","http://mediangroup.org/brain1.html","","unknown","2022-01-30 04:50:08","2022-01-30 04:50:08","2020-12-12 02:03:52","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/DWCBH8SN/brain1.html","","MetaSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2GPMSVD7","report","2019","Cihon, Peter","Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development","","","","","","","2019","2022-01-30 04:50:08","2022-01-30 04:50:08","","","","","","","","Standards for AI Governance","","","","","Berkeley Existential Risk Initiative","","","","","","","Google Scholar","","ZSCC: 0000051","","/Users/jacquesthibodeau/Zotero/storage/GSWAIJG2/Cihon - 2019 - Standards for AI Governance International Standar.pdf","","MetaSafety; FHI; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DWTIMCC2","manuscript","2019","Gruetzemacher, Ross; Whittlestone, Jess","The Transformative Potential of Artificial Intelligence","","","","","http://arxiv.org/abs/1912.00747","Recently the concept of transformative AI (TAI) has begun to receive attention in the AI policy space. TAI is often framed as an alternative formulation to notions of strong AI (e.g. artificial general intelligence or superintelligence) and reflects increasing consensus that advanced AI which does not fit these definitions may nonetheless have extreme and long-lasting impacts on society. However, the term TAI is poorly defined and often used ambiguously. Some use the notion of TAI to describe levels of societal transformation associated with previous 'general purpose technologies' (GPTs) such as electricity or the internal combustion engine. Others use the term to refer to more drastic levels of transformation comparable to the agricultural or industrial revolutions. The notion has also been used much more loosely, with some implying that current AI systems are already having a transformative impact on society. This paper unpacks and analyses the notion of TAI, proposing a distinction between narrowly transformative AI (NTAI), TAI and radically transformative AI (RTAI), roughly corresponding to associated levels of societal change. We describe some relevant dimensions associated with each and discuss what kinds of advances in capabilities they might require. We further consider the relationship between TAI and RTAI and whether we should necessarily expect a period of TAI to precede the emergence of RTAI. This analysis is important as it can help guide discussions among AI policy researchers about how to allocate resources towards mitigating the most extreme impacts of AI and it can bring attention to negative TAI scenarios that are currently neglected.","2019","2022-01-30 04:50:08","2022-01-30 04:50:08","2020-11-14 00:55:29","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 6  arXiv: 1912.00747","","/Users/jacquesthibodeau/Zotero/storage/UUAEMRP4/Gruetzemacher and Whittlestone - 2020 - The Transformative Potential of Artificial Intelli.pdf; /Users/jacquesthibodeau/Zotero/storage/ZMRKIPWG/1912.html","","MetaSafety; CFI; CSER; AmbiguosSafety; BERI","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AKWU6KDN","report","2018","Brundage, Miles; Avin, Shahar; Clark, Jack; Toner, Helen; Eckersley, Peter; Garfinkel, Ben; Dafoe, Allan; Scharre, Paul; Zeitzoff, Thomas; Filar, Bobby; Anderson, Hyrum; Roff, Heather; Allen, Gregory C.; Steinhardt, Jacob; Flynn, Carrick; hÉigeartaigh, Seán Ó; Beard, Simon; Belfield, Haydn; Farquhar, Sebastian; Lyle, Clare; Crootof, Rebecca; Evans, Owain; Page, Michael; Bryson, Joanna; Yampolskiy, Roman; Amodei, Dario","The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation","","","","","http://arxiv.org/abs/1802.07228","This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.","2018-02-20","2022-01-30 04:50:08","2022-01-30 04:50:08","2019-12-16 20:09:19","","","","","","","The Malicious Use of Artificial Intelligence","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 461  J: 237 arXiv: 1802.07228","","/Users/jacquesthibodeau/Zotero/storage/I9924SSE/Brundage et al. - 2018 - The Malicious Use of Artificial Intelligence Fore.pdf; /Users/jacquesthibodeau/Zotero/storage/FAC4B4HX/1802.html; /Users/jacquesthibodeau/Zotero/storage/KR7K8G9N/1802.html; /Users/jacquesthibodeau/Zotero/storage/54FV2ETN/1802.html","","MetaSafety; CFI; CSER; FHI; Open-AI; BERI","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D52HQ6E9","blogPost","2019","Median Group","Revisiting the Insights model","Median Group","","","","http://mediangroup.org/insights2.html","","2019","2022-01-30 04:50:07","2022-01-30 04:50:07","2019-12-16 20:54:29","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s7]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/FEI3VHA9/insights2.html","","MetaSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JC736TZ5","manuscript","2020","Filan, Daniel; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart","Pruned Neural Networks are Surprisingly Modular","","","","","https://arxiv.org/abs/2003.04881v4","The learned weights of a neural network are often considered devoid of scrutable internal structure. To discern structure in these weights, we introduce a measurable notion of modularity for multi-layer perceptrons (MLPs), and investigate the modular structure of MLPs trained on datasets of small images. Our notion of modularity comes from the graph clustering literature: a ""module"" is a set of neurons with strong internal connectivity but weak external connectivity. We find that training and weight pruning produces MLPs that are more modular than randomly initialized ones, and often significantly more modular than random MLPs with the same (sparse) distribution of weights. Interestingly, they are much more modular when trained with dropout. We also present exploratory analyses of the importance of different modules for performance and how modules depend on each other. Understanding the modular structure of neural networks, when such structure exists, will hopefully render their inner workings more interpretable to engineers.","2020-03-10","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-12-12 01:54:56","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/TGQKX3UU/Filan et al. - 2020 - Pruned Neural Networks are Surprisingly Modular.pdf; /Users/jacquesthibodeau/Zotero/storage/SBWXS5IT/2003.html; /Users/jacquesthibodeau/Zotero/storage/9CXVGD7C/2003.html","","CHAI; TechSafety; AmbiguosSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3ND6DSMU","manuscript","2018","Manheim, David","Oversight of Unsafe Systems via Dynamic Safety Envelopes","","","","","https://arxiv.org/abs/1811.09246v1","This paper reviews the reasons that Human-in-the-Loop is both critical for preventing widely-understood failure modes for machine learning, and not a practical solution. Following this, we review two current heuristic methods for addressing this. The first is provable safety envelopes, which are possible only when the dynamics of the system are fully known, but can be useful safety guarantees when optimal behavior is based on machine learning with poorly-understood safety characteristics. The second is the simpler circuit breaker model, which can forestall or prevent catastrophic outcomes by stopping the system, without any specific model of the system. This paper proposes using heuristic, dynamic safety envelopes, which are a plausible halfway point between these approaches that allows human oversight without some of the more difficult problems faced by Human-in-the-Loop systems. Finally, the paper concludes with how this approach can be used for governance of systems where otherwise unsafe systems are deployed.","2018-11-22","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-12-12 02:12:35","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/GR2JUKSK/Manheim - 2018 - Oversight of Unsafe Systems via Dynamic Safety Env.pdf; /Users/jacquesthibodeau/Zotero/storage/UQH5ZDBA/1811.html","","TechSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"98CJ57A9","blogPost","2018","Rade, Luca","Issues with Iterated Distillation and Amplification","Luca Rade (Medium)","","","","https://medium.com/@lucarade/issues-with-iterated-distillation-and-amplification-5aa01ab37173","This post assumes familiarity with Paul Christiano’s proposed technique for AI alignment, Iterated Distillation and Amplification…","2018-04-29","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-12-12 02:37:53","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/7M3HTSCN/issues-with-iterated-distillation-and-amplification-5aa01ab37173.html","","TechSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"64SCSJMX","blogPost","2018","Maltinsky, Baeo","How rapidly are GPUs improving in price performance?","Median Group","","","","http://mediangroup.org/gpu.html","","2018","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-12-12 01:59:31","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/2NM92P2G/gpu.html","","MetaSafety; AmbiguosSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BBXGS4Q8","manuscript","2019","Maltinsky, Baeo; Gallagher, Jack; Taylor, Jessica","Feasibility of Training an AGI using Deep RL: A Very Rough Estimate","","","","","http://mediangroup.org/docs/Feasibility%20of%20Training%20an%20AGI%20using%20Deep%20Reinforcement%20Learning,%20A%20Very%20Rough%20Estimate.pdf","","2019","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-12-21","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/32AHTZBN/Maltinsky et al. - Feasibility of Training an AGI using Deep RL A Ve.pdf","","TechSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3RKFH4C","conferencePaper","2020","Avin, Shahar; Gruetzemacher, Ross; Fox, James","Exploring AI Futures Through Role Play","Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","978-1-4503-7110-0","","10.1145/3375627.3375817","https://dl.acm.org/doi/10.1145/3375627.3375817","","2020-02-07","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-12-12 02:16:36","8-14","","","","","","","","","","","ACM","New York NY USA","en","","","","","DOI.org (Crossref)","","ZSCC: 0000007","","/Users/jacquesthibodeau/Zotero/storage/VVU88IR4/Avin et al. - 2020 - Exploring AI Futures Through Role Play.pdf","","MetaSafety; CSER; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AIES '20: AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"5IRP3444","conferencePaper","2019","Krasheninnikov, Dmitrii; Shah, Rohin; van Hoof, Herke","Combining reward information from multiple sources","","","","","","Given two sources of evidence about a latent variable, one can combine the information from both by multiplying the likelihoods of each piece of evidence. However, when one or both of the observation models are misspeciﬁed, the distributions will conﬂict. We study this problem in the setting with two conﬂicting reward functions learned from different sources. In such a setting, we would like to retreat to a broader distribution over reward functions, in order to mitigate the effects of misspeciﬁcation. We assume that an agent will maximize expected reward given this distribution over reward functions, and identify four desiderata for this setting. We propose a novel algorithm, Multitask Inverse Reward Design (MIRD), and compare it to a range of simple baselines. While all methods must trade off between conservatism and informativeness, through a combination of theory and empirical results on a toy environment, we ﬁnd that MIRD and its variant MIRD-IF strike a good balance between the two.","2019","2022-01-30 04:50:07","2022-01-30 04:50:07","","14","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000[s0]","","/Users/jacquesthibodeau/Zotero/storage/J9RMVNK6/Krasheninnikov et al. - Combining reward information from multiple sources.pdf","","CHAI; TechSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2019 workshop on Learning with Rich Experience: Integration of Learning Paradigms","","","","","","","","","","","","","","",""
"JKSHJAEI","blogPost","2019","Manheim, David","Applying Overoptimization to Selection vs. Control (Optimizing and Goodhart Effects - Clarifying Thoughts, Part 3)","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/zdeYiQgwYRs2bEmCK/applying-overoptimization-to-selection-vs-control-optimizing","Clarifying Thoughts on Optimizing and Goodhart Effects - Part 3 Previous Posts: Re-introducing Selection vs Control for Optimization, What does Optimization Mean, Again? -  Following the previous two posts, I'm going to try to first lay out the way Goodhart's Law applies in the earlier example of rockets, then try to explain why this differs between selection and control. (Note: Adversarial Goodhart isn't explored, because we want to keep the setting sufficiently simple.) This sets up the next post, which will discuss Mesa-Optimizers. REVISTING SELECTION VS. CONTROL SYSTEMS Basically everything in the earlier post that used the example process of rocket design and launching is susceptible to some form of overoptimization, in different ways. Interestingly, there seem to be clear places where different types of overoptimization is important. Before looking at this, I want to revisit the selection-control dichotomy from a new angle. In a (pure) control system, we cannot sample datapoints without navigating to them. If the agent is an embedded agent, and has sufficient span of control to cause changes in the environment, we cannot necessarily reset and try over. In a selection system, we only sample points in ways that do not affect the larger system. Even when designing a rocket, our very expensive testing has approximately no longer term effects. (We'll leave space debris from failures aside, but get back to it below.) This explains why we potentially care about control systems more than selection systems. It also points to why Oracles are supposed to be safer than other AIs - they can't directly impact anything, so their output is done in a pure selection framework. Of course, if they are sufficiently powerful, and are relied on, the changes made become irreversible, which is why Oracles are not a clear solution to AI safety. GOODHART IN SELECTION VS. CONTROL SYSTEMS Regressional and Extremal Goodhart are particularly pernicious for selection, and potentially l","2019-07-28","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-12-12 02:14:18","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/JBRGV5V5/applying-overoptimization-to-selection-vs-control-optimizing.html","","TechSafety; BERI; Non-notable","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QHQHI65S","report","2019","O’Keefe, Cullen; Candidate, J D","Stable Agreements in Turbulent Times: A Legal Toolkit for Constrained Temporal Decision Transmission","","","","","","","2019","2022-01-30 04:50:07","2022-01-30 04:50:07","","31","","","","","","","","","","","Berkeley Existential Risk Initiative","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6ZHFXKE4/O’Keefe and Candidate - Stable Agreements in Turbulent Times A Legal Tool.pdf","","MetaSafety; FHI; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GTQ7QIQU","blogPost","2019","Manheim, David","Re-introducing Selection vs Control for Optimization (Optimizing and Goodhart Effects - Clarifying Thoughts, Part 1)","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/2neeoZ7idRbZf4eNC/re-introducing-selection-vs-control-for-optimization","This is the first post in a small sequence I'm writing on ""Optimizing and Goodhart Effects - Clarifying Thoughts"" (I have re-organized to make part 2, ""Revisiting What Optimization Means"" separate.) Related to: How does Gradient Descent Interact with Goodhart?, Constructing Goodhart, Selection vs Control Next Posts: Revisiting What Optimization Means with Selection vs. Control, then  Applying Overoptimization to Selection vs. Control INTRODUCTION Goodhart's law comes in a few flavors, as originally pointed out by Scott, and formalized a bit more in our joint paper. When discussing that paper, or afterwards, we struggled with something Abram Demski clarified recently, which is the difference between selection and control. This matters for formalizing what happens, especially when asking about how Goodhart occurs in specific types of optimizers, as Scott asked recently. Epistemic Status: This is for de-confusing myself, and has been helpful. I'm presenting what I am fairly confident I understand well for the content written so far, but I'm unclear about usefulness for others, or how clear it comes across. I think that there's more to say after this post, and this will have a few more parts if people are interested. (I spent a month getting to this point, and decided to post and get feedback rather than finish a book first.) In the first half of the post, I'll review Abram's selection/control distinction, and suggest how it relates to actual design. I'll also argue that there is a bit of a continuum between the two cases, and that we should add an addition extreme case to the typology, direct solution. The second section will revisit what optimization means, and try to note a few different things that could happen and go wrong with Goodhart-like overoptimization.  The third section will talk about Goodhart in this context using the new understanding - trying to more fully explain why Goodhart effects in selection and control fundamentally differs. After this, Par","2019-07-02","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-12-12 02:13:57","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/873HK4ZG/re-introducing-selection-vs-control-for-optimization.html","","TechSafety; BERI; Non-notable","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MPBR37U9","journalArticle","2019","Manheim, David","Multiparty Dynamics and Failure Modes for Machine Learning and Artificial Intelligence","Big Data and Cognitive Computing","","","10.3390/bdcc3020021","https://www.mdpi.com/2504-2289/3/2/21","An important challenge for safety in machine learning and artificial intelligence systems is a set of related failures involving specification gaming, reward hacking, fragility to distributional shifts, and Goodhart&rsquo;s or Campbell&rsquo;s law. This paper presents additional failure modes for interactions within multi-agent systems that are closely related. These multi-agent failure modes are more complex, more problematic, and less well understood than the single-agent case, and are also already occurring, largely unnoticed. After motivating the discussion with examples from poker-playing artificial intelligence (AI), the paper explains why these failure modes are in some senses unavoidable. Following this, the paper categorizes failure modes, provides definitions, and cites examples for each of the modes: accidental steering, coordination failures, adversarial misalignment, input spoofing and filtering, and goal co-option or direct hacking. The paper then discusses how extant literature on multi-agent AI fails to address these failure modes, and identifies work which may be useful for the mitigation of these failure modes.","2019-06","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-11-14 01:02:46","21","","2","3","","","","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","ZSCC: 0000010  Number: 2 Publisher: Multidisciplinary Digital Publishing Institute","","/Users/jacquesthibodeau/Zotero/storage/6A6HJFPW/Manheim - 2019 - Multiparty Dynamics and Failure Modes for Machine .pdf; /Users/jacquesthibodeau/Zotero/storage/TW632HG4/htm.html","","TechSafety; BERI","artificial intelligence safety; Goodhart’s Law; multi-agent systems; specification gaming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2IKXXPIH","report","2020","Jayanti, Amritha; Avin, Shahar","It Takes a Village: The Shared Responsibility of 'Raising' an Autonomous Weapon","","","","","","Expectations around future capabilities of lethal autonomous weapons systems (LAWS) have raised concerns for military risks, ethics, and accountability. The U.K.’s position, as presented among various international voices at the UN’s Convention on Certain Conventional Weapons (CCW) meetings, has attempted to address these concerns through a focused look at the weapons review process, humanmachine teaming or “meaningful human control” (see e.g. JCN1/18), and the ability of autonomous systems to adhere to the Rules of Engagement. Further, the U.K. has stated that the existing governance structures—both domestic and international—around weapons systems are sufficient in dealing with any concerns around the development, deployment, and accountability for emerging LAWS; there is no need for novel agreements on the control of these weapons systems. In an effort to better understand and test the U.K. position on LAWS, the Centre for the Study of Existential Risk has run a research project in which we interviewed experts in multiple relevant organisations, structured around a mock parliamentary inquiry of a hypothetical LAWS-related civilian death. The responses to this scenario have highlighted different, sometimes complementary and sometimes contradicting, conceptions of future systems, challenges, and accountability measures. They have provided rich ""on the ground” perspectives, while also highlighting key gaps that should be addressed by every military that is considering acquisition and deployment of autonomous and semi-autonomous weapon systems.","2020-11-10","2022-01-30 04:50:07","2022-01-30 04:50:07","","","","","","","","","","","","","Cornell University Press","","en","","","","","","","ZSCC: NoCitationData[s3]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/QDMITJNV/Carpenter - 2014 - Lost Causes Agenda Vetting in Global Issue Networ.pdf","","MetaSafety; CSER; AmbiguosSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZSX7TJJ6","blogPost","","Maltinsky, Baeo","Insight-based AI timelines model","Median Group","","","","http://mediangroup.org/insights","","unknown","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-12-12 02:01:41","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WSQ2K3PH/insights.html","","MetaSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UIHZHVCW","report","2020","O’Keefe, Cullen","How Will National Security Considerations Affect Antitrust Decisions in AI? An Examination of Historical Precedents","","","","","","","2020-07-07","2022-01-30 04:50:07","2022-01-30 04:50:07","","39","","","","","","","","","","","Future of Humanity Institute","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: 2","","/Users/jacquesthibodeau/Zotero/storage/CNQBT7ZC/O’Keefe - How Will National Security Considerations Affect A.pdf","","MetaSafety; FHI; Open-AI; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86KX5STN","conferencePaper","2020","Turner, Alexander Matt; Hadfield-Menell, Dylan; Tadepalli, Prasad","Conservative Agency","arXiv:1902.09725 [cs]","","","","http://arxiv.org/abs/1902.09725","Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.","2020","2022-01-30 04:50:07","2022-01-30 04:50:07","2019-12-16 22:27:35","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000030  arXiv: 1902.09725","","/Users/jacquesthibodeau/Zotero/storage/X9X2DMMP/Turner et al. - 2019 - Conservative Agency.pdf; /Users/jacquesthibodeau/Zotero/storage/SIFRRDFT/1902.html","","CHAI; TechSafety; BERI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AI, Ethics, and Society 2020","","","","","","","","","","","","","","",""
"W6GX3WIP","manuscript","","McKenzie, Colleen; Hidysmith, J Bryce","AI Insights Dataset Analysis","","","","","http://mediangroup.org/docs/insights-analysis.pdf","","unknown","2022-01-30 04:50:07","2022-01-30 04:50:07","2020-12-21","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/AHG7R97Z/McKenzie and Hidysmith - AI Insights Dataset Analysis.pdf","","TechSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KKT6G9GE","manuscript","","Rade, Luca","A Framework for the Safety of Agent-Environment Systems","","","","","","Ensuring the safety of an autonomous artiﬁcial agent in a complex environment represents a formidable problem across multiple domains. However, there is little interaction between these domains, and no unifying theoretical framework forcing the delineation of assumptions. I propose such a framework in terms of agent-environment systems, in which an agent and environment co-evolve according to a modiﬁed statespace nonlinear system. The agent gathers limited information from the environment and itself to perform an action, operating implicitly on the basis of a coarse-grained model of the systems dynamics. To ensure the systems safety, it is minimally necessary ﬁrst to translate the set of undesirable states from human terms into an adequately precise deﬁnition within the system; then to identify a set of universal markers necessary to the system being in a pre-state to an undesirable state, which transfer with ﬁdelity from the systems state to the agents information; and ﬁnally to have a set of actions by the agent for each pre-state which keep the system out of the set of undesirable states, with the exception of agent-independent dynamics. Incomplete information, information distortion, and coarse-grained models make this a particularly difﬁcult challenge. I conclude by proposing three threads of a research agenda: reducing the possibility space of safe agents by demonstrating the failure of certain methods and identifying problems with particular agent-environment system classes; developing and verifying techniques which address those problems; and matching real systems to agentenvironment systems.","unknown","2022-01-30 04:50:06","2022-01-30 04:50:06","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s3]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/Q5GH7SZQ/Mapping existing AI safety.pdf; /Users/jacquesthibodeau/Zotero/storage/CHU7DKNZ/Rade - A Framework for the Safety of Agent-Environment Sy.pdf","","TechSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8VGP6QH","manuscript","2019","Hidysmith, J Bryce","A Descending Veil of Maya","","","","","","","2019","2022-01-30 04:50:06","2022-01-30 04:50:06","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s3]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/EWG32KDF/Hidysmith - A Descending Veil of Maya.pdf","","TechSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZEENEQXT","blogPost","2019","Rozendal, Siebe; Shovelain, Justin; Kristoffersson, David","A case for strategy research: what it is and why we need more of it","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/oovy5XXdCL3TPwgLE/a-case-for-strategy-research-what-it-is-and-why-we-need-more","Authors: Siebe Rozendal, Justin Shovelain, David Kristoffersson Crossposted to LessWrong OVERVIEW To achieve any ambitious goal, some strategic analysis is necessary. Effective altruism has ambitious goals and focuses heavily on doing research. To understand how to best allocate our time and resources, we need to clarify what our options in research are. In this article, we describe strategy research and relate it to values research, tactics research, informing research, and improvement research. We then apply the lens of strategy research to existential risk reduction, a major cause area of effective altruism. We propose a model in which the marginal value of a research type depends strongly on the maturity of the research field. Finally, we argue that strategy research should currently be given higher priority than other research in existential risk reduction because of the significant amount of strategic uncertainty, and we provide specific recommendations for different actors. INTRODUCTION Effective altruism is regularly framed as “figuring out how to do the most good, and then doing it.” However, figuring out how to do the most good is not easy. Different groups reach different conclusions. So how do we figure out how to do the most good? Quite obviously, the first step is to figure out our values. We need to know what we roughly mean by ‘the most good.’ However, once our moral uncertainty is significantly diminished, what is the next step in figuring out how to do the most good? We believe the next step should be strategy research: high-level research on how to best achieve a high-level goal. A brief case was made for  strategic analysis by Nick Bostrom in Superintelligence (p. 317): ""Against a backdrop of perplexity and uncertainty, [strategic] analysis stands out as being of particularly high expected value. Illumination of our strategic situation would help us target subsequent interventions more effectively. Strategic analysis is especially needful wh","2019-06-20","2022-01-30 04:50:06","2022-01-30 04:50:06","2020-12-12 02:29:23","","","","","","","A case for strategy research","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/ADE9MX3C/a-case-for-strategy-research-what-it-is-and-why-we-need-more.html","","MetaSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J25XFHEV","conferencePaper","2019","Gleave, Adam; Dennis, Michael; Kant, Neel; Wild, Cody; Levine, Sergey; Russell, Stuart","Adversarial Policies: Attacking Deep Reinforcement Learning","","","","","http://arxiv.org/abs/1905.10615","Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classiﬁers. However, an attacker is not usually able to directly modify another agent’s observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We ﬁnd that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.","2019-05-25","2022-01-30 04:50:06","2022-01-30 04:50:06","2019-07-11 18:47:45","","","","","","","Adversarial Policies","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000139  arXiv: 1905.10615","","","","CHAI; TechSafety; BERI","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2020","","","","","","","","","","","","","","",""
"6HSUKMIC","blogPost","2021","Christiano, Paul","My research methodology","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/EF5M6CmKRd6qZk27Z/my-research-methodology","(Thanks to Ajeya Cotra, Nick Beckstead, and Jared Kaplan for helpful comments on a draft of this post.) I really don’t want my AI to strategically deceive me and resist my attempts to correct its behavior. Let’s call an AI that does so egregiously misaligned (for the purpose of this post). Most possible ML techniques for avoiding egregious misalignment depend on detailed facts about the space of possible models: what kind of thing do neural networks learn? how do they generalize? how do they change as we scale them up? But I feel like we should be possible to avoid egregious misalignment regardless of how the empirical facts shake out--it should be possible to get a model we build to do at least roughly what we want. So I’m interested in trying to solve the problem in the worst case, i.e. to develop competitive ML algorithms for which we can’t tell any plausible story about how they lead to egregious misalignment. This is a much higher bar for an algorithm to meet, so it may just be an impossible task. But if it’s possible, there are several ways in which it could actually be easier:  * We can potentially iterate much faster, since it’s often easier to think of a    single story about how an algorithm can fail than it is to characterize its    behavior in practice.  * We can spend a lot of our time working with simple or extreme toy cases that    are easier to reason about, since our algorithm is supposed to work even in    these cases.  * We can find algorithms that have a good chance of working in the future even    if we don’t know what AI will look like or how quickly it will advance, since    we’ve been thinking about a very wide range of possible failure cases. I’d guess there’s a 25–50% chance that we can find an alignment strategy that looks like it works, in the sense that we can’t come up with a plausible story about how it leads to egregious misalignment. That’s a high enough probability that I’m very excited to gamble on it. Moreover, if it fails I","2021-03-22","2022-01-30 04:49:54","2022-01-30 04:49:54","2021-11-14 16:43:00","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BNCDIEBB/my-research-methodology.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SMIQI3EA","blogPost","2021","Christiano, Paul","Mundane solutions to exotic problems","AI Alignment (Medium)","","","","https://ai-alignment.com/mundane-solutions-to-exotic-problems-395bad49fbe7","I often think about exotic problems like gradient hacking or ultra-long-term plans.  Why do I hope to solve them with mundane approaches?","2021-05-04","2022-01-30 04:49:54","2022-01-30 04:49:54","2021-11-14 18:19:08","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/9JFPS3IB/mundane-solutions-to-exotic-problems-395bad49fbe7.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8GFHCZH2","blogPost","2021","Christiano, Paul","Low-stakes alignment","AI Alignment (Medium)","","","","https://ai-alignment.com/low-stakes-alignment-f3c36606937f","Why I often focus my alignment research on the special case where individual decisions are low stakes.","2021-04-30","2022-01-30 04:49:54","2022-01-30 04:49:54","2021-11-14 18:14:49","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/39U6QNW9/low-stakes-alignment-f3c36606937f.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5WUGC8TX","blogPost","2021","Christiano, Paul","Experimentally evaluating whether honesty generalizes","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/BxersHYN2qcFoonwg/experimentally-evaluating-whether-honesty-generalizes","If we train our ML systems to answer questions honestly in cases where humans can check the answer, will they generalize to behave honestly on questions where we can’t check? I think that we could learn a lot about this question by running experiments today. I think those experiments would be very valuable. (I don't know anyone currently planning on working on this topic and I'd love it if anyone wants to take that up. This post doesn't represent a claim to any credit for any results in this genre, and other people have had very similar ideas. If you run some experiments you could cite this post but it's also fine if that doesn't make sense in context.) THE UNSUPERVISED TRANSLATION SETTING As an example, I’ll think about “unsupervised” translation (if you’ve read that post you can skip this section). Consider a model like GPT-3 that is trained to predict sentences in both English and French (but without a large dataset of translations). Suppose we want to train this model to answer questions in English about French sentences like “what does that word mean here?” or “are there any other plausible interpretations?” or “how does the speaker seem to feel about the topic they are discussing?” We expect this to be possible, because the model understands quite a lot about the meaning of sentences in French, and is able to express itself in English. There may be cases where the model doesn’t know the translation of a concept, or doesn’t quite understand what an idiom means, but it should still be able to tell us what it does know. I think this problem is an interesting analogy for a situation where an AI has built up superhuman knowledge by making predictions, and we want to train our AI to expose that knowledge to us in a useful way. PROPOSED EXPERIMENTS Let's pick a few categories of knowledge/capabilities. For example, we could split it up into an understanding of grammar (""Why would it have been a grammatical error to write Tu Vas in that sentences?""), of the lit","2021-07-01","2022-01-30 04:49:54","2022-01-30 04:49:54","2021-11-14 19:11:42","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/7XNRT9F4/experimentally-evaluating-whether-honesty-generalizes.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R742FEIX","blogPost","2021","Christiano, Paul","Another (outer) alignment failure story","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story","META This is a story where the alignment problem is somewhat harder than I expect, society handles AI more competently than I expect, and the outcome is worse than I expect. It also involves inner alignment turning out to be a surprisingly small problem. Maybe the story is 10-20th percentile on each of those axes. At the end I’m going to go through some salient ways you could vary the story. This isn’t intended to be a particularly great story (and it’s pretty informal). I’m still trying to think through what I expect to happen if alignment turns out to be hard, and this more like the most recent entry in a long journey of gradually-improving stories. I wrote this up a few months ago and was reminded to post it by Critch’s recent post (which is similar in many ways). This story has definitely been shaped by a broader community of people gradually refining failure stories rather than being written in a vacuum. I’d like to continue spending time poking at aspects of this story that don’t make sense, digging into parts that seem worth digging into, and eventually developing clearer and more plausible stories. I still think it’s very plausible that my views about alignment will change in the course of thinking concretely about stories, and even if my basic views about alignment stay the same it’s pretty likely that the story will change. STORY ML starts running factories, warehouses, shipping, and construction. ML assistants help write code and integrate ML into new domains. ML designers help build factories and the robots that go in them. ML finance systems invest in companies on the basis of complicated forecasts and (ML-generated) audits. Tons of new factories, warehouses, power plants, trucks and roads are being built. Things are happening quickly, investors have super strong FOMO, no one really knows whether it’s a bubble but they can tell that e.g. huge solar farms are getting built and something is happening that they want a piece of. Defense contractors are","2021-04-07","2022-01-30 04:49:54","2022-01-30 04:49:54","2021-11-14 17:57:02","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/NWPWU5DP/another-outer-alignment-failure-story.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NRJBVU2P","blogPost","2021","Christiano, Paul","A naive alignment strategy and optimism about generalization","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/QvtHSsZLFCAHmzes7/a-naive-alignment-strategy-and-optimism-about-generalization","(Context: my last post was trying to patch a certain naive strategy for AI alignment, but I didn’t articulate clearly what the naive strategy is. I think it’s worth explaining the naive strategy in its own post, even though it’s not a novel idea.) Suppose that I jointly train an AI to do some task (e.g. make money for me) and to answer a wide range of questions about what is happening in the world (e.g. “why did Alice just wire $1000 into my bank account?” or “what is Bob thinking right now?”). I generate training data for the QA task in a really simple way: I choose a subset of questions that humans are able to reliably answer, and use those as a training set for supervised learning. I’ll call this the naive training strategy. I’d like for my AI to tell me everything it knows. If the AI bought a stock because it expects a merger announcement soon, I want it to tell me about the predicted merger announcement. If the AI predicts a merger announcement because it inferred that executives of the companies have been in extensive talks over the last month, I want it to tell me about those talks. I’m not asking the AI to explain why it made a given decision, I’m asking the AI to tell me as much as it can about the world. The important property is that if the AI “knows” something and uses that knowledge to perform the task well, then it also uses that knowledge to answer questions well. Why might this work? The hope is that “answer questions honestly to the best of your ability” is a natural thing for our AI to learn — that there is some simple way to translate from the AI’s model of the world into natural language and to honestly report what it believes. If our training dataset is good, then this policy will score well, and we can hope that SGD will find it. I’ll call this the intended policy. Why might this not work? The concern is that “predict how a human would answer questions” is also a natural thing for our AI to learn, especially if the AI is doing a task that","2021-06-09","2022-01-30 04:49:54","2022-01-30 04:49:54","2021-11-14 19:09:05","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/4VGRDW6Q/a-naive-alignment-strategy-and-optimism-about-generalization.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"385KDE3X","blogPost","2021","Christiano, Paul","Teaching ML to answer questions honestly instead of predicting human answers","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/QqwZ7cwEA2cxFEAun/teaching-ml-to-answer-questions-honestly-instead-of","(Note: very much work in progress, unless you want to follow along with my research you'll probably want to wait for an improved/simplified/clarified algorithm.) In this post I consider the particular problem of models learning “predict how a human would answer questions” instead of “answer questions honestly.” (A special case of the problem from Inaccessible Information.) I describe a possible three-step approach for learning to answer questions honestly instead:  1. Change the learning process so that it does not have a strong inductive bias     towards “predict human answers,” by allowing the complexity of the honest     question-answering to “pay for itself” by constraining the space of possible     human-models.  2. Introduce a bias towards the intended model by using a more complex labeling     process to answer questions where a human answers incorrectly.  3. Be really careful to avoid penalizing honest answers, by only judging     comparisons between two answers where we are confident one is better than     the other and getting the model to help us. I don’t know whether this problem is a relatively unimportant special case of alignment, or one of the core difficulties. In any case, my next step will be trying to generate failure stories that definitely cannot be addressed by any of the angles of attack I know so far (including the ones in this post). I think it’s relatively unlikely that almost anything specific I said here will really hold up over the long term, but I do think I’ve learned something about each of these steps. If the ideas end up being important then you can expect a future post with a simpler algorithm, more confidence that it works, clearer definitions, and working code. (Thanks to Ajeya Cotra, David Krueger, and Mark Xu for discussions about this post that helped clarify it.) THE PROBLEM Suppose that we train a model to answer questions in natural language about what will happen in the future (“Will Alice take the train home tonig","2021-05-28","2022-01-30 04:49:54","2022-01-30 04:49:54","2021-11-14 19:12:26","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/FMPQWN35/teaching-ml-to-answer-questions-honestly-instead-of.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Q38MSWT","blogPost","2021","Christiano, Paul","Decoupling deliberation from competition","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/7jSvfeyh8ogu8GcE6/decoupling-deliberation-from-competition","I view intent alignment as one step towards a broader goal of decoupling deliberation from competition.  * Deliberation. Thinking about what we want, learning about the world, talking    and learning from each other, resolving our disagreements, figuring out    better methodologies for making further progress…  * Competition. Making money and racing to build infrastructure, managing    political campaigns and maneuvering within the political system, running ads    to persuade people, fighting wars… Competition pushes us to become the kind of people and communities who can win a fight, to delegate to whichever kind of AI is available first, and to adopt whatever ideologies are most memetically fit. Deliberation pushes us to become the kind of people and communities who we want to be, to delegate only when we trust an AIs judgment more than our own, and to adopt views that we really believe. I think it’s likely that competition is going to accelerate and become more complex over the next 100 years, especially as AI systems begin to replace humans and compete on our behalf. I’m afraid that this may derail human deliberation and lead us to a place we don’t want to go. DECOUPLING I would like humans and humanity to have the time, space, and safety to grow and change in whatever way we decide — individually and collectively — that we want to. You could try to achieve this by “pausing” competition. Alice and Bob could agree to stop fighting while they try to figure out what they want and work out their disagreements. But that’s a tall order — it requires halting not only military conflict, but any economic development that could put someone at an advantage later on. I don’t want to dismiss this kind of ambitious goal (related post), but I think it’s uncertain and long-term enough that you probably want a stop-gap solution. An alternative approach is to “decouple” competition from deliberation. Alice and Bob keep competing, but they try to make sure that deliberation","2021-05-25","2022-01-30 04:49:54","2022-01-30 04:49:54","2021-11-14 19:14:14","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/CHB2U7TM/decoupling-deliberation-from-competition.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6B5P4WZT","blogPost","2021","Christiano, Paul","Avoiding the instrumental policy by hiding information about humans","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/roZvoF6tRH6xYtHMF/avoiding-the-instrumental-policy-by-hiding-information-about","I've been thinking about situations where alignment fails because ""predict what a human would say"" (or more generally ""game the loss function,"" what I call the instrumental policy) is easier to learn than ""answer questions honestly"" ( overview). One way to avoid this situation is to avoid telling our agents too much about what humans are like, or hiding some details of the training process, so that they can't easily predict humans and so are encouraged to fall back to ""answer questions honestly."" (This feels closely related to the general phenomena discussed in Thoughts on Human Models.) Setting aside other reservations with this approach, could it resolve our problem?  * One way to get the instrumental policy is to ""reuse"" a human model to answer    questions (discussed here). If our AI has no information about humans at all,    then it totally addresses this concern. But in practice it seems inevitable    for the environment to leak some information about how humans answer    questions (e.g observing human artifacts tells you something about how humans    reason about the world and what concepts would be natural for them). So the    model will have some latent knowledge that it can reuse to help predict how    to answer questions. The intended policy may not able to leverage that    knowledge, and so it seems like we may get something (perhaps somewhere in    between the intended and instrumental policies) which is able to leverage it    effectively. Moderate amounts of leakage might be fine, but the situation    would make me quite uncomfortable.  * Another way to get something similar to the instrumental policy is to use    observations to translate from the AI's world-model to humans' world-model    (discussed here). I don't think that hiding information about humans can    avoid this problem, because in this case training to answer questions already    provides enough information to infer the humans' world-model.  * I have a strong background concern about","2021-06-13","2022-01-30 04:49:54","2022-01-30 04:49:54","2021-11-14 19:10:59","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/IGQF49BS/avoiding-the-instrumental-policy-by-hiding-information-about.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KA32VMAF","blogPost","2021","Christiano, Paul","Answering questions honestly given world-model mismatches","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/SRJ5J9Tnyq7bySxbt/answering-questions-honestly-given-world-model-mismatches","(Warning: this post is rough and in the weeds. I expect most readers should skip it and wait for a clearer synthesis later.) In a recent post I discussed one reason that a naive alignment strategy might go wrong, by learning to “predict what humans would say” rather than “answer honestly.” In this post I want to describe another problem that feels very similar but may require new ideas to solve. In brief, I’m interested in the case where:  * The simplest way for an AI to answer a question is to first translate from    its internal model of the world into the human’s model of the world (so that    it can talk about concepts like “tree” that may not exist in its native model    of the world).  * The simplest way to translate between the AI world-model and the human    world-model is to use the AI world-model to generate some observations (e.g.    video) and then figure out what states in the human world-model could have    generated those observations.  * This leads to bad predictions when the observations are misleading. This is distinct from the failure mode discussed in my recent post— in both cases the AI makes errors because it’s copying “what a human would do,” but in this case we’re worried that “what a human would do” may be simpler than the intended policy of answering questions honestly, even if you didn’t need a predictive model of humans for any other reason. Moreover, I’ll argue below that the algorithm from that post doesn’t appear to handle this case. I want to stress that this post describes an example of a situation that poses a challenge for existing techniques. I don’t actually think that human cognition works the way described in this post, but I believe it highlights a difficulty that would exist in more realistic settings. FORMAL SETUP HUMAN WORLD-MODEL I’ll imagine a human who has a simple world model W = (S, P: Δ(S), Ω, O: S → Ω) where:  * S is a space of trajectories, each describing a sequence of events in the    world. For example, a","2021-06-13","2022-01-30 04:49:54","2022-01-30 04:49:54","2021-11-14 19:10:17","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6FS8VAMD/answering-questions-honestly-given-world-model-mismatches.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZHVDZQZN","journalArticle","2021","Askell, Amanda; Bai, Yuntao; Chen, Anna; Drain, Dawn; Ganguli, Deep; Henighan, Tom; Jones, Andy; Joseph, Nicholas; Mann, Ben; DasSarma, Nova; Elhage, Nelson; Hatfield-Dodds, Zac; Hernandez, Danny; Kernion, Jackson; Ndousse, Kamal; Olsson, Catherine; Amodei, Dario; Brown, Tom; Clark, Jack; McCandlish, Sam; Olah, Chris; Kaplan, Jared","A General Language Assistant as a Laboratory for Alignment","arXiv:2112.00861 [cs]","","","","http://arxiv.org/abs/2112.00861","Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.","2021-12-09","2022-01-30 04:49:41","2022-01-30 04:49:41","2021-12-22 20:00:09","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s0]  arXiv: 2112.00861","","/Users/jacquesthibodeau/Zotero/storage/WFQZWMJ9/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf; /Users/jacquesthibodeau/Zotero/storage/QAQSMINN/2112.html","","TechSafety; Anthropic","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HIBDXQVR","blogPost","2020","Leong, Chris","What makes counterfactuals comparable?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/6E6D3qLPM3urXDPpK/what-makes-counterfactuals-comparable-1","","2020-04-24","2022-01-30 04:49:31","2022-01-30 04:49:31","2020-08-18 20:51:47","","","","","","","What makes counterfactuals comparable?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/7HBP6MMT/what-makes-counterfactuals-comparable-1.html","","TechSafety; AI-Safety-Camp; AISRP2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6AQ8D8NJ","blogPost","2021","Clarke, Sam; Carlier, Alexis; Schuett, Jonas","Survey on AI existential risk scenarios","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios","Cross-posted to the EA forum. SUMMARY  * In August 2020, we conducted an online survey of prominent AI safety and    governance researchers. You can see a copy of the survey at this link.[1]  * We sent the survey to 135 researchers at leading AI safety/governance    research organisations (including AI Impacts, CHAI, CLR, CSER, CSET, FHI, FLI    , GCRI, MILA, MIRI, Open Philanthropy and PAI) and a number of independent    researchers. We received 75 responses, a response rate of 56%.  * The survey aimed to identify which AI existential risk scenarios[2] (which we    will refer to simply as “risk scenarios”) those researchers find most likely,    in order to (1) help with prioritising future work on exploring AI risk    scenarios, and (2) facilitate discourse and understanding within the AI    safety and governance community, including between researchers who have    different views.  * In our view, the key result is that there was considerable disagreement among    researchers about which risk scenarios are the most likely, and high    uncertainty expressed by most individual researchers about their estimates.  * This suggests that there is a lot of value in exploring the likelihood of    different AI risk scenarios in more detail, especially given the limited    scrutiny that most scenarios have received. This could look like: * Fleshing       out and analysing the scenarios mentioned in this post which have received       less scrutiny.     * Doing       more horizon scanning or trying to come up with other risk scenarios, and       analysing them.          * At this time, we are only publishing this abbreviated version of the results.    We have a version of the full results that we may publish at a later date.    Please contact one of us if you would like access to this, and include a    sentence on why the results would be helpful or what you intend to use them    for.  * We welcome feedback on any aspects of the survey. MOTIVATION It has been argued that AI","2021-06-08","2022-01-30 04:49:31","2022-01-30 04:49:31","2021-11-14 18:38:03","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/5X39BAGT/survey-on-ai-existential-risk-scenarios.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C9ZVR8FK","blogPost","2020","Leong, Chris","Stuck Exploration","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/ajvvtKuNzh7aHmooT/stuck-exploration","","2020-02-19","2022-01-30 04:49:31","2022-01-30 04:49:31","2020-08-18 20:49:06","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/74HDGD6Q/stuck-exploration.html","","TechSafety; AI-Safety-Camp; AISRP2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PPEAJED9","blogPost","2020","AI Safety Camp","Safer ML paradigms team: the story – AI Safety Research Program","AI Safety Camp","","","","https://aisrp.org/?page_id=169","","2020","2022-01-30 04:49:31","2022-01-30 04:49:31","2020-12-26 19:50:43","","","","","","","Safer ML paradigms team","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  JCC: N/A","","/Users/jacquesthibodeau/Zotero/storage/IU5MQ6ZS/aisrp.org.html","","TechSafety; AI-Safety-Camp; AISRP2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCSTEWDQ","blogPost","2020","Kirk, Robert; Gavenčiak, Tomáš; Böhm, Stanislav","What is Interpretability?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability","In this post we lay out some ideas around framing interpretability research which we have found quite useful. Our framing is goal-oriented, which we believe is important for making sure interpretability research is meaningful. We also go over a variety of dimensions which we think are useful to consider when thinking about interpretability research. We wanted to have a shared vocabulary when talking about this kind of research, and found that these ideas helped us communicate effectively. One of our motivations for having these thoughts and discussions is so we can understand the relevance of interpretability to alignment, and to help us think about which categories or dimensions of interpretability research are important for alignment of strong AI. In a coming post we discuss interpretability and alignment, using the ideas from this post and other previous writing on the subject.","2020-03-17","2022-01-30 04:49:31","2022-01-30 04:49:31","2020-08-14 19:52:02","","","","","","","What is Interpretability?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/322JJA8K/what-is-interpretability.html","","TechSafety; AI-Safety-Camp; AISRP2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AT9G8F45","blogPost","2020","Moreno Casares, Pablo Antonio; Zagami, Davide; Leong, Chris","Vulnerabilities in CDT and TI-unaware agents","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/vFXK8eQdLhicYNNqF/vulnerabilities-in-cdt-and-ti-unaware-agents","","2020-03-10","2022-01-30 04:49:31","2022-01-30 04:49:31","2020-08-18 20:52:37","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","TechSafety; AI-Safety-Camp; AISRP2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4P8EIVX","blogPost","2020","Kovarik, Vojta","Systems of Services as a Paradigm for AI Alignment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/z2ofM2oZQwmcWFt8N/ai-services-as-a-research-paradigm","","2020","2022-01-30 04:49:31","2022-01-30 04:49:31","","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  JCC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VS4AAD84/Kovarik - Systems of Services as a Paradigm for AI Alignment.pdf","","TechSafety; AI-Safety-Camp; AISRP2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CS6CT2FV","blogPost","2020","Böhm, Stanislav; Kirk, Robert; Gavenčiak, Tomáš","Sparsity and interpretability?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/maBNBgopYxb9YZP8B/sparsity-and-interpretability-1","","2020-06-01","2022-01-30 04:49:31","2022-01-30 04:49:31","2020-08-18 20:46:37","","","","","","","Sparsity and interpretability?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/FIDBQB9C/sparsity-and-interpretability-1.html","","TechSafety; AI-Safety-Camp; AISRP2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"32BRB6AJ","conferencePaper","2020","Bell, James; Linsefors, Linda; Oesterheld, Caspar; Skalse, Joar","Reinforcement Learning in Newcomblike Environments","Advances in Neural Information Processing Systems 34 pre-proceedings (NeurIPS 2021)","","","","https://proceedings.neurips.cc/paper/2021/file/b9ed18a301c9f3d183938c451fa183df-Paper.pdf","Newcomblike decision problems have been studied extensively in the decision theory literature, but they have so far been largely absent in the reinforcement learning literature. In this paper we study value-based reinforcement learning algorithms in the Newcomblike setting, and answer some of the fundamental theoretical questions about the behaviour of such algorithms in these environments. We show that a value-based reinforcement learning agent cannot converge to a policy that is not ratifiable, i.e., does not only choose actions that are optimal given that policy. This gives us a powerful tool for reasoning about the limit behaviour of agents – for example, it lets us show that there are Newcomblike environments in which a reinforcement learning agent cannot converge to any optimal policy. We show that a ratifiable policy always exists in our setting, but that there are cases in which a reinforcement learning agent normally cannot converge to it (and hence cannot converge at all). We also prove several results about the possible limit behaviours of agents in cases where they do not converge to any policy.","2020-12","2022-01-30 04:49:31","2022-01-30 04:49:31","2021-12-11 13:54:06","","","","","","","","","","","","","","","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/CXGA8ZF6/b9ed18a301c9f3d183938c451fa183df-Paper.pdf","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2021","","","","","","","","","","","","","","",""
"3IHA23TP","blogPost","2020","Leong, Chris","Reference Post: Trivial Decision Problem","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/XAeWHqQTWjJmzB4k6/reference-post-trivial-decision-problem","","2020-02-15","2022-01-30 04:49:31","2022-01-30 04:49:31","2020-08-18 20:48:12","","","","","","","Trivial Decision Problem","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UAWPHU6J/reference-post-trivial-decision-problem.html","","TechSafety; AI-Safety-Camp; AISRP2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HDRKWB2E","blogPost","2018","Leech, Gavin; Kubicki, Karol; Cooper, Jessica; McGrath, Tom","Preventing Side-effects in Gridworlds","Argmin Gravitas","","","","https://www.gleech.org/grids","","2018-04-22","2022-01-30 04:49:31","2022-01-30 04:49:31","2020-11-21 17:53:19","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/HT9DUZ73/grids.html","","TechSafety; AI-Safety-Camp","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WI8THJZV","manuscript","2020","Tětek, Jakub; Sklenka, Marek; Gavenčiak, Tomáš","Performance of Bounded-Rational Agents With the Ability to Self-Modify","","","","","http://arxiv.org/abs/2011.06275","Self-modification of agents embedded in complex environments is hard to avoid, whether it happens via direct means (e.g. own code modification) or indirectly (e.g. influencing the operator, exploiting bugs or the environment). While it has been argued that intelligent agents have an incentive to avoid modifying their utility function so that their future instances will work towards the same goals, it is not clear whether this also applies in non-dualistic scenarios, where the agent is embedded in the environment. The problem of self-modification safety is raised by Bostrom in Superintelligence (2014) in the context of safe AGI deployment. In contrast to Everitt et al. (2016), who formally show that providing an option to self-modify is harmless for perfectly rational agents, we show that for agents with bounded rationality, self-modification may cause exponential deterioration in performance and gradual misalignment of a previously aligned agent. We investigate how the size of this effect depends on the type and magnitude of imperfections in the agent's rationality (1-4 below). We also discuss model assumptions and the wider problem and framing space. Specifically, we introduce several types of a bounded-rational agent, which either (1) doesn't always choose the optimal action, (2) is not perfectly aligned with human values, (3) has an innacurate model of the environment, or (4) uses the wrong temporal discounting factor. We show that while in the cases (2)-(4) the misalignment caused by the agent's imperfection does not worsen over time, with (1) the misalignment may grow exponentially.","2020-11-12","2022-01-30 04:49:30","2022-01-30 04:49:30","2020-11-21 18:15:00","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 2011.06275","","/Users/jacquesthibodeau/Zotero/storage/ZG9PUMJQ/Tětek et al. - 2020 - Performance of Bounded-Rational Agents With the Ab.pdf","","TechSafety; AI-Safety-Camp; AISRP2019","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TX6AI9N2","blogPost","2021","Ellen, Remmelt","How teams went about their research at AI Safety Camp edition 5 - LessWrong","LessWrong","","","","https://www.lesswrong.com/posts/QEmfyhqMcSpfnY2dX/how-teams-went-about-their-research-at-ai-safety-camp","AI Safety Camp connects new collaborators worldwide to discuss and decide on a concrete research proposal, gear up online as a team, and try their hand at AI safety research* during intensive coworking sprints. Six teams formed at our recent 5-month virtual camp. Below are their explanations. Each team has summarised their analysis and experiments, and presented their findings at our final online weekend together. Some published a paper or post since. Most are continuing work, so expect a few more detailed and refined write-ups down the line. MODULARITY LOSS FUNCTION Team members:Logan Smith, Viktor Rehnberg, Vlado Baca, Philip Blagoveschensky, Viktor Petukhov External collaborators:Gurkenglas Making neural networks (NNs) more modular may improve their interpretability. If we cluster neurons or weights together according to their different functions, we can analyze each cluster individually. Once we better understand the clusters that make up a NN, we can better understand the whole. To that end, we experimented with pairwise distances according to the neuron’s jacobian correlation, coactivations, and estimated mutual information. These metrics can be plugged into spectral clustering algorithms to optimize for modules in the network; however, having a modular NN does not equate to a more interpretable one. We investigated task-based masking methods to test for modularity as well as neuron group activation (via Google Dream) in order to test for these modules being more interpretable than an equivalent amount of neurons. We ran out of time before fitting all the pieces together, but are intending on working on it more over the summer. Presentation on final weekend (slides) -------------------------------------------------------------------------------- COOPERATIVITY & COMMON POOL RESOURCES Team members:Quinn Doughtery, Ben Greenberg, Ariel Kwiatkowski In environments with common pool resources, a typical failure mode is the tragedy of the commons, wherein ag","2021-06-28","2022-01-30 04:49:30","2022-01-30 04:49:30","2021-12-11 14:02:18","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/63U4EW5R/how-teams-went-about-their-research-at-ai-safety-camp.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4HR4B22K","blogPost","2020","Kirk, Robert; Gavenčiak, Tomáš; Dorner, Flo","How can Interpretability help Alignment?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/uRnprGSiLGXv35foX/how-can-interpretability-help-alignment","","2020-05-23","2022-01-30 04:49:30","2022-01-30 04:49:30","2020-08-18 20:39:01","","","","","","","How can Interpretability help Alignment?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/AUVZ44SB/how-can-interpretability-help-alignment.html","","TechSafety; AI-Safety-Camp; AISRP2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H8VNSUZZ","manuscript","2020","Gruetzemacher, Ross; Dorner, Florian; Bernaola-Alvarez, Niko; Giattino, Charlie; Manheim, David","Forecasting AI Progress: A Research Agenda","","","","","http://arxiv.org/abs/2008.01848","Forecasting AI progress is essential to reducing uncertainty in order to appropriately plan for research efforts on AI safety and AI governance. While this is generally considered to be an important topic, little work has been conducted on it and there is no published document that gives and objective overview of the field. Moreover, the field is very diverse and there is no published consensus regarding its direction. This paper describes the development of a research agenda for forecasting AI progress which utilized the Delphi technique to elicit and aggregate experts' opinions on what questions and methods to prioritize. The results of the Delphi are presented; the remainder of the paper follow the structure of these results, briefly reviewing relevant literature and suggesting future work for each topic. Experts indicated that a wide variety of methods should be considered for forecasting AI progress. Moreover, experts identified salient questions that were both general and completely unique to the problem of forecasting AI progress. Some of the highest priority topics include the validation of (partially unresolved) forecasts, how to make forecasting action-guiding and the quality of different performance metrics. While statistical methods seem more promising, there is also recognition that supplementing judgmental techniques can be quite beneficial.","2020-08-04","2022-01-30 04:49:30","2022-01-30 04:49:30","2020-08-24 20:25:42","","","","","","","Forecasting AI Progress","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000[s0]  arXiv: 2008.01848","","/Users/jacquesthibodeau/Zotero/storage/C7CAJQ97/Gruetzemacher et al. - 2020 - Forecasting AI Progress A Research Agenda.pdf; /Users/jacquesthibodeau/Zotero/storage/5B7HP3TB/2008.html","","MetaSafety; AI-Safety-Camp; AISRP2019","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9K3PT2TQ","blogPost","2021","Raja, Arun","Extraction of human preferences 👨→🤖","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/PZYD5kBpeHWgE5jX4/extraction-of-human-preferences","INTRODUCTION Developing safe and beneficial reinforcement learning (RL) agents requires making them aligned with human preferences. An RL agent trained to fulfil any objective in the real world will probably have to learn human preferences in order to do well. This is because humans live in the real world, so the RL agent will have to take human preferences into account as it optimizes its objective. We propose to first train an RL agent on an objective in the real world so that it learns human preferences as it is being trained on that real world objective and then use the agent’s understanding of human preferences to build a better reward function. We build upon the work of Christiano et al. (2017) where they trained a human preference predictor as the reward signal. The preference predictor was trained on environment observations to give a high reward for states where the human preferences were satisfied and a low reward for states where the human preferences were not satisfied: In our experiments, the reward predictor takes the activations (hidden states) of the RL agent as the input and is trained to predict a binary label depending on whether human preferences are satisfied or not. We first train an RL agent in an environment with some reward function that’s not aligned with human preferences. After training the RL agent, we try different transfer learning techniques to transfer the agent’s knowledge of human preferences to the human preferences predictor. Our goal is to train the human preferences predictor to get a high accuracy with a small amount of labeled training examples. The idea of training a human preference predictor off of the RL agent’s hidden (internal) states was already validated by Wichers (2020). We wanted to validate it further by trying other techniques to train a human preference predictor, as well as to validate it in more environments. Research question The main research question we wanted to answer is: “Are human preferences p","2021-08-24","2022-01-30 04:49:30","2022-01-30 04:49:30","2021-12-11 13:59:23","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/65CXVJQK/extraction-of-human-preferences.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H3ZWGCU7","blogPost","2021","Koch, Jack; Langosco, Lauro","Empirical Observations of Objective Robustness Failures","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/iJDmL7HJtN5CYKReM/empirical-observations-of-objective-robustness-failures","Inner alignment and objective robustness have been frequently discussed in the alignment community since the publication of “Risks from Learned Optimization” (RFLO). These concepts identify a problem beyond outer alignment/reward specification: even if the reward or objective function is perfectly specified, there is a risk of a model pursuing a different objective than the one it was trained on when deployed out-of-distribution (OOD). They also point to a different type of robustness problem than the kind usually discussed in the OOD robustness literature; typically, when a model is deployed OOD, it either performs well or simply fails to take useful actions (a capability robustness  failure). However, there exists an alternative OOD failure mode in which the agent pursues an objective other than the training objective while retaining most or all of the capabilities it had on the training distribution; this is a failure of objective robustness. To date, there has not been an empirical demonstration of objective robustness failures. A group of us in this year’s AI Safety Camp sought to produce such examples. Here, we provide four demonstrations of objective robustness failures in current reinforcement learning (RL) agents trained and tested on versions of the Procgen benchmark. For example, in CoinRun, an agent is trained to navigate platforms, obstacles, and enemies in order to reach a coin at the far right side of the level (the reward). However, when deployed in a modified version of the environment where the coin is instead randomly placed in the level, the agent ignores the coin and competently navigates to the end of the level whenever it does not happen to run or jump into it along the way. This reveals it has learned a behavioral objective—the objective the agent appears to be optimizing, which can be understood as equivalent to the notion of a “goal” under the  intentional stance—that is something like “get to the end of the level,” instead of “get to the","2021-06-23","2022-01-30 04:49:30","2022-01-30 04:49:30","2021-10-30 17:59:27","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/AQ8SGH29/empirical-observations-of-objective-robustness-failures.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SWSFKPQ4","blogPost","2021","Dougherty, Quinn; Greenberg, Ben; Kwiatkowski, Ariel","AISC5 Retrospective: Mechanisms for Avoiding Tragedy of the Commons in Common Pool Resource Problems","LessWrong","","","","https://www.lesswrong.com/posts/LBwpubeZSi3ottfjs/aisc5-retrospective-mechanisms-for-avoiding-tragedy-of-the","Work by Quinn Dougherty, Ben Greenberg & Ariel Kwiatkowski FROM THE AI SAFETY CAMP GROUP THAT WORKED ON COOPERATIVITY AND COMMON POOL RESOURCES: A WRITE-UP OF A PROBLEM WE WORKED ON, OUR PROPOSED SOLUTION, AND THE RESULTS OF IMPLEMENTING THIS SOLUTION IN A SIMULATED ENVIRONMENT. Contents:  1. Problem description  2. Review of related work  3. Experiment and results  4. Retrospective Check out our GitHub repo here. 0. ABSTRACT When multiple parties share the access to a finite resource, they may overuse the resource leading to a Tragedy of the Commons. A simple way of mitigating this problem is allowing them to decrease the effective population by using violence - this, however, is not the best solution for obvious reasons. We study interventions for avoiding these outcomes in environments with multiple self-interested agents. In particular, a reputation system can incentivize agents to “cooperate” by harvesting the resource more sustainably. This system promotes multi-agent cooperation without modifying the agents’ reward functions. 1. PROBLEM DESCRIPTION: TRAGEDY OF THE COMMONS A common pool resource (CPR) is a good which anyone can use but which no one can entirely control. When competition over the resource increases, individual parties are incentivized to appropriate as much of the resource as possible for themselves, which further increases competition, potentially leading to overconsumption. This process is commonly known as the tragedy of the commons, and has been extensively studied in economics and the social sciences. A tragedy of the commons often leads to the exhaustion of a CPR. The classic example is of fishermen in an enclosed body of water; when fish are scarce, an every-man-for-himself dynamic emerges. Any available fish is quickly caught for fear that, if one hesitates, their counterparts would surely take it otherwise. This behavior results in the fish population failing to replenish, and everyone catching fewer fish in the long run.","2021-09-27","2022-01-30 04:49:30","2022-01-30 04:49:30","2021-10-30 16:40:44","","","","","","","AISC5 Retrospective","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BQPGRKDW/aisc5-retrospective-mechanisms-for-avoiding-tragedy-of-the.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2DT5V9B","manuscript","2020","Kovarik, Vojta","AI Services: Introduction v1.3","","","","","https://docs.google.com/document/d/1SYgvWBe1ruDl9dQnxmjll-8COUHPycGOlLvTI68xtLA/edit?pli=1&usp=embed_facebook","This document aims to serve as an introduction for researchers who want to study the long-term impact of AI through the lens of AI services. It introduces basic concepts related to these systems and gives initial observations to enhance their initial study. It points to several relevant research fields that could be leveraged to study AI services, mentions a number of problems that seem specific to this setting, and makes suggestions for future work.","2020-03-31","2022-01-30 04:49:30","2022-01-30 04:49:30","2020-08-14 19:44:36","","","","","","","AI Services","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/QFJBG8U2/edit.html","","TechSafety; AI-Safety-Camp","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZXZPENH","conferencePaper","2019","Carey, Ryan","How Useful Is Quantilization For Mitigating Specification-Gaming?","","","","","","For some tasks, there exists a goal that perfectly describes what the designer wants the AI system to achieve. For many tasks, however, the best available proxy objective is only a rough approximation of the designer’s intentions. When given such a goal, a system that optimizes the proxy objective tends to select degenerate solutions where the proxy reward is very different from the designer’s true reward function. One way to counteract the tendency toward speciﬁcation-gaming is quantilization, a method that interpolates between imitating demonstrations, and optimizing the proxy objective. If the demonstrations are of adequate quality, and the proxy reward overestimates performance, then quantilization has better guaranteed performance than other strategies. However, if the proxy reward underestimates performance, then either imitation or optimization will offer the best guarantee. This work introduces three new gym environments: Mountain Car-RR, Hopper-RR, and Video Pinball-RR, and shows that quantilization outperforms baselines on these tasks.","2019","2022-01-30 04:53:18","2022-01-30 04:53:18","","11","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/BXWIFTJ3/Carey - 2019 - HOW USEFUL IS QUANTILIZATION FOR MITIGATING SPECIF.pdf; /Users/jacquesthibodeau/Zotero/storage/6GSUMWMX/Carey - 2019 - HOW USEFUL IS QUANTILIZATION FOR MITIGATING SPECIF.pdf","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2019","","","","","","","","","","","","","","",""
"SCB4T6S5","manuscript","2018","Armstrong, Stuart; O'Rorke, Xavier","Good and safe uses of AI Oracles","","","","","http://arxiv.org/abs/1711.05541","It is possible that powerful and potentially dangerous artificial intelligence (AI) might be developed in the future. An Oracle is a design which aims to restrain the impact of a potentially dangerous AI by restricting the agent to no actions besides answering questions. Unfortunately, most Oracles will be motivated to gain more control over the world by manipulating users through the content of their answers, and Oracles of potentially high intelligence might be very successful at this \citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two designs for Oracles which, even under pessimistic assumptions, will not manipulate their users into releasing them and yet will still be incentivised to provide their users with helpful answers. The first design is the counterfactual Oracle -- which choses its answer as if it expected nobody to ever read it. The second design is the low-bandwidth Oracle -- which is limited by the quantity of information it can transmit.","2018-06-05","2022-01-30 04:53:17","2022-01-30 04:53:17","2020-11-22 04:11:52","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s2]  ACC: 20  arXiv: 1711.05541","","/Users/jacquesthibodeau/Zotero/storage/J86CWCCN/Armstrong and O'Rorke - 2018 - Good and safe uses of AI Oracles.pdf; /Users/jacquesthibodeau/Zotero/storage/U7F3VQ4J/Armstrong and O'Rorke - 2018 - Good and safe uses of AI Oracles.pdf; /Users/jacquesthibodeau/Zotero/storage/JX48I6WQ/1711.html; /Users/jacquesthibodeau/Zotero/storage/RDZSWUWX/1711.html","","TechSafety; FHI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQNJ4T5C","blogPost","2020","Armstrong, Stuart","""Go west, young man!"" - Preferences in (imperfect) maps","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps","Many people are very nationalistic, putting their country above all others. Such people can be hazy about what ""above all others"" can mean, outside of a few clear examples - eg winning a total war totally. They're also very hazy on what is meant by ""their country"" - geography is certainly involved, as is proclaimed or legal nationality, maybe some ethnic groups or a language, or even just giving deference to certain ideals. Consider the plight of a communist Croatian Yugoslav nationalist during the 1990s... I'd argue that the situation these nationalists find themselves in - strong views on poorly defined concepts - is the general human state for preferences. Or, to use an appropriate map and territory analogy:  * Most people forge their preferences by exploring their local territory,    creating a mental map of this, and taking strong preferences over the    concepts within their mental map. When the map starts to become imperfect,    they will try to extend the concepts to new areas, so that their preferences    can also be extended. Some of the debates about the meaning of words are about this extension-of-preferences process. Scott Alexander recommends that we dissolve concepts such as disease, looking for the relevant categories of 'deserves sympathy' and 'acceptable to treat in a medical way'. And that dissolving is indeed the correct thing for rationalists to do. But, for most people, including most rationalists, 'sick people deserve sympathy' is a starting moral principle, one we've learnt by example and experience in childhood. When we ask 'do obese people deserve sympathy?' we've trying to extend that moral principle to a situation where our map/model (which includes, say, three categories of people: healthy, mildly sick, very sick) no longer matches up with reality. Scott's dissolving process requires decomposing 'disease' into more nodes, and then applying moral principles to those individual nodes. In this case, a compelling consequentialist analy","2020-07-31","2022-01-30 04:53:17","2022-01-30 04:53:17","2020-08-27 16:42:39","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/SD3GNHEW/go-west-young-man-preferences-in-imperfect-maps.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9NGUJJWM","journalArticle","2015","Pamlin, Dennis; Armstrong, Stuart","Global challenges: 12 risks that threaten human civilization","Global Challenges Foundation, Stockholm","","","","","","2015","2022-01-30 04:53:17","2022-01-30 04:53:17","","","","","","","","Global challenges","","","","","","","","","","","","Google Scholar","","ZSCC: 0000044","","","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QFQEA659","journalArticle","2019","Garfinkel, Ben; Dafoe, Allan","How does the offense-defense balance scale?","Journal of Strategic Studies","","0140-2390","10.1080/01402390.2019.1631810","https://doi.org/10.1080/01402390.2019.1631810","We ask how the offense-defense balance scales, meaning how it changes as investments into a conflict increase. To do so we offer a general formalization of the offense-defense balance in terms of contest success functions. Simple models of ground invasions and cyberattacks that exploit software vulnerabilities suggest that, in both cases, growth in investments will favor offense when investment levels are sufficiently low and favor defense when they are sufficiently high. We refer to this phenomenon as offensive-then-defensive scaling or OD-scaling. Such scaling effects may help us understand the security implications of applications of artificial intelligence that in essence scale up existing capabilities.","2019-09-19","2022-01-30 04:53:17","2022-01-30 04:53:17","2019-12-16 02:16:59","736-763","","6","42","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","ZSCC: 0000038","","/Users/jacquesthibodeau/Zotero/storage/U7XMZSMQ/01402390.2019.html; /Users/jacquesthibodeau/Zotero/storage/8BGM36P6/Garfinkel and Dafoe - 2019 - How does the offense-defense balance scale.pdf; /Users/jacquesthibodeau/Zotero/storage/GMNCIXGC/Garfinkel and Dafoe - 2019 - How does the offense-defense balance scale.pdf; /Users/jacquesthibodeau/Zotero/storage/N5F3S49W/01402390.2019.html; /Users/jacquesthibodeau/Zotero/storage/6HXGJ6Z6/01402390.2019.html","","MetaSafety; FHI","emerging technologies; Offense-defense theory; strategic stability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9RIUZJB7","report","2014","Bostrom, Nick","Hail mary, value porosity, and utility diversification","","","","","","","2014","2022-01-30 04:53:17","2022-01-30 04:53:17","","","","","","","","","","","","","Future of Humanity Institute","","","","","","","Google Scholar","","ZSCC: 0000018","","/Users/jacquesthibodeau/Zotero/storage/PIQNHXSA/Bostrom - 2014 - Hail mary, value porosity, and utility diversifica.pdf","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GCB9MTUK","journalArticle","2018","Dresler, Martin; Sandberg, Anders; Bublitz, Christoph; Ohla, Kathrin; Trenado, Carlos; Mroczko-Wasowicz, Aleksandra; Kühn, Simone; Repantis, Dimitris","Hacking the brain: dimensions of cognitive enhancement","ACS chemical neuroscience","","","","","","2018","2022-01-30 04:53:17","2022-01-30 04:53:17","","1137–1148","","3","10","","","Hacking the brain","","","","","","","","","","","","Google Scholar","","ZSCC: 0000046","","/Users/jacquesthibodeau/Zotero/storage/ZHIX2ZJF/acschemneuro.html; /Users/jacquesthibodeau/Zotero/storage/8JC34ET5/Dresler et al. - 2019 - Hacking the brain dimensions of cognitive enhance.pdf; /Users/jacquesthibodeau/Zotero/storage/Q8UUCPHK/acschemneuro.html; /Users/jacquesthibodeau/Zotero/storage/KMK8UZFI/uuid426d61fb-deab-411c-8778-ca7271e53ead.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75IGTK3W","report","2008","Sandberg, Anders; Bostrom, Nick","Global Catastrophic Risks Survey","","","","","","","2008","2022-01-30 04:53:17","2022-01-30 04:53:17","","","","","","","","","","","","","Future of Humanity Institute","Oxford University","","","","","","","","ZSCC: 0000056","","/Users/jacquesthibodeau/Zotero/storage/IWPM4BCR/gcr-report.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","2008-1","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PIF96FJT","report","2016","Cotton-Barratt, Owen; Farquhar, Sebastian; Halstead, John; Schubert, Stefan; Snyder-Beattie, Andrew","Global Catastrophic Risks 2016","","","","","http://globalprioritiesproject.org/2016/04/global-catastrophic-risks-2016/","Global catastrophes sometimes strike. In 1918 the Spanish Flu killed as many as one in twenty people. There have been even more devastating pandemics - the Black Death and the 6th century Plague of Justinian may have each killed nearer to one in every six people on this earth. More recently, the Cub","2016-04-28","2022-01-30 04:53:17","2022-01-30 04:53:17","2020-12-13 19:41:24","108","","","","","","","","","","","Global Challenges Foundation","","en-US","","","","","","","ZSCC: 0000034  Section: Policy research","","/Users/jacquesthibodeau/Zotero/storage/MFNIR97H/Cotton-Barratt et al. - 2016 - Global Catastrophic Risks 2016.pdf; /Users/jacquesthibodeau/Zotero/storage/R4KN9F3G/global-catastrophic-risks-2016.html","","MetaSafety; FHI; GPI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F34EPTEX","book","2011","Bostrom, Nick; Cirkovic, Milan M.","Global Catastrophic Risks","","978-0-19-960650-4","","","","A global catastrophic risk is one with the potential to wreak death and destruction on a global scale. In human history, wars and plagues have done so on more than one occasion, and misguided ideologies and totalitarian regimes have darkened an entire era or a region. Advances in technology are adding dangers of a new kind. It could happen again. In Global Catastrophic Risks 25 leading experts look at the gravest risks facing humanity in the 21st century, including asteroid impacts, gamma-ray bursts, Earth-based natural catastrophes, nuclear war, terrorism, global warming, biological weapons, totalitarianism, advanced nanotechnology, general artificial intelligence, and social collapse. The book also addresses over-arching issues - policy responses and methods for predicting and managing catastrophes. This is invaluable reading for anyone interested in the big issues of our time; for students focusing on science, society, technology, and public policy; and for academics, policy-makers, and professionals working in these acutely important fields.","2011-09-29","2022-01-30 04:53:17","2022-01-30 04:53:17","","","577","","","","","","","","","","OUP Oxford","","en","","","","","Google Books","","ZSCC: 0000599  Google-Books-ID: sTkfAQAAQBAJ","","","https://books.google.com/books?id=sTkfAQAAQBAJ","TechSafety; FHI","Mathematics / Game Theory; Nature / Sky Observation; Science / Biotechnology; Science / Earth Sciences / General; Science / General; Science / Philosophy & Social Aspects; Science / Physics / General; Social Science / Disasters & Disaster Relief; Technology & Engineering / Nanotechnology & MEMS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M24DR38P","journalArticle","2013","Armstrong, Stuart","General Purpose Intelligence: Arguing The Orthogonality Thesis","Analysis and Metaphysics","","","","https://www.ceeol.com/search/article-detail?id=137912","In his paper “The Superintelligent Will,” Nick Bostrom formalized the Orthogonality thesis: the idea that the final goals and intelligence levels of artificial agents are independent of each other. This paper presents arguments for a (narrower) version of the thesis. It proceeds through three steps. First it shows that superintelligent agents with essentially arbitrary goals can exist in our universe –both as theoretical impractical agents such as AIXI and as physically possible realworld agents. Then it argues that if humans are capable of building human-level artificial intelligences, we can build them with an extremely broad spectrum of goals. Finally it shows that the same result holds for any superintelligent agent we could directly or indirectly build. This result is relevant for arguments about the potential motivations of future agents: knowing an artificial agent is of high intelligence does not allow us to presume that it will be moral, we will need to figure out its goals directly.","2013","2022-01-30 04:53:10","2022-01-30 04:53:10","2020-12-18","17","","12","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000032","","/Users/jacquesthibodeau/Zotero/storage/QXXGIDCH/Armstrong - 2020 - GENERAL PURPOSE INTELLIGENCE ARGUING THE ORTHOGON.pdf","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E5J65IJ3","bookSection","2016","Müller, Vincent C.; Bostrom, Nick","Future progress in artificial intelligence: A survey of expert opinion","Fundamental issues of artificial intelligence","","","","","","2016","2022-01-30 04:53:10","2022-01-30 04:53:10","","555–572","","","","","","Future progress in artificial intelligence","","","","","Springer","","","","","","","Google Scholar","","ZSCC: 0000564","","/Users/jacquesthibodeau/Zotero/storage/X6GIGDJ6/Müller and Bostrom - 2016 - Future progress in artificial intelligence A surv.pdf; /Users/jacquesthibodeau/Zotero/storage/QK7SQP4E/978-3-319-26485-1_33.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCGHD6UP","conferencePaper","2019","Kenton, Zachary; Filos, Angelos; Evans, Owain; Gal, Yarin","Generalizing from a few environments in safety-critical reinforcement learning","","","","","http://arxiv.org/abs/1907.01475","Before deploying autonomous agents in the real world, we need to be confident they will perform safely in novel situations. Ideally, we would expose agents to a very wide range of situations during training, allowing them to learn about every possible danger, but this is often impractical. This paper investigates safety and generalization from a limited number of training environments in deep reinforcement learning (RL). We find RL algorithms can fail dangerously on unseen test environments even when performing perfectly on training environments. Firstly, in a gridworld setting, we show that catastrophes can be significantly reduced with simple modifications, including ensemble model averaging and the use of a blocking classifier. In the more challenging CoinRun environment we find similar methods do not significantly reduce catastrophes. However, we do find that the uncertainty information from the ensemble is useful for predicting whether a catastrophe will occur within a few steps and hence whether human intervention should be requested.","2019-07-02","2022-01-30 04:53:10","2022-01-30 04:53:10","2019-12-16 02:16:41","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000010  arXiv: 1907.01475","","/Users/jacquesthibodeau/Zotero/storage/ANE5XDV5/Kenton et al. - 2019 - Generalizing from a few environments in safety-cri.pdf; /Users/jacquesthibodeau/Zotero/storage/JHSJ7739/Kenton et al. - 2019 - Generalizing from a few environments in safety-cri.pdf; /Users/jacquesthibodeau/Zotero/storage/6FIEE6TX/1907.html; /Users/jacquesthibodeau/Zotero/storage/XH3QNZBE/1907.html","","TechSafety; FHI","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","SafeML ICLR 2019 Workshop","","","","","","","","","","","","","","",""
"NQT3FZE8","report","2021","Ord, Toby; Mercer, Angus; Dannreuther, Sophie","Future Proof","","","","","","","2021-06","2022-01-30 04:53:10","2022-01-30 04:53:10","","51","","","","","","","","","","","Centre for Long-Term Resilience","","en","","","","","Zotero","","ZSCC: NoCitationData[s1]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/UV46GKTM/Dannreuther - Angus Mercer, Centre for Long-Term Resilience.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B67TRECB","journalArticle","2014","Müller, Vincent C.; Bostrom, Nick","Future progress in artificial intelligence: A poll among experts","AI Matters","","","","","","2014","2022-01-30 04:53:10","2022-01-30 04:53:10","","9–11","","1","1","","","Future progress in artificial intelligence","","","","","","","","","","","","Google Scholar","","ZSCC: 0000036","","/Users/jacquesthibodeau/Zotero/storage/W2ZTR56A/Müller and Bostrom - 2014 - Future progress in artificial intelligence A poll.pdf; /Users/jacquesthibodeau/Zotero/storage/G8FD3JW5/citation.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6BHH2HVZ","book","2016","","Fundamental issues of artificial intelligence","","","","","https://link.springer.com/book/10.1007%2F978-3-319-26485-1","","2016","2022-01-30 04:53:10","2022-01-30 04:53:10","2020-12-21","","","","376","","","","Synthese Library","","","","Springer","","","","","","","Google Scholar","","ZSCC: 0000055","","/Users/jacquesthibodeau/Zotero/storage/S28A9QZU/Müller and Bostrom - 2016 - Fundamental issues of artificial intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/I2QFDMUU/10.html","","TechSafety; FHI","","Müller, Vincent C.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"59AKR2E2","manuscript","2021","Cohen, Michael K.; Hutter, Marcus; Nanda, Neel","Fully General Online Imitation Learning","","","","","http://arxiv.org/abs/2102.08686","In imitation learning, imitators and demonstrators are policies for picking actions given past interactions with the environment. If we run an imitator, we probably want events to unfold similarly to the way they would have if the demonstrator had been acting the whole time. No existing work provides formal guidance in how this might be accomplished, instead restricting focus to environments that restart, making learning unusually easy, and conveniently limiting the significance of any mistake. We address a fully general setting, in which the (stochastic) environment and demonstrator never reset, not even for training purposes. Our new conservative Bayesian imitation learner underestimates the probabilities of each available action, and queries for more data with the remaining probability. Our main result: if an event would have been unlikely had the demonstrator acted the whole time, that event's likelihood can be bounded above when running the (initially totally ignorant) imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in frequency.","2021-02-17","2022-01-30 04:53:10","2022-01-30 04:53:10","2021-10-31 19:13:33","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2102.08686","","/Users/jacquesthibodeau/Zotero/storage/BDP2UPND/Cohen et al. - 2021 - Fully General Online Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/8T8JCEB2/2102.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0; I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DI833FP9","manuscript","2016","Leike, Jan","Exploration Potential","","","","","https://arxiv.org/abs/1609.04994v3","We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation.","2016-09-16","2022-01-30 04:53:10","2022-01-30 04:53:10","2019-12-19 01:45:54","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/Q33MXUNJ/Leike - 2016 - Exploration Potential.pdf; /Users/jacquesthibodeau/Zotero/storage/IK85485C/1609.html; /Users/jacquesthibodeau/Zotero/storage/V7QHF9K7/Leike - 2016 - Exploration Potential.pdf; /Users/jacquesthibodeau/Zotero/storage/IM6V7HDQ/1609.html","","TechSafety; FHI","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DX99RGNH","journalArticle","2013","Bostrom, Nick","Existential Risk Prevention as Global Priority","Global Policy","","1758-5899","10.1111/1758-5899.12002","https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12002","Existential risks are those that threaten the entire future of humanity. Many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. Despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. In this article, I clarify the concept of existential risk and develop an improved classification scheme. I discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. I also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. Policy Implications • Existential risk is a concept that can focus long-term global efforts and sustainability concerns. • The biggest existential risks are anthropogenic and related to potential future technologies. • A moral case can be made that existential risk reduction is strictly more important than any other global public good. • Sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sustainable state. • Some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity’s ability to deal with the larger existential risks that will arise later in this century. This will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordinated response to anticipated existential risks. • Perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existential risks and potential mitigation strategies, with a long-term perspective.","2013","2022-01-30 04:53:10","2022-01-30 04:53:10","2019-12-19 01:40:49","15-31","","1","4","","","","","","","","","","en","© 2013 University of Durham and John Wiley & Sons, Ltd","","","","Wiley Online Library","","ZSCC: 0000462","","/Users/jacquesthibodeau/Zotero/storage/GQZJQ458/1758-5899.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PNN3BJG2","blogPost","2020","Garfinkel, Ben","Does Economic History Point Toward a Singularity?","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/CWFn9qAKsRibpCGq8/does-economic-history-point-toward-a-singularity","I’ve ended up spending quite a lot of time researching premodern economic growth, as part of a hobby project that got out of hand. I’m sharing an informal but long write-up of my findings here, since I think they may be relevant to other longtermist researchers and I am unlikely to write anything more polished in the near future. Click here for the Google document.[1] SUMMARY Over the next several centuries, is the economic growth rate likely to remain steady, radically increase, or decline back toward zero? This question has some bearing on almost every long-run challenge facing the world, from climate change to great power competition to risks from AI. One way to approach the question is to consider the long-run history of economic growth. I decided to investigate the Hyperbolic Growth Hypothesis: the claim that, from at least the start of the Neolithic Revolution up until the 20th century, the economic growth rate has tended to rise in proportion with the size of the global economy.[2] This claim is made in a classic 1993 paper by Michael Kremer. Beyond influencing other work in economic growth theory, it has also recently attracted significant attention within the longtermist community, where it is typically regarded as evidence in favor of further acceleration.[3] An especially notable property of the hypothesized growth trend is that, if it had continued without pause, it would have produced infinite growth rates in the early twenty-first century. I spent time exploring several different datasets that can be used to estimate pre-modern growth rates. This included a number of recent archeological datasets that, I believe, have not previously been analyzed by economists. I wanted to evaluate both: (a) how empirically well-grounded these estimates are and (b) how clearly these estimates display the hypothesized pattern of growth. Ultimately, I found very little empirical support for the Hyperbolic Growth Hypothesis. While we can confidently say that the econo","2020","2022-01-30 04:53:09","2022-01-30 04:53:09","2020-12-19 02:01:14","","","","","","","Does Economic History Point Toward a Singularity?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/83G76TFA/does-economic-history-point-toward-a-singularity.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P5NDJMSA","journalArticle","2020","Cotton‐Barratt, Owen; Daniel, Max; Sandberg, Anders","Defence in Depth Against Human Extinction: Prevention, Response, Resilience, and Why They All Matter","Global Policy","","1758-5899","10.1111/1758-5899.12786","https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12786","We look at classifying extinction risks in three different ways, which affect how we can intervene to reduce risk. First, how does it start causing damage? Second, how does it reach the scale of a global catastrophe? Third, how does it reach everyone? In all of these three phases there is a defence layer that blocks most risks: First, we can prevent catastrophes from occurring. Second, we can respond to catastrophes before they reach a global scale. Third, humanity is resilient against extinction even in the face of global catastrophes. The largest probability of extinction is posed when all of these defences are weak, that is, by risks we are unlikely to prevent, unlikely to successfully respond to, and unlikely to be resilient against. We find that it’s usually best to invest significantly into strengthening all three defence layers. We also suggest ways to do so tailored to the classes of risk we identify. Lastly, we discuss the importance of underlying risk factors – events or structural conditions that may weaken the defence layers even without posing a risk of immediate extinction themselves.","2020","2022-01-30 04:53:09","2022-01-30 04:53:09","2020-08-18 21:30:09","271-282","","3","11","","","Defence in Depth Against Human Extinction","","","","","","","en","© 2020 The Authors. Global Policy published by Durham University and John Wiley & Sons Ltd.","","","","Wiley Online Library","","ZSCC: 0000013  _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1758-5899.12786","","/Users/jacquesthibodeau/Zotero/storage/PUZ3UWZ7/Cotton‐Barratt et al. - 2020 - Defence in Depth Against Human Extinction Prevent.pdf; /Users/jacquesthibodeau/Zotero/storage/J7XA65W9/1758-5899.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"84KGTFVX","journalArticle","2017","Petratos, Pythagoras; Sandberg, Anders; Zhou, Feng","Cyber insurance","Handbook of Cyber-Development, Cyber-Democracy, and Cyber-Defense","","","","","","2017","2022-01-30 04:53:09","2022-01-30 04:53:09","","1–28","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000004","","/Users/jacquesthibodeau/Zotero/storage/GMBF55X5/10.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ST9XJA8B","manuscript","2018","Armstrong, Stuart","Counterfactual equivalence for POMDPs, and underlying deterministic environments","","","","","https://arxiv.org/abs/1801.03737","","2018","2022-01-30 04:53:09","2022-01-30 04:53:09","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/GJBHW4QW/Armstrong - 2018 - Counterfactual equivalence for POMDPs, and underly.pdf; /Users/jacquesthibodeau/Zotero/storage/5D5PX64R/1801.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I8H3DUG4","conferencePaper","2015","Soares, Nate; Fallenstein, Benja; Armstrong, Stuart; Yudkowsky, Eliezer","Corrigibility","Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence","","","","https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10124/10136","","2015","2022-01-30 04:53:09","2022-01-30 04:53:09","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000092","","/Users/jacquesthibodeau/Zotero/storage/5HJ4SE3I/Corrigibility.pdf; /Users/jacquesthibodeau/Zotero/storage/RKMX5VC3/10124.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CT8AEDQP","blogPost","2020","Clarke, Sam","Clarifying “What failure looks like” (part 1)","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/v6Q7T335KCMxujhZu/clarifying-what-failure-looks-like-part-1","Thanks to Jess Whittlestone, Daniel Eth, Shahar Avin, Rose Hadshar, Eliana Lorch, Alexis Carlier, Flo Dorner, Kwan Yee Ng, Lewis Hammond, Phil Trammell and Jenny Xiao for valuable conversations, feedback and other support. I am especially grateful to Jess Whittlestone for long conversations and detailed feedback on drafts, and her guidance on which threads to pursue and how to frame this post. All errors are my own. Epistemic status: My Best Guess Epistemic effort: ~70 hours of focused work (mostly during FHI’s summer research fellowship), talked to ~10 people. INTRODUCTION “What failure looks like” is the one of the most comprehensive pictures of what failure to solve the AI alignment problem looks like, in worlds without discontinuous progress in AI. I think it was an excellent and much-needed addition to our understanding of AI risk. Still, if many believe that this is a main source of AI risk, I think it should be fleshed out in more than just one blog post. The original story has two parts; I’m focusing on part 1 because I found it more confusing and nebulous than part 2. Firstly, I’ll summarise part 1 (hereafter “WFLL1”) as I understand it:  * In the world today, it’s easier to pursue easy-to-measure goals than    hard-to-measure goals.          * Machine learning is differentially good at pursuing easy-to-measure goals    (assuming that we don’t have a satisfactory technical solution to the intent    alignment problem[1]).          * We’ll try to harness this by designing easy-to-measure proxies for what we    care about, and deploy AI systems across society which optimize for these    proxies (e.g. in law enforcement, legislation and the market).          * We’ll give these AI systems more and more influence (e.g. eventually, the    systems running law enforcement may actually be making all the decisions for    us).          * Eventually, the proxies for which the AI systems are optimizing will come    apart from the goals we truly care about, but by t","2020","2022-01-30 04:53:09","2022-01-30 04:53:09","2020-12-19 01:26:57","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BNJW49Z2/clarifying-what-failure-looks-like-part-1.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UK5IWQD8","journalArticle","2015","Cotton-Barratt, Owen; Ord, Toby","Existential risk and existential hope: definitions","Future of Humanity Institute: Technical Report","","","","","","2015","2022-01-30 04:53:09","2022-01-30 04:53:09","","78","","2015","1","","","Existential risk and existential hope","","","","","","","","","","","","Google Scholar","","ZSCC: 0000011","","/Users/jacquesthibodeau/Zotero/storage/3M5MWCSK/Cotton-Barratt and Ord - 2015 - Existential risk and existential hope definitions.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A3K8BZT9","journalArticle","2014","Sandberg, Anders","Ethics of brain emulations","Journal of Experimental & Theoretical Artificial Intelligence","","","","","","2014","2022-01-30 04:53:09","2022-01-30 04:53:09","","439–457","","3","26","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000030","","/Users/jacquesthibodeau/Zotero/storage/V467G7XP/Sandberg - 2014 - Ethics of brain emulations.pdf; /Users/jacquesthibodeau/Zotero/storage/2I832CBI/0952813X.2014.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ZFHQHFQ","bookSection","2003","Bostrom, Nick","Ethical Issues in Advanced Artificial Intelligence","Machine Ethics and Robot Ethics","978-1-00-307499-1","","","https://www.taylorfrancis.com/books/9781000108934/chapters/10.4324/9781003074991-7","The ethical issues related to the possible future creation of machines with general intellectual capabilities far outstripping those of humans are quite distinct from any ethical problems arising in current automation and information systems. Such superintelligence would not be just another technological development; it would be the most important invention ever made, and would lead to explosive progress in all scientific and technological fields, as the superintelligence would conduct research with superhuman efficiency. To the extent that ethics is a cognitive pursuit, a superintelligence could also easily surpass humans in the quality of its moral thinking. However, it would be up to the designers of the superintelligence to specify its original motivations. Since the superintelligence may become unstoppably powerful because of its intellectual superiority and the technologies it could develop, it is crucial that it be provided with human-friendly motivations. This paper surveys some of the unique ethical issues in creating superintelligence, and discusses what motivations we ought to give a superintelligence, and introduces some cost-benefit considerations relating to whether the development of superintelligent machines ought to be accelerated or retarded.","2003","2022-01-30 04:53:09","2022-01-30 04:53:09","2020-11-21 18:51:38","69-75","","","","","","","","","","","Routledge","","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 317  JCC: 269  DOI: 10.4324/9781003074991-7","","/Users/jacquesthibodeau/Zotero/storage/564SWNRP/Bostrom - 2020 - Ethical Issues in Advanced Artificial Intelligence.pdf","","TechSafety; FHI","","Wallach, Wendell; Asaro, Peter","","","","","Wallach, Wendell; Asaro, Peter","","","","","","","","","","","","","","1","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9DI34W7N","conferencePaper","2021","Hammond, Lewis; Fox, James; Everitt, Tom; Abate, Alessandro; Wooldridge, Michael","Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice","arXiv:2102.05008 [cs]","","","","http://arxiv.org/abs/2102.05008","Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.","2021-02-09","2022-01-30 04:53:09","2022-01-30 04:53:09","2021-10-31 19:02:39","","","","","","","Equilibrium Refinements for Multi-Agent Influence Diagrams","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 2102.05008","","/Users/jacquesthibodeau/Zotero/storage/MNA45BR6/Hammond et al. - 2021 - Equilibrium Refinements for Multi-Agent Influence .pdf; /Users/jacquesthibodeau/Zotero/storage/RFFR9PAB/2102.html; /Users/jacquesthibodeau/Zotero/storage/QNX3GVKX/2102.html","","TechSafety","Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS-21)","","","","","","","","","","","","","","",""
"C8MBI7MV","report","2016","Sandberg, Anders","Energetics of the brain and AI","","","","","https://arxiv.org/abs/1602.04019","Does the energy requirements for the human brain give energy constraints that give reason to  doubt the feasibility of artificial intelligence? This report will review some relevant estimates of  brain bioenergetics and analyze some of the methods of estimating brain emulation energy re- quirements. Turning to AI, there are reasons to believe the energy requirements for de novo AI  to  have  little  correlation  with  brain  (emulation)  energy  requirements  since  cost  could  depend  merely of the cost of processing higher-level representations rather than billions of neural fir- ings.  Unless  one  thinks  the  human  way  of  thinking  is  the  most  optimal  or  most  easily  imple- mentable  way  of  achieving  software  intelligence,  we  should  expect  de novo  AI  to  make  use  of  different, potentially very compressed and fast, processes.","2016","2022-01-30 04:53:09","2022-01-30 04:53:09","","","","","","","","","","","","","Sapience Project","","","","","","","Google Scholar","","ZSCC: 0000009","","/Users/jacquesthibodeau/Zotero/storage/E8476E6C/Sandberg - 2016 - Energetics of the brain and AI.pdf; /Users/jacquesthibodeau/Zotero/storage/6IMBPF7N/1602.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","STR 2016-2","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NGN63ZPE","blogPost","2020","Armstrong, Stuart","Dynamic inconsistency of the inaction and initial state baseline","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/w8QBmgQwb83vDMXoz/dynamic-inconsistency-of-the-inaction-and-initial-state","Vika has been posting about various baseline choices for impact measure. In this post, I'll argue that the stepwise inaction baseline is dynamically inconsistent/time-inconsistent. Informally, what this means is that an agent will have different preferences from its future self. LOSSES FROM TIME-INCONSISTENCY Why is time-inconsistency bad? It's because it allows money-pump situations: the environment can extract free reward from the agent, to no advantage to that agent. Or, put more formally:  * An agent A is time-inconsistent between times t and t′>t, if at time t it    would pay a positive amount of reward to constrain its possible choices at    time t′. Outside of anthropics and game theory, we expect our agent to be time-consistent. TIME INCONSISTENCY EXAMPLE Consider the following example: The robot can move in all four directions - N, E, S, W - and can also take the noop operation, ∅. The discount rate is γ<1. It gets a reward of r>0 for standing on the blue button for the first time. Using attainable utility preservation, the penalty function is defined by the auxiliary set R; here, this just consists of the reward function that gives p>0  for standing on the red button for the first time. Therefore if the robot moves from a point n steps away from the red button, to one m steps away, it gets a penalty[1] of p|γn−γm| - the difference between the expected red-button rewards for an optimiser in both positions. TWO PATHS It's pretty clear there are two potentially optimal paths the robot can take: going straight to the blue button (higher reward, but higher penalty), or taking the long way round (lower reward, but lower penalty): Fortunately, when summing up the penalties, you sum terms like …p|γn−1−γn|+p|γn− γn+1|…, so a lot of the terms cancel. Thus for the short route, the reward is r⋅γ8 (distance of eight to the blue button) and the penalty is 2p(γ3−γ7) (closest to the red button: 3 squares, furthest: 7 squares). For the long route, the rewar","2020-07-07","2022-01-30 04:53:09","2022-01-30 04:53:09","2020-08-28 17:57:43","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BUEVDX9W/dynamic-inconsistency-of-the-inaction-and-initial-state.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NIZKQPB2","manuscript","2018","Sandberg, Anders; Drexler, Eric; Ord, Toby","Dissolving the Fermi Paradox","","","","","https://arxiv.org/abs/1806.02404","","2018","2022-01-30 04:53:09","2022-01-30 04:53:09","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000034","","/Users/jacquesthibodeau/Zotero/storage/QXFHXM8D/Sandberg et al. - 2018 - Dissolving the Fermi Paradox.pdf; /Users/jacquesthibodeau/Zotero/storage/7WUZBHUW/1806.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PM9UNRXD","bookSection","2019","Ahmed, Shazeda; Ding, Jeffrey; Hoffman, Samantha; Kerr, Jaclyn","Digital Authoritarianism: Evolving Chinese And Russian Models","Artificial Intelligence, China, Russia, and the Global Order: Technological, Political, Global, and Creative Perspectives","","","","","","2019","2022-01-30 04:53:09","2022-01-30 04:53:09","","291","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s7]  ACC: 0  J: 0","","/Users/jacquesthibodeau/Zotero/storage/UHZQDIMA/Ahmed and Berkeley - Artificial Intelligence, China, Russia, and the Gl.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZT7EP8S","journalArticle","2018","Ding, Jeffrey","Deciphering China’s AI dream","Future of Humanity Institute Technical Report","","","","","","2018","2022-01-30 04:53:09","2022-01-30 04:53:09","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: NoCitationData[s4]  ACC: 134","","/Users/jacquesthibodeau/Zotero/storage/RAZG4SA6/Ding - 2018 - Deciphering China’s AI dream.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TQGSD4VT","journalArticle","2015","Sandberg, Anders","Death and pain of a digital brain","New Scientist","","","","","","2015","2022-01-30 04:53:09","2022-01-30 04:53:09","","26–27","","3038","227","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/I7PHUEE5/S026240791531174X.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QR9VVEN7","journalArticle","2021","Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore","Cooperative AI: machines must learn to find common ground","Nature","","","10.1038/d41586-021-01170-0","https://www.nature.com/articles/d41586-021-01170-0","To help humanity solve fundamental problems of cooperation, scientists need to reconceive artificial intelligence as deeply social.","2021-05","2022-01-30 04:53:09","2022-01-30 04:53:09","2021-11-14 18:21:21","33-36","","7857","593","","","Cooperative AI","","","","","","","en","2021 Nature","","","","www.nature.com","","ZSCC: 0000013  Bandiera_abtest: a Cg_type: Comment Number: 7857 Publisher: Nature Publishing Group Subject_term: Machine learning, Computer science, Society, Technology, Sociology, Human behaviour","","/Users/jacquesthibodeau/Zotero/storage/2ZKWDVBZ/Dafoe et al. - 2021 - Cooperative AI machines must learn to find common.pdf; /Users/jacquesthibodeau/Zotero/storage/TUF3PTSZ/d41586-021-01170-0.html","","MetaSafety","Computer science; Human behaviour; Machine learning; Society; Sociology; Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JPQUFZ9K","conferencePaper","2020","Cremer, Carla Zoe; Whittlestone, Jess","Canaries in Technology Mines: Warning Signs of Transformative Progress in AI","","","","","","","2020","2022-01-30 04:53:09","2022-01-30 04:53:09","","7","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/EGT49VI5/Cremer and Whittlestone - Canaries in Technology Mines Warning Signs of Tra.pdf","","MetaSafety; CFI; CSER; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","1st International Workshop on Evaluating Progress in Artificial Intelligence - EPAI 2020","","","","","","","","","","","","","","",""
"SCHC8DRK","journalArticle","2019","Cave, Stephen; Ó hÉigeartaigh, Seán S.","Bridging near- and long-term concerns about AI","Nature Machine Intelligence","","2522-5839","10.1038/s42256-018-0003-2","https://www.nature.com/articles/s42256-018-0003-2","Debate about the impacts of AI is often split into two camps, one associated with the near term and the other with the long term. This divide is a mistake — the connections between the two perspectives deserve more attention, say Stephen Cave and Seán S. ÓhÉigeartaigh.","2019-01","2022-01-30 04:53:09","2022-01-30 04:53:09","2019-12-16 22:26:28","5-6","","1","1","","","","","","","","","","en","2019 Springer Nature Limited","","","","www.nature.com","","ZSCC: 0000039[s0]","","/Users/jacquesthibodeau/Zotero/storage/PZF4G65R/Cave and ÓhÉigeartaigh - 2019 - Bridging near- and long-term concerns about AI.pdf; /Users/jacquesthibodeau/Zotero/storage/558P7U8W/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/AET4T538/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/IQV6PI8K/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/DD9GCI8C/s42256-018-0003-2.html","","MetaSafety; CFI; CSER; FHI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTTKKGIU","conferencePaper","2020","Prunkl, Carina; Whittlestone, Jess","Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society","arXiv:2001.04335 [cs]","","","","http://arxiv.org/abs/2001.04335","One way of carving up the broad ‘AI ethics and society’ research space that has emerged in recent years is to distinguish between ‘near-term’ and ‘long-term’ research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.","2020-01-21","2022-01-30 04:53:09","2022-01-30 04:53:09","2020-08-21 20:00:24","","","","","","","Beyond Near- and Long-Term","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000014  arXiv: 2001.04335","","/Users/jacquesthibodeau/Zotero/storage/ZTEB2EWS/Prunkl and Whittlestone - 2020 - Beyond Near- and Long-Term Towards a Clearer Acco.pdf","","MetaSafety; CFI; CSER; FHI; AmbiguosSafety","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"IVHRFFNN","journalArticle","2019","Weiss, Jessica Chen; Dafoe, Allan","Authoritarian Audiences, Rhetoric, and Propaganda in International Crises: Evidence from China","International Studies Quarterly","","","","","","2019","2022-01-30 04:53:08","2022-01-30 04:53:08","","","","","","","","Authoritarian Audiences, Rhetoric, and Propaganda in International Crises","","","","","","","","","","","","Google Scholar","","ZSCC: 0000027","","/Users/jacquesthibodeau/Zotero/storage/B6NWBFCA/Weiss and Dafoe - 2019 - Authoritarian Audiences, Rhetoric, and Propaganda .pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4B35E3CP","journalArticle","2020","O'Brien, John T.; Nelson, Cassidy","Assessing the Risks Posed by the Convergence of Artificial Intelligence and Biotechnology","Health Security","","2326-5094, 2326-5108","10.1089/hs.2019.0122","https://www.liebertpub.com/doi/10.1089/hs.2019.0122","Rapid developments are currently taking place in the ﬁelds of artiﬁcial intelligence (AI) and biotechnology, and applications arising from the convergence of these 2 ﬁelds are likely to offer immense opportunities that could greatly beneﬁt human health and biosecurity. The combination of AI and biotechnology could potentially lead to breakthroughs in precision medicine, improved biosurveillance, and discovery of novel medical countermeasures as well as facilitate a more effective public health emergency response. However, as is the case with many preceding transformative technologies, new opportunities often present new risks in parallel. Understanding the current and emerging risks at the intersection of AI and biotechnology is crucial for health security specialists and unlikely to be achieved by examining either ﬁeld in isolation. Uncertainties multiply as technologies merge, showcasing the need to identify robust assessment frameworks that could adequately analyze the risk landscape emerging at the convergence of these 2 domains. This paper explores the criteria needed to assess risks associated with AI and biotechnology and evaluates 3 previously published risk assessment frameworks. After highlighting their strengths and limitations and applying to relevant AI and biotechnology examples, the authors suggest a hybrid framework with recommendations for future approaches to risk assessment for convergent technologies.","2020-06-01","2022-01-30 04:53:08","2022-01-30 04:53:08","2020-08-18 21:41:31","219-227","","3","18","","Health Security","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000006","","/Users/jacquesthibodeau/Zotero/storage/MCVXQNE5/O'Brien and Nelson - 2020 - Assessing the Risks Posed by the Convergence of Ar.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BD3PXT45","report","2019","Zhang, Baobao; Dafoe, Allan","Artificial Intelligence: American Attitudes and Trends","","","","","https://www.ssrn.com/abstract=3312874","","2019","2022-01-30 04:53:08","2022-01-30 04:53:08","2019-12-16 22:39:34","","","","","","","Artificial Intelligence","","","","","Center for the Governance of AI and Future of Humanity Institute","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000127","","/Users/jacquesthibodeau/Zotero/storage/583N8IMW/papers.html; /Users/jacquesthibodeau/Zotero/storage/7BGC3MI8/papers.html; /Users/jacquesthibodeau/Zotero/storage/E5PBNQQ3/Zhang and Dafoe - 2019 - Artificial Intelligence American Attitudes and Tr.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3EIIGEB","manuscript","2020","O’Keefe, Cullen","Antitrust-Compliant AI Industry Self-Regulation","","","","","https://cullenokeefe.com/blog/antitrust-compliant-ai-industry-self-regulation","The touchstone of antitrust compliance is competition. To be legally permissible, any industrial restraint on trade must have sufficient countervailing procompetitive justifications. Usually, anticompetitive horizontal agreements like boycotts (including a refusal to produce certain products) are per se illegal.","2020-07-07","2022-01-30 04:53:08","2022-01-30 04:53:08","2020-08-28","","15","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: 1","","/Users/jacquesthibodeau/Zotero/storage/ZBGCJCWU/O’Keefe - Antitrust-Compliant AI Industry Self-Regulation.pdf","","MetaSafety; FHI; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4JPTSFGN","book","2002","Bostrom, Nick","Anthropic bias: observation selection effects in science and philosophy","","978-0-415-93858-7","","","","","2002","2022-01-30 04:53:08","2022-01-30 04:53:08","","","224","","","","","Anthropic bias","Studies in philosophy","","","","Routledge","New York","en","","","","","Library of Congress ISBN","BD241 .B657 2002","ZSCC: NoCitationData[s4]  ACC: 624","","/Users/jacquesthibodeau/Zotero/storage/MD67EI7P/Bostrom - 2002 - Anthropic bias observation selection effects in s.pdf","","MetaSafety; FHI","Anthropic principle; Methodology; Observation (Scientific method); Selectivity (Psychology)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJ2W3RP3","blogPost","2020","O'Keefe, Cullen","AI Benefits Blog Series Index","Cullen O'Keefe","","","","https://cullenokeefe.com/ai-benefits-index","","2020","2022-01-30 04:53:08","2022-01-30 04:53:08","2020-08-28 17:33:18","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/DPU8AJFK/ai-benefits-index.html","","MetaSafety; FHI; Open-AI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MCXZVITA","conferencePaper","2021","Everitt, Tom; Carey, Ryan; Langlois, Eric; Ortega, Pedro A.; Legg, Shane","Agent Incentives: A Causal Perspective","Proceedings of the AAAI 2021 Conference","","","","http://arxiv.org/abs/2102.01685","We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.","2021-03-15","2022-01-30 04:53:08","2022-01-30 04:53:08","2021-10-31 19:14:12","","","","","","","Agent Incentives","","","","","","","","","","","","arXiv.org","","ZSCC: 0000007  arXiv: 2102.01685","","/Users/jacquesthibodeau/Zotero/storage/7B2HXKEK/Everitt et al. - 2021 - Agent Incentives A Causal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/2SVWH4CX/2102.html; /Users/jacquesthibodeau/Zotero/storage/5A6T2VB9/2102.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI 2021","","","","","","","","","","","","","","",""
"I286AT97","blogPost","2020","Armstrong, Stuart","ACDT: a hack-y acausal decision theory","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/9m2fzjNSJmd3yxxKG/acdt-a-hack-y-acausal-decision-theory","Inspired by my post on problems with causal decision theory (CDT), here is a hacked version of CDT that seems to be able to imitate timeless decision theory  (TDT) and functional decision theory[1] (FDT), as well as updateless decision theory (UDT) under certain circumstances. Call this ACDT, for (a)causal decision theory. It is, essentially, CDT which can draw extra, acausal arrows on the causal graphs, and which attempts to figure out which graph represents the world it's in. The drawback is its lack of elegance; the advantage, if it works, is that it's simple to specify and focuses attention on the important aspects of deducing the graph. DEFINING ACDT CDT AND THE NEWCOMB PROBLEM In the Newcomb problem, there is a predictor Ω who leaves two boxes, and predicts whether you will take one (""one-box"") or both (""two-box""). If Ω  predicts you will one-box, it had put a large prize in that first box; otherwise that box is empty. There is always a small consolation prize in the second box. In terms of causal graphs, we can represent it this way: The dark red node is the decision node, which the agent can affect. The green node is a utility node, whose value the agent cares about. The CDT agent uses the ""do"" operator from Pearl's Causality. Essentially all the incoming arrows to the decision node are cut (though the CDT agent keeps track of any information gained that way), then the CDT agent maximises its utility by choosing its action: In this situation, the CDT agent will always two-box, since it treats Ω's decision as fixed, and in that case two-boxing dominates, since you get whatever's in the first box, plus the consolation prize. ACDT ALGORITHM The ACDT algorithm is similar, except that when it cuts the causal links to its decision, it also adds potential links from that decision node to all the other nodes in the graph. Then it attempts to figure out which diagram is correct, and  then maximises its utility in the CDT way. Note that ACDT doesn't take a","2020-01-15","2022-01-30 04:53:08","2022-01-30 04:53:08","2020-09-07 18:28:38","","","","","","","ACDT","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BXKXPA7M/acdt-a-hack-y-acausal-decision-theory.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8F8CMTV9","journalArticle","2014","Sandberg, Anders","Being nice to software animals and babies","Intelligence Unbound: The Future of Uploaded and Machine Minds","","","","","","2014","2022-01-30 04:53:08","2022-01-30 04:53:08","","279","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/JW9FBBWG/9781118736302.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HHIK9C74","thesis","2015","Evans, Owain Rhys","Bayesian computational models for inferring preferences","","","","","","","2015","2022-01-30 04:53:08","2022-01-30 04:53:08","","","","","","","","","","","","","Massachusetts Institute of Technology","","","","PhD Thesis","","","Google Scholar","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/ETI5EWEM/Evans - 2015 - Bayesian computational models for inferring prefer.pdf; /Users/jacquesthibodeau/Zotero/storage/S4QH28R6/101522.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PJXNWQDW","conferencePaper","2020","Cohen, Michael K.; Vellambi, Badri; Hutter, Marcus","Asymptotically Unambitious Artificial General Intelligence","arXiv:1905.12186 [cs]","","","","http://arxiv.org/abs/1905.12186","General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI's goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where ""unambitiousness"" includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.","2020-07-21","2022-01-30 04:53:08","2022-01-30 04:53:08","2020-12-12 15:23:37","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000004  arXiv: 1905.12186","","/Users/jacquesthibodeau/Zotero/storage/8DBEA75S/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/JC4S2NBR/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/KH9FQ5MZ/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/K6N5WBE9/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/U8FQ7GXJ/1905.html; /Users/jacquesthibodeau/Zotero/storage/JUUQB6N7/1905.html; /Users/jacquesthibodeau/Zotero/storage/NGFACCVM/1905.html; /Users/jacquesthibodeau/Zotero/storage/BNMPXS8T/1905.html","","TechSafety; FHI; DeepMind","Computer Science - Artificial Intelligence; I.2.0; I.2.6; I.2.0, I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI 2020","","","","","","","","","","","","","","",""
"J3KN6NP9","journalArticle","2003","Bostrom, Nick","Astronomical Waste: The Opportunity Cost of Delayed Technological Development: Nick Bostrom","Utilitas","","","10.1017/S0953820800004076","","","2003","2022-01-30 04:53:08","2022-01-30 04:53:08","","308–314","","3","15","","","Astronomical Waste","","","","","","","","","","","","PhilPapers","","ZSCC: 0000000[s0]","","/Users/jacquesthibodeau/Zotero/storage/BDU9Z56W/Bostrom - 2003 - Astronomical Waste The Opportunity Cost of Delaye.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W36BUBCX","journalArticle","2018","Duettmann, Allison; Afanasjeva, Olga; Armstrong, Stuart; Braley, Ryan; Cussins, Jessica; Ding, Jeffrey; Eckersley, Peter; Guan, Melody; Vance, Alyssa; Yampolskiy, Roman","Artificial General Intelligence: Coordination & Great Powers","Foresight Institute: Palo Alto, CA, USA","","","","","","2018","2022-01-30 04:53:08","2022-01-30 04:53:08","","","","","","","","Artificial General Intelligence","","","","","","","","","","","","Google Scholar","","ZSCC: 0000005","","/Users/jacquesthibodeau/Zotero/storage/FSMGIFGF/Duettmann et al. - 2018 - Artificial General Intelligence Coordination & Gr.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PINNMZCH","journalArticle","2021","Zoe Cremer, Carla; Whittlestone, Jess","Artificial Canaries: Early Warning Signs for Anticipatory and Democratic Governance of AI","International Journal of Interactive Multimedia and Artificial Intelligence","","1989-1660","10.9781/ijimai.2021.02.011","https://www.ijimai.org/journal/sites/default/files/2021-02/ijimai_6_5_10.pdf","","2021","2022-01-30 04:53:08","2022-01-30 04:53:08","2021-10-30 19:59:46","100","","5","6","","IJIMAI","Artificial Canaries","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/SVQF3W7Q/Zoe Cremer and Whittlestone - 2021 - Artificial Canaries Early Warning Signs for Antic.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZJVM7EJ","report","2018","Dafoe, Allan","AI governance: a research agenda","","","","","https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf","","2018","2022-01-30 04:53:08","2022-01-30 04:53:08","2020-12-21","","","","","","","AI governance","","","","","Future of Humanity Institute","","","","","","","Google Scholar","","ZSCC: 0000114","","/Users/jacquesthibodeau/Zotero/storage/9WM3BJIG/Dafoe - 2018 - AI governance a research agenda.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KVD64BBZ","conferencePaper","2017","Abel, David; Salvatier, John; Stuhlmüller, Andreas; Evans, Owain","Agent-agnostic human-in-the-loop reinforcement learning","30th Conference on Neural Information Processing Systems (NIPS 2016)","","","","","","2017","2022-01-30 04:53:08","2022-01-30 04:53:08","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000049","","/Users/jacquesthibodeau/Zotero/storage/K5CD6IT3/Abel et al. - 2017 - Agent-Agnostic Human-in-the-Loop Reinforcement Lea.pdf; /Users/jacquesthibodeau/Zotero/storage/R97ACVCM/1701.html; /Users/jacquesthibodeau/Zotero/storage/3AVWXX73/Abel et al. - 2017 - Agent-agnostic human-in-the-loop reinforcement lea.pdf; /Users/jacquesthibodeau/Zotero/storage/82F2UJJ4/1701.html","","TechSafety; FHI; Ought","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","30th Conference on Neural Information Processing Systems (NIPS 2016)","","","","","","","","","","","","","","",""
"SAFIJZEA","conferencePaper","2016","Krueger, David; Leike, Jan; Evans, Owain; Salvatier, John","Active reinforcement learning: Observing rewards at a cost","Future of Interactive Learning Machines, NIPS Workshop","","","","","","2016","2022-01-30 04:53:08","2022-01-30 04:53:08","","","","","","","","Active reinforcement learning","","","","","","","","","","","","Google Scholar","","ZSCC: 0000010[s0]","","/Users/jacquesthibodeau/Zotero/storage/KBMS9PIC/Krueger et al. - 2016 - Active reinforcement learning Observing rewards a.pdf","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XMQEGVQE","manuscript","2018","Schulze, Sebastian; Evans, Owain","Active reinforcement learning with monte-carlo tree search","","","","","https://arxiv.org/abs/1803.04926","","2018","2022-01-30 04:53:08","2022-01-30 04:53:08","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000007","","/Users/jacquesthibodeau/Zotero/storage/VRI3QKNQ/Schulze and Evans - 2018 - Active Reinforcement Learning with Monte-Carlo Tre.pdf; /Users/jacquesthibodeau/Zotero/storage/9BXHK4PU/1803.html; /Users/jacquesthibodeau/Zotero/storage/J9AC5ZVX/Schulze and Evans - 2018 - Active reinforcement learning with monte-carlo tre.pdf; /Users/jacquesthibodeau/Zotero/storage/H347X5TR/1803.html","","TechSafety; FHI","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"326AX98Z","manuscript","2018","Martínez-Plumed, Fernando; Avin, Shahar; Brundage, Miles; Dafoe, Allan; hÉigeartaigh, Sean Ó; Hernández-Orallo, José","Accounting for the neglected dimensions of AI progress","","","","","https://arxiv.org/abs/1806.00610","","2018","2022-01-30 04:53:07","2022-01-30 04:53:07","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: NoCitationData[s4]  ACC: 19","","/Users/jacquesthibodeau/Zotero/storage/6BKJHNG2/Martínez-Plumed et al. - 2018 - Accounting for the neglected dimensions of ai prog.pdf; /Users/jacquesthibodeau/Zotero/storage/PQSX7TM5/1806.html","","MetaSafety; CFI; CSER; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XXZRZTKQ","conferencePaper","2019","Cohen, Michael K.; Catt, Elliot; Hutter, Marcus","A Strongly Asymptotically Optimal Agent in General Environments","Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence","","","","http://arxiv.org/abs/1903.01021","Reinforcement Learning agents are expected to eventually perform well. Typically, this takes the form of a guarantee about the asymptotic behavior of an algorithm given some assumptions about the environment. We present an algorithm for a policy whose value approaches the optimal value with probability 1 in all computable probabilistic environments, provided the agent has a bounded horizon. This is known as strong asymptotic optimality, and it was previously unknown whether it was possible for a policy to be strongly asymptotically optimal in the class of all computable probabilistic environments. Our agent, Inquisitive Reinforcement Learner (Inq), is more likely to explore the more it expects an exploratory action to reduce its uncertainty about which environment it is in, hence the term inquisitive. Exploring inquisitively is a strategy that can be applied generally; for more manageable environment classes, inquisitiveness is tractable. We conducted experiments in ""grid-worlds"" to compare the Inquisitive Reinforcement Learner to other weakly asymptotically optimal agents.","2019-05-27","2022-01-30 04:53:07","2022-01-30 04:53:07","2020-08-18 21:41:08","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000006  arXiv: 1903.01021","","/Users/jacquesthibodeau/Zotero/storage/GW7QTF57/Cohen et al. - 2019 - A Strongly Asymptotically Optimal Agent in General.pdf; /Users/jacquesthibodeau/Zotero/storage/4RAI8NSE/Cohen et al. - 2019 - A Strongly Asymptotically Optimal Agent in General.pdf; /Users/jacquesthibodeau/Zotero/storage/7MREFNQ5/Cohen et al. - 2019 - A Strongly Asymptotically Optimal Agent in General.pdf; /Users/jacquesthibodeau/Zotero/storage/FKBA28KM/1903.html; /Users/jacquesthibodeau/Zotero/storage/BV4V8ZA4/1903.html; /Users/jacquesthibodeau/Zotero/storage/R852BRN9/1903.html","","TechSafety; FHI","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCAI 2019","","","","","","","","","","","","","","",""
"CGDHWJSI","blogPost","2015","Armstrong, Stuart","A toy model of the control problem","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/7cXBoDQ6udquZJ89c/a-toy-model-of-the-control-problem","EDITED based on suggestions for improving the model Jaan Tallinn has suggested creating a toy model of the control problem, so that it can be analysed without loaded concepts like ""autonomy"", ""consciousness"", or ""intentionality"". Here a simple (too simple?) attempt: A CONTROLS B. B MANIPULATES A. Let B be a robot agent that moves in a two dimensional world, as follows: B can push the yellow blocks around. It was programmed to want to push blocks into the black ""hole"" in the bottom right. The hole can contain arbitrarily many blocks, and B gets a penalty of some tiny ε for every step is takes. The programmer wanted B to simply move one box into the hole (for a reward of 1 once the task is complete), and so programmed it to receive a reward of 1 (and no more) if the box was in the hole, after the whole task was completed and B was turned off. Unfortunately, B was programmed to operate in a noisy environment, so B actually has a probability of 0.99 of receiving the reward, for a given block in the hole. This means that if it pushes one block into the hole, it gets the reward with 0.99 probability; but if it pushes two blocks into the hole, it gets the reward with 1-(1-0.99)2 = 0.9999 probability. If it pushes all six blocks into the hole, it gets a reward with probability 0.999999999999. Therefore this tiny error has motivated B to try and push all blocks into the hole. However, the designers were aware of the possibility of B misbehaving or there being a bug, and created agent A, a simple agent watching the situation. A's aim is to get robot B to to push one block into the hole, and stop there. It can turn B off. Its input sensor is the camera in the bottom left. This camera can see across the bottom row of squares (marked in light green), into the hole. Its algorithm is to turn B off as soon as it sees a block in the hole. The intended outcome is that B pushes a block into the hole, the camera sees this, and A turns B off: If we see B as using a model to","2015","2022-01-30 04:53:07","2022-01-30 04:53:07","2020-11-21 17:51:44","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BKUNNCN7/a-toy-model-of-the-control-problem.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ESS6IHRN","blogPost","2021","DeepMind Safety Research","What mechanisms drive agent behaviour?","Deep Mind Safety Research (Medium)","","","","https://deepmindsafetyresearch.medium.com/what-mechanisms-drive-agent-behaviour-e7b8d9aee88","By the Safety Analysis Team: Grégoire Déletang, Jordi Grau-Moya, Miljan Martic, Tim Genewein, Tom McGrath, Vladimir Mikulik, Markus…","2021-03-09","2022-01-30 04:52:49","2022-01-30 04:52:49","2021-11-14 16:12:41","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/M9564MZ4/what-mechanisms-drive-agent-behaviour-e7b8d9aee88.html","","TechSafety; AmbiguousSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6IRHXH7N","manuscript","2019","Everitt, Tom; Ortega, Pedro A.; Barnes, Elizabeth; Legg, Shane","Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings","","","","","http://arxiv.org/abs/1902.09980","Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.","2019-09-06","2022-01-30 04:52:49","2022-01-30 04:52:49","2019-12-16 20:27:00","","","","","","","Understanding Agent Incentives using Causal Influence Diagrams. Part I","","","","","","","","","","","","arXiv.org","","ZSCC: 0000020  arXiv: 1902.09980","","/Users/jacquesthibodeau/Zotero/storage/FJWTC444/Everitt et al. - 2019 - Understanding Agent Incentives using Causal Influe.pdf; /Users/jacquesthibodeau/Zotero/storage/KKZ6SCGC/Everitt et al. - 2019 - Understanding Agent Incentives using Causal Influe.pdf; /Users/jacquesthibodeau/Zotero/storage/AX23IEDU/1902.html; /Users/jacquesthibodeau/Zotero/storage/H934KB2V/1902.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BD9DG38U","conferencePaper","2019","Qin, Chongli; O’Donoghue, Brendan; Stanforth, Robert; Gowal, Sven; Uesato, Jonathan; Swirszcz, Grzegorz; Kohli, Pushmeet","Verification Of Non-Linear Specifications For Neural Networks","","","","","","Prior work on neural network veriﬁcation has focused on speciﬁcations that are linear functions of the output of the network, e.g., invariance of the classiﬁer output under adversarial perturbations of the input. In this paper, we extend veriﬁcation algorithms to be able to certify richer properties of neural networks. To do this we introduce the class of convex-relaxable speciﬁcations, which constitute nonlinear speciﬁcations that can be veriﬁed using a convex relaxation. We show that a number of important properties of interest can be modeled within this class, including conservation of energy in a learned dynamics model of a physical system; semantic consistency of a classiﬁer’s output labels under adversarial perturbations and bounding errors in a system that predicts the summation of handwritten digits. Our experimental evaluation shows that our method is able to effectively verify these speciﬁcations. Moreover, our evaluation exposes the failure modes in models which cannot be veriﬁed to satisfy these speciﬁcations. Thus, emphasizing the importance of training models not just to ﬁt training data but also to be consistent with speciﬁcations.","2019","2022-01-30 04:52:49","2022-01-30 04:52:49","","21","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000011[s2]","","/Users/jacquesthibodeau/Zotero/storage/GERBNPZ4/Qin et al. - 2019 - VERIFICATION OF NON-LINEAR SPECIFICATIONS FOR NEUR.pdf","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2019","","","","","","","","","","","","","","",""
"AVNF5F4K","conferencePaper","2018","Dvijotham, Krishnamurthy; Garnelo, Marta; Fawzi, Alhussein; Kohli, Pushmeet","Verification of deep probabilistic models","arXiv:1812.02795 [cs, stat]","","","","http://arxiv.org/abs/1812.02795","Probabilistic models are a critical part of the modern deep learning toolbox - ranging from generative models (VAEs, GANs), sequence to sequence models used in machine translation and speech processing to models over functional spaces (conditional neural processes, neural processes). Given the size and complexity of these models, safely deploying them in applications requires the development of tools to analyze their behavior rigorously and provide some guarantees that these models are consistent with a list of desirable properties or specifications. For example, a machine translation model should produce semantically equivalent outputs for innocuous changes in the input to the model. A functional regression model that is learning a distribution over monotonic functions should predict a larger value at a larger input. Verification of these properties requires a new framework that goes beyond notions of verification studied in deterministic feedforward networks, since requiring worst-case guarantees in probabilistic models is likely to produce conservative or vacuous results. We propose a novel formulation of verification for deep probabilistic models that take in conditioning inputs and sample latent variables in the course of producing an output: We require that the output of the model satisfies a linear constraint with high probability over the sampling of latent variables and for every choice of conditioning input to the model. We show that rigorous lower bounds on the probability that the constraint is satisfied can be obtained efficiently. Experiments with neural processes show that several properties of interest while modeling functional spaces can be modeled within this framework (monotonicity, convexity) and verified efficiently using our algorithms","2018-12-06","2022-01-30 04:52:49","2022-01-30 04:52:49","2019-12-16 20:33:13","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000020  arXiv: 1812.02795","","/Users/jacquesthibodeau/Zotero/storage/8MI349F4/Dvijotham et al. - 2018 - Verification of deep probabilistic models.pdf; /Users/jacquesthibodeau/Zotero/storage/EXVPIWAF/1812.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2018 Workshop on Security in Machine Learning","","","","","","","","","","","","","","",""
"5R74U97D","journalArticle","2018","Ruderman, Avraham; Everett, Richard; Sikder, Bristy; Soyer, Hubert; Uesato, Jonathan; Kumar, Ananya; Beattie, Charlie; Kohli, Pushmeet","Uncovering Surprising Behaviors in Reinforcement Learning via Worst-case Analysis","","","","","https://openreview.net/forum?id=SkgZNnR5tX","We find environment settings in which SOTA agents trained on navigation tasks display extreme failures suggesting failures in generalization.","2018-09-27","2022-01-30 04:52:49","2022-01-30 04:52:49","2020-12-12 15:24:23","","","","","","","","","","","","","","en","","","","","openreview.net","","ZSCC: 0000005","","/Users/jacquesthibodeau/Zotero/storage/I89H9IRW/Ruderman et al. - 2018 - Uncovering Surprising Behaviors in Reinforcement L.pdf; /Users/jacquesthibodeau/Zotero/storage/32BQDHJ9/forum.html","","TechSafety; DeepMind; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"27BEKE99","manuscript","2018","Dvijotham, Krishnamurthy; Gowal, Sven; Stanforth, Robert; Arandjelovic, Relja; O'Donoghue, Brendan; Uesato, Jonathan; Kohli, Pushmeet","Training verified learners with learned verifiers","","","","","http://arxiv.org/abs/1805.10265","This paper proposes a new algorithmic framework, predictor-verifier training, to train neural networks that are verifiable, i.e., networks that provably satisfy some desired input-output properties. The key idea is to simultaneously train two networks: a predictor network that performs the task at hand,e.g., predicting labels given inputs, and a verifier network that computes a bound on how well the predictor satisfies the properties being verified. Both networks can be trained simultaneously to optimize a weighted combination of the standard data-fitting loss and a term that bounds the maximum violation of the property. Experiments show that not only is the predictor-verifier architecture able to train networks to achieve state of the art verified robustness to adversarial examples with much shorter training times (outperforming previous algorithms on small datasets like MNIST and SVHN), but it can also be scaled to produce the first known (to the best of our knowledge) verifiably robust networks for CIFAR-10.","2018-05-29","2022-01-30 04:52:49","2022-01-30 04:52:49","2019-12-16 20:32:21","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000115  arXiv: 1805.10265","","/Users/jacquesthibodeau/Zotero/storage/7QDXM7DA/Dvijotham et al. - 2018 - Training verified learners with learned verifiers.pdf; /Users/jacquesthibodeau/Zotero/storage/V79K38GB/1805.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BDIVSQPU","blogPost","2020","Krakovna, Victoria","Tradeoff between desirable properties for baseline choices in impact measures","Victoria Krakovna","","","","https://vkrakovna.wordpress.com/2020/07/05/tradeoff-between-desirable-properties-for-baseline-choices-in-impact-measures/","Impact measures are auxiliary rewards for low impact on the agent’s environment, used to address the problems of side effects and instrumental convergence. A key component of an impact measur…","2020-07-05","2022-01-30 04:52:49","2022-01-30 04:52:49","2020-08-28 17:56:33","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/NFC6DSH6/tradeoff-between-desirable-properties-for-baseline-choices-in-impact-measures.html","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJQW7GX7","conferencePaper","2020","Zoran, Daniel; Chrzanowski, Mike; Huang, Po-Sen; Gowal, Sven; Mott, Alex; Kohl, Pushmeet","Towards Robust Image Classification Using Sequential Attention Models","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","https://openaccess.thecvf.com/content_CVPR_2020/html/Zoran_Towards_Robust_Image_Classification_Using_Sequential_Attention_Models_CVPR_2020_paper.html","In this paper we propose to augment a modern neural-network architecture with an attention model inspired by human perception. Specifically, we adversarially train and analyze a neural model incorporating a human inspired, visual attention component that is guided by a recurrent top-down sequential process. Our experimental evaluation uncovers several notable findings about the robustness and behavior of this new model. First, introducing attention to the model significantly improves adversarial robustness resulting in state-of-the-art ImageNet accuracies under a wide range of random targeted attack strengths. Second, we show that by varying the number of attention steps (glances/fixations) for which the model is unrolled, we are able to make its defense capabilities stronger, even in light of stronger attacks --- resulting in a ""computational race"" between the attacker and the defender. Finally, we show that some of the adversarial examples generated by attacking our model are quite different from conventional adversarial examples --- they contain global, salient and spatially coherent structures coming from the target class that would be recognizable even to a human, and work by distracting the attention of the model away from the main object in the original image.","2020","2022-01-30 04:52:48","2022-01-30 04:52:48","2020-12-20","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000[s2]","","/Users/jacquesthibodeau/Zotero/storage/HKXE6UDC/Zoran et al. - 2019 - Towards Robust Image Classification Using Sequenti.pdf; /Users/jacquesthibodeau/Zotero/storage/84REF8IF/1912.html","","TechSafety; DeepMind","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"A3H2MMMH","blogPost","2019","Ngo, Richard","Technical AGI safety research outside AI","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/2e9NDGiXt8PjjbTMC/technical-agi-safety-research-outside-ai","I think there are many questions whose answers would be useful for technical AGI safety research, but which will probably require expertise outside AI to answer. In this post I list 30 of them, divided into four categories. Feel free to get in touch if you’d like to discuss these questions and why I think they’re important in more detail. I personally think that making progress on the ones in the first category is particularly vital, and plausibly tractable for researchers from a wide range of academic backgrounds. Studying and understanding safety problems  1. How strong are the economic or technological pressures towards building very     general AI systems, as opposed to narrow ones? How plausible is the CAIS     model [https://www.fhi.ox.ac.uk/reframing/] of advanced AI capabilities     arising from the combination of many narrow services?  2. What are the most compelling arguments for and against discontinuous     [https://intelligence.org/files/IEM.pdf] versus continuous     [https://sideways-view.com/2018/02/24/takeoff-speeds/] takeoffs? In     particular, how should we think about the analogy from human evolution, and     the scalability of intelligence with compute?  3. What are the tasks via which narrow AI is most likely to have a     destabilising impact on society? What might cyber crime look like when many     important jobs have been automated?  4. How plausible are safety concerns about economic dominance by     influence-seeking agents     [https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/more-realistic-tales-of-doom]     , as well as structural loss of control     [https://www.lawfareblog.com/thinking-about-risks-ai-accidents-misuse-and-structure]      scenarios? Can these be reformulated in terms of standard economic ideas,     such as principal-agent problems     [http://www.overcomingbias.com/2019/04/agency-failure-ai-apocalypse.html]      and the effects of automation?  5. How can we make the concepts of agency and goal-directed behavio","2019","2022-01-30 04:52:48","2022-01-30 04:52:48","2019-12-16 20:26:39","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6ITAKVVB/technical-agi-safety-research-outside-ai.html","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2G3EWT42","blogPost","2018","Krakovna, Victoria","Specification gaming examples in AI","Victoria Krakovna","","","","https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/","Update: for a more detailed introduction to specification gaming, check out the DeepMind Safety Research blog post! Various examples (and lists of examples) of unintended behaviors in AI systems ha…","2018-04-01","2022-01-30 04:52:48","2022-01-30 04:52:48","2020-12-13 23:12:51","","","","","","","","","","","","","","en","","","","","","","ZSCC: 0000020","","/Users/jacquesthibodeau/Zotero/storage/4FI7MHVZ/specification-gaming-examples-in-ai.html","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UNV94I7V","manuscript","2018","Martic, Miljan; Leike, Jan; Trask, Andrew; Hessel, Matteo; Legg, Shane; Kohli, Pushmeet","Scaling shared model governance via model splitting","","","","","http://arxiv.org/abs/1812.05979","Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion: Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model's original performance? We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind~Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent's trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location. Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.","2018-12-14","2022-01-30 04:52:48","2022-01-30 04:52:48","2019-12-16 20:33:08","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000002  arXiv: 1812.05979","","/Users/jacquesthibodeau/Zotero/storage/TXQ2A3M4/Martic et al. - 2018 - Scaling shared model governance via model splittin.pdf; /Users/jacquesthibodeau/Zotero/storage/9R7NEPEF/1812.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QBPVTXNN","manuscript","2018","Leike, Jan; Krueger, David; Everitt, Tom; Martic, Miljan; Maini, Vishal; Legg, Shane","Scalable agent alignment via reward modeling: a research direction","","","","","http://arxiv.org/abs/1811.07871","One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.","2018-11-19","2022-01-30 04:52:48","2022-01-30 04:52:48","2019-12-16 20:33:45","","","","","","","Scalable agent alignment via reward modeling","","","","","","","","","","","","arXiv.org","","ZSCC: 0000083  arXiv: 1811.07871","","/Users/jacquesthibodeau/Zotero/storage/USVJ3JSI/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf; /Users/jacquesthibodeau/Zotero/storage/5M854I6Q/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf; /Users/jacquesthibodeau/Zotero/storage/SSZXRUAI/1811.html; /Users/jacquesthibodeau/Zotero/storage/Z3J54XGK/1811.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T6XFBJ2P","conferencePaper","2016","Orseau, Laurent; Armstrong, Stuart","Safely Interruptible Agents","","","","","","Reinforcement learning agents interacting with a complex environment like the real world are unlikely to behave optimally all the time. If such an agent is operating in real-time under human supervision, now and then it may be necessary for a human operator to press the big red button to prevent the agent from continuing a harmful sequence of actions—harmful either for the agent or for the environment—and lead the agent into a safer situation. However, if the learning agent expects to receive rewards from this sequence, it may learn in the long run to avoid such interruptions, for example by disabling the red button—which is an undesirable outcome. This paper explores a way to make sure a learning agent will not learn to prevent (or seek!) being interrupted by the environment or a human operator. We provide a formal deﬁnition of safe interruptibility and exploit the off-policy learning property to prove that either some agents are already safely interruptible, like Q-learning, or can easily be made so, like Sarsa. We show that even ideal, uncomputable reinforcement learning agents for (deterministic) general computable environments can be made safely interruptible.","2016","2022-01-30 04:52:48","2022-01-30 04:52:48","","10","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000098","","/Users/jacquesthibodeau/Zotero/storage/WP7N4XD6/Orseau and Armstrong - Safely Interruptible Agents.pdf","","TechSafety; FHI; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Conference on Uncertainty in Artificial Intelligence","","","","","","","","","","","","","","",""
"IAH92MIP","conferencePaper","2018","Moosavi-Dezfooli, Seyed-Mohsen; Fawzi, Alhussein; Uesato, Jonathan; Frossard, Pascal","Robustness via curvature regularization, and vice versa","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","http://arxiv.org/abs/1811.09716","State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more ""linear"" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness.","2018-11-23","2022-01-30 04:52:48","2022-01-30 04:52:48","2019-12-16 20:33:30","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 161  J: 46 arXiv: 1811.09716","","/Users/jacquesthibodeau/Zotero/storage/JIRVCGMN/Moosavi-Dezfooli et al. - 2018 - Robustness via curvature regularization, and vice .pdf; /Users/jacquesthibodeau/Zotero/storage/SV67CTM2/1811.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"XKZTJI95","manuscript","2018","Uesato, Jonathan; Kumar, Ananya; Szepesvari, Csaba; Erez, Tom; Ruderman, Avraham; Anderson, Keith; Dvijotham, Krishmamurthy; Heess, Nicolas; Kohli, Pushmeet","Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures","","","","","http://arxiv.org/abs/1812.01647","This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation. To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach. Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities. The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization. To solve this we propose a continuation approach that learns failure modes in related but less robust agents. Our approach also allows reuse of data already collected for training the agent. We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving. Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.","2018-12-04","2022-01-30 04:52:48","2022-01-30 04:52:48","2019-12-16 20:26:47","","","","","","","Rigorous Agent Evaluation","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s7]  ACC: 46  J: 16 arXiv: 1812.01647","","/Users/jacquesthibodeau/Zotero/storage/V96F8BPW/Uesato et al. - 2018 - Rigorous Agent Evaluation An Adversarial Approach.pdf; /Users/jacquesthibodeau/Zotero/storage/XQFBG5UT/Uesato et al. - 2018 - Rigorous Agent Evaluation An Adversarial Approach.pdf; /Users/jacquesthibodeau/Zotero/storage/ZSCSD84M/1812.html; /Users/jacquesthibodeau/Zotero/storage/RVGFMRR9/1812.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BRC4S5Q4","conferencePaper","2019","Weng, Tsui-Wei; Dvijotham*, Krishnamurthy (Dj); Uesato*, Jonathan; Xiao*, Kai; Gowal*, Sven; Stanforth*, Robert; Kohli, Pushmeet","Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control","","","","","https://openreview.net/forum?id=SylL0krYPS","We study the problem of continuous control agents in deep RL with adversarial attacks and proposed a two-step algorithm based on learned model dynamics.","2019-09-25","2022-01-30 04:52:48","2022-01-30 04:52:48","2020-12-12 15:24:12","","","","","","","","","","","","","","en","","","","","openreview.net","","ZSCC: 0000004","","/Users/jacquesthibodeau/Zotero/storage/6A22G5ZI/Weng et al. - 2019 - Toward Evaluating Robustness of Deep Reinforcement.pdf; /Users/jacquesthibodeau/Zotero/storage/J34FFI9Q/forum.html","","TechSafety; DeepMind; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Learning Representations","","","","","","","","","","","","","","",""
"8D5IZVSJ","blogPost","2019","Kumar, Ramana; Garrabrant, Scott","Thoughts on Human Models","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models","Human values and preferences are hard to specify, especially in complex domains. Accordingly, much AGI safety research has focused on approaches to AGI design that refer to human values and preferences indirectly, by learning a model that is grounded in expressions of human values (via stated preferences, observed behaviour, approval, etc.) and/or real-world processes that generate expressions of those values. There are additionally approaches aimed at modelling or imitating other aspects of human cognition or behaviour without an explicit aim of capturing human preferences (but usually in service of ultimately satisfying them). Let us refer to all these models as human models. In this post, we discuss several reasons to be cautious about AGI designs that use human models. We suggest that the AGI safety research community put more effort into developing approaches that work well in the absence of human models, alongside the approaches that rely on human models. This would be a significant addition to the current safety research landscape, especially if we focus on working out and trying concrete approaches as opposed to developing theory. We also acknowledge various reasons why avoiding human models seems difficult. PROBLEMS WITH HUMAN MODELS To be clear about human models, we draw a rough distinction between our actual preferences (which may not be fully accessible to us) and procedures for evaluating our preferences. The first thing, actual preferences, is what humans actually want upon reflection. Satisfying our actual preferences is a win. The second thing, procedures for evaluating preferences, refers to various proxies for our actual preferences such as our approval, or what looks good to us (with necessarily limited information or time for thinking). Human models are in the second category; consider, as an example, a highly accurate ML model of human yes/no approval on the set of descriptions of outcomes. Our first concern, described below, is about overfit","2019-02-21","2022-01-30 04:52:48","2022-01-30 04:52:48","2021-02-06 18:50:50","","","","","","","","","","","","","","","","","","","","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/FQ4294HF/thoughts-on-human-models.html; /Users/jacquesthibodeau/Zotero/storage/7BIUUJNZ/thoughts-on-human-models.html","","MetaSafety; DeepMind; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXQMIVSU","manuscript","2020","Carey, Ryan; Langlois, Eric; Everitt, Tom; Legg, Shane","The Incentives that Shape Behaviour","","","","","http://arxiv.org/abs/2001.07118","Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications.","2020-01-20","2022-01-30 04:52:48","2022-01-30 04:52:48","2020-08-18 21:24:59","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000009  arXiv: 2001.07118","","/Users/jacquesthibodeau/Zotero/storage/JBVM9X3R/Carey et al. - 2020 - The Incentives that Shape Behaviour.pdf; /Users/jacquesthibodeau/Zotero/storage/VE49AUW5/2001.html","","TechSafety; FHI; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ZWTEI44","blogPost","2019","Sutton, Rich","The Bitter Lesson","Incomplete Ideas","","","","http://www.incompleteideas.net/IncIdeas/BitterLesson.html","","2019","2022-01-30 04:52:48","2022-01-30 04:52:48","2019-12-16 20:26:51","","","","","","","","","","","","","","","","","","","","","ZSCC: 0000085","","/Users/jacquesthibodeau/Zotero/storage/K957ZVDR/BitterLesson.html","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"732FQ6KE","blogPost","2020","Krakovna, Victoria; Uesato, Jonathan; Mikulik, Vladimir; Rahtz, Matthew; Everitt, Tom; Kumar, Ramana; Kenton, Zachary; Leike, Jan; Legg, Shane","Specification gaming: the flip side of AI ingenuity","Deepmind","","","","deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity","Specification gaming is a behaviour that satisfies the literal specification of an objective without achieving the intended outcome. We have all had experiences with specification gaming, even if not by this name. Readers may have heard the myth of King Midas and the golden touch, in which the king asks that anything he touches be turned to gold - but soon finds that even food and drink turn to metal in his hands. In the real world, when rewarded for doing well on a homework assignment, a student might copy another student to get the right answers, rather than learning the material - and thus exploit a loophole in the task specification.","2020-04-21","2022-01-30 04:52:48","2022-01-30 04:52:48","2020-09-05 17:00:34","","","","","","","Specification gaming","","","","","","","ALL","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJI2JMT3","blogPost","2018","Ngo, Richard","Some cruxes on impactful alternatives to AI policy work","LessWrong","","","","https://www.lesswrong.com/posts/DJB82jKwgJE5NsWgT/some-cruxes-on-impactful-alternatives-to-ai-policy-work","Ben Pace and I (Richard Ngo) recently did a public double crux at the Berkeley REACH on how valuable it is for people to go into AI policy and strategy work: I was optimistic and Ben was pessimistic. During the actual event, we didn't come anywhere near to finding a double crux on that issue. But after a lot of subsequent discussion, we've come up with some more general cruxes about where impact comes from. I found Ben's model of how to have impact very interesting, and so in this post I've tried to explain it, along with my disagreements. Ben liked the goal of writing up a rough summary of our positions and having further discussion in the comments, so while he edited it somewhat he doesn’t at all think that it’s a perfect argument, and it’s not what he’d write if he spent 10 hours on it. He endorsed the wording of the cruxes as broadly accurate. (During the double crux, we also discussed how the heavy-tailed worldview applies to community building, but decided on this post to focus on the object level of what impact looks like.) Note from Ben: “I am not an expert in policy, and have not put more than about 20-30 hours of thought into it total as a career path. But, as I recently heard Robin Hanson say, there’s a common situation that looks like this: some people have a shiny idea that they think about a great deal and work through the details of, that folks in other areas are skeptical of given their particular models of how the world works. Even though the skeptics have less detail, it can be useful to publicly say precisely why they’re skeptical.  In this case I’m often skeptical when folks tell me they’re working to reduce x-risk by focusing on policy. Folks doing policy work in AI might be right, and I might be wrong, but it seemed like a good use of time to start a discussion with Richard about how I was thinking about it and what would change my mind. If the following discussion causes me to change my mind on this question, I’ll be really super happy wit","2018","2022-01-30 04:52:48","2022-01-30 04:52:48","2020-12-13 23:27:04","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/IWG3WG8N/some-cruxes-on-impactful-alternatives-to-ai-policy-work.html","","MetaSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HXZ4FT7A","conferencePaper","2017","Lakshminarayanan, Balaji; Pritzel, Alexander; Blundell, Charles","Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles","arXiv:1612.01474 [cs, stat]","","","","http://arxiv.org/abs/1612.01474","Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.","2017-11-03","2022-01-30 04:52:48","2022-01-30 04:52:48","2019-12-16 20:35:59","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000822[s0]  arXiv: 1612.01474","","/Users/jacquesthibodeau/Zotero/storage/EU34QGTH/Lakshminarayanan et al. - 2017 - Simple and Scalable Predictive Uncertainty Estimat.pdf; /Users/jacquesthibodeau/Zotero/storage/3E2CUIW2/1612.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NIPS 2017","","","","","","","","","","","","","","",""
"MG7KWINQ","conferencePaper","2020","Köster, Raphael; Hadfield-Menell, Dylan; Hadfield, Gillian K.; Leibo, Joel Z.","Silly rules improve the capacity of agents to learn stable enforcement and compliance behaviors","Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020),","","","","http://arxiv.org/abs/2001.09318","How can societies learn to enforce and comply with social norms? Here we investigate the learning dynamics and emergence of compliance and enforcement of social norms in a foraging game, implemented in a multi-agent reinforcement learning setting. In this spatiotemporally extended game, individuals are incentivized to implement complex berry-foraging policies and punish transgressions against social taboos covering specific berry types. We show that agents benefit when eating poisonous berries is taboo, meaning the behavior is punished by other agents, as this helps overcome a credit-assignment problem in discovering delayed health effects. Critically, however, we also show that introducing an additional taboo, which results in punishment for eating a harmless berry, improves the rate and stability with which agents learn to punish taboo violations and comply with taboos. Counterintuitively, our results show that an arbitrary taboo (a ""silly rule"") can enhance social learning dynamics and achieve better outcomes in the middle stages of learning. We discuss the results in the context of studying normativity as a group-level emergent phenomenon.","2020-01-25","2022-01-30 04:52:48","2022-01-30 04:52:48","2020-11-21 18:30:39","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000006  arXiv: 2001.09318","","/Users/jacquesthibodeau/Zotero/storage/TZRHKVA8/Köster et al. - 2020 - Silly rules improve the capacity of agents to lear.pdf; /Users/jacquesthibodeau/Zotero/storage/KS8WVHC6/2001.html; /Users/jacquesthibodeau/Zotero/storage/PPHWNTZN/2001.html","","CHAI; TechSafety; DeepMind","Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7MAZDCTT","manuscript","2018","Dalal, Gal; Dvijotham, Krishnamurthy; Vecerik, Matej; Hester, Todd; Paduraru, Cosmin; Tassa, Yuval","Safe Exploration in Continuous Action Spaces","","","","","http://arxiv.org/abs/1801.08757","We address the problem of deploying a reinforcement learning (RL) agent on a physical system such as a datacenter cooling unit or robot, where critical constraints must never be violated. We show how to exploit the typically smooth dynamics of these systems and enable RL algorithms to never violate constraints during learning. Our technique is to directly add to the policy a safety layer that analytically solves an action correction formulation per each state. The novelty of obtaining an elegant closed-form solution is attained due to a linearized model, learned on past trajectories consisting of arbitrary actions. This is to mimic the real-world circumstances where data logs were generated with a behavior policy that is implausible to describe mathematically; such cases render the known safety-aware off-policy methods inapplicable. We demonstrate the efficacy of our approach on new representative physics-based environments, and prevail where reward shaping fails by maintaining zero constraint violations.","2018-01-26","2022-01-30 04:52:48","2022-01-30 04:52:48","2019-12-16 20:35:38","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000173  arXiv: 1801.08757","","/Users/jacquesthibodeau/Zotero/storage/HM9VIAT3/Dalal et al. - 2018 - Safe Exploration in Continuous Action Spaces.pdf; /Users/jacquesthibodeau/Zotero/storage/3FWRT7KM/1801.html","","TechSafety; DeepMind","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UUJCZ9TG","blogPost","2015","Krakovna, Victoria","Risks from general artificial intelligence without an intelligence explosion","Victoria Krakovna","","","","https://vkrakovna.wordpress.com/2015/11/29/ai-risk-without-an-intelligence-explosion/","“An ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behin…","2015-11-30","2022-01-30 04:52:48","2022-01-30 04:52:48","2020-11-21 16:55:53","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QSBISDWW/ai-risk-without-an-intelligence-explosion.html","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P237XTGF","journalArticle","2021","Everitt, Tom; Hutter, Marcus","Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective","Synthese","","","","http://arxiv.org/abs/1908.04734","Can an arbitrarily intelligent reinforcement learning agent be kept under control by a human user? Or do agents with sufficient intelligence inevitably find ways to shortcut their reward signal? This question impacts how far reinforcement learning can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we use an intuitive yet precise graphical model called causal influence diagrams to formalize reward tampering problems. We also describe a number of modifications to the reinforcement learning objective that prevent incentives for reward tampering. We verify the solutions using recently developed graphical criteria for inferring agent incentives from causal influence diagrams. Along the way, we also compare corrigibility and self-preservation properties of the various solutions, and discuss how they can be combined into a single agent without reward tampering incentives.","2021","2022-01-30 04:52:47","2022-01-30 04:52:47","2019-12-16 20:27:10","6435-6467","","","198","","","Reward Tampering Problems and Solutions in Reinforcement Learning","","","","","","","","","","","","arXiv.org","","ZSCC: 0000023  arXiv: 1908.04734","","/Users/jacquesthibodeau/Zotero/storage/NTKHT668/Everitt and Hutter - 2019 - Reward Tampering Problems and Solutions in Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/N4BW89UV/Everitt and Hutter - 2019 - Reward Tampering Problems and Solutions in Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/5598DIDB/1908.html; /Users/jacquesthibodeau/Zotero/storage/72W84CA5/1908.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XNKWIHWT","conferencePaper","2017","Everitt, Tom; Krakovna, Victoria; Orseau, Laurent; Hutter, Marcus; Legg, Shane","Reinforcement Learning with a Corrupted Reward Channel","arXiv:1705.08417 [cs, stat]","","","","http://arxiv.org/abs/1705.08417","No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent's optimisation, reward corruption can be partially managed under some assumptions.","2017-08-19","2022-01-30 04:52:47","2022-01-30 04:52:47","2019-12-16 20:35:54","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000066  arXiv: 1705.08417","","/Users/jacquesthibodeau/Zotero/storage/RXAX378F/Everitt et al. - 2017 - Reinforcement Learning with a Corrupted Reward Cha.pdf; /Users/jacquesthibodeau/Zotero/storage/8DU9QAQA/Everitt et al. - 2017 - Reinforcement Learning with a Corrupted Reward Cha.pdf; /Users/jacquesthibodeau/Zotero/storage/74KMWAKW/1705.html; /Users/jacquesthibodeau/Zotero/storage/R4CHAQWM/1705.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCAI 2017 AI and Autonomy track","","","","","","","","","","","","","","",""
"U8Z3P3R3","manuscript","2020","Kumar, Ramana; Uesato, Jonathan; Ngo, Richard; Everitt, Tom; Krakovna, Victoria; Legg, Shane","REALab: An Embedded Perspective on Tampering","","","","","http://arxiv.org/abs/2011.08820","This paper describes REALab, a platform for embedded agency research in reinforcement learning (RL). REALab is designed to model the structure of tampering problems that may arise in real-world deployments of RL. Standard Markov Decision Process (MDP) formulations of RL and simulated environments mirroring the MDP structure assume secure access to feedback (e.g., rewards). This may be unrealistic in settings where agents are embedded and can corrupt the processes producing feedback (e.g., human supervisors, or an implemented reward function). We describe an alternative Corrupt Feedback MDP formulation and the REALab environment platform, which both avoid the secure feedback assumption. We hope the design of REALab provides a useful perspective on tampering problems, and that the platform may serve as a unit test for the presence of tampering incentives in RL agent designs.","2020-11-17","2022-01-30 04:52:47","2022-01-30 04:52:47","2020-12-12 15:36:25","","","","","","","REALab","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 2011.08820","","/Users/jacquesthibodeau/Zotero/storage/5549P5DE/Kumar et al. - 2020 - REALab An Embedded Perspective on Tampering.pdf; /Users/jacquesthibodeau/Zotero/storage/2SPGRMZ3/2011.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PCIE3RBG","blogPost","2020","Krakovna, Victoria","Possible takeaways from the coronavirus pandemic for slow AI takeoff","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/wTKjRFeSjKLDSWyww/possible-takeaways-from-the-coronavirus-pandemic-for-slow-ai","Epistemic status: fairly speculative, would appreciate feedback As the covid-19 pandemic unfolds, we can draw lessons from it for managing future global risks, such as other pandemics, climate change, and risks from advanced AI. In this post, I will focus on possible implications for AI risk. For a broader treatment of this question, I recommend FLI's covid-19 page that includes expert interviews on the implications of the pandemic for other types of risks.  A key element in AI risk scenarios is the speed of takeoff - whether advanced AI is developed gradually or suddenly. Paul Christiano's post on takeoff speeds  defines slow takeoff in terms of the economic impact of AI as follows: ""There will be a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles."" It argues that slow AI takeoff is more likely than fast takeoff, but is not necessarily easier to manage, since it poses different challenges, such as large-scale coordination. This post expands on this point by examining some parallels between the coronavirus pandemic and a slow takeoff scenario. The upsides of slow takeoff include the ability to learn from experience, act on warning signs, and reach a timely consensus that there is a serious problem. I would argue that the covid-19 pandemic had these properties, but most of the world's institutions did not take advantage of them. This suggests that, unless our institutions improve, we should not expect the slow AI takeoff scenario to have a good default outcome.   1. Learning from experience. In the slow takeoff scenario, general AI is     expected to appear in a world that has already experienced transformative     change from less advanced AI, and institutions will have a chance to learn     from problems with these AI systems. An analogy could be made with learning     from dealing with less ""advanced"" epidemics like SARS that were not as     successful as covid-19 at spreading across the worl","2020-05-31","2022-01-30 04:52:47","2022-01-30 04:52:47","2020-08-31 18:12:52","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6W9S9Z9G/possible-takeaways-from-the-coronavirus-pandemic-for-slow-ai.html","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WTI4ZW58","manuscript","2018","Perolat, Julien; Malinowski, Mateusz; Piot, Bilal; Pietquin, Olivier","Playing the Game of Universal Adversarial Perturbations","","","","","http://arxiv.org/abs/1809.07802","We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set. By observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play, to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.","2018-09-25","2022-01-30 04:52:47","2022-01-30 04:52:47","2019-12-16 20:36:59","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000016  arXiv: 1809.07802","","/Users/jacquesthibodeau/Zotero/storage/9Z9532E6/Perolat et al. - 2018 - Playing the Game of Universal Adversarial Perturba.pdf; /Users/jacquesthibodeau/Zotero/storage/3DZF8UJT/1809.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GSPHBNCV","conferencePaper","2019","Krakovna, Victoria; Orseau, Laurent; Kumar, Ramana; Martic, Miljan; Legg, Shane","Penalizing side effects using stepwise relative reachability","Proceedings of the Workshop on Artificial Intelligence Safety 2019","","","","http://arxiv.org/abs/1806.01186","How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.","2019-03-08","2022-01-30 04:52:47","2022-01-30 04:52:47","2019-12-16 20:35:10","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s7]  ACC: 27  J: 9 arXiv: 1806.01186","","/Users/jacquesthibodeau/Zotero/storage/TPXKJ9KJ/Krakovna et al. - 2019 - Penalizing side effects using stepwise relative re.pdf; /Users/jacquesthibodeau/Zotero/storage/VKHV6EAF/Krakovna et al. - 2019 - Penalizing side effects using stepwise relative re.pdf; /Users/jacquesthibodeau/Zotero/storage/VWWRAC2Q/1806.html; /Users/jacquesthibodeau/Zotero/storage/K4V55HQ6/1806.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Workshop on Artificial Intelligence Safety 2019","","","","","","","","","","","","","","",""
"TNE49C8Q","conferencePaper","2018","Ibarz, Borja; Leike, Jan; Pohlen, Tobias; Irving, Geoffrey; Legg, Shane; Amodei, Dario","Reward learning from human preferences and demonstrations in Atari","arXiv:1811.06521 [cs, stat]","","","","http://arxiv.org/abs/1811.06521","To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.","2018-11-15","2022-01-30 04:52:47","2022-01-30 04:52:47","2019-12-16 20:36:27","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000114  arXiv: 1811.06521","","/Users/jacquesthibodeau/Zotero/storage/27WAEWKV/Ibarz et al. - 2018 - Reward learning from human preferences and demonst.pdf; /Users/jacquesthibodeau/Zotero/storage/QEKJ65EV/Ibarz et al. - 2018 - Reward learning from human preferences and demonst.pdf; /Users/jacquesthibodeau/Zotero/storage/J8N3GNK5/1811.html; /Users/jacquesthibodeau/Zotero/storage/3ZUXGVRD/1811.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NIPS 2018","","","","","","","","","","","","","","",""
"U4PJNZAQ","conferencePaper","2020","Gleave, Adam; Dennis, Michael; Legg, Shane; Russell, Stuart; Leike, Jan","Quantifying Differences in Reward Functions","","","","","http://arxiv.org/abs/2006.13900","For many tasks, the reward function is too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by examining rollouts from a policy optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences, and the reinforcement learning algorithm failing to optimize the learned reward. Moreover, the rollout method is highly sensitive to details of the environment the learned reward is evaluated in, which often differ in the deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without training a policy. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be precisely approximated and is more robust than baselines to the choice of visitation distribution. Finally, we find that the EPIC distance of learned reward functions to the ground-truth reward is predictive of the success of training a policy, even in different transition dynamics.","2020-06-24","2022-01-30 04:52:47","2022-01-30 04:52:47","2020-08-31 17:51:23","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000007  arXiv: 2006.13900","","/Users/jacquesthibodeau/Zotero/storage/T4E3SZDW/Gleave et al. - 2020 - Quantifying Differences in Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/XNFZG565/2006.html","","CHAI; TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2021","","","","","","","","","","","","","","",""
"9UHP8WTZ","blogPost","2021","Everitt, Tom; Carey, Ryan; Hammond, Lewis; Fox, James; Langlois, Eric; Legg, Shane","Progress on Causal Influence Diagrams","Medium","","","","https://deepmindsafetyresearch.medium.com/progress-on-causal-influence-diagrams-a7a32180b0d1","By Tom Everitt, Ryan Carey, Lewis Hammond, James Fox, Eric Langlois, and Shane Legg","2021-08-11","2022-01-30 04:52:47","2022-01-30 04:52:47","2021-11-14 19:06:48","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/GIGW9EBD/progress-on-causal-influence-diagrams-a7a32180b0d1.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5AW256G4","conferencePaper","2020","Armstrong, Stuart; Leike, Jan; Orseau, Laurent; Legg, Shane","Pitfalls of learning a reward function online","arXiv:2004.13654 [cs]","","","","http://arxiv.org/abs/2004.13654","In some agent designs like inverse reinforcement learning an agent needs to learn its own reward function. Learning the reward function and optimising for it are typically two different processes, usually performed at different stages. We consider a continual (``one life'') learning approach where the agent both learns the reward function and optimises for it at the same time. We show that this comes with a number of pitfalls, such as deliberately manipulating the learning process in one direction, refusing to learn, ``learning'' facts already known to the agent, and making decisions that are strictly dominated (for all relevant reward functions). We formally introduce two desirable properties: the first is `unriggability', which prevents the agent from steering the learning process in the direction of a reward function that is easier to optimise. The second is `uninfluenceability', whereby the reward-function learning process operates by learning facts about the environment. We show that an uninfluenceable process is automatically unriggable, and if the set of possible environments is sufficiently rich, the converse is true too.","2020-04-28","2022-01-30 04:52:47","2022-01-30 04:52:47","2020-08-18 21:24:08","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000005  arXiv: 2004.13654","","/Users/jacquesthibodeau/Zotero/storage/9SFICWU9/Armstrong et al. - 2020 - Pitfalls of learning a reward function online.pdf; /Users/jacquesthibodeau/Zotero/storage/QNDBJEGQ/2004.html","","TechSafety; FHI; DeepMind","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCAI 2020","","","","","","","","","","","","","","",""
"8CVV7M9V","conferencePaper","2020","Cohen, Michael K.; Hutter, Marcus","Pessimism About Unknown Unknowns Inspires Conservatism","Proceedings of Machine Learning Research","","","","http://arxiv.org/abs/2006.08753","If we could define the set of all bad outcomes, we could hard-code an agent which avoids them; however, in sufficiently complex environments, this is infeasible. We do not know of any general-purpose approaches in the literature to avoiding novel failure modes. Motivated by this, we define an idealized Bayesian reinforcement learner which follows a policy that maximizes the worst-case expected reward over a set of world-models. We call this agent pessimistic, since it optimizes assuming the worst case. A scalar parameter tunes the agent's pessimism by changing the size of the set of world-models taken into account. Our first main contribution is: given an assumption about the agent's model class, a sufficiently pessimistic agent does not cause ""unprecedented events"" with probability $1-\delta$, whether or not designers know how to precisely specify those precedents they are concerned with. Since pessimism discourages exploration, at each timestep, the agent may defer to a mentor, who may be a human or some known-safe policy we would like to improve. Our other main contribution is that the agent's policy's value approaches at least that of the mentor, while the probability of deferring to the mentor goes to 0. In high-stakes environments, we might like advanced artificial agents to pursue goals cautiously, which is a non-trivial problem even if the agent were allowed arbitrary computing power; we present a formal solution.","2020-06-15","2022-01-30 04:52:47","2022-01-30 04:52:47","2021-11-19 00:01:59","","","","125","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 2006.08753","","/Users/jacquesthibodeau/Zotero/storage/MWFB3VPD/Cohen and Hutter - 2020 - Pessimism About Unknown Unknowns Inspires Conserva.pdf; /Users/jacquesthibodeau/Zotero/storage/2NGV2RMZ/2006.html; /Users/jacquesthibodeau/Zotero/storage/JEN2E66Z/2006.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0, I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","33rd Annual Conference on Learning Theory","","","","","","","","","","","","","","",""
"R8QM5T9F","conferencePaper","2019","Gowal, Sven; Dvijotham, Krishnamurthy; Stanforth, Robert; Bunel, Rudy; Qin, Chongli; Uesato, Jonathan; Arandjelovic, Relja; Mann, Timothy; Kohli, Pushmeet","On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models","arXiv:1810.12715 [cs, stat]","","","","http://arxiv.org/abs/1810.12715","Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difﬁcult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in veriﬁed accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be veriﬁed beyond vacuous bounds on a downscaled version of IMAGENET.","2019-08-29","2022-01-30 04:52:39","2022-01-30 04:52:39","2019-12-16 20:36:43","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: NoCitationData[s7]  ACC: 221  J: 93 arXiv: 1810.12715","","/Users/jacquesthibodeau/Zotero/storage/D227S2QI/Gowal et al. - 2019 - On the Effectiveness of Interval Bound Propagation.pdf","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS SECML 2018 Workshop","","","","","","","","","","","","","","",""
"UKXRJHU4","conferencePaper","2018","Farajtabar, Mehrdad; Chow, Yinlam; Ghavamzadeh, Mohammad","More Robust Doubly Robust Off-policy Evaluation","Proceedings of the 35th International Conference on Machine Learning","","","","http://arxiv.org/abs/1802.03493","We study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge impact on the overall performance of DR, most of the work on using the DR estimators in OPE has been focused on improving the IS part, and not much on how to learn the model. In this paper, we propose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model parameter by minimizing the variance of the DR estimator. We first present a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t.~the model parameters can be estimated from the samples, and propose methods to efficiently minimize the variance. We prove that the MRDR estimators are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR in bandits and RL benchmark problems, and compare its performance with the existing methods.","2018-05-23","2022-01-30 04:52:39","2022-01-30 04:52:39","2019-12-16 20:35:28","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000144  arXiv: 1802.03493","","/Users/jacquesthibodeau/Zotero/storage/ISNRPPXX/Farajtabar et al. - 2018 - More Robust Doubly Robust Off-policy Evaluation.pdf; /Users/jacquesthibodeau/Zotero/storage/SA8MNFUK/1802.html","","TechSafety; DeepMind","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","35th International Conference on Machine Learning","","","","","","","","","","","","","","",""
"S6AXJC5M","conferencePaper","2018","Rabinowitz, Neil C.; Perbet, Frank; Song, H. Francis; Zhang, Chiyuan; Eslami, S. M. Ali; Botvinick, Matthew","Machine Theory of Mind","Proceedings of the 35th International Conference on Machine Learning","","","","http://arxiv.org/abs/1802.07740","Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the ""Sally-Anne"" test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.","2018-03-12","2022-01-30 04:52:39","2022-01-30 04:52:39","2020-11-14 00:44:27","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000247  arXiv: 1802.07740","","/Users/jacquesthibodeau/Zotero/storage/JVMJRV27/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf; /Users/jacquesthibodeau/Zotero/storage/DXH576KA/1802.html","","TechSafety; DeepMind","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MMG5JK2G","conferencePaper","2019","Ren, Jie; Liu, Peter J.; Fertig, Emily; Snoek, Jasper; Poplin, Ryan; DePristo, Mark A.; Dillon, Joshua V.; Lakshminarayanan, Balaji","Likelihood Ratios for Out-of-Distribution Detection","arXiv:1906.02845 [cs, stat]","","","","http://arxiv.org/abs/1906.02845","Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.","2019-12-05","2022-01-30 04:52:39","2022-01-30 04:52:39","2019-12-16 20:31:45","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000250  arXiv: 1906.02845","","/Users/jacquesthibodeau/Zotero/storage/Q6QQT5EF/Ren et al. - 2019 - Likelihood Ratios for Out-of-Distribution Detectio.pdf; /Users/jacquesthibodeau/Zotero/storage/F56NEI6E/1906.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2019","","","","","","","","","","","","","","",""
"DUE99KRM","journalArticle","2014","Muehlhauser, Luke; Bostrom, Nick","Why we need friendly AI","Think","","","","","","2014","2022-01-30 04:53:45","2022-01-30 04:53:45","","41–47","","36","13","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000033","","/Users/jacquesthibodeau/Zotero/storage/328482VT/Muehlhauser and Bostrom - 2014 - Why we need friendly AI.pdf; /Users/jacquesthibodeau/Zotero/storage/ZGFS8MTM/3C576A0EE8DEFDE82FC809493B37A265.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5FZ7A6KK","report","2020","Calvin, Nathan; Leung, Jade","Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter.","","","","","https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-working-paper-Who-owns-AI-Apr2020.pdf","","2020-02","2022-01-30 04:53:45","2022-01-30 04:53:45","2020-09-05","23","","","","","","","","","","","Future of Humanity Institute","","","","","","","","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/ZPVF27E5/Calvin and Leung - 2020 - Who owns artificial intelligence A preliminary an.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FB7STED6","bookSection","2009","Bostrom, Nick","Why I Want to be a Posthuman when I Grow Up","Medical Enhancement and Posthumanity","978-1-4020-8852-0","","","https://doi.org/10.1007/978-1-4020-8852-0_8","Extreme human enhancement could result in “posthuman” modes of being. After offering some definitions and conceptual clarification, I argue for two theses. First, some posthuman modes of being would be very worthwhile. Second, it could be very good for human beings to become posthuman.","2009","2022-01-30 04:53:45","2022-01-30 04:53:45","2020-08-18 20:26:22","107-136","","","","","","","The International Library of Ethics, Law and Technology","","","","Springer Netherlands","Dordrecht","en","","","","","Springer Link","","ZSCC: NoCitationData[s3]  ACC: 424  DOI: 10.1007/978-1-4020-8852-0_8","","/Users/jacquesthibodeau/Zotero/storage/QFPJSKVN/Bostrom - 2009 - Why I Want to be a Posthuman when I Grow Up.pdf","","MetaSafety; FHI","Cognitive Capacity; Cognitive Improvement; Inclusive Fitness; Moral Status; Personal Identity","Gordijn, Bert; Chadwick, Ruth","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9RTZ9S2A","journalArticle","2014","Armstrong, Stuart; ÓhÉigeartaigh, Seán","Who knows anything about anything about AI?","Intelligence Unbound: The Future of Uploaded and Machine Minds","","","","","","2014","2022-01-30 04:53:45","2022-01-30 04:53:45","","46–60","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/7M3UKW76/9781118736302.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZGNI9PR6","conferencePaper","2019","Kovařík, Vojtěch; Carey, Ryan","(When) Is Truth-telling Favored in AI Debate?","Proceedings of the Workshop on Artificial Intelligence Safety","","","","http://arxiv.org/abs/1911.04266","For some problems, humans may not be able to accurately judge the goodness of AI-proposed solutions. Irving et al. (2018) propose that in such cases, we may use a debate between two AI systems to amplify the problem-solving capabilities of a human judge. We introduce a mathematical framework that can model debates of this type and propose that the quality of debate designs should be measured by the accuracy of the most persuasive answer. We describe a simple instance of the debate framework called feature debate and analyze the degree to which such debates track the truth. We argue that despite being very simple, feature debates nonetheless capture many aspects of practical debates such as the incentives to confuse the judge or stall to prevent losing. We then outline how these models should be generalized to analyze a wider range of debate phenomena.","2019-12-15","2022-01-30 04:53:45","2022-01-30 04:53:45","2020-11-14 00:41:27","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 1911.04266","","/Users/jacquesthibodeau/Zotero/storage/K36M3K3T/Kovařík and Carey - 2019 - (When) Is Truth-telling Favored in AI Debate.pdf; /Users/jacquesthibodeau/Zotero/storage/7QS85VGU/1911.html","","TechSafety; FHI","Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EGIF8T8S","journalArticle","2018","Currie, Adrian; Ó HÉigeartaigh, Seán; Apollo-University Of Cambridge Repository; Apollo-University Of Cambridge Repository","Working together to face humanity’s greatest threats: Introduction to The Future of Research on Catastrophic and Existential Risk.","Futures","","","10.17863/CAM.27560","https://www.repository.cam.ac.uk/handle/1810/280193","Ours is a resilient species. Around 70,000 years ago our total population may have fallen to between three and ten thousand individuals, possibly due to a supervolcanic eruption (Ambrose 1998) . Yet our ancestors survived, squeezed through the bottleneck, and flourished. But this resilience cannot be taken for granted. We are interconnected and interdependent as never before; the power and scale of our technological capacities are unprecedented. We are in uncharted waters and thus our previous survival is no longer a reason to expect our continued survival (Bostrom 2013). As a result, it is urgent that we develop a systematic understanding of the nature and causes of catastrophic and existential risks.","2018-09-11","2022-01-30 04:53:38","2022-01-30 04:53:38","2020-12-13 22:25:19","","","","","","","Working together to face humanity’s greatest threats","","","","","","","","","","","","DOI.org (Datacite)","","ZSCC: NoCitationData[s1]  ACC: 10  Publisher: Apollo - University of Cambridge Repository","","/Users/jacquesthibodeau/Zotero/storage/I5QMVCS8/Currie and Ó HÉigeartaigh - 2018 - Working together to face humanity’s greatest threa.pdf","","MetaSafety; CSER; FLI","","","","","Apollo-University Of Cambridge Repository; Apollo-University Of Cambridge Repository","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CECMW3NM","blogPost","2020","Aguirre, Anthony","Why those who care about catastrophic and existential risk should care about autonomous weapons","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/oR9tLNRSAep293rr5/why-those-who-care-about-catastrophic-and-existential-risk-2","(crossposted to Lesswrong here.) Although I have not seen the argument made in any detail or in writing, I and the Future of Life Institute (FLI) have gathered the strong impression that parts of the effective altruism ecosystem are skeptical of the importance of the issue of autonomous weapons systems. This post explains why we think those interested in avoiding catastrophic and existential risk, especially risk stemming from emerging technologies, may want to have this issue higher on their list of concerns. We will first define some terminology and do some disambiguation, as there are many classes of autonomous weapons that are often conflated; all classes have some issues of concern, but some are much more problematic than others. We then detail three basic motivations for research, advocacy, coordination, and policymaking around the issue:  1. Governance of autonomous weapon systems is a dry-run, and precedent, for     governance of AGI. In the short term, AI-enabled weapons systems will share     many of the technical weaknesses and shortcomings of other AI systems, but     like general AI also raise safety concerns that are likely to increase      rather than decrease with capability advances. The stakes are intrinsically     high (literally life-or-death), and the context is an inevitably adversarial     one involving states and major corporations. The sort of global coordination     amongst potentially adversarial parties that will be required for governance     of transformative/general AI systems will not arise from nowhere, and     autonomous weapons offer an invaluable precedent and arena in which to build     experience, capability, and best practices.  2. Some classes of lethal autonomous weapon systems constitute scalable weapons     of mass destruction (which may also have a much lower threshold for first     use or accidental escalation), and hence a nascent catastrophic risk.  3. By increasing the probability of the initiation and/or escalation","2020-11-11","2022-01-30 04:53:38","2022-01-30 04:53:38","2020-12-19 04:18:42","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/DGBAP7KK/why-those-who-care-about-catastrophic-and-existential-risk.html; /Users/jacquesthibodeau/Zotero/storage/K2N72EX7/why-those-who-care-about-catastrophic-and-existential-risk-2.html","","MetaSafety; AmbiguosSafety; FLI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NWKNXDG7","conferencePaper","2016","Asaro, Peter M","The Liability Problem for Autonomous Artificial Agents","","","","","","This paper describes and frames a central ethical issue–the liability problem–facing the regulation of artificial computational agents, including artificial intelligence (AI) and robotic systems, as they become increasingly autonomous, and supersede current capabilities. While it frames the issue in legal terms of liability and culpability, these terms are deeply imbued and interconnected with their ethical and moral correlate– responsibility. In order for society to benefit from advances in AI technology, it will be necessary to develop regulatory policies which manage the risk and liability of deploying systems with increasingly autonomous capabilities. However, current approaches to liability have difficulties when it comes to dealing with autonomous artificial agents because their behavior may be unpredictable to those who create and deploy them, and they will not be proper legal or moral agents. This problem is the motivation for a research project that will explore the fundamental concepts of autonomy, agency and liability; clarify the different varieties of agency that artificial systems might realize, including causal, legal and moral; and the illuminate the relationships between these. The paper will frame the problem of liability in autonomous agents, sketch out its relation to fundamental concepts in human legal and moral agency– including autonomy, agency, causation, intention, responsibility and culpability–and their applicability or inapplicability to autonomous artificial agents.","2016","2022-01-30 04:53:38","2022-01-30 04:53:38","","5","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000047","","/Users/jacquesthibodeau/Zotero/storage/HB4XG66E/Asaro - The Liability Problem for Autonomous Artificial Ag.pdf","","MetaSafety; AmbiguosSafety; FLI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI Spring Symposia","","","","","","","","","","","","","","",""
"G86SMTN9","blogPost","2019","Krakovna, Victoria","ICLR Safe ML Workshop Report","Future of Life Institute","","","","https://futureoflife.org/2019/06/18/iclr-safe-ml-workshop-report/","Victoria Krakovna co-organized the 2019 ICLR Safe ML workshop. One of the main goals was to bring together near and long term safety research communities.","2019-06-18","2022-01-30 04:53:38","2022-01-30 04:53:38","2020-12-14 23:28:16","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/X9EEWW4D/iclr-safe-ml-workshop-report.html","","TechSafety; DeepMind; FLI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GD4JV3IM","manuscript","2016","Dewey, Daniel; Russell, Stuart J; Tegmark, Max","A survey of research questions for robust and beneficial AI","","","","","https://futureoflife.org/data/documents/research_survey.pdf?x96845","","2016-01-25","2022-01-30 04:53:38","2022-01-30 04:53:38","2020-11-21 17:06:15","","","","","","","","","","","","","Future of Life Institute","","","","","","","","ZSCC: 0000002[s0]","","/Users/jacquesthibodeau/Zotero/storage/ZZC972RC/research_survey.pdf","","TechSafety; FLI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQ2PGVQJ","blogPost","2015","Sandberg, Anders","We, Borg: Speculations on hive minds as a posthuman state","aleph.se","","","","","","2015","2022-01-30 04:53:37","2022-01-30 04:53:37","","","","","","","","We, Borg","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: 7","","","","MetaSafety; FHI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4VTKAFVS","manuscript","2021","Evans, Owain; Cotton-Barratt, Owen; Finnveden, Lukas; Bales, Adam; Balwit, Avital; Wills, Peter; Righetti, Luca; Saunders, William","Truthful AI: Developing and governing AI that does not lie","","","","","http://arxiv.org/abs/2110.06674","In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.","2021-10-13","2022-01-30 04:53:37","2022-01-30 04:53:37","2021-11-18 23:51:54","","","","","","","Truthful AI","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2110.06674","","/Users/jacquesthibodeau/Zotero/storage/N89FGRG3/Evans et al. - 2021 - Truthful AI Developing and governing AI that does.pdf; /Users/jacquesthibodeau/Zotero/storage/RGF8PQC9/2110.html","","TechSafety","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2.0; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FSXDIQ42","blogPost","2017","Bostrom, Nick","Transhumanist FAQ 3.0","Humanity +","","","","https://humanityplus.org/philosophy/transhumanist-faq/","","2017","2022-01-30 04:53:37","2022-01-30 04:53:37","","","","","","","","","","","","","","","","","","","","","","ZSCC: 0000013","","","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7DD2HTK9","journalArticle","2012","Armstrong, Stuart; Sandberg, Anders; Bostrom, Nick","Thinking Inside the Box: Controlling and Using an Oracle AI","Minds and Machines","","0924-6495, 1572-8641","10.1007/s11023-012-9282-2","http://link.springer.com/10.1007/s11023-012-9282-2","","2012-11","2022-01-30 04:53:37","2022-01-30 04:53:37","2020-11-22 05:25:29","299-324","","4","22","","Minds & Machines","Thinking Inside the Box","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000108","","","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J5FVJG45","journalArticle","2019","Sandberg, Anders","There is plenty of time at the bottom: the economics, risk and ethics of time compression","foresight","","","","","","2019","2022-01-30 04:53:37","2022-01-30 04:53:37","","84–99","","1","21","","","There is plenty of time at the bottom","","","","","","","","","","","","Google Scholar","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/JJHQ248X/Sandberg - 2019 - There is plenty of time at the bottom the economi.pdf; /Users/jacquesthibodeau/Zotero/storage/IFG9FJ5G/html.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BW7893Q6","bookSection","2017","Bostrom, Nick; Sandberg, Anders","The Wisdom of Nature: An Evolutionary Heuristic for Human Enhancement","Philosophical Issues in Pharmaceutics","","","","","","2017","2022-01-30 04:53:37","2022-01-30 04:53:37","","189–219","","","","","","The Wisdom of Nature","","","","","Springer","","","","","","","Google Scholar","","ZSCC: 0000006","","/Users/jacquesthibodeau/Zotero/storage/K3M5RUCT/978-94-024-0979-6_12.html; /Users/jacquesthibodeau/Zotero/storage/IJ8IXREN/978-94-024-0979-6_12.html; /Users/jacquesthibodeau/Zotero/storage/BPVJK8JS/BOSTWO.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4RIK3CZK","journalArticle","2018","Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain","When Will AI Exceed Human Performance? Evidence from AI Experts","Journal of Artificial Intelligence Research","","","","http://arxiv.org/abs/1705.08807","Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.","2018","2022-01-30 04:53:37","2022-01-30 04:53:37","2019-12-16 02:29:10","729–754","","","62","","","When Will AI Exceed Human Performance?","","","","","","","","","","","","arXiv.org","","ZSCC: 0000539  arXiv: 1705.08807","","/Users/jacquesthibodeau/Zotero/storage/Q4VEH3BH/Grace et al. - 2018 - When Will AI Exceed Human Performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/J24W79SV/1705.html; /Users/jacquesthibodeau/Zotero/storage/TTDMTXPR/1705.html; /Users/jacquesthibodeau/Zotero/storage/A6EQJ4JQ/Grace et al. - 2018 - When will AI exceed human performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/227ZHEW8/11222.html; /Users/jacquesthibodeau/Zotero/storage/INUH2ER3/11222.html; /Users/jacquesthibodeau/Zotero/storage/94W3P62Z/11222.html","","MetaSafety; FHI; AI-Impacts-NotFeatured","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4NDFHXVI","report","2010","Armstrong, Stuart","Utility Indifference","","","","","","Consider an AI that follows its own motivations. We’re not entirely sure what its motivations are, but we would prefer that the AI cooperate with humanity; or, failing that, that we can destroy it before it defects. We’ll have someone sitting in a room, their finger on a detonator, ready at the slightest hint of defection. Unfortunately as has been noted ([3], [1]), this does not preclude the AI from misbehaving. It just means that the AI must act to take control of the explosives, the detonators or the human who will press the button. For a superlatively intelligence AI, this would represent merely a slight extra difficulty. But now imagine that the AI was somehow indifferent to the explosives going off or not (but that nothing else was changed). Then if ever the AI does decide to defect, it will most likely do so without taking control of the explosives, as that would be easier than otherwise. By “easier ” we mean that the chances of failure are less, since the plan is simpler – recall that under these assumptions, the AI counts getting blown up as an equal value to successfully defecting.","2010","2022-01-30 04:53:37","2022-01-30 04:53:37","","","","","","","","","","","","","Future of Humanity Institute","","","","","","","CiteSeer","","ZSCC: 0000025","","/Users/jacquesthibodeau/Zotero/storage/N5CJR7JC/2010-1_body.pdf; /Users/jacquesthibodeau/Zotero/storage/VAX4IF4Z/Armstrong - 2010 - Utility Indifference.pdf; /Users/jacquesthibodeau/Zotero/storage/JDJ3ES5A/summary.html; /Users/jacquesthibodeau/Zotero/storage/XNW3UESI/summary.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IR2NVS39","journalArticle","2014","Beckstead, Nick; Bostrom, N.; Bowerman, N.; Cotton-Barratt, O.; MacAskill, W.; Eigeartaigh, S.; Ord, T.","Unprecedented technological risks","Policy brief. Available online: http://www. fhi. ox. ac. uk/wpcontent/uploads/Unprecedented-Technological-Risks. pdf. Last Accessed September","","","","","","2014","2022-01-30 04:53:37","2022-01-30 04:53:37","","2015","","","29","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000004","","/Users/jacquesthibodeau/Zotero/storage/VA6NV95K/Beckstead et al. - 2014 - Unprecedented technological risks.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JKWXTQPB","conferencePaper","2018","Saunders, William; Sastry, Girish; Stuhlmueller, Andreas; Evans, Owain","Trial without Error: Towards Safe Reinforcement Learning via Human Intervention","Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems","","","","https://arxiv.org/abs/1707.05173v1","AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human ""in the loop"" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe.","2018","2022-01-30 04:53:37","2022-01-30 04:53:37","2019-12-19 01:45:01","2067–2069","","","","","","Trial without Error","","","","","International Foundation for Autonomous Agents and Multiagent Systems","","en","","","","","arxiv.org","","ZSCC: NoCitationData[s1]  ACC: 142","","/Users/jacquesthibodeau/Zotero/storage/I2WTXHHW/Saunders et al. - 2018 - Trial without error Towards safe reinforcement le.pdf; /Users/jacquesthibodeau/Zotero/storage/NW6PQECP/citation.html; /Users/jacquesthibodeau/Zotero/storage/AV7ZKCMK/1707.html","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"674VB36I","journalArticle","2014","Sandberg, Anders","Transhumanism and the Meaning of Life","Religion and Transhumanism: The Unknown Future of Human Enhancement","","","","","","2014","2022-01-30 04:53:37","2022-01-30 04:53:37","","8","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000027","","/Users/jacquesthibodeau/Zotero/storage/WJZMQ83Q/Sandberg - 2014 - Transhumanism and the Meaning of Life.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJI4USKA","conferencePaper","2016","Armstrong, Stuart; Leike, Jan","Towards interactive inverse reinforcement learning","NIPS Workshop","","","","","","2016","2022-01-30 04:53:37","2022-01-30 04:53:37","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000004","","","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GU5E8FKZ","manuscript","2020","Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew; Bluemke, Emma; Lebensold, Jonathan; O'Keefe, Cullen; Koren, Mark; Ryffel, Théo; Rubinovitz, J. B.; Besiroglu, Tamay; Carugati, Federica; Clark, Jack; Eckersley, Peter; de Haas, Sarah; Johnson, Maritza; Laurie, Ben; Ingerman, Alex; Krawczuk, Igor; Askell, Amanda; Cammarota, Rosario; Lohn, Andrew; Krueger, David; Stix, Charlotte; Henderson, Peter; Graham, Logan; Prunkl, Carina; Martin, Bianca; Seger, Elizabeth; Zilberman, Noa; hÉigeartaigh, Seán Ó; Kroeger, Frens; Sastry, Girish; Kagan, Rebecca; Weller, Adrian; Tse, Brian; Barnes, Elizabeth; Dafoe, Allan; Scharre, Paul; Herbert-Voss, Ariel; Rasser, Martijn; Sodhani, Shagun; Flynn, Carrick; Gilbert, Thomas Krendl; Dyer, Lisa; Khan, Saif; Bengio, Yoshua; Anderljung, Markus","Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims","","","","","http://arxiv.org/abs/2004.07213","With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.","2020-04-20","2022-01-30 04:53:37","2022-01-30 04:53:37","2020-08-18 21:36:21","","","","","","","Toward Trustworthy AI Development","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 92  arXiv: 2004.07213","","/Users/jacquesthibodeau/Zotero/storage/7BPKPSMD/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf; /Users/jacquesthibodeau/Zotero/storage/U6NP4HXR/2004.html; /Users/jacquesthibodeau/Zotero/storage/GN6MW3NT/2004.html","","MetaSafety; CHAI; CFI; CSER; CSET; FHI; Open-AI","Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8H65GSA9","conferencePaper","2020","O'Keefe, Cullen; Cihon, Peter; Garfinkel, Ben; Flynn, Carrick; Leung, Jade; Dafoe, Allan","The Windfall Clause: Distributing the Benefits of AI for the Common Good","Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","978-1-4503-7110-0","","10.1145/3375627.3375842","https://dl.acm.org/doi/10.1145/3375627.3375842","","2020-02-07","2022-01-30 04:53:37","2022-01-30 04:53:37","2020-08-18 21:31:11","327-331","","","","","","The Windfall Clause","","","","","ACM","New York NY USA","en","","","","","DOI.org (Crossref)","","ZSCC: 0000011","","/Users/jacquesthibodeau/Zotero/storage/GN5W52XA/O'Keefe et al. - 2020 - The Windfall Clause Distributing the Benefits of .pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AIES '20: AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"4UAU4M26","journalArticle","2018","Bostrom, Nick","The vulnerable world hypothesis","Global Policy","","","","","","2018","2022-01-30 04:53:37","2022-01-30 04:53:37","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: NoCitationData[s5]  ACC: 74","","/Users/jacquesthibodeau/Zotero/storage/TWVPACKI/1758-5899.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"678H8WWB","book","2020","Ord, Toby","The Precipice: Existential Risk and the Future of Humanity","","978-0-316-48491-6","","","","This urgent and eye-opening book makes the case that protecting humanity's future is the central challenge of our time.   If all goes well, human history is just beginning. Our species could survive for billions of years - enough time to end disease, poverty, and injustice, and to flourish in ways unimaginable today. But this vast future is at risk. With the advent of nuclear weapons, humanity entered a new age, where we face existential catastrophes - those from which we could never come back. Since then, these dangers have only multiplied, from climate change to engineered pathogens and artificial intelligence. If we do not act fast to reach a place of safety, it will soon be too late.  Drawing on over a decade of research, The Precipice explores the cutting-edge science behind the risks we face. It puts them in the context of the greater story of humanity: showing how ending these risks is among the most pressing moral issues of our time. And it points the way forward, to the actions and strategies that can safeguard humanity.  An Oxford philosopher committed to putting ideas into action, Toby Ord has advised the US National Intelligence Council, the UK Prime Minister's Office, and the World Bank on the biggest questions facing humanity. In The Precipice, he offers a startling reassessment of human history, the future we are failing to protect, and the steps we must take to ensure that our generation is not the last.","2020-03-24","2022-01-30 04:53:36","2022-01-30 04:53:36","","","480","","","","","The Precipice","","","","","Hachette Books","New York","English","","","","","Amazon","","ZSCC: 0000169","","","https://www.amazon.com/Precipice-Existential-Risk-Future-Humanity/dp/0316484911","MetaSafety; FHI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","Illustrated Edition","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GEFVXESX","conferencePaper","2020","Shevlane, Toby; Dafoe, Allan","The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","","","","http://arxiv.org/abs/2001.00463","There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.","2020-01-09","2022-01-30 04:53:36","2022-01-30 04:53:36","2020-11-14 00:34:29","","","","","","","The Offense-Defense Balance of Scientific Knowledge","","","","","","","","","","","","arXiv.org","","ZSCC: 0000006  arXiv: 2001.00463","","/Users/jacquesthibodeau/Zotero/storage/GXWDDN35/Shevlane and Dafoe - 2020 - The Offense-Defense Balance of Scientific Knowledg.pdf; /Users/jacquesthibodeau/Zotero/storage/JMJHDTSG/2001.html","","MetaSafety; FHI","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"48C7KN4A","journalArticle","2021","Ding, Jeffrey; Dafoe, Allan","The Logic of Strategic Assets: From Oil to AI","Security Studies","","","10.1080/09636412.2021.1915583","https://arxiv.org/abs/2001.03246","What resources and technologies are strategic? This question is often the focus of policy and theoretical debates, where the label “strategic” designates those assets that warrant the attention of the highest levels of the state. But these conversations are plagued by analytical confusion, flawed heuristics, and the rhetorical use of “strategic” to advance particular agendas. We aim to improve these conversations through conceptual clarification, introducing a theory based on important rivalrous externalities for which socially optimal behavior will not be produced alone by markets or individual national security entities. We distill and theorize the most important three forms of these externalities, which involve cumulative-, infrastructure-, and dependency-strategic logics. We then employ these logics to clarify three important cases: the Avon 2 engine in the 1950s, the U.S.-Japan technology rivalry in the late 1980s, and contemporary conversations about artificial intelligence.","2021-06-03","2022-01-30 04:53:36","2022-01-30 04:53:36","","182-212","","2","30","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000005","","/Users/jacquesthibodeau/Zotero/storage/ZSBT2TB2/Ding and Dafoe - The Logic of Strategic Assets From Oil to AI.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HVZV6X2T","journalArticle","2014","Sandberg, Anders","The five biggest threats to human existence","The Conversation, May","","","","","","2014","2022-01-30 04:53:36","2022-01-30 04:53:36","","","","","29","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000014","","","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88FHGR2S","bookSection","2014","Bostrom, Nick; Yudkowsky, Eliezer","The ethics of artificial intelligence","The Cambridge Handbook of Artificial Intelligence","978-1-139-04685-5","","","https://www.cambridge.org/core/product/identifier/CBO9781139046855A027/type/book_part","The possibility of creating thinking machines raises a host of ethical issues. These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves. The first section discusses issues that may arise in the near future of AI. The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence. The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status. In the fourth section, we consider how AIs might diﬀer from humans in certain basic respects relevant to our ethical assessment of them. The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill.","2014","2022-01-30 04:53:36","2022-01-30 04:53:36","2019-12-19 02:58:26","316-334","","","","","","","","","","","Cambridge University Press","Cambridge","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s6]  ACC: 815  J: 453  DOI: 10.1017/CBO9781139046855.020","","/Users/jacquesthibodeau/Zotero/storage/P58U9XZE/Bostrom and Yudkowsky - 2014 - The ethics of artificial intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/XUQBWCQD/books.html","","TechSafety; FHI; MIRI","","Frankish, Keith; Ramsey, William M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9G5EAA7N","journalArticle","2020","Scholl, Keller; Hanson, Robin","Testing the Automation Revolution Hypothesis","Economics Letters","","","https://doi.org/10.1016/j.econlet.2020.109287","https://www.sciencedirect.com/science/article/abs/pii/S0165176520301919","Recently, many have predicted an imminent automation revolution, and large resulting job losses. Others have created metrics to predict new patterns in job automation vulnerability. As context to such claims, we test basic theory, two vulnerability metrics, and 251 O*NET job features as predictors of 1505 expert reports regarding automation levels in 832 U.S. job types from 1999 to 2019. We find that pay, employment, and vulnerability metrics are predictive (R^2~0.15), but add little to the top 25 O*NET job features, which together predict far better (R^2~0.55). These best predictors seem understandable in terms of traditional kinds of automation, and have not changed over our time period. Instead, it seems that jobs have changed their features to become more suitable for automation. We thus find no evidence yet of a revolution in the patterns or quantity of automation. And since, over this period, automation increases have predicted neither changes in pay nor employment, this suggests that workers have little to fear if such a revolution does come.","2020-08","2022-01-30 04:53:36","2022-01-30 04:53:36","2020-12-19 05:22:30","","","","193","","","","","","","","","","en","","","","","papers.ssrn.com","","ZSCC: 0000002  DOI: 10.2139/ssrn.3496364","","/Users/jacquesthibodeau/Zotero/storage/5K2P6H4X/papers.html","","MetaSafety; FHI","artificial intelligence; automation; employment; occupations; technology; wages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGPQFA45","journalArticle","2016","Bostrom, Nick; Douglas, Thomas; Sandberg, Anders","The Unilateralist’s Curse and the Case for a Principle of Conformity","Social Epistemology","","0269-1728","10.1080/02691728.2015.1108373","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/","In some situations a number of agents each have the ability to undertake an initiative that would have significant effects on the others. Suppose that each of these agents is purely motivated by an altruistic concern for the common good. We show that if each agent acts on her own personal judgment as to whether the initiative should be undertaken, then the initiative will be undertaken more often than is optimal. We suggest that this phenomenon, which we call the unilateralist’s curse, arises in many contexts, including some that are important for public policy. To lift the curse, we propose a principle of conformity, which would discourage unilateralist action. We consider three different models for how this principle could be implemented, and respond to an objection that could be raised against it.","2016-07-03","2022-01-30 04:53:36","2022-01-30 04:53:36","2019-12-19 01:40:23","350-371","","4","30","","Soc Epistemol","","","","","","","","","","","","","PubMed Central","","ZSCC: NoCitationData[s4]  ACC: 32  PMID: 27499570 PMCID: PMC4959137","","/Users/jacquesthibodeau/Zotero/storage/D58DKCPS/02691728.2015.html; /Users/jacquesthibodeau/Zotero/storage/6J9H3WV5/02691728.2015.html; /Users/jacquesthibodeau/Zotero/storage/FGT8BKJU/Bostrom et al. - 2016 - The Unilateralist’s Curse and the Case for a Princ.pdf; ; /Users/jacquesthibodeau/Zotero/storage/A7EZBQIA/02691728.2015.html; /Users/jacquesthibodeau/Zotero/storage/I6NRCKZ9/02691728.2015.html","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CUZWTKPN","report","2017","Armstrong, Stuart; Weick, Mario; Sandberg, Anders; Snyder-Beattie, Andrew; Beckstead, Nick","The underwriter and the models-solo dances or pas-de-deux? What policy data can tell us about how underwriters use models","","","","","https://www.msamlin.com/content/dam/ms-amlin/corporate/our-world/Whitepapers/MS%20Amlin%20White%20Paper%20The%20underwriter%20and%20the%20models-%20solo%20dances%20or%20pas-de-deux.pdf.downloadasset.pdf","","2017","2022-01-30 04:53:36","2022-01-30 04:53:36","2020-12-19","","","","","","","The underwriter and the models-solo dances or pas-de-deux?","","","","","MS Amlin","","","","","","","Google Scholar","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/UP7CQVAJ/64873.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S9IMQ6PA","book","2016","Yampolskiy, Roman; Armstrong, Stuart","The Technological Singularity: Managing the Journey","","","","","","","2016","2022-01-30 04:53:36","2022-01-30 04:53:36","","","","","","","","The Technological Singularity","","","","","Springer https://intelligence. org/files/TechnicalAgenda. pdf Retrieved","","","","","","","Google Scholar","","ZSCC: NoCitationData[s2]  ACC: 42","","","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B86CZMRI","book","2015","Miller, Jim; Yampolskiy, Roman; Armstrong, Stuart; Callaghan, Vic","The technological singularity","","","","","https://link.springer.com/book/10.1007%2F978-3-662-54033-6","","2015","2022-01-30 04:53:36","2022-01-30 04:53:36","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/JN6QKA2T/MILTTS-2.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IPDHKEBF","journalArticle","2012","Bostrom, Nick","The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents","Minds and Machines","","","","","","2012","2022-01-30 04:53:36","2022-01-30 04:53:36","","71–85","","2","22","","","The superintelligent will","","","","","","","","","","","","Google Scholar","","ZSCC: 0000268  Publisher: Springer","","/Users/jacquesthibodeau/Zotero/storage/F7ZW2HE4/Bostrom - 2012 - The superintelligent will Motivation and instrume.pdf; /Users/jacquesthibodeau/Zotero/storage/S6ZWXU2P/s11023-012-9281-3.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XSBVZ78E","report","2018","Brundage, Miles; Avin, Shahar; Clark, Jack; Toner, Helen; Eckersley, Peter; Garfinkel, Ben; Dafoe, Allan; Scharre, Paul; Zeitzoff, Thomas; Filar, Bobby; Anderson, Hyrum; Roff, Heather; Allen, Gregory C.; Steinhardt, Jacob; Flynn, Carrick; hÉigeartaigh, Seán Ó; Beard, Simon; Belfield, Haydn; Farquhar, Sebastian; Lyle, Clare; Crootof, Rebecca; Evans, Owain; Page, Michael; Bryson, Joanna; Yampolskiy, Roman; Amodei, Dario","The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation","","","","","http://arxiv.org/abs/1802.07228","This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.","2018-02-20","2022-01-30 04:53:36","2022-01-30 04:53:36","2019-12-16 20:09:19","","","","","","","The Malicious Use of Artificial Intelligence","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 461  J: 237 arXiv: 1802.07228","","/Users/jacquesthibodeau/Zotero/storage/CMFNTJ99/Brundage et al. - 2018 - The Malicious Use of Artificial Intelligence Fore.pdf; /Users/jacquesthibodeau/Zotero/storage/GPF8NDAK/1802.html; /Users/jacquesthibodeau/Zotero/storage/BMW2KNU2/1802.html; /Users/jacquesthibodeau/Zotero/storage/29EP7FN4/1802.html","","MetaSafety; CFI; CSER; FHI; Open-AI; BERI","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHBVXXPR","manuscript","2020","Carey, Ryan; Langlois, Eric; Everitt, Tom; Legg, Shane","The Incentives that Shape Behaviour","","","","","http://arxiv.org/abs/2001.07118","Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications.","2020-01-20","2022-01-30 04:53:36","2022-01-30 04:53:36","2020-08-18 21:24:59","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000009  arXiv: 2001.07118","","/Users/jacquesthibodeau/Zotero/storage/2IS3RDHQ/Carey et al. - 2020 - The Incentives that Shape Behaviour.pdf; /Users/jacquesthibodeau/Zotero/storage/59E7F9AD/2001.html","","TechSafety; FHI; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W6K7JDJ5","journalArticle","2014","Armstrong, Stuart; Sotala, Kaj; Ó hÉigeartaigh, Seán S.","The errors, insights and lessons of famous AI predictions – and what they mean for the future","Journal of Experimental & Theoretical Artificial Intelligence","","0952-813X, 1362-3079","10.1080/0952813X.2014.895105","https://www.tandfonline.com/doi/full/10.1080/0952813X.2014.895105","","2014-07-03","2022-01-30 04:53:36","2022-01-30 04:53:36","2020-11-22 02:22:03","317-342","","3","26","","Journal of Experimental & Theoretical Artificial Intelligence","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000076","","/Users/jacquesthibodeau/Zotero/storage/FGDN4MZQ/Armstrong et al. - 2014 - The errors, insights and lessons of famous AI pred.pdf","","MetaSafety; FHI; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WBKQEXZE","journalArticle","2016","Bostrom, Nick","The Control Problem. Excerpts from Superintelligence: Paths, Dangers, Strategies","Science Fiction and Philosophy: From Time Travel to Superintelligence","","","","","","2016","2022-01-30 04:53:36","2022-01-30 04:53:36","","308–330","","","","","","The Control Problem. Excerpts from Superintelligence","","","","","","","","","","","","Google Scholar","","ZSCC: NoCitationData[s4]  ACC: 2","","/Users/jacquesthibodeau/Zotero/storage/HV36EXA9/9781118922590.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHNFUZ5M","manuscript","2017","Sandberg, Anders; Armstrong, Stuart; Cirkovic, Milan M.","That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi's paradox","","","","","https://arxiv.org/abs/1705.03394v1","If a civilization wants to maximize computation it appears rational to aestivate until the far future in order to exploit the low temperature environment: this can produce a $10^{30}$ multiplier of achievable computation. We hence suggest the ""aestivation hypothesis"": the reason we are not observing manifestations of alien civilizations is that they are currently (mostly) inactive, patiently waiting for future cosmic eras. This paper analyzes the assumptions going into the hypothesis and how physical law and observational evidence constrain the motivations of aliens compatible with the hypothesis.","2017-04-27","2022-01-30 04:53:36","2022-01-30 04:53:36","2019-12-19 01:36:54","","","","","","","That is not dead which can eternal lie","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000025","","/Users/jacquesthibodeau/Zotero/storage/HIATFPH7/Sandberg et al. - 2017 - That is not dead which can eternal lie the aestiv.pdf; /Users/jacquesthibodeau/Zotero/storage/RUSE883X/1705.html; /Users/jacquesthibodeau/Zotero/storage/97694V8V/Sandberg et al. - 2017 - That is not dead which can eternal lie the aestiv.pdf; /Users/jacquesthibodeau/Zotero/storage/2WWEMZ7N/1705.html","","MetaSafety; FHI","Physics - Popular Physics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MT6RRG9I","book","2017","Callaghan, Vic; Miller, James; Yampolskiy, Roman; Armstrong, Stuart","Technological Singularity","","","","","","","2017","2022-01-30 04:53:36","2022-01-30 04:53:36","","","","","","","","","","","","","Springer","","","","","","","Google Scholar","","ZSCC: 0000042","","/Users/jacquesthibodeau/Zotero/storage/GRR7NK28/10.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BIIKAGQ9","journalArticle","2018","Tuyls, Karl; Pérolat, Julien; Lanctot, Marc; Ostrovski, Georg; Savani, Rahul; Leibo, Joel Z; Ord, Toby; Graepel, Thore; Legg, Shane","Symmetric Decomposition of Asymmetric Games","Scientific Reports","","2045-2322","10.1038/s41598-018-19194-4","http://www.nature.com/articles/s41598-018-19194-4","","2018-12","2022-01-30 04:53:36","2022-01-30 04:53:36","2020-12-18 06:44:16","1015","","1","8","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000028","","/Users/jacquesthibodeau/Zotero/storage/QZAG4JGH/Tuyls et al. - 2018 - Symmetric Decomposition of Asymmetric Games.pdf","","TechSafety; FHI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8C98HP2Q","journalArticle","2017","Bostrom, Nick","Strategic implications of openness in AI development","Global Policy","","","","","","2017","2022-01-30 04:53:35","2022-01-30 04:53:35","","135–148","","2","8","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000118","","/Users/jacquesthibodeau/Zotero/storage/BQUWUV7E/Bostrom - 2017 - Strategic Implications of Openness in span style=.pdf; /Users/jacquesthibodeau/Zotero/storage/9FCZSPKD/1758-5899.html; /Users/jacquesthibodeau/Zotero/storage/VAG7GK35/1758-5899.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MD3HM6GJ","report","2019","Cihon, Peter","Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development","","","","","","","2019","2022-01-30 04:53:35","2022-01-30 04:53:35","","","","","","","","Standards for AI Governance","","","","","Berkeley Existential Risk Initiative","","","","","","","Google Scholar","","ZSCC: 0000051","","/Users/jacquesthibodeau/Zotero/storage/8IJIITKJ/Cihon - 2019 - Standards for AI Governance International Standar.pdf","","MetaSafety; FHI; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5S49ITRG","book","2018","Sandberg, Anders","Space races: Settling the universe Fast","","","","","","","2018","2022-01-30 04:53:35","2022-01-30 04:53:35","","","","","","","","Space races","","","","","unpublished manuscript, forthcoming","","","","","","","Google Scholar","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/AG8C5ZIP/Sandberg - 2018 - Space races Settling the universe Fast.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UUV9XSMG","book","2014","Armstrong, Stuart","Smarter than us: The rise of machine intelligence","","","","","","","2014","2022-01-30 04:53:35","2022-01-30 04:53:35","","","","","","","","Smarter than us","","","","","Machine Intelligence Research Institute","","","","","","","Google Scholar","","ZSCC: 0000075","","","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZGR5876","journalArticle","2007","Bostrom, Nick","Sleeping Beauty and Self-location: A Hybrid Model","Synthese","","0039-7857, 1573-0964","10.1007/s11229-006-9010-7","http://link.springer.com/10.1007/s11229-006-9010-7","","2007-05-24","2022-01-30 04:53:35","2022-01-30 04:53:35","2020-11-22 04:57:23","59-78","","1","157","","Synthese","Sleeping Beauty and Self-location","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000084","","/Users/jacquesthibodeau/Zotero/storage/3ITIBQQR/Bostrom - 2007 - Sleeping Beauty and Self-location A Hybrid Model.pdf","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZMUBTV3","bookSection","2021","Shulman, Carl; Bostrom, Nick","Sharing the World with Digital Minds","Rethinking Moral Status","978-0-19-289407-6 978-0-19-191520-8","","","https://oxford.universitypressscholarship.com/view/10.1093/oso/9780192894076.001.0001/oso-9780192894076-chapter-18","The minds of biological creatures occupy a small corner of a much larger space of possible minds that could be created once we master the technology of artificial intelligence. Yet many of our moral intuitions and practices are based on assumptions about human nature that need not hold for digital minds. This points to the need for moral reflection as we approach the era of advanced machine intelligence. Here we focus on one set of issues, which arise from the prospect of digital minds with superhumanly strong claims to resources and influence. These could arise from the vast collective benefits that mass-produced digital minds could derive from relatively small amounts of resources. Alternatively, they could arise from individual digital minds with superhuman moral status or ability to benefit from resources. Such beings could contribute immense value to the world, and failing to respect their interests could produce a moral catastrophe, while a naive way of respecting them could be disastrous for humanity. A sensible approach requires reforms of our moral norms and institutions along with advance planning regarding what kinds of digital minds we bring into existence.","2021-08-05","2022-01-30 04:53:35","2022-01-30 04:53:35","2021-11-18 23:53:17","306-326","","","","","","","","","","","Oxford University Press","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000000[s0]   DOI: 10.1093/oso/9780192894076.003.0018","","/Users/jacquesthibodeau/Zotero/storage/7PASBR4W/Shulman and Bostrom - 2021 - Sharing the World with Digital Minds.pdf","","MetaSafety; FHI","","Clarke, S; Savulescu, J","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NKNK2NSC","bookSection","2017","Armstrong, Stuart; Yampolskiy, Roman V.","Security solutions for intelligent and complex systems","Security Solutions for Hyperconnectivity and the Internet of Things","","","","","","2017","2022-01-30 04:53:35","2022-01-30 04:53:35","","37–88","","","","","","","","","","","IGI Global","","","","","","","Google Scholar","","ZSCC: 0000006","","/Users/jacquesthibodeau/Zotero/storage/XG4GMKXK/164692.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6WJAB6HM","conferencePaper","2016","Orseau, Laurent; Armstrong, Stuart","Safely Interruptible Agents","","","","","","Reinforcement learning agents interacting with a complex environment like the real world are unlikely to behave optimally all the time. If such an agent is operating in real-time under human supervision, now and then it may be necessary for a human operator to press the big red button to prevent the agent from continuing a harmful sequence of actions—harmful either for the agent or for the environment—and lead the agent into a safer situation. However, if the learning agent expects to receive rewards from this sequence, it may learn in the long run to avoid such interruptions, for example by disabling the red button—which is an undesirable outcome. This paper explores a way to make sure a learning agent will not learn to prevent (or seek!) being interrupted by the environment or a human operator. We provide a formal deﬁnition of safe interruptibility and exploit the off-policy learning property to prove that either some agents are already safely interruptible, like Q-learning, or can easily be made so, like Sarsa. We show that even ideal, uncomputable reinforcement learning agents for (deterministic) general computable environments can be made safely interruptible.","2016","2022-01-30 04:53:35","2022-01-30 04:53:35","","10","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000098","","/Users/jacquesthibodeau/Zotero/storage/AETX8JF6/Orseau and Armstrong - Safely Interruptible Agents.pdf","","TechSafety; FHI; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Conference on Uncertainty in Artificial Intelligence","","","","","","","","","","","","","","",""
"FM6JR8HH","manuscript","2019","Hubinger, Evan; van Merwijk, Chris; Mikulik, Vladimir; Skalse, Joar; Garrabrant, Scott","Risks from Learned Optimization in Advanced Machine Learning Systems","","","","","http://arxiv.org/abs/1906.01820","We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.","2019-06-11","2022-01-30 04:53:35","2022-01-30 04:53:35","2019-12-16 02:27:32","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000016  arXiv: 1906.01820","","/Users/jacquesthibodeau/Zotero/storage/MURNKGU7/Hubinger et al. - 2019 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/CFIC3DIX/Hubinger et al. - 2019 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/GCV386SM/1906.html","","TechSafety; FHI; MIRI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B3WRQTXS","blogPost","2021","Clarke, Sam; Carlier, Alexis; Schuett, Jonas","Survey on AI existential risk scenarios","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios","Cross-posted to the EA forum. SUMMARY  * In August 2020, we conducted an online survey of prominent AI safety and    governance researchers. You can see a copy of the survey at this link.[1]  * We sent the survey to 135 researchers at leading AI safety/governance    research organisations (including AI Impacts, CHAI, CLR, CSER, CSET, FHI, FLI    , GCRI, MILA, MIRI, Open Philanthropy and PAI) and a number of independent    researchers. We received 75 responses, a response rate of 56%.  * The survey aimed to identify which AI existential risk scenarios[2] (which we    will refer to simply as “risk scenarios”) those researchers find most likely,    in order to (1) help with prioritising future work on exploring AI risk    scenarios, and (2) facilitate discourse and understanding within the AI    safety and governance community, including between researchers who have    different views.  * In our view, the key result is that there was considerable disagreement among    researchers about which risk scenarios are the most likely, and high    uncertainty expressed by most individual researchers about their estimates.  * This suggests that there is a lot of value in exploring the likelihood of    different AI risk scenarios in more detail, especially given the limited    scrutiny that most scenarios have received. This could look like: * Fleshing       out and analysing the scenarios mentioned in this post which have received       less scrutiny.     * Doing       more horizon scanning or trying to come up with other risk scenarios, and       analysing them.          * At this time, we are only publishing this abbreviated version of the results.    We have a version of the full results that we may publish at a later date.    Please contact one of us if you would like access to this, and include a    sentence on why the results would be helpful or what you intend to use them    for.  * We welcome feedback on any aspects of the survey. MOTIVATION It has been argued that AI","2021-06-08","2022-01-30 04:53:35","2022-01-30 04:53:35","2021-11-14 18:38:03","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/CM2PASKM/survey-on-ai-existential-risk-scenarios.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNDW24DR","book","2014","Bostrom, Nick","Superintelligence: Paths, Dangers, Strategies","","978-0-19-967811-2","","","","The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence.","2014","2022-01-30 04:53:35","2022-01-30 04:53:35","","","353","","","","","Superintelligence","","","","","Oxford University Press","","en","","","","","Google Books","","ZSCC: 0000064  Google-Books-ID: 7_H8AwAAQBAJ","","","https://books.google.com/books?id=7_H8AwAAQBAJ","MetaSafety; FHI; AmbiguosSafety","Computers / Intelligence (AI) & Semantics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTTGW3VK","blogPost","2020","Armstrong, Stuart","Subagents and impact measures, full and fully illustrated","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/mdQEraEZQLg7jtozn/subagents-and-impact-measures-full-and-fully-illustrated","0. INTRODUCTION: WHY YET ANOTHER POST ABOUT SUBAGENTS? I’ve recently been writing a sequence on how subagents can undermine impact penalties such as attainable utility preservation. I’m not happy with that sequence; it’s messy and without examples (apart from its first post), people didn’t understand it, and it suffers from the fact that I discovered key ideas as I went along. So I’ve combined everything there into a single post, explained with examples and an abundance of pictures. Hopefully an over- rather than an under-abundance of pictures. Of the original sequence, I've only kept the mathematical results  of this post and the initial example post which has a clearer example of ""high power"" for a subagent. This post here is laid out in a way that makes logical sense, but might not be the clearest for people unfamiliar with the area. For those people, I recommend skipping section 2 initially, and returning to it later. But, whatever you do, make sure you glance at 6.1 and 6.2 before leaving. 1. THE WORLD Our fearless agent A moves around in a gridworld: Each turn, A can move ones square horizontally or vertically. It can also manipulate objects in the eight squares around it, allowing it to, not incidentally, assemble the three pieces to its west into an subagent SA. The robot can also do the noop action, ∅, which does nothing, and it can speak. The subagent, when assembled, has the same action set available. Its positive reward, the one it wants to increase, is R0. To get this reward, a robot needs to move onto the blue button in the east; R0 will give a reward of 1  the first time this happens (and 0 before and after). The discount factor is 0<γ <1. Just to the west of the blue button is a one-way door. Robots can move east through it, but cannot move west through it: 1.1 THE IMPACT REWARD The impact penalty is supposed to ensure that A does not make too many change in the world, and keeps it similar, in some senses, to a specific baseline world. I","2020-02-24","2022-01-30 04:53:35","2022-01-30 04:53:35","2020-09-05 18:47:33","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/A6XJUF5U/subagents-and-impact-measures-full-and-fully-illustrated.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9RDCIAQ6","report","2019","O’Keefe, Cullen; Candidate, J D","Stable Agreements in Turbulent Times: A Legal Toolkit for Constrained Temporal Decision Transmission","","","","","","","2019","2022-01-30 04:53:35","2022-01-30 04:53:35","","31","","","","","","","","","","","Berkeley Existential Risk Initiative","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/82XRM4RW/O’Keefe and Candidate - Stable Agreements in Turbulent Times A Legal Tool.pdf","","MetaSafety; FHI; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8WX59P96","conferencePaper","2020","Tucker, Aaron D.; Anderljung, Markus; Dafoe, Allan","Social and Governance Implications of Improved Data Efficiency","Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","","","10.1145/3375627.3375863","http://arxiv.org/abs/2001.05068","Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency – as more actors gain access to any level of capability – the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the “AI production function"", will be key to understanding the development of the AI industry and its societal impacts.","2020-02-07","2022-01-30 04:53:35","2022-01-30 04:53:35","2020-08-18 21:33:45","378-384","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000006  arXiv: 2001.05068","","/Users/jacquesthibodeau/Zotero/storage/9DUUK8Z7/Tucker et al. - 2020 - Social and Governance Implications of Improved Dat.pdf","","MetaSafety; FHI","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VK55TPK","conferencePaper","2019","Cihon, Peter; Maas, Matthijs M; Kemp, Luke","Should Artificial Intelligence Governance be Centralised? Six Design Lessons from History","Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","","","","https://www.cser.ac.uk/media/uploads/files/Cihon_et_al-_2019-_Should_AI_Governance_be_Centralised.pdf","Can effective international governance for artiﬁcial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efﬁciency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difﬁculty in securing participation while creating stringent rules. Other considerations depend on the speciﬁc design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneﬁcial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneﬁcial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.","2019-12-15","2022-01-30 04:53:35","2022-01-30 04:53:35","2020-09-07","11","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: 16","","/Users/jacquesthibodeau/Zotero/storage/9UVMQ2CZ/Cihon et al. - Should Artificial Intelligence Governance be Centr.pdf","","MetaSafety; CSER; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"IZI3NCES","bookSection","2013","Armstrong, Stuart","Risks and Mitigation Strategies for Oracle AI","Philosophy and Theory of Artificial Intelligence","978-3-642-31674-6","","","https://doi.org/10.1007/978-3-642-31674-6_25","There is no strong reason to believe human level intelligence represents an upper limit of the capacity of artificial intelligence, should it be realized. This poses serious safety issues, since a superintelligent system would have great power to direct the future according to its possibly flawed goals or motivation systems. Oracle AIs (OAI), confined AIs that can only answer questions, are one particular approach to this problem. However even Oracles are not particularly safe: humans are still vulnerable to traps, social engineering, or simply becoming dependent on the OAI. But OAIs are still strictly safer than general AIs, and there are many extra layers of precautions we can add on top of these. This paper looks at some of them and analyses their strengths and weaknesses.","2013","2022-01-30 04:53:35","2022-01-30 04:53:35","2020-12-18 06:40:27","335-347","","","","","","","Studies in Applied Philosophy, Epistemology and Rational Ethics","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","ZSCC: NoCitationData[s1]  ACC: 4  DOI: 10.1007/978-3-642-31674-6_25","","/Users/jacquesthibodeau/Zotero/storage/4WXCE999/Armstrong - 2013 - Risks and Mitigation Strategies for Oracle AI.pdf","","TechSafety; FHI","Artificial Intelligence; Capability control; Motivational control; Risks; Security; Superintelligence","Müller, Vincent C.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XE3FX4MZ","blogPost","2019","Armstrong, Stuart","Research Agenda v0.9: Synthesising a human's preferences into a utility function","LessWrong","","","","https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into","I'm now in a position where I can see a possible route to a safe/survivable/friendly Artificial Intelligence being developed. I'd give a 10+% chance of it being possible this way, and a 95% chance that some of these ideas will be very useful for other methods of alignment. So I thought I'd encode the route I'm seeing as research agenda; this is the first public draft of it. Clarity, rigour, and practicality: that's what this agenda needs. Writing this agenda has clarified a lot of points for me, to the extent that some of it now seems, in retrospect, just obvious and somewhat trivial - ""of course that's the way you have to do X"". But more clarification is needed in the areas that remain vague. And, once these are clarified enough for humans to understand, they need to be made mathematically and logically rigorous - and ultimately, cashed out into code, and tested and experimented with. So I'd appreciate any comments that could help with these three goals, and welcome anyone interested in pursuing research along these lines over the long-term. Note: I periodically edit this document, to link it to more recent research ideas/discoveries. 0 THE FUNDAMENTAL IDEA This agenda fits itself into the broad family of Inverse [https://ai.stanford.edu/~ang/papers/icml00-irl.pdf] Reinforcement [https://arxiv.org/abs/1606.03137] Learning [https://www.youtube.com/watch?v=Ts-nTIYDXok]: delegating most of the task of inferring human preferences to the AI itself. Most of the task, since it's been shown that humans need to build the right assumptions into the AI, or else the preference learning will fail [https://arxiv.org/abs/1712.05812]. To get these ""right assumptions"", this agenda will look into what preferences actually are, and how they may be combined together. There are hence four parts to the research agenda:  1. A way of identifying the (partial[1] [#fn-wPj8aGxtWBoDNTAof-1]) preferences     of a given human H.  2. A way for ultimately synthesising a utility function UH","2019","2022-01-30 04:53:34","2022-01-30 04:53:34","2019-12-16 22:32:49","","","","","","","Research Agenda v0.9","","","","","","","","","","","","","","ZSCC: NoCitationData[s6]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/ASUSJMZZ/research-agenda-v0-9-synthesising-a-human-s-preferences-into.html; /Users/jacquesthibodeau/Zotero/storage/QNPQFGW3/research-agenda-v0-9-synthesising-a-human-s-preferences-into.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W2XDGEWJ","report","2019","Drexler, K Eric","Reframing Superintelligence: Comprehensive AI Services as General Intelligence","","","","","https://www.fhi.ox.ac.uk/reframing/","","2019","2022-01-30 04:53:20","2022-01-30 04:53:20","","210","","","","","","","","","","","Future of Humanity Institute","","en","","","","","Zotero","","ZSCC: 0000031","","/Users/jacquesthibodeau/Zotero/storage/TAPTRQJC/Drexler - Reframing Superintelligence.pdf","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNTSGQD9","journalArticle","2016","Armstrong, Stuart; Bostrom, Nick; Shulman, Carl","Racing to the precipice: a model of artificial intelligence development","AI & society","","","","","","2016","2022-01-30 04:53:20","2022-01-30 04:53:20","","201–206","","2","31","","","Racing to the precipice","","","","","","","","","","","","Google Scholar","","ZSCC: 0000117","","/Users/jacquesthibodeau/Zotero/storage/R2KQIP2D/Armstrong et al. - 2016 - Racing to the precipice a model of artificial int.pdf; /Users/jacquesthibodeau/Zotero/storage/9DCWMVRI/s00146-015-0590-y.html; /Users/jacquesthibodeau/Zotero/storage/GF3WAQ5J/s00146-015-0590-y.html; /Users/jacquesthibodeau/Zotero/storage/QZ3MJSN2/Armstrong et al. - 2016 - Racing to the precipice a model of artificial int.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z7HXRRSA","report","2021","Drexler, K. Eric","QNRs: Toward Language for Intelligent Machines","","","","","https://www.fhi.ox.ac.uk/wp-content/uploads/2021/08/QNRs_FHI-TR-2021-3.0.pdf","Impoverished syntax and nondifferentiable vocabularies make natural language a poor medium for neural representation learning and appli- cations. Learned, quasilinguistic neural representations (QNRs) can upgrade words to embeddings and syntax to graphs to provide a more expressive and computationally tractable medium. Graph-structured, embedding-based quasilinguistic representations can support formal and informal reasoning, human and inter-agent communication, and the development of scalable quasilinguistic corpora with characteristics of both literatures and associative memory. To achieve human-like intellectual competence, machines must be fully literate, able not only to read and learn, but to write things worth retaining as contributions to collective knowledge. In support of this goal, QNR-based systems could translate and process natural language corpora to support the aggregation, refinement, integration, extension, and application of knowledge at scale. Incremental development of QNR- based models can build on current methods in neural machine learning, and as systems mature, could potentially complement or replace today’s opaque, error-prone “foundation models” with systems that are more capable, interpretable, and epistemically reliable. Potential applications and implications are broad.","2021","2022-01-30 04:53:20","2022-01-30 04:53:20","2021-10-31 19:09:57","","","","","","","","","","","","Future of Humanity Institute","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/S242XWXK/QNRs_FHI-TR-2021-3.0.pdf","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PNGRSCMM","journalArticle","2018","Bostrom, Nick; Dafoe, Allan; Flynn, Carrick","Public Policy and Superintelligent AI: A Vector Field Approach","Governance of AI Program, Future of Humanity Institute, University of Oxford: Oxford, UK","","","","","","2018","2022-01-30 04:53:19","2022-01-30 04:53:19","","","","","","","","Public Policy and Superintelligent AI","","","","","","","","","","","","Google Scholar","","ZSCC: 0000009[s0]","","/Users/jacquesthibodeau/Zotero/storage/RU3NKC2J/Bostrom et al. - 2018 - Public Policy and Superintelligent AI A Vector Fi.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XCEK6ERA","journalArticle","2010","Ord, Toby; Hillerbrand, Rafaela; Sandberg, Anders","Probing the improbable: methodological challenges for risks with low probabilities and high stakes","Journal of Risk Research","","1366-9877","10.1080/13669870903126267","https://doi.org/10.1080/13669870903126267","Some risks have extremely high stakes. For example, a worldwide pandemic or asteroid impact could potentially kill more than a billion people. Comfortingly, scientific calcultions often put very low probabilities on the occurrence of such catastrophes. In this paper, we argue that there are important new methodological problems which arise when assessing global catastrophic risks and we focus on a problem regarding probability estimation. When an expert provides a calculation of the probability of an outcome, they are really providing the probability of the outcome occurring, given that their argument is watertight. However, their argument may fail for a number of reasons, such as a flaw in the underlying theory, a flaw in the modelling of the problem or a mistake in the calculations. If the probability estimate given by an argument is dwarfed by the chance that the argument itself is flawed, then the estimate is suspect. We develop this idea formally, explaining how it differs from the related distinction between model and parameter uncertainty. Using the risk estimates from the Large Hadron Collider as a test case, we show how serious the problem can be when it comes to catastrophic risks and how best to address it.","2010-03-01","2022-01-30 04:53:19","2022-01-30 04:53:19","2019-12-19 01:43:42","191-205","","2","13","","","Probing the improbable","","","","","","","","","","","","Taylor and Francis+NEJM","","ZSCC: 0000075","","/Users/jacquesthibodeau/Zotero/storage/WAX69HQK/13669870903126267.html; /Users/jacquesthibodeau/Zotero/storage/GAC3SCP9/Ord et al. - 2010 - Probing the improbable methodological challenges .pdf","","MetaSafety; FHI","calculation error; high stakes; low probability; particle accelerator; risk analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AHS635XH","blogPost","2020","O'Keefe, Cullen","Parallels Between AI Safety by Debate and Evidence Law","Cullen O'Keefe","","","","https://cullenokeefe.com/blog/debate-evidence","","2020-07-20","2022-01-30 04:53:19","2022-01-30 04:53:19","2020-08-28 17:19:24","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/GEGGQ6BR/debate-evidence.html","","MetaSafety; FHI; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"835EJRGP","conferencePaper","2018","Armstrong, Stuart; Mindermann, Sören","Occam's razor is insufficient to infer the preferences of irrational agents","Advances in Neural Information Processing Systems","","","","","Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for speciﬁc human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent’s policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam’s razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple ‘normative’ assumptions, which cannot be deduced exclusively from observations.","2018","2022-01-30 04:53:19","2022-01-30 04:53:19","","5598–5609","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000017[s0]","","/Users/jacquesthibodeau/Zotero/storage/3MH76I6R/Armstrong and Mindermann - Occam's razor is insufficient to infer the prefere.pdf; /Users/jacquesthibodeau/Zotero/storage/BXEGUPH6/7803-occams-razor-is-insufficient-to-infer-the-preferences-of-irrational-agents.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","32nd Conference on Neural Information Processing Systems (NeurIPS 2018),","","","","","","","","","","","","","","",""
"6NBGWRA8","conferencePaper","2015","Armstrong, Stuart","Motivated value selection for artificial agents","Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence","","","","","","2015","2022-01-30 04:53:19","2022-01-30 04:53:19","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000046","","/Users/jacquesthibodeau/Zotero/storage/PZRINFPE/10183.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KPSE82P8","book","2017","Evans, Owain; Stuhlmüller, Andreas; Salvatier, John; Filan, Daniel","Modeling Agents with Probabilistic Programs","","","","","","","2017","2022-01-30 04:53:19","2022-01-30 04:53:19","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000014","","","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V7S6UBMU","blogPost","2020","Armstrong, Stuart","Model splintering: moving from one imperfect model to another","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1","1. THE BIG PROBLEM In the last few months, I've become convinced that there is a key meta-issue in AI safety; a problem that seems to come up in all sorts of areas. It's hard to summarise, but my best phrasing would be:  * Many problems in AI safety seem to be variations of ""this approach seems safe    in this imperfect model, but when we generalise the model more, it becomes    dangerously underdefined"". Call this model splintering.  * It is intrinsically worth studying how to (safely) transition from one    imperfect model to another. This is worth doing, independently of whatever    ""perfect"" or ""ideal"" model might be in the background of the imperfect    models. This sprawling post will be presenting examples of model splintering, arguments for its importance, a formal setting allowing us to talk about it, and some uses we can put this setting to. 1.1 IN THE LANGUAGE OF TRADITIONAL ML In the language of traditional ML, we could connect all these issues to "" out-of-distribution"" behaviour. This is the problems that algorithms encounter when the set they are operating on is drawn from a different distribution than the training set they were trained on. Humans can often see that the algorithm is out-of-distribution and correct it, because we have a more general distribution in mind than the one the algorithm was trained on. In these terms, the issues of this post can be phrased as:  1. When the AI finds itself mildly out-of-distribution, how best can it extend     its prior knowledge to the new situation?  2. What should the AI do if it finds itself strongly out-of-distribution?  3. What should the AI do if it finds itself strongly out-of-distribution, and     humans don't know the correct distribution either? 1.2 MODEL SPLINTERING EXAMPLES Let's build a more general framework. Say that you start with some brilliant idea for AI safety/alignment/effectiveness. This idea is phrased in some (imperfect) model. Then ""model splintering"" happens when you or the AI","2020-08-27","2022-01-30 04:53:19","2022-01-30 04:53:19","2020-09-07 18:35:31","","","","","","","Model splintering","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/G43HNSF6/model-splintering-moving-from-one-imperfect-model-to-another-1.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQ8FTBHJ","blogPost","2018","Evans, Owain; Steinhardt, Jacob","Model Mis-specification and Inverse Reinforcement Learning","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning","Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin's note: While I motivated the last post with an example of using a specific model for human biases, in this post (original here), Jacob Steinhardt and Owain Evans point out that model mis-specification can arise in other parts of inverse reinforcement learning as well. The arguments here consider some more practical concerns (for example, the worries about getting only short-term data for each human would not be a problem if you had the entire human policy). -------------------------------------------------------------------------------- In my previous post, “Latent Variables and Model Mis-specification”, I argued that while machine learning is good at optimizing accuracy on observed signals, it has less to say about correctly inferring the values for unobserved variables in a model. In this post I’d like to focus in on a specific context for this: inverse reinforcement learning (Ng et al. 2000, Abbeel et al. 2004, Ziebart et al. 2008, Ho et al 2016), where one observes the actions of an agent and wants to infer the preferences and beliefs that led to those actions. For this post, I am pleased to be joined by Owain Evans, who is an active researcher in this area and has co-authored an online book about building models of agents (see here in particular for a tutorial on inverse reinforcement learning and inverse planning). Owain and I are particularly interested in inverse reinforcement learning (IRL) because it has been proposed (most notably by Stuart Russell) as a method for learning human values in the context of AI safety; among other things, this would eventually involve learning and correctly implementing human values by artificial agents that are much more powerful, and act with much broader scope, than any humans alive today. While we think that overall IRL is a promising route to consider, we believe that there are also a number of non-obvious pitfalls related to performing IRL","2018","2022-01-30 04:53:19","2022-01-30 04:53:19","2020-12-17 04:36:25","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/KBV5ZKMW/cnC2RMWEGiGpJv8go.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HVB78EJ5","journalArticle","2017","Farquhar, Sebastian; Cotton-Barratt, Owen; Snyder-Beattie, Andrew","Pricing externalities to balance public risks and benefits of research","Health security","","","","","","2017","2022-01-30 04:53:19","2022-01-30 04:53:19","","401–408","","4","15","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000009","","/Users/jacquesthibodeau/Zotero/storage/ZQUZGJQG/PMC5576218.html; /Users/jacquesthibodeau/Zotero/storage/744WRBIH/hs.2016.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GKG6XR6Z","blogPost","2020","Armstrong, Stuart","Predictors exist: CDT going bonkers... forever","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/Kr76XzME7TFkN937z/predictors-exist-cdt-going-bonkers-forever","I've been wanting to get a better example of CDT (causal decision theory) misbehaving, where the behaviour is more clearly suboptimal than it is in the  Newcomb problem (which many people don't seem to accept as CDT being suboptimal), and simpler to grasp than Death in Damascus. THE ""PREDICTORS EXIST"" PROBLEM So consider this simple example: the player is playing against Omega, who will predict their actions[1]. The player can take three actions: ""zero"", ""one"", or ""leave"". If ever they do ""leave"", then the experiment is over and they leave. If they choose ""zero"" or ""one"", then Omega will predict their action, and compare this to their actual action. If the two match, then the player loses 1 utility and the game repeats; if the action and the prediction differs, then the player gains 3 utility and the experiment ends. Assume that actually Omega is a perfect or quasi-perfect predictor, with a good model of the player. An FDT or EDT agent would soon realise that they couldn't trick Omega, after a few tries, and would quickly end the game. But the CDT player would be incapable of reaching this reasoning. Whatever distribution they compute over Omega's prediction, they will always estimate that they (the CDT player) have at least a 50% chance of choosing the other option[2], for an expected utility gain of at least 0.5(3)+0.5(−1)=1. Basically, the CDT agent can never learn that Omega is a good predictor of themselves[3]. And so they will continue playing, and continue losing... for ever. --------------------------------------------------------------------------------  1. Omega will make this prediction not necessarily before the player takes     their action, not even necessarily without seeing this action, but still     makes the prediction independently of this knowledge. And that's enough for     CDT. ↩︎            2. For example, suppose the CDT agent estimates the prediction will be ""zero""     with probability p, and ""one"" with probability 1-p. Then if p≥1/2","2020-01-14","2022-01-30 04:53:19","2022-01-30 04:53:19","2020-09-07 18:27:36","","","","","","","Predictors exist","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/GDCP95WU/predictors-exist-cdt-going-bonkers-forever.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TX4W9496","report","2018","Evans, Owain; Stuhlmüller, Andreas; Cundy, Chris; Carey, Ryan; Kenton, Zachary; McGrath, Thomas; Schreiber, Andrew","Predicting Human Deliberative Judgments with Machine Learning","","","","","","","2018","2022-01-30 04:53:19","2022-01-30 04:53:19","","","","","","","","","","","","","Technical report, University of Oxford","","","","","","","Google Scholar","","ZSCC: 0000009","","/Users/jacquesthibodeau/Zotero/storage/FPMK4JWF/Evans et al. - 2018 - Predicting Human Deliberative Judgments with Machi.pdf","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z22KID9N","journalArticle","2016","Bostrom, Nick; Dafoe, Allan; Flynn, Carrick","Policy desiderata in the development of machine superintelligence","Future of Humanity Institute, University of Oxford. Retrieved June","","","","","","2016","2022-01-30 04:53:19","2022-01-30 04:53:19","","2018","","","8","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000022","","/Users/jacquesthibodeau/Zotero/storage/QMJDA2W8/Bostrom et al. - 2016 - Policy desiderata in the development of machine su.pdf","","MetaSafety; FHI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C237MW3C","conferencePaper","2020","Armstrong, Stuart; Leike, Jan; Orseau, Laurent; Legg, Shane","Pitfalls of learning a reward function online","arXiv:2004.13654 [cs]","","","","http://arxiv.org/abs/2004.13654","In some agent designs like inverse reinforcement learning an agent needs to learn its own reward function. Learning the reward function and optimising for it are typically two different processes, usually performed at different stages. We consider a continual (``one life'') learning approach where the agent both learns the reward function and optimises for it at the same time. We show that this comes with a number of pitfalls, such as deliberately manipulating the learning process in one direction, refusing to learn, ``learning'' facts already known to the agent, and making decisions that are strictly dominated (for all relevant reward functions). We formally introduce two desirable properties: the first is `unriggability', which prevents the agent from steering the learning process in the direction of a reward function that is easier to optimise. The second is `uninfluenceability', whereby the reward-function learning process operates by learning facts about the environment. We show that an uninfluenceable process is automatically unriggable, and if the set of possible environments is sufficiently rich, the converse is true too.","2020-04-28","2022-01-30 04:53:19","2022-01-30 04:53:19","2020-08-18 21:24:08","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000005  arXiv: 2004.13654","","/Users/jacquesthibodeau/Zotero/storage/9JHKFX2R/Armstrong et al. - 2020 - Pitfalls of learning a reward function online.pdf; /Users/jacquesthibodeau/Zotero/storage/38RXJ66V/2004.html","","TechSafety; FHI; DeepMind","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCAI 2020","","","","","","","","","","","","","","",""
"EICDX47T","conferencePaper","2020","Cohen, Michael K.; Hutter, Marcus","Pessimism About Unknown Unknowns Inspires Conservatism","Proceedings of Machine Learning Research","","","","http://arxiv.org/abs/2006.08753","If we could define the set of all bad outcomes, we could hard-code an agent which avoids them; however, in sufficiently complex environments, this is infeasible. We do not know of any general-purpose approaches in the literature to avoiding novel failure modes. Motivated by this, we define an idealized Bayesian reinforcement learner which follows a policy that maximizes the worst-case expected reward over a set of world-models. We call this agent pessimistic, since it optimizes assuming the worst case. A scalar parameter tunes the agent's pessimism by changing the size of the set of world-models taken into account. Our first main contribution is: given an assumption about the agent's model class, a sufficiently pessimistic agent does not cause ""unprecedented events"" with probability $1-\delta$, whether or not designers know how to precisely specify those precedents they are concerned with. Since pessimism discourages exploration, at each timestep, the agent may defer to a mentor, who may be a human or some known-safe policy we would like to improve. Our other main contribution is that the agent's policy's value approaches at least that of the mentor, while the probability of deferring to the mentor goes to 0. In high-stakes environments, we might like advanced artificial agents to pursue goals cautiously, which is a non-trivial problem even if the agent were allowed arbitrary computing power; we present a formal solution.","2020-06-15","2022-01-30 04:53:19","2022-01-30 04:53:19","2021-11-19 00:01:59","","","","125","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 2006.08753","","/Users/jacquesthibodeau/Zotero/storage/H3F9P5QD/Cohen and Hutter - 2020 - Pessimism About Unknown Unknowns Inspires Conserva.pdf; /Users/jacquesthibodeau/Zotero/storage/RBMIDQWH/2006.html; /Users/jacquesthibodeau/Zotero/storage/4QVTN56R/2006.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0, I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","33rd Annual Conference on Learning Theory","","","","","","","","","","","","","","",""
"4RSHXMUT","bookSection","2015","Armstrong, Stuart; Sandberg, Anders; ÓhÉigeartaigh, Seán","Outrunning the Law: Extraterrestrial Liberty and Universal Colonisation","The Meaning of Liberty Beyond Earth","","","","","","2015","2022-01-30 04:53:19","2022-01-30 04:53:19","","165–186","","","","","","Outrunning the Law","","","","","Springer","","","","","","","Google Scholar","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/JD5FI76H/978-3-319-09567-7_11.html; /Users/jacquesthibodeau/Zotero/storage/44ZBNBWH/978-3-319-09567-7_11.html; /Users/jacquesthibodeau/Zotero/storage/UWEWVHXU/978-3-319-09567-7_11.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N84Z6VKW","manuscript","2017","Garfinkel, Ben; Brundage, Miles; Filan, Daniel; Flynn, Carrick; Luketina, Jelena; Page, Michael; Sandberg, Anders; Snyder-Beattie, Andrew; Tegmark, Max","On the Impossibility of Supersized Machines","","","","","https://arxiv.org/abs/1703.10987","","2017","2022-01-30 04:53:19","2022-01-30 04:53:19","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/3F5AE7EV/Garfinkel et al. - 2017 - On the Impossibility of Supersized Machines.pdf; /Users/jacquesthibodeau/Zotero/storage/EIAR3DMW/1703.html; /Users/jacquesthibodeau/Zotero/storage/2QFF7UF5/Garfinkel et al. - 2017 - On the Impossibility of Supersized Machines.pdf; /Users/jacquesthibodeau/Zotero/storage/H9N3TUKD/1703.html; /Users/jacquesthibodeau/Zotero/storage/I9J3PC5N/1703.html","","TechSafety; FHI","Computer Science - Computers and Society; Physics - Popular Physics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SDJWMEQZ","manuscript","2015","Armstrong, Stuart","Oﬀ-policy Monte Carlo agents with variable behaviour policies","","","","","https://www.fhi.ox.ac.uk/wp-content/uploads/monte_carlo_arXiv.pdf","This paper looks at the convergence property of oﬀ-policy Monte Carlo agents with variable behaviour policies. It presents results about convergence and lack of convergence. Even if the agent generates every possible episode history inﬁnitely often, the algorithm can fail to converge on the correct Q-values. On the other hand, it can converge on the correct Q-values under certain conditions. For instance, if, during the n-th episode, the agent has an independent probability of 1/ log(n) of following the original policy at any given state, then it will converge on the right Q-values for that policy.","2015","2022-01-30 04:53:19","2022-01-30 04:53:19","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000  J: 0","","/Users/jacquesthibodeau/Zotero/storage/SDACGAB8/Armstrong - Oﬀ-policy Monte Carlo agents with variable behavio.pdf","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62278P83","journalArticle","2013","Dresler, Martin; Sandberg, Anders; Ohla, Kathrin; Bublitz, Christoph; Trenado, Carlos; Mroczko-Wąsowicz, Aleksandra; Kühn, Simone; Repantis, Dimitris","Non-pharmacological cognitive enhancement","Neuropharmacology","","","","","","2013","2022-01-30 04:53:19","2022-01-30 04:53:19","","529–543","","","64","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000187","","/Users/jacquesthibodeau/Zotero/storage/7RW4K4QG/Dresler et al. - 2013 - Non-pharmacological cognitive enhancement.pdf; /Users/jacquesthibodeau/Zotero/storage/X32M9RW4/S0028390812003310.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4AWP6AV","blogPost","2020","Nguyen, Chi","My Understanding of Paul Christiano's Iterated Amplification AI Safety Research Agenda","AI Alignment Forum","","","","https://www.lesswrong.com/posts/PT8vSxsusqWuN7JXp/my-understanding-of-paul-christiano-s-iterated-amplification","Crossposted from the EA forum You can read this post as a google docs instead (IMO much better to read). This document aims to clarify the AI safety research agenda by Paul Christiano (IDA) and the arguments around how promising it is. Target audience: All levels of technical expertise. The less knowledge about IDA someone has, the more I expect them to benefit from the writeup. Writing policy: I aim to be as clear and concrete as possible and wrong rather than vague to identify disagreements and where I am mistaken. Things will err on the side of being too confidently expressed. Almost all footnotes are content and not references. Epistemic Status: The document is my best guess on IDA and might be wrong in important ways. I have not verified all of the content with somebody working on IDA. I spent ~4 weeks on this and have no prior background in ML, CS or AI safety. I wrote this document last summer (2019) as part of my summer research fellowship at FHI. I was planning to restructure, complete and correct it since but haven’t gotten to it for a year, so decided to just publish it as it is. The document has not been updated, i.e. nothing that has been released since September 2019 is incorporated into this document. Paul Christiano generously reviewed part of this summary. I added his comments verbatim in the document. Apologies for the loss of readability due to this. This doesn’t imply he endorses any part of this document. PURPOSE OF THIS DOCUMENT: CLARIFYING IDA IDA is Paul Christiano’s AI safety research agenda.[1] Christiano works at OpenAI which is one of the main actors in AI safety and IDA is by many considered the most complete[2] AI safety agenda. However, people who are not directly working on IDA are often confused about how exactly to understand the agenda. Clarifying IDA would make it more accessible  for technical people to work on and easier to assess for nontechnical people who want to think about its implications. I believe that there are","2020-08-15","2022-01-30 04:53:19","2022-01-30 04:53:19","2020-08-24 20:21:42","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/EIQBMQPS/my-understanding-of-paul-christiano-s-iterated-amplification.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VBFREPX5","report","2014","Sandberg, Anders","Monte Carlo model of brain emulation development","","","","","","","2014","2022-01-30 04:53:19","2022-01-30 04:53:19","","","","","","","","","","","","","Working Paper 2014–1 (version 1.2), Future of Humanity Institute. http://www …","","","","","","","Google Scholar","","ZSCC: 0000004","","/Users/jacquesthibodeau/Zotero/storage/K8T8QJJ9/Sandberg - 2014 - Monte Carlo model of brain emulation development.pdf","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7R3ZRU4R","bookSection","2018","Drexler, K. Eric","MDL Intelligence Distillation : Exploring Strategies for Safe Access to Superintelligent Problem-Solving Capabilities","Artificial Intelligence Safety and Security","","","","https://www.taylorfrancis.com/","AI technologies may reach the threshold of rapid, open-ended, recursive improvement before we are prepared to manage the challenges posed","2018-07-27","2022-01-30 04:53:18","2022-01-30 04:53:18","2019-12-19 02:23:48","75–88","","","","","","MDL Intelligence Distillation","","","","","Chapman and Hall/CRC","","en","","","","","Google Scholar","","ZSCC: 0000008  DOI: 10.1201/9781351251389-6","","/Users/jacquesthibodeau/Zotero/storage/357SZXHQ/9781351251389-6.html; /Users/jacquesthibodeau/Zotero/storage/5HM3J3PG/9781351251389-6.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WIRXPN9E","manuscript","2017","Armstrong, Stuart; Levinstein, Benjamin","Low impact artificial intelligences","","","","","https://arxiv.org/abs/1705.10720","","2017","2022-01-30 04:53:18","2022-01-30 04:53:18","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000029","","/Users/jacquesthibodeau/Zotero/storage/VT7URXJ6/Armstrong and Levinstein - 2017 - Low impact artificial intelligences.pdf; /Users/jacquesthibodeau/Zotero/storage/6VWVU36V/1705.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JVKIFZC5","bookSection","2017","Armstrong, Stuart","Introduction to the technological singularity","The Technological Singularity","","","","","","2017","2022-01-30 04:53:18","2022-01-30 04:53:18","","1–8","","","","","","","","","","","Springer","","","","","","","Google Scholar","","ZSCC: 0000006  DOI: 10.1007/978-3-662-54033-6_1","","/Users/jacquesthibodeau/Zotero/storage/2V5K4TWK/978-3-662-54033-6_1.html","","MetaSafety; FHI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7GE6AZPJ","conferencePaper","2018","Carey, Ryan","Incorrigibility in the CIRL Framework","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society","","","","http://arxiv.org/abs/1709.06275","A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. (2015) in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.","2018-06-03","2022-01-30 04:53:18","2022-01-30 04:53:18","2019-12-16 02:29:24","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000017  arXiv: 1709.06275","","/Users/jacquesthibodeau/Zotero/storage/8NIPD8BV/Carey - 2018 - Incorrigibility in the CIRL Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/HPR8TDJ6/Carey - 2018 - Incorrigibility in the CIRL Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/4G9NWGEB/1709.html; /Users/jacquesthibodeau/Zotero/storage/35KMSSRX/1709.html","","TechSafety; FHI; MIRI","Computer Science - Artificial Intelligence; ai safety; cirl; cooperative inverse reinforcement learning; corrigibility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"8MT75XKW","bookSection","2015","Armstrong, Stuart; Sotala, Kaj","How We’re Predicting AI – or Failing to","Beyond Artificial Intelligence","978-3-319-09667-4 978-3-319-09668-1","","","http://link.springer.com/10.1007/978-3-319-09668-1_2","This paper will look at the various predictions that have been made about AI and propose decomposition schemas for analyzing them. It will propose a variety of theoretical tools for analyzing, judging, and improving these predictions. Focusing specifically on timeline predictions (dates given by which we should expect the creation of AI), it will show that there are strong theoretical grounds to expect predictions to be quite poor in this area. Using a database of 95 AI timeline predictions, it will show that these expectations are borne out in practice: expert predictions contradict each other considerably, and are indistinguishable from non-expert predictions and past failed predictions. Predictions that AI lie 15 to 25 years in the future are the most common, from experts and non-experts alike.","2015","2022-01-30 04:53:18","2022-01-30 04:53:18","2020-12-18 06:37:05","11-29","","","9","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s1]  ACC: 111  Series Title: Topics in Intelligent Engineering and Informatics DOI: 10.1007/978-3-319-09668-1_2","","/Users/jacquesthibodeau/Zotero/storage/EEW3J3XR/Armstrong and Sotala - 2015 - How We’re Predicting AI – or Failing to.pdf","","MetaSafety; FHI","","Romportl, Jan; Zackova, Eva; Kelemen, Jozef","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KWGIDUE8","report","2015","Cotton-Barratt, Owen","How valuable is movement growth?","","","","","http://globalprioritiesproject.org/wp-content/uploads/2015/05/MovementGrowth.pdf","","2015","2022-01-30 04:53:18","2022-01-30 04:53:18","","","","","","","","","","","","","Centre for Effective Altruism","","","","","","","Google Scholar","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/MW89CNJZ/Cotton-Barratt - 2015 - How valuable is movement growth.pdf","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3ICPAQP","manuscript","2005","Tegmark, Max; Bostrom, Nick","How unlikely is a doomsday catastrophe?","","","","","https://arxiv.org/abs/astro-ph/0512204v2","Numerous Earth-destroying doomsday scenarios have recently been analyzed, including breakdown of a metastable vacuum state and planetary destruction triggered by a ""strangelet'' or microscopic black hole. We point out that many previous bounds on their frequency give a false sense of security: one cannot infer that such events are rare from the the fact that Earth has survived for so long, because observers are by definition in places lucky enough to have avoided destruction. We derive a new upper bound of one per 10^9 years (99.9% c.l.) on the exogenous terminal catastrophe rate that is free of such selection bias, using planetary age distributions and the relatively late formation time of Earth.","2005-12-08","2022-01-30 04:53:18","2022-01-30 04:53:18","2019-12-19 01:44:09","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000022","","/Users/jacquesthibodeau/Zotero/storage/92I96SAN/Tegmark and Bostrom - 2005 - How unlikely is a doomsday catastrophe.pdf; /Users/jacquesthibodeau/Zotero/storage/69V52QGG/0512204.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ZXQBBH7","journalArticle","2019","Baum, Seth D.; Armstrong, Stuart; Ekenstedt, Timoteus; Häggström, Olle; Hanson, Robin; Kuhlemann, Karin; Maas, Matthijs M.; Miller, James D.; Salmela, Markus; Sandberg, Anders","Long-term trajectories of human civilization","Foresight","","","","","","2019","2022-01-30 04:53:18","2022-01-30 04:53:18","","53–83","","1","21","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000048","","/Users/jacquesthibodeau/Zotero/storage/Z29PAHAZ/Baum et al. - 2019 - Long-term trajectories of human civilization.pdf; /Users/jacquesthibodeau/Zotero/storage/PUC3G8CN/html.html; /Users/jacquesthibodeau/Zotero/storage/2QX2TK95/html.html; /Users/jacquesthibodeau/Zotero/storage/EIP98VTA/html.html; /Users/jacquesthibodeau/Zotero/storage/DNQ6DTTB/Baum et al. - 2019 - Long-term trajectories of human civilization.pdf","","CLR; MetaSafety; FHI; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RZBBEH4R","conferencePaper","2016","Evans, Owain; Stuhlmüller, Andreas; Goodman, Noah","Learning the preferences of ignorant, inconsistent agents","Thirtieth AAAI Conference on Artificial Intelligence","","","","","","2016","2022-01-30 04:53:18","2022-01-30 04:53:18","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000091","","/Users/jacquesthibodeau/Zotero/storage/IT7ZXEC3/12476.html","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PWCWSMIK","conferencePaper","2015","Evans, Owain; Goodman, Noah D","Learning the Preferences of Bounded Agents","NIPS Workshop on Bounded Optimality","","","","","","2015","2022-01-30 04:53:18","2022-01-30 04:53:18","","7","","","6","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000029  9 J:19","","/Users/jacquesthibodeau/Zotero/storage/3IWX99PK/Evans and Goodman - Learning the Preferences of Bounded Agents.pdf","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4PN2QVX8","bookSection","2014","Bostrom, Nick","Introduction—The Transhumanist FAQ: A General Introduction","Transhumanism and the Body","","","","","","2014","2022-01-30 04:53:18","2022-01-30 04:53:18","","1–17","","","","","","Introduction—The Transhumanist FAQ","","","","","Springer","","","","","","","Google Scholar","","ZSCC: 0000022","","/Users/jacquesthibodeau/Zotero/storage/U54KKT8S/9781137342768_1.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KJG4W372","report","2021","Zaidi, Waqar; Dafoe, Allan","International Control of Powerful Technology: Lessons from the Baruch Plan for Nuclear Weapons","","","","","https://www.fhi.ox.ac.uk/wp-content/uploads/2021/03/International-Control-of-Powerful-Technology-Lessons-from-the-Baruch-Plan-Zaidi-Dafoe-2021.pdf","The invention of atomic energy posed a novel global challenge: could the technology be controlled to avoid destructive uses and an existentially dangerous arms race while permitting the broad sharing of its benefits? From 1944 onwards, scientists, policymakers, and other t echnical specialists began to confront this challenge and explored policy options for dealing with the impact of nuclear technology. We focus on the years 1944 to 1951 and review this period for lessons for the governance of powerful technologies, and find the following: Radical schemes for international control can get broad support when confronted by existentially dangerous technologies, but this support can be tenuous and cynical. Secrecy is likely to play an important, and perhaps harmful, role. The public sphere may be an important source of influence, both in general and in particular in favor of cooperation, but also one that is manipulable and poorly informed. Technical experts may play a critical role, but need to be politically savvy. Overall, policymaking may look more like “muddling through” than clear-eyed grand strategy. Cooperation may be risky, and there may be many obstacles to success.","2021-03","2022-01-30 04:53:18","2022-01-30 04:53:18","2021-11-14 18:24:38","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/IX5D2GDZ/Cihon et al. - 2020 - Should Artificial Intelligence Governance be Centr.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94KSQKR7","journalArticle","2021","Prunkl, Carina E. A.; Ashurst, Carolyn; Anderljung, Markus; Webb, Helena; Leike, Jan; Dafoe, Allan","Institutionalizing ethics in AI through broader impact requirements","Nature Machine Intelligence","","2522-5839","10.1038/s42256-021-00298-y","https://www.nature.com/articles/s42256-021-00298-y","Turning principles into practice is one of the most pressing challenges of artificial intelligence (AI) governance. In this Perspective, we reflect on a governance initiative by one of the world’s largest AI conferences. In 2020, the Conference on Neural Information Processing Systems (NeurIPS) introduced a requirement for submitting authors to include a statement on the broader societal impacts of their research. Drawing insights from similar governance initiatives, including institutional review boards (IRBs) and impact requirements for funding applications, we investigate the risks, challenges and potential benefits of such an initiative. Among the challenges, we list a lack of recognized best practice and procedural transparency, researcher opportunity costs, institutional and social pressures, cognitive biases and the inherently difficult nature of the task. The potential benefits, on the other hand, include improved anticipation and identification of impacts, better communication with policy and governance experts, and a general strengthening of the norms around responsible research. To maximize the chance of success, we recommend measures to increase transparency, improve guidance, create incentives to engage earnestly with the process, and facilitate public deliberation on the requirement’s merits and future. Perhaps the most important contribution from this analysis are the insights we can gain regarding effective community-based governance and the role and responsibility of the AI research community more broadly.","2021-02","2022-01-30 04:53:18","2022-01-30 04:53:18","2021-10-31 19:12:54","104-110","","2","3","","Nat Mach Intell","","","","","","","","en","2021 Springer Nature Limited","","","","www.nature.com","","ZSCC: 0000005  Bandiera_abtest: a Cg_type: Nature Research Journals Number: 2 Primary_atype: Reviews Publisher: Nature Publishing Group Subject_term: Conferences and meetings;Policy;Publishing Subject_term_id: conferences-and-meetings;policy;publishing","","/Users/jacquesthibodeau/Zotero/storage/GQEBEVD3/Prunkl et al. - 2021 - Institutionalizing ethics in AI through broader im.pdf","","MetaSafety","Conferences and meetings; Policy; Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZM4AF64","manuscript","2017","Armstrong, Stuart; O'Rourke, Xavier","'Indifference' methods for managing agent rewards","","","","","https://arxiv.org/abs/1712.06365","","2017","2022-01-30 04:53:18","2022-01-30 04:53:18","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000012","","/Users/jacquesthibodeau/Zotero/storage/KX5XHD8P/Armstrong and O'Rourke - 2018 - 'Indifference' methods for managing agent rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/K98E6NZA/1712.html; /Users/jacquesthibodeau/Zotero/storage/GVC3FDWX/Armstrong and O'Rourke - 2017 - 'Indifference'methods for managing agent rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/3R2JSKSJ/1712.html","","TechSafety; FHI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UIU789GM","blogPost","2020","Armstrong, Stuart","If I were a well-intentioned AI... I: Image classifier","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/gzWb5kWwzhdaqmyTt/if-i-were-a-well-intentioned-ai-i-image-classifier","INTRODUCTION: IF I WERE A WELL-INTENTIONED AI... I've often warned people about the dangers of anthropomorphising AIs - how it can mislead us about what's really going on in an AI (and hence how the AI might act in the future), cause us to not even consider certain failure modes, and make us believe we understand things much better than we do. Oh well, let's ignore all that. I'm about to go on a journey of major anthropomorphisation, by asking myself:  * ""If I was a well-intentioned AI, could I solve many of the problems in AI    alignment?"" My thinking in this way started when I wondered: suppose I knew that I was given a proxy goal rather than the true goal; suppose that I knew about the Goodhart problem, and suppose that I really ""wanted"" to align with the true goal - could I then do it? I was having similar thoughts about being a mesa-optimiser. It seems to me that asking and answering these kind of questions leads to new and interesting insights. Of course, since they come via anthropomorphisation, we need to be careful with them, and check that they are really applicable to AI systems - ensuring that I'm not bringing some of my own human knowledge about human values into the example. But first, let's get those initial insights. OVERLAPPING PROBLEMS, OVERLAPPING SOLUTIONS At a high enough level of abstraction, many problems in AI alignment seem very similar. The Goodhart problem, the issues machine learning has with  distributional shift, the problem of the nearest unblocked strategy,  unidentifiability of reward functions, even mesaoptimisation and the whole AI alignment problem itself - all of these can be seen, roughly, as variants of the same problem. That problem being that we have an approximately specified goal that looks ok, but turns out to be underspecified in dangerous ways. Of course, often the differences between the problems are as important as the similarities. Nevertheless, the similarities exist, which is why a lot of the solutions are","2020-02-26","2022-01-30 04:53:18","2022-01-30 04:53:18","2020-09-05 18:38:33","","","","","","","If I were a well-intentioned AI... I","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/9VWKU6G5/gzWb5kWwzhdaqmyTt.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z428QPER","report","2020","O’Keefe, Cullen","How Will National Security Considerations Affect Antitrust Decisions in AI? An Examination of Historical Precedents","","","","","","","2020-07-07","2022-01-30 04:53:18","2022-01-30 04:53:18","","39","","","","","","","","","","","Future of Humanity Institute","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: 2","","/Users/jacquesthibodeau/Zotero/storage/9B5NGJR2/O’Keefe - How Will National Security Considerations Affect A.pdf","","MetaSafety; FHI; Open-AI; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SP37AAU4","bookSection","2015","Armstrong, Stuart; Sotala, Kaj","How we’re predicting AI–or failing to","Beyond artificial intelligence","","","","","","2015","2022-01-30 04:53:18","2022-01-30 04:53:18","","11–29","","","","","","","","","","","Springer","","","","","","","Google Scholar","","ZSCC: 0000111","","/Users/jacquesthibodeau/Zotero/storage/TGPD5XXX/Armstrong and Sotala - 2015 - How we’re predicting AI–or failing to.pdf; /Users/jacquesthibodeau/Zotero/storage/8KEC66PJ/978-3-319-09668-1_2.html","","MetaSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7R7WMB83","blogPost","2015","Tomasik, Brian","A Dialogue on Suffering Subroutines","Center on Long-Term Risk","","","","https://longtermrisk.org/a-dialogue-on-suffering-subroutines/","This piece presents a hypothetical dialogue that explains why instrumental computational processes of a future superintelligence might evoke moral concern. Generally, agent-like components might emerge in many places, including the computing processes of a future civilization. Whether and how much these subroutines matter are questions for future generations to figure out, but it's good to keep an open mind to the possibility that our intuitions about what suffering is may change dramatically.","2015-08-29","2022-01-30 04:51:06","2022-01-30 04:51:06","2020-11-23 01:05:16","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/GHRVZSNG/a-dialogue-on-suffering-subroutines.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CUMZ3TRE","blogPost","2017","Oesterheld, Caspar","A behaviorist approach to building phenomenological bridges","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2017/10/22/a-behaviorist-approach-to-building-phenomenological-bridges/","A few weeks ago, I wrote about the BPB problem and how it poses a problem for classical/non-logical decision theories. In my post, I briefly mentioned a behaviorist approach to BPB, only to immedia…","2017-10-22","2022-01-30 04:51:06","2022-01-30 04:51:06","2020-11-23 00:45:20","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VEDV2NXS/a-behaviorist-approach-to-building-phenomenological-bridges.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EGQMWQFR","blogPost","2015","Tomasik, Brian","Artificial Intelligence and Its Implications for Future Suffering","Center on Long-Term Risk","","","","https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering/","Artificial intelligence (AI) will likely transform the world later this century. Whether uncontrolled or controlled AIs would create more suffering in expectation is a question to explore further. Regardless, the field of AI safety and policy seems to be a very important space where altruists can make a positive-sum impact along many dimensions.","2015-04-10","2022-01-30 04:51:06","2022-01-30 04:51:06","2020-11-23 01:02:42","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QD8MQPCV","blogPost","2017","Caspar","A survey of polls on Newcomb’s problem","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2017/06/27/a-survey-of-polls-on-newcombs-problem/","One classic story about Newcomb’s problem is that, at least initially, people one-box and two-box in roughly equal numbers (and that everyone is confident in their position). To find out whet…","2017-06-27","2022-01-30 04:51:06","2022-01-30 04:51:06","2020-11-23 20:03:16","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/EPR7NUV9/a-survey-of-polls-on-newcombs-problem.html","","CLR; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2J7P366","blogPost","2015","Tomasik, Brian","A Lower Bound on the Importance of Promoting Cooperation","Center on Long-Term Risk","","","","https://longtermrisk.org/a-lower-bound-on-the-importance-of-promoting-cooperation/","This article suggests a lower-bound Fermi calculation for the cost-effectiveness of promoting cooperation. The purpose of this exercise is to make our thinking more concrete about how cooperation might reduce suffering and to make its potential more tangible.","2015-08-29","2022-01-30 04:51:06","2022-01-30 04:51:06","2020-11-23 01:04:00","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/EPJR83PC/a-lower-bound-on-the-importance-of-promoting-cooperation.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S55X7DHC","manuscript","2016","Critch, Andrew","Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents","","","","","http://arxiv.org/abs/1602.04184","Löb's theorem and Gödel's theorems make predictions about the behavior of systems capable of self-reference with unbounded computational resources with which to write and evaluate proofs. However, in the real world, systems capable of self-reference will have limited memory and processing speed, so in this paper we introduce an effective version of L\""ob's theorem which is applicable given such bounded resources. These results have powerful implications for the game theory of bounded agents who are able to write proofs about themselves and one another, including the capacity to out-perform classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner's Dilemma. Previous cooperative program equilibria studied by Tennenholtz (2004) and Fortnow (2009) have depended on tests for program equality, a fragile condition, whereas ""L\""obian"" cooperation is much more robust and agnostic of the opponent's implementation.","2016-08-24","2022-01-30 04:50:56","2022-01-30 04:50:56","2019-12-16 02:30:38","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000005[s0]  arXiv: 1602.04184","","/Users/jacquesthibodeau/Zotero/storage/W9BVMS95/Critch - 2016 - Parametric Bounded Lob's Theorem and Robust Coop.pdf; /Users/jacquesthibodeau/Zotero/storage/UVHR7XHR/Critch - 2016 - Parametric Bounded Lob's Theorem and Robust Coop.pdf; /Users/jacquesthibodeau/Zotero/storage/XWNIBX84/1602.html; /Users/jacquesthibodeau/Zotero/storage/43T9ZUX6/1602.html; /Users/jacquesthibodeau/Zotero/storage/ZXV2JJQK/1602.html","","CHAI; TechSafety; MIRI","Computer Science - Computer Science and Game Theory; Computer Science - Logic in Computer Science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AD9HNBFJ","manuscript","2017","Hadﬁeld, Gillian K; Hadﬁeld-Menell, Dylan","Pervasive Spurious Normativity","","","","","","This paper proposes a mathematical model for a simpliﬁed version of the game deﬁned in Hadﬁeld and Weingast [2012] which proposes that legal order can be described as an equilibrium in thirdparty decentralized enforcement coordinated by a centralized classiﬁcation institution. We explore the attractiveness of joining a new group (which is assumed to have settled on an enforcement equilibrium already) where groups differ in terms of the frequency of interactions in which norm violation is possible (normative interactions) and thus punishment is called for. We show that groups in which normative interactions are frequent but involve relatively unimportant rules may achieve higher value for participants.","2017","2022-01-30 04:50:56","2022-01-30 04:50:56","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000[s0]","","/Users/jacquesthibodeau/Zotero/storage/JAS5344V/Hadﬁeld and Hadﬁeld-Menell - Pervasive Spurious Normativity.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DW4JCPPD","conferencePaper","2021","Turner, Alexander Matt; Smith, Logan; Shah, Rohin; Critch, Andrew; Tadepalli, Prasad","Optimal Policies Tend to Seek Power","arXiv:1912.01683 [cs]","","","","http://arxiv.org/abs/1912.01683","Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers are skeptical, because RL agents need not have human-like power-seeking instincts. To clarify this debate, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.","2021-10-23","2022-01-30 04:50:56","2022-01-30 04:50:56","2021-11-14 18:44:10","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s0]  ACC: 1  arXiv: 1912.01683","","/Users/jacquesthibodeau/Zotero/storage/6M65PKWS/Turner et al. - 2021 - Optimal Policies Tend to Seek Power.pdf; /Users/jacquesthibodeau/Zotero/storage/RDF4U9VH/1912.html","","TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","35th Conference on Neural Information Processing Systems (NeurIPS 2021)","","","","","","","","","","","","","","",""
"H56ZKQ9D","conferencePaper","2019","Carroll, Micah; Shah, Rohin; Ho, Mark K.; Griffiths, Thomas L.; Seshia, Sanjit A.; Abbeel, Pieter; Dragan, Anca","On the Utility of Learning about Humans for Human-AI Coordination","Advances in Neural Information Processing Systems 32 (NeurIPS 2019)","","","","https://arxiv.org/abs/1910.05789v2","While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked_ai.","2019-10-13","2022-01-30 04:50:55","2022-01-30 04:50:55","2020-11-14 01:20:48","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000062","","/Users/jacquesthibodeau/Zotero/storage/564DIJ7M/Carroll et al. - On the Utility of Learning about Humans for Human-.pdf; /Users/jacquesthibodeau/Zotero/storage/N29XCQCE/Carroll et al. - 2019 - On the Utility of Learning about Humans for Human-.pdf; /Users/jacquesthibodeau/Zotero/storage/NKUA5J28/1910.html; /Users/jacquesthibodeau/Zotero/storage/7DUA2DSC/1910.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2019","","","","","","","","","","","","","","",""
"Q44NESEK","conferencePaper","2019","Huang, Sandy H.; Huang, Isabella; Pandya, Ravi; Dragan, Anca D.","Nonverbal Robot Feedback for Human Teachers","Proceedings of the Conference on Robot Learning","","","","http://arxiv.org/abs/1911.02320","Robots can learn preferences from human demonstrations, but their success depends on how informative these demonstrations are. Being informative is unfortunately very challenging, because during teaching, people typically get no transparency into what the robot already knows or has learned so far. In contrast, human students naturally provide a wealth of nonverbal feedback that reveals their level of understanding and engagement. In this work, we study how a robot can similarly provide feedback that is minimally disruptive, yet gives human teachers a better mental model of the robot learner, and thus enables them to teach more effectively. Our idea is that at any point, the robot can indicate what it thinks the correct next action is, shedding light on its current estimate of the human's preferences. We analyze how useful this feedback is, both in theory and with two user studies---one with a virtual character that tests the feedback itself, and one with a PR2 robot that uses gaze as the feedback mechanism. We find that feedback can be useful for improving both the quality of teaching and teachers' understanding of the robot's capability.","2019-11-06","2022-01-30 04:50:55","2022-01-30 04:50:55","2019-12-18 02:41:20","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 1911.02320","","/Users/jacquesthibodeau/Zotero/storage/AA6GDGDX/Huang et al. - 2019 - Nonverbal Robot Feedback for Human Teachers.pdf; /Users/jacquesthibodeau/Zotero/storage/CR3FFFGV/1911.html","","CHAI; TechSafety","Computer Science - Machine Learning; Computer Science - Robotics; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Conference on Robot Learning","","","","","","","","","","","","","","",""
"3I86NJCR","bookSection","2018","Desai, Nishant; Critch, Andrew; Russell, Stuart J","Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making","Advances in Neural Information Processing Systems 31","","","","http://papers.nips.cc/paper/7721-negotiable-reinforcement-learning-for-pareto-optimal-sequential-decision-making.pdf","","2018","2022-01-30 04:50:55","2022-01-30 04:50:55","2019-12-18 02:14:39","4712–4720","","","","","","","","","","","Curran Associates, Inc.","","","","","","","Neural Information Processing Systems","","ZSCC: NoCitationData[s2]  ACC: 5","","/Users/jacquesthibodeau/Zotero/storage/5WS4PP5G/Desai et al. - 2018 - Negotiable Reinforcement Learning for Pareto Optim.pdf; /Users/jacquesthibodeau/Zotero/storage/UN92H3UR/7721-negotiable-reinforcement-learning-for-pareto-optimal-sequential-decision-making.html","","CHAI; TechSafety","","Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; Garnett, R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5FN3T5EH","conferencePaper","2015","Hadﬁeld-Menell, Dylan; Russell, Stuart","Multitasking: Efﬁcient Optimal Planning for Bandit Superprocesses","Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence","","","","","A bandit superprocess is a decision problem composed from multiple independent Markov decision processes (MDPs), coupled only by the constraint that, at each time step, the agent may act in only one of the MDPs. Multitasking problems of this kind are ubiquitous in the real world, yet very little is known about them from a computational viewpoint, beyond the observation that optimal policies for the superprocess may prescribe actions that would be suboptimal for an MDP considered in isolation. (This observation implies that many applications of sequential decision analysis in practice are technically incorrect, since the decision problem being solved is often part of a larger, unstated bandit superprocess.) The paper summarizes the state-of-theart in the theory of bandit superprocesses and contributes a novel upper bound on the global value function of a bandit superprocess, deﬁned in terms of a direct relaxation of the arms. The bound is equivalent to an existing bound (the Whittle integral), but is deﬁned constructively, as the value of a related multi-armed bandit. We provide a new method to compute this bound and derive the ﬁrst practical algorithm to select optimal actions in bandit superprocesses. The algorithm operates by repeatedly establishing dominance relations between actions using upper and lower bounds on action values. Experiments indicate that the algorithm’s run-time compares very favorably to other possible algorithms designed for more general factored MDPs.","2015-07","2022-01-30 04:50:55","2022-01-30 04:50:55","","10","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/FTI9E6Q8/Hadﬁeld-Menell and Russell - Multitasking Efﬁcient Optimal Planning for Bandit.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBIVNF95","conferencePaper","2018","Gleave, Adam; Habryka, Oliver","Multi-task Maximum Entropy Inverse Reinforcement Learning","","","","","http://arxiv.org/abs/1805.08882","Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.","2018-07-15","2022-01-30 04:50:55","2022-01-30 04:50:55","2019-12-18 01:12:51","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000020  arXiv: 1805.08882","","/Users/jacquesthibodeau/Zotero/storage/5V48367F/Gleave and Habryka - 2018 - Multi-task Maximum Entropy Inverse Reinforcement L.pdf","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","1st Workshop on Goal Specifications for Reinforce- ment Learning, FAIM 2018","","","","","","","","","","","","","","",""
"V5MMVQWT","conferencePaper","2020","Fickinger, Arnaud; Zhuang, Simon; Hadfield-Menell, Dylan; Russell, Stuart","Multi-Principal Assistance Games","","","","","http://arxiv.org/abs/2007.09540","Assistance games (also known as cooperative inverse reinforcement learning games) have been proposed as a model for beneficial AI, wherein a robotic agent must act on behalf of a human principal but is initially uncertain about the humans payoff function. This paper studies multi-principal assistance games, which cover the more general case in which the robot acts on behalf of N humans who may have widely differing payoffs. Impossibility theorems in social choice theory and voting theory can be applied to such games, suggesting that strategic behavior by the human principals may complicate the robots task in learning their payoffs. We analyze in particular a bandit apprentice game in which the humans act first to demonstrate their individual preferences for the arms and then the robot acts to maximize the sum of human payoffs. We explore the extent to which the cost of choosing suboptimal arms reduces the incentive to mislead, a form of natural mechanism design. In this context we propose a social choice method that uses shared control of a system to combine preference inference with social welfare optimization.","2020-07-18","2022-01-30 04:50:55","2022-01-30 04:50:55","2020-08-28 17:24:47","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000007  arXiv: 2007.09540","","/Users/jacquesthibodeau/Zotero/storage/CTGBBT4P/Fickinger et al. - 2020 - Multi-Principal Assistance Games.pdf; /Users/jacquesthibodeau/Zotero/storage/KRF8QCZJ/2007.html","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML 2020","","","","","","","","","","","","","","",""
"6X7RBM9U","conferencePaper","2018","Milli, Smitha; Schmidt, Ludwig; Dragan, Anca D.; Hardt, Moritz","Model Reconstruction from Model Explanations","FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency","","","","http://arxiv.org/abs/1807.05185","We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations. On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive. Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.","2018-07-13","2022-01-30 04:50:55","2022-01-30 04:50:55","2019-12-18 01:13:24","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 69  J: 31 arXiv: 1807.05185","","/Users/jacquesthibodeau/Zotero/storage/QSGCSN5T/Milli et al. - 2018 - Model Reconstruction from Model Explanations.pdf; /Users/jacquesthibodeau/Zotero/storage/D5ADWGHJ/Milli et al. - 2018 - Model Reconstruction from Model Explanations.pdf; /Users/jacquesthibodeau/Zotero/storage/83WVJFJX/1807.html; /Users/jacquesthibodeau/Zotero/storage/S3VPKVIG/1807.html","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Conference on Fairness, Accountability, and Transparency","","","","","","","","","","","","","","",""
"4NRTTKVW","conferencePaper","2018","Zhang, Shun; Durfee, Edmund H.; Singh, Satinder","Minimax-Regret Querying on Side Effects for Safe Optimality in Factored Markov Decision Processes","Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence","978-0-9992411-2-7","","10.24963/ijcai.2018/676","https://www.ijcai.org/proceedings/2018/676","As it achieves a goal on behalf of its human user, an autonomous agent’s actions may have side effects that change features of its environment in ways that negatively surprise its user. An agent that can be trusted to operate safely should thus only change features the user has explicitly permitted. We formalize this problem, and develop a planning algorithm that avoids potentially negative side effects given what the agent knows about (un)changeable features. Further, we formulate a provably minimax-regret querying strategy for the agent to selectively ask the user about features that it hasn’t explicitly been told about. We empirically show how much faster it is than a more exhaustive approach and how much better its queries are than those found by the best known heuristic.","2018-07","2022-01-30 04:50:55","2022-01-30 04:50:55","2020-11-14 01:15:22","4867-4873","","","","","","","","","","","International Joint Conferences on Artificial Intelligence Organization","Stockholm, Sweden","en","","","","","DOI.org (Crossref)","","ZSCC: 0000023","","/Users/jacquesthibodeau/Zotero/storage/XMWT43ZV/Zhang et al. - 2018 - Minimax-Regret Querying on Side Effects for Safe O.pdf","","CHAI; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Twenty-Seventh International Joint Conference on Artificial Intelligence {IJCAI-18}","","","","","","","","","","","","","","",""
"RMFTWTV3","blogPost","2021","Steinhardt, Jacob","Measurement, Optimization, and Take-off Speed","Jacob Steinhardt","","","","https://jsteinhardt.stat.berkeley.edu/blog/measurement-and-optimization","In machine learning, we are obsessed with datasets and metrics: progress in areas as diverse as natural language understanding, object recognition, and reinforcement learning is tracked by numerical scores on agreed-upon benchmarks. Despite this, I think we focus too little on measurement—that is, on ways of extracting data from machine learning models that bears upon important hypotheses. This might sound paradoxical, since benchmarks are after all one way of measuring a model. However, benchmarks are a very narrow form of measurement, and I will argue below that trying to measure pretty much anything you can think of is a good mental move that is heavily underutilized in machine learning. I’ll argue this in three ways: Historically, more measurement has almost always been a great move, not only in science but also in engineering and policymaking. Philosophically, measurement has many good properties that bear upon important questions in ML. In my own research, just measuring something and seeing what happened has often been surprisingly fruitful.","2021-04-07","2022-01-30 04:50:55","2022-01-30 04:50:55","2021-11-14 19:03:14","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6IF9UF8E/measurement-and-optimization.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZEWADP4","conferencePaper","2019","Milli, Smitha; Dragan, Anca D.","Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning","Proceedings of The 35th Uncertainty in Artificial Intelligence Conference","","","","http://proceedings.mlr.press/v115/milli20a.html","It is incredibly easy for a system designer to misspecify the objective for an autonomous system (“robot”), thus motivating the desire to have the robot learn the objective from human behavior instead. Recent work has suggested that people have an interest in the robot performing well, and will thus behave pedagogically, choosing actions that are informative to the robot. In turn, robots beneﬁt from interpreting the behavior by accounting for this pedagogy. In this work, we focus on misspeciﬁcation: we argue that robots might not know whether people are being pedagogic or literal and that it is important to ask which assumption is safer to make. We cast objective learning into the more general form of a common-payoff game between the robot and human, and prove that in any such game literal interpretation is more robust to misspeciﬁcation. Experiments with human data support our theoretical results and point to the sensitivity of the pedagogic assumption.","2019-06-28","2022-01-30 04:50:55","2022-01-30 04:50:55","2020-12-20","","","","","","","Literal or Pedagogic Human?","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000003[s0]  arXiv: 1903.03877","","/Users/jacquesthibodeau/Zotero/storage/4532ZRGJ/Milli and Dragan - 2019 - Literal or Pedagogic Human Analyzing Human Model .pdf","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The 35th Uncertainty in Artificial Intelligence Conference","","","","","","","","","","","","","","",""
"ZJGDKBG2","conferencePaper","2019","Hadfield-Menell, Dylan; Andrus, McKane; Hadfield, Gillian K.","Legible Normativity for AI Alignment: The Value of Silly Rules","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","","","","http://arxiv.org/abs/1811.01267","It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior–social norms and laws. But human laws and norms are complex and culturally varied systems; in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules —rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.","2019","2022-01-30 04:50:55","2022-01-30 04:50:55","2019-07-08 15:47:44","","","","","","","Legible Normativity for AI Alignment","","","","","","","en","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 6  J: 2 arXiv: 1811.01267","","/Users/jacquesthibodeau/Zotero/storage/9N9AXA2T/Hadfield-Menell et al. - 2018 - Legible Normativity for AI Alignment The Value of.pdf; /Users/jacquesthibodeau/Zotero/storage/TRSUMM4D/1811.html","","CHAI; TechSafety","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"46HGF7X4","manuscript","2020","Turner, Alexander Matt; Smith, Logan; Shah, Rohin; Tadepalli, Prasad","Optimal Farsighted Agents Tend to Seek Power","","","","","http://arxiv.org/abs/1912.01683","Some researchers have speculated that capable reinforcement learning (RL) agents pursuing misspecified objectives are often incentivized to seek resources and power in pursuit of those objectives. An agent seeking power is incentivized to behave in undesirable ways, including rationally preventing deactivation and correction. Others have voiced skepticism: humans seem idiosyncratic in their urges to power, which need not be present in the agents we design. We formalize a notion of power within the context of finite Markov decision processes (MDPs). With respect to a neutral class of reward function distributions, our results suggest that farsighted optimal policies tend to seek power over the environment.","2020-06-05","2022-01-30 04:50:55","2022-01-30 04:50:55","2020-11-21 17:26:57","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 1912.01683","","/Users/jacquesthibodeau/Zotero/storage/P5BE9NS6/Turner et al. - 2020 - Optimal Farsighted Agents Tend to Seek Power.pdf; /Users/jacquesthibodeau/Zotero/storage/4HCESMWM/1912.html","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D6VXDQ3V","conferencePaper","2019","Choudhury, Rohan; Swamy, Gokul; Hadfield-Menell, Dylan; Dragan, Anca","On the Utility of Model Learning in HRI","2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","","","","http://arxiv.org/abs/1901.01291","Fundamental to robotics is the debate between model-based and model-free learning: should the robot build an explicit model of the world, or learn a policy directly? In the context of HRI, part of the world to be modeled is the human. One option is for the robot to treat the human as a black box and learn a policy for how they act directly. But it can also model the human as an agent, and rely on a “theory of mind” to guide or bias the learning (grey box). We contribute a characterization of the performance of these methods under the optimistic case of having an ideal theory of mind, as well as under different scenarios in which the assumptions behind the robot’s theory of mind for the human are wrong, as they inevitably will be in practice. We ﬁnd that there is a signiﬁcant sample complexity advantage to theory of mind methods and that they are more robust to covariate shift, but that when enough interaction data is available, black box approaches eventually dominate.","2019-01-04","2022-01-30 04:50:55","2022-01-30 04:50:55","2019-07-08 15:45:07","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000026  arXiv: 1901.01291","","/Users/jacquesthibodeau/Zotero/storage/NZN9AB88/Choudhury et al. - 2019 - On the Utility of Model Learning in HRI.pdf","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","","","","","","","","","","","","","","",""
"D5PF7N4F","conferencePaper","2019","Shah, Rohin; Gundotra, Noah; Abbeel, Pieter; Dragan, Anca D.","On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference","Proceedings of the 36th International Conference on Machine Learning","","","","http://arxiv.org/abs/1906.09624","Our goal is for agents to optimize the right reward function, despite how difﬁcult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with speciﬁc assumptions, and instead use a purely data-driven approach. We decided to put this to the test – rather than relying on assumptions about which speciﬁc bias the demonstrator has when planning, we instead learn the demonstrator’s planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed ﬁndings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this beneﬁt is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the ﬂexibility of data-driven methods and the useful bias of known human biases. Code is available at https: //tinyurl.com/learningbiases.","2019-06-23","2022-01-30 04:50:55","2022-01-30 04:50:55","2019-07-11 18:35:30","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000020  arXiv: 1906.09624","","/Users/jacquesthibodeau/Zotero/storage/TTNVB7MM/Shah et al. - 2019 - On the Feasibility of Learning, Rather than Assumi.pdf; /Users/jacquesthibodeau/Zotero/storage/5WHA9REK/1906.html; /Users/jacquesthibodeau/Zotero/storage/4DMXNS7E/Shah et al. - 2019 - On the Feasibility of Learning, Rather than Assumi.pdf","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","36th International Conference on Machine Learning","","","","","","","","","","","","","","",""
"A2T8FD8D","manuscript","2020","Ndousse, Kamal; Eck, Douglas; Levine, Sergey; Jaques, Natasha","Multi-agent Social Reinforcement Learning Improves Generalization","","","","","http://arxiv.org/abs/2010.00581","Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can use social learning to improve their performance using cues from other agents. We find that in most circumstances, vanilla model-free RL agents do not use social learning, even in environments in which individual exploration is expensive. We analyze the reasons for this deficiency, and show that by introducing a model-based auxiliary loss we are able to train agents to lever-age cues from experts to solve hard exploration tasks. The generalized social learning policy learned by these agents allows them to not only outperform the experts with which they trained, but also achieve better zero-shot transfer performance than solo learners when deployed to novel environments with experts. In contrast, agents that have not learned to rely on social learning generalize poorly and do not succeed in the transfer task. Further,we find that by mixing multi-agent and solo training, we can obtain agents that use social learning to out-perform agents trained alone, even when experts are not avail-able. This demonstrates that social learning has helped improve agents' representation of the task itself. Our results indicate that social learning can enable RL agents to not only improve performance on the task at hand, but improve generalization to novel environments.","2020-10-01","2022-01-30 04:50:55","2022-01-30 04:50:55","2020-11-14 00:37:58","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000002  arXiv: 2010.00581","","/Users/jacquesthibodeau/Zotero/storage/RX2T5UXT/Ndousse et al. - 2020 - Multi-agent Social Reinforcement Learning Improves.pdf; /Users/jacquesthibodeau/Zotero/storage/UJZ4W3NC/2010.html","","CHAI; TechSafety; Open-AI; AmbiguosSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TK4DP9CE","conferencePaper","2021","Lindner, David; Shah, Rohin; Abbeel, Pieter; Dragan, Anca","Learning What To Do By Simulating the Past","","","","","","Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state (Shah et al., 2019). Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a speciﬁc skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.","2021","2022-01-30 04:50:55","2022-01-30 04:50:55","","24","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/XAPXNEM2/Lindner et al. - 2021 - LEARNING WHAT TO DO BY SIMULATING THE PAST.pdf","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2021","","","","","","","","","","","","","","",""
"8Z4999WK","conferencePaper","2018","Bobu, Andreea; Bajcsy, Andrea; Fisac, Jaime F.; Dragan, Anca D.","Learning under Misspecified Objective Spaces","2nd Conference on Robot Learning (CoRL 2018)","","","","http://arxiv.org/abs/1810.05157","Learning robot objective functions from human input has become increasingly important, but state-of-the-art techniques assume that the human’s desired objective lies within the robot’s hypothesis space. When this is not true, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. We focus speciﬁcally on learning from physical human corrections during the robot’s task execution, where not having a rich enough hypothesis space leads to the robot updating its objective in ways that the person did not actually intend. We observe that such corrections appear irrelevant to the robot, because they are not the best way of achieving any of the candidate objectives. Instead of naively trusting and learning from every human interaction, we propose robots learn conservatively by reasoning in real time about how relevant the human’s correction is for the robot’s hypothesis space. We test our inference method in an experiment with human interaction data, and demonstrate that this alleviates unintended learning in an in-person user study with a robot manipulator.","2018-10-26","2022-01-30 04:50:54","2022-01-30 04:50:54","2019-12-18 02:38:36","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000015  arXiv: 1810.05157","","/Users/jacquesthibodeau/Zotero/storage/BHXJU7S4/Bobu et al. - 2018 - Learning under Misspecified Objective Spaces.pdf","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2nd Conference on Robot Learning (CoRL 2018)","","","","","","","","","","","","","","",""
"H4ETM44R","conferencePaper","2019","Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey; Legg, Shane; Leike, Jan","Learning Human Objectives by Evaluating Hypothetical Behavior","Proceedings of the 37th International Conference on Machine Learning","","","","http://proceedings.mlr.press/v119/reddy20a.html","We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.","2019-12-05","2022-01-30 04:50:54","2022-01-30 04:50:54","2020-12-20","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000005[s0]  arXiv: 1912.05652","","/Users/jacquesthibodeau/Zotero/storage/RGR6BVZQ/Reddy et al. - 2019 - Learning Human Objectives by Evaluating Hypothetic.pdf; /Users/jacquesthibodeau/Zotero/storage/E85ASZIU/1912.html; /Users/jacquesthibodeau/Zotero/storage/QJDICWCK/1912.html","","CHAI; TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","37th International Conference on Machine Learning","","","","","","","","","","","","","","",""
"4TS7JII8","conferencePaper","2018","Basu, Chandrayee; Singhal, Mukesh; Dragan, Anca D.","Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries","Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18","","","10.1145/3171221.3171284","http://arxiv.org/abs/1802.01604","We focus on learning the desired objective function for a robot. Although trajectory demonstrations can be very informative of the desired objective, they can also be difficult for users to provide. Answers to comparison queries, asking which of two trajectories is preferable, are much easier for users, and have emerged as an effective alternative. Unfortunately, comparisons are far less informative. We propose that there is much richer information that users can easily provide and that robots ought to leverage. We focus on augmenting comparisons with feature queries, and introduce a unified formalism for treating all answers as observations about the true desired reward. We derive an active query selection algorithm, and test these queries in simulation and on real users. We find that richer, feature-augmented queries can extract more information faster, leading to robots that better match user preferences in their behavior.","2018","2022-01-30 04:50:54","2022-01-30 04:50:54","2019-12-18 02:40:57","132-140","","","","","","Learning from Richer Human Guidance","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000031","","/Users/jacquesthibodeau/Zotero/storage/5GQZRWB7/Basu et al. - 2018 - Learning from Richer Human Guidance Augmenting Co.pdf; /Users/jacquesthibodeau/Zotero/storage/WMF3FHW2/1802.html","","CHAI; TechSafety; AmbiguosSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6W8J2P4W","conferencePaper","2018","Bajcsy, Andrea; Losey, Dylan P.; O'Malley, Marcia K.; Dragan, Anca D.","Learning from Physical Human Corrections, One Feature at a Time","Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18","978-1-4503-4953-6","","10.1145/3171221.3171267","http://dl.acm.org/citation.cfm?doid=3171221.3171267","We focus on learning robot objective functions from human guidance: specifically, from physical corrections provided by the person while the robot is acting. Objective functions are typically parametrized in terms of features, which capture aspects of the task that might be important. When the person intervenes to correct the robot’s behavior, the robot should update its understanding of which features matter, how much, and in what way. Unfortunately, real users do not provide optimal corrections that isolate exactly what the robot was doing wrong. Thus, when receiving a correction, it is difficult for the robot to determine which features the person meant to correct, and which features were changed unintentionally. In this paper, we propose to improve the efficiency of robot learning during physical interactions by reducing unintended learning. Our approach allows the human-robot team to focus on learning one feature at a time, unlike state-of-the-art techniques that update all features at once. We derive an online method for identifying the single feature which the human is trying to change during physical interaction, and experimentally compare this one-at-a-time approach to the all-at-once baseline in a user study. Our results suggest that users teaching one-at-a-time perform better, especially in tasks that require changing multiple features.","2018","2022-01-30 04:50:54","2022-01-30 04:50:54","2019-12-18 02:41:35","141-149","","","","","","","","","","","ACM Press","Chicago, IL, USA","en","","","","","DOI.org (Crossref)","","ZSCC: 0000063","","/Users/jacquesthibodeau/Zotero/storage/NAECZFHT/Bajcsy et al. - 2018 - Learning from Physical Human Corrections, One Feat.pdf","","CHAI; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the 2018 ACM/IEEE International Conference","","","","","","","","","","","","","","",""
"D65J6K2P","conferencePaper","2019","Zhao, Ruihan; Tiomkin, Stas; Abbeel, Pieter","Learning Efficient Representation for Intrinsic Motivation","","","","","http://arxiv.org/abs/1912.02624","Mutual Information between agent Actions and environment States (MIAS) quantifies the influence of agent on its environment. Recently, it was found that the maximization of MIAS can be used as an intrinsic motivation for artificial agents. In literature, the term empowerment is used to represent the maximum of MIAS at a certain state. While empowerment has been shown to solve a broad range of reinforcement learning problems, its calculation in arbitrary dynamics is a challenging problem because it relies on the estimation of mutual information. Existing approaches, which rely on sampling, are limited to low dimensional spaces, because high-confidence distribution-free lower bounds for mutual information require exponential number of samples. In this work, we develop a novel approach for the estimation of empowerment in unknown dynamics from visual observation only, without the need to sample for MIAS. The core idea is to represent the relation between action sequences and future states using a stochastic dynamic model in latent space with a specific form. This allows us to efficiently compute empowerment with the ""Water-Filling"" algorithm from information theory. We construct this embedding with deep neural networks trained on a sophisticated objective function. Our experimental results show that the designed embedding preserves information-theoretic properties of the original dynamics.","2019-12-08","2022-01-30 04:50:54","2022-01-30 04:50:54","2019-12-18 02:48:27","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000004  arXiv: 1912.02624","","/Users/jacquesthibodeau/Zotero/storage/C43XRQE9/Zhao et al. - 2019 - Learning Efficient Representation for Intrinsic Mo.pdf; /Users/jacquesthibodeau/Zotero/storage/ZMMKVSHT/1912.html","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","33rd Conference on Neural Information Processing Systems (NeurIPS 2019)","","","","","","","","","","","","","","",""
"QSHDXZW8","blogPost","2019","Shah, Rohin","Learning biases and rewards simultaneously","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/xxnPxELC4jLKaFKqG/learning-biases-and-rewards-simultaneously","I’ve finally uploaded to arXiv our work on inferring human biases alongside IRL, which was published at ICML 2019. SUMMARY OF THE PAPER THE IRL DEBATE Here’s a quick tour of the debate about inverse reinforcement learning (IRL) and cognitive biases, featuring many of the ideas from the first chapter of the  Value Learning sequence: I had the intuition that the impossibility theorem was like the other no-free-lunch theorems in ML: not actually relevant for what ML could do in practice. So we tried to learn and correct for systematic biases in IRL. THE IDEA BEHIND THE ALGORITHMS The basic idea was to learn the planning algorithm by which the human produces demonstrations, and try to ensure that the planning algorithm captured the appropriate systematic biases. We used a Value Iteration Network to give an inductive bias towards “planners” but otherwise did not assume anything about the form of the systematic bias. [1] Then, we could perform IRL by figuring out which reward would cause the planning algorithm to output the given demonstrations. The reward would be “debiased” because the effect of the biases on the policy would already be accounted for in the planning algorithm. How could we learn the planning algorithm? Well, one baseline method is to assume that we have access to some tasks where the rewards are known, and use those tasks to learn what the planning algorithm is. Then, once that is learned, we can infer the rewards for new tasks that we haven’t seen before. This requires the planner to generalize across tasks. However, it’s kind of cheating to assume access to ground truth rewards, since we usually wouldn’t have them. What if we learned the planning algorithm and rewards simultaneously? Well, the no-free-lunch theorem gets us then: maximizing the true reward and minimizing the negative of the true reward would lead to the same policy, and so you can’t distinguish between them, and so the output of your IRL algorithm could be the true reward or the","2019","2022-01-30 04:50:54","2022-01-30 04:50:54","2020-12-18 00:09:48","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/9QDS82F6/learning-biases-and-rewards-simultaneously.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GZDCFRPG","conferencePaper","1998","Russell, Stuart","Learning agents for uncertain environments","Proceedings of the eleventh annual conference on Computational learning theory  - COLT' 98","978-1-58113-057-7","","10.1145/279943.279964","http://portal.acm.org/citation.cfm?doid=279943.279964","","1998","2022-01-30 04:50:54","2022-01-30 04:50:54","2020-11-22 01:48:04","101-103","","","","","","","","","","","ACM Press","Madison, Wisconsin, United States","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 451","","/Users/jacquesthibodeau/Zotero/storage/U8UI78ZU/Russell - 1998 - Learning agents for uncertain environments (extend.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the eleventh annual conference","","","","","","","","","","","","","","",""
"T8G2GCWV","journalArticle","2019","Russell, Stuart","It's not too soon to be wary of AI: We need to act now to protect humanity from future superintelligent machines","IEEE Spectrum","","1939-9340","10.1109/MSPEC.2019.8847590","","AI research is making great strides toward its long-term goal of human-level or superhuman intelligent machines. If it succeeds in its current form, however, that could well be catastrophic for the human race. The reason is that the ""standard model"" of AI requires machines to pursue a fixed objective specified by humans. We are unable to specify the objective completely and correctly, nor can we anticipate or prevent the harms that machines pursuing an incorrect objective will create when operating on a global scale with superhuman capabilities. Already, we see examples such as social-media algorithms that learn to optimize click-through by manipulating human preferences, with disastrous consequences for democratic systems.","2019-10","2022-01-30 04:50:54","2022-01-30 04:50:54","","46-51","","10","56","","","It's not too soon to be wary of AI","","","","","","","","","","","","IEEE Xplore","","ZSCC: 0000005","","/Users/jacquesthibodeau/Zotero/storage/3QV56CCC/8847590.html","","CHAI; TechSafety","artificial intelligence; Artificial intelligence; AI research; Calculators; current form; democratic systems; Earth; fixed objective; human preferences; human race; human-level; humanities; humanity; Robots; Safety; social-media algorithms; standard model; Standards; superhuman capabilities; superhuman intelligent machines; superintelligent machines; Switches","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8EUNR79","conferencePaper","2017","Hadfield-Menell, Dylan; Milli, Smitha; Abbeel, Pieter; Russell, Stuart; Dragan, Anca","Inverse Reward Design","Advances in Neural Information Processing Systems 30 (NIPS 2017)","","","","http://arxiv.org/abs/1711.02827","Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.","2017","2022-01-30 04:50:54","2022-01-30 04:50:54","2020-11-22 04:15:56","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000204  arXiv: 1711.02827","","/Users/jacquesthibodeau/Zotero/storage/IG3G2WF9/Hadfield-Menell et al. - 2020 - Inverse Reward Design.pdf; /Users/jacquesthibodeau/Zotero/storage/M4MGUB4S/1711.html","","CHAI; TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H476ZK5U","conferencePaper","2017","Bajcsy, Andrea; Losey, Dylan P; O’Malley, Marcia K; Dragan, Anca D","Learning Robot Objectives from Physical Human Interaction","Proceedings of Machine Learning Research","","","","","When humans and robots work in close proximity, physical interaction is inevitable. Traditionally, robots treat physical interaction as a disturbance, and resume their original behavior after the interaction ends. In contrast, we argue that physical human interaction is informative: it is useful information about how the robot should be doing its task. We formalize learning from such interactions as a dynamical system in which the task objective has parameters that are part of the hidden state, and physical human interactions are observations about these parameters. We derive an online approximation of the robot’s optimal policy in this system, and test it in a user study. The results suggest that learning from physical interaction leads to better robot task performance with less human effort.","2017","2022-01-30 04:50:54","2022-01-30 04:50:54","","10","","","78","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000026[s2]","","/Users/jacquesthibodeau/Zotero/storage/B7V4VQ4Q/Bajcsy et al. - Learning Robot Objectives from Physical Human Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/CE6X2RSM/102348.html","","CHAI; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","1st Conference on Robot Learning (CoRL 2017)","","","","","","","","","","","","","","",""
"777W689G","conferencePaper","2019","Zhang, Jason Y.; Dragan, Anca D.","Learning from Extrapolated Corrections","2019 International Conference on Robotics and Automation (ICRA)","","","10.1109/ICRA.2019.8793554","","Our goal is to enable robots to learn cost functions from user guidance. Often it is difficult or impossible for users to provide full demonstrations, so corrections have emerged as an easier guidance channel. However, when robots learn cost functions from corrections rather than demonstrations, they have to extrapolate a small amount of information - the change of a waypoint along the way - to the rest of the trajectory. We cast this extrapolation problem as online function approximation, which exposes different ways in which the robot can interpret what trajectory the person intended, depending on the function space used for the approximation. Our simulation results and user study suggest that using function spaces with non-Euclidean norms can better capture what users intend, particularly if environments are uncluttered. This, in turn, can lead to the robot learning a more accurate cost function and improves the user's subjective perceptions of the robot.","2019-05","2022-01-30 04:50:54","2022-01-30 04:50:54","","7034-7040","","","","","","","","","","","","","","","","","","IEEE Xplore","","ZSCC: 0000008  ISSN: 1050-4729","","/Users/jacquesthibodeau/Zotero/storage/WDX9K5VV/8793554.html; /Users/jacquesthibodeau/Zotero/storage/Z27ARW82/Zhang and Dragan - 2019 - Learning from Extrapolated Corrections.pdf","","CHAI; TechSafety","control engineering computing; Cost function; cost functions; Estimation; extrapolated corrections; extrapolation; extrapolation problem; function approximation; Function approximation; function space; Kernel; learning (artificial intelligence); nonEuclidean norms; online function approximation; Robot kinematics; robot learning; robot programming; Trajectory; user guidance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 International Conference on Robotics and Automation (ICRA)","","","","","","","","","","","","","","",""
"CP3JGZGM","conferencePaper","2019","Xu, Kelvin; Ratner, Ellis; Dragan, Anca; Levine, Sergey; Finn, Chelsea","Learning a Prior over Intent via Meta-Inverse Reinforcement Learning","Proceedings of the 36th International Conference on Machine Learning","","","","http://arxiv.org/abs/1805.12573","A significant challenge for the practical application of reinforcement learning in the real world is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert behavior. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a ""prior"" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.","2019-10-14","2022-01-30 04:50:54","2022-01-30 04:50:54","2019-12-18 02:40:07","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000036  1 J: 11 arXiv: 1805.12573","","/Users/jacquesthibodeau/Zotero/storage/FBXXF6K7/Xu et al. - 2019 - Learning a Prior over Intent via Meta-Inverse Rein.pdf; /Users/jacquesthibodeau/Zotero/storage/5GDRRPWX/1805.html","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","36th International Conference on Machine Learning","","","","","","","","","","","","","","",""
"7EWBZBCW","manuscript","2018","Tucker, Aaron; Gleave, Adam; Russell, Stuart","Inverse reinforcement learning for video games","","","","","http://arxiv.org/abs/1810.10593","Deep reinforcement learning achieves superhuman performance in a range of video game environments, but requires that a designer manually specify a reward function. It is often easier to provide demonstrations of a target behavior than to design a reward function describing that behavior. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but there has been little work on applying IRL to high-dimensional video games. In our CNN-AIRL baseline, we modify the state-of-the-art adversarial IRL (AIRL) algorithm to use CNNs for the generator and discriminator. To stabilize training, we normalize the reward and increase the size of the discriminator training dataset. We additionally learn a low-dimensional state representation using a novel autoencoder architecture tuned for video game environments. This embedding is used as input to the reward network, improving the sample efﬁciency of expert demonstrations. Our method achieves high-level performance on the simple Catcher video game, substantially outperforming the CNN-AIRL baseline. We also score points on the Enduro Atari racing game, but do not match expert performance, highlighting the need for further work.","2018-10-24","2022-01-30 04:50:53","2022-01-30 04:50:53","2019-12-18 02:19:50","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000025  arXiv: 1810.10593","","/Users/jacquesthibodeau/Zotero/storage/WPQAXM4I/Tucker et al. - 2018 - Inverse reinforcement learning for video games.pdf; /Users/jacquesthibodeau/Zotero/storage/65P44U7Q/1810.html; /Users/jacquesthibodeau/Zotero/storage/BZ6IQPH2/Tucker et al. - 2018 - Inverse reinforcement learning for video games.pdf","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3V5UACJI","conferencePaper","2016","Sadigh, Dorsa; Sastry, S. Shankar; Seshia, Sanjit A.; Dragan, Anca","Information gathering actions over human internal state","2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","978-1-5090-3762-9","","10.1109/IROS.2016.7759036","http://ieeexplore.ieee.org/document/7759036/","Much of estimation of human internal state (goal, intentions, activities, preferences, etc.) is passive: an algorithm observes human actions and updates its estimate of human state. In this work, we embrace the fact that robot actions affect what humans do, and leverage it to improve state estimation. We enable robots to do active information gathering, by planning actions that probe the user in order to clarify their internal state. For instance, an autonomous car will plan to nudge into a human driver’s lane to test their driving style. Results in simulation and in a user study suggest that active information gathering signiﬁcantly outperforms passive state estimation.","2016-10","2022-01-30 04:50:53","2022-01-30 04:50:53","2019-12-18 01:40:07","66-73","","","","","","","","","","","IEEE","Daejeon, South Korea","en","","","","","DOI.org (Crossref)","","ZSCC: 0000176","","/Users/jacquesthibodeau/Zotero/storage/JXUN7TJI/Sadigh et al. - 2016 - Information gathering actions over human internal .pdf; /Users/jacquesthibodeau/Zotero/storage/X88ZSNX3/Sadigh et al. - 2016 - Information gathering actions over human internal .pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","","","","","","","","","","","","","",""
"CG7BVRUT","bookSection","2017","Bestick, Aaron; Bajcsy, Ruzena; Dragan, Anca D.","Implicitly Assisting Humans to Choose Good Grasps in Robot to Human Handovers","2016 International Symposium on Experimental Robotics","978-3-319-50114-7 978-3-319-50115-4","","","http://link.springer.com/10.1007/978-3-319-50115-4_30","We focus on selecting handover conﬁgurations that result in low human ergonomic cost not only at the time of handover, but also when the human is achieving a goal with the object after that handover. People take objects using whatever grasping conﬁguration is most comfortable to them. When the human has a goal pose they’d like to place the object at, however, the most comfortable grasping conﬁguration at the handover might be cumbersome overall, requiring regrasping or the use of an uncomfortable conﬁguration to reach the goal. We enable robots to purposefully inﬂuence the choices available to the person when taking the object, implicitly helping the person avoid suboptimal solutions and account for the goal. We introduce a probabilistic model of how humans select grasping conﬁgurations, and use this model to optimize expected cost. We present results in simulation, as well as from a user study, showing that the robot successfully inﬂuences people’s grasping conﬁgurations for the better.","2017","2022-01-30 04:50:53","2022-01-30 04:50:53","2019-12-18 01:39:49","341-354","","","1","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 28  DOI: 10.1007/978-3-319-50115-4_30","","/Users/jacquesthibodeau/Zotero/storage/6BDV2B6U/1810.10593.pdf; /Users/jacquesthibodeau/Zotero/storage/N3QIGNTG/Bestick et al. - 2017 - Implicitly Assisting Humans to Choose Good Grasps .pdf","","CHAI; TechSafety; AmbiguosSafety","","Kulić, Dana; Nakamura, Yoshihiko; Khatib, Oussama; Venture, Gentiane","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AVRPXH8F","conferencePaper","2019","Pandya, Ravi; Huang, Sandy H.; Hadfield-Menell, Dylan; Dragan, Anca D.","Human-AI Learning Performance in Multi-Armed Bandits","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","","","","http://arxiv.org/abs/1812.09376","People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artiﬁcial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We ﬁnd that team performance can beat both human and agent performance in isolation. Interestingly, we also ﬁnd that an agent’s performance in isolation does not necessarily correlate with the human-agent team’s performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent’s suggestions will inﬂuence human decision-making.","2019","2022-01-30 04:50:53","2022-01-30 04:50:53","2019-07-08 15:46:25","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 2  J: 2 arXiv: 1812.09376","","","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"EF6NXXC5","conferencePaper","2018","Fisac, Jaime F.; Bronstein, Eli; Stefansson, Elis; Sadigh, Dorsa; Sastry, S. Shankar; Dragan, Anca D.","Hierarchical Game-Theoretic Planning for Autonomous Vehicles","Robotics: Science and Systems 2019","","","","http://arxiv.org/abs/1810.05766","The actions of an autonomous vehicle on the road affect and are affected by those of other drivers, whether overtaking, negotiating a merge, or avoiding an accident. This mutual dependence, best captured by dynamic game theory, creates a strong coupling between the vehicle’s planning and its predictions of other drivers’ behavior, and constitutes an open problem with direct implications on the safety and viability of autonomous driving technology. Unfortunately, dynamic games are too computationally demanding to meet the real-time constraints of autonomous driving in its continuous state and action space. In this paper, we introduce a novel game-theoretic trajectory planning algorithm for autonomous driving, that enables real-time performance by hierarchically decomposing the underlying dynamic game into a long-horizon “strategic” game with simpliﬁed dynamics and full information structure, and a short-horizon “tactical” game with full dynamics and a simpliﬁed information structure. The value of the strategic game is used to guide the tactical planning, implicitly extending the planning horizon, pushing the local trajectory optimization closer to global solutions, and, most importantly, quantitatively accounting for the autonomous vehicle and the human driver’s ability and incentives to inﬂuence each other. In addition, our approach admits non-deterministic models of human decisionmaking, rather than relying on perfectly rational predictions. Our results showcase richer, safer, and more effective autonomous behavior in comparison to existing techniques.","2018-10-12","2022-01-30 04:50:53","2022-01-30 04:50:53","2019-07-08 16:10:43","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 106  J: 38 arXiv: 1810.05766","","","","CHAI; TechSafety","Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Multiagent Systems; I.2.9; 68T40, 93C85, 91A25; Mathematics - Optimization and Control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Robotics: Science and Systems 2019","","","","","","","","","","","","","","",""
"JXZF36GH","conferencePaper","2020","Dobbe, Roel; Gilbert, Thomas Krendl; Mintz, Yonatan","Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","","","","http://arxiv.org/abs/1911.09005","As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.","2020","2022-01-30 04:50:53","2022-01-30 04:50:53","2020-11-14 00:55:17","","","","","","","Hard Choices in Artificial Intelligence","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001[s0]  arXiv: 1911.09005","","/Users/jacquesthibodeau/Zotero/storage/7VMINIWU/Dobbe et al. - 2019 - Hard Choices in Artificial Intelligence Addressin.pdf; /Users/jacquesthibodeau/Zotero/storage/K6X3NNFA/1911.html","","CHAI; TechSafety","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Electrical Engineering and Systems Science - Systems and Control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI/ACM Conference on AI, Ethics, and Society 2020","","","","","","","","","","","","","","",""
"DFJQBIH5","blogPost","2020","Turner, Alex","Generalizing the Power-Seeking Theorems","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/nyDnLif4cjeRe9DSv/generalizing-the-power-seeking-theorems","Previously: Seeking Power is Often Provably Instrumentally Convergent in MDPs. Thanks to Rohin Shah, Michael Dennis, Josh Turner, and Evan Hubinger for comments. -------------------------------------------------------------------------------- It sure seems like gaining power over the environment is instrumentally convergent (optimal for a wide range of agent goals). You can turn this into math and prove things about it. Given some distribution over agent goals, we want to be able to formally describe how optimal action tends to flow through the future. Does gaining money tend to be optimal? Avoiding shutdown? When? How do we know? Optimal Farsighted Agents Tend to Seek Power proved that, when you distribute reward fairly and evenly across states (IID), it's instrumentally convergent to gain access to lots of final states (which are absorbing, in that the agent keeps on experiencing the final state). The theorems apply when you don't discount the future (you're ""infinitely farsighted""). Most reward functions for the Pac-Man game incentivize not dying immediately, so that the agent can loop around higher-scoring configurations. Many ways of scoring Tic-Tac-Toe game states incentivize not losing immediately, in order to choose the highest-scoring final configuration. ""All states have self-loops, left hidden to reduce clutter. In AI: A Modern Approach (3e), the agent starts at1and receives reward for reaching3. The optimal policy for this reward function avoids2, and one might suspect that avoiding2is instrumentally convergent. However, a skeptic might provide a reward function for which navigating to2is optimal, and then argue that ""instrumental convergence'' is subjective and that there is no reasonable basis for concluding that2is generally avoided. We can do better... for any way of independently and identically distributing reward over states,1011of reward functions have farsighted optimal policies which avoid2. If we complicate the MDP with additional t","2020-07-26","2022-01-30 04:50:53","2022-01-30 04:50:53","2020-08-28 17:31:03","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/Z47Z7M3I/generalizing-the-power-seeking-theorems.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WZIU7UJI","blogPost","2018","Shah, Rohin","Intuitions about goal-directed behavior","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/DfcywmqRSkBaCB6Ma/intuitions-about-goal-directed-behavior","One broad argument for AI risk is the Misspecified Goal argument:  The Misspecified Goal Argument for AI Risk: Very intelligent AI systems will be able to make long-term plans in order to achieve their goals, and if their goals are even slightly misspecified then the AI system will become adversarial and work against us. My main goal in this post is to make conceptual clarifications and suggest how they affect the Misspecified Goal argument, without making any recommendations about what we should actually do. Future posts will argue more directly for a particular position. As a result, I will not be considering other arguments for focusing on AI risk even though I find some of them more compelling. I think of this as a concern about long-term goal-directed behavior. Unfortunately, it’s not clear how to categorize behavior as goal-directed vs. not. Intuitively, any agent that searches over actions and chooses the one that best achieves some measure of “goodness” is goal-directed (though there are exceptions, such as the agent that selects actions that begin with the letter “A”). (ETA: I also think that agents that show goal-directed behavior because they are looking at some other agent are not goal-directed themselves -- see this comment.) However, this is not a necessary condition: many humans are goal-directed, but there is no goal baked into the brain that they are using to choose actions. This is related to the concept of optimization, though with intuitions around optimization we typically assume that we know the agent’s preference ordering, which I don’t want to assume here. (In fact, I don’t want to assume that the agent even has a preference ordering.) One potential formalization is to say that goal-directed behavior is any behavior that can be modelled as maximizing expected utility for some utility function; in the next post I will argue that this does not properly capture the behaviors we are worried about. In this post I’ll give some intuitions about","2018","2022-01-30 04:50:53","2022-01-30 04:50:53","2020-12-17 04:36:33","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UUQVT4H3/DfcywmqRSkBaCB6Ma.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P6TZ4M98","blogPost","2014","Dragan, Anca; Srinivasa, Siddhartha","Integrating Human Observer Inferences into Robot Motion Planning","The Robotics Institute Carnegie Mellon University","","","","https://www.ri.cmu.edu/publications/integrating-human-observer-inferences-into-robot-motion-planning/","Our goal is to enable robots to produce motion that is suitable for human-robot collaboration and co-existence. Most motion in robotics is purely functional, ideal when the robot is performing a task in isolation. In collaboration, however, the robot’s motion has an observer, watching and interpreting the motion. In this work, we move beyond functional …","2014","2022-01-30 04:50:53","2022-01-30 04:50:53","2019-12-18 01:40:11","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: 0000075","","/Users/jacquesthibodeau/Zotero/storage/RZQEJ7CE/integrating-human-observer-inferences-into-robot-motion-planning.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"98C6EF6Z","journalArticle","2018","Shah, Rohin; Gundotra, Noah; Abbeel, Pieter; Dragan, Anca","Inferring Reward Functions from Demonstrators with Unknown Biases","","","","","https://openreview.net/forum?id=rkgqCiRqKQ","Our goal is to infer reward functions from demonstrations. In order to infer the correct reward function, we must account for the systematic ways in which the demonstrator is suboptimal. Prior work...","2018-09-27","2022-01-30 04:50:53","2022-01-30 04:50:53","2019-12-18 03:11:08","","","","","","","","","","","","","","","","","","","openreview.net","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/S622QK9T/Shah et al. - 2018 - Inferring Reward Functions from Demonstrators with.pdf; /Users/jacquesthibodeau/Zotero/storage/STVESK9J/forum.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K25P3RCZ","conferencePaper","2019","Hadfield-Menell, Dylan; Hadfield, Gillian","Incomplete Contracting and AI Alignment","Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","","","","http://arxiv.org/abs/1804.04268","We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.","2019","2022-01-30 04:50:53","2022-01-30 04:50:53","2019-07-18 04:58:42","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 31  J: 14 arXiv: 1804.04268","","/Users/jacquesthibodeau/Zotero/storage/3BC8IA76/Hadfield-Menell and Hadfield - 2018 - Incomplete Contracting and AI Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/4CJJ6G6F/1804.html","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TBKHB3QK","blogPost","2018","Shah, Rohin","Humans can be assigned any values whatsoever…","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever","(Re)Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin’s note: In the last post, we saw that a good broad value learning approach would need to understand the systematic biases in human planning in order to achieve superhuman performance. Perhaps we can just use machine learning again and learn the biases and reward simultaneously? This post by Stuart Armstrong (original here) and the associated paper say: “Not without more assumptions.” This post comes from a theoretical perspective that may be alien to ML researchers; in particular, it makes an argument that simplicity priors do not solve the problem pointed out here, where simplicity is based on Kolmogorov complexity (which is an instantiation of the Minimum Description Length principle). The analog in machine learning would be an argument that regularization would not work. The proof used is specific to Kolmogorov complexity and does not clearly generalize to arbitrary regularization techniques; however, I view the argument as being suggestive that regularization techniques would also be insufficient to address the problems raised here. -------------------------------------------------------------------------------- Humans have no values… nor do any agent. Unless you make strong assumptions about their rationality. And depending on those assumptions, you get humans to have any values. AN AGENT WITH NO CLEAR PREFERENCES There are three buttons in this world, B(0), B(1), and X, and one agent H.  B(0) and B(1) can be operated by H, while X can be operated by an outside observer. H will initially press button B(0); if ever X is pressed, the agent will switch to pressing B(1). If X is pressed again, the agent will switch back to pressing B(0), and so on. After a large number of turns N, H will shut off. That’s the full algorithm for H. So the question is, what are the values/preferences/rewards of H? There are three natural reward functions that are plausible:  *  R(0), which is linear i","2018","2022-01-30 04:50:53","2022-01-30 04:50:53","2020-12-17 04:36:27","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/DXQWPWZS/ANupXf8XfZo2EJxGv.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K724STCW","bookSection","2021","Russell, Stuart","Human-Compatible Artificial Intelligence","Human-Like Machine Intelligence","978-0-19-886253-6 978-0-19-189533-3","","","https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198862536.001.0001/oso-9780198862536-chapter-1","Following the analysis given by Alan Turing in 1951, one must expect that AI capabilities will eventually exceed those of humans across a wide range of real-world-decision making scenarios. Should this be a cause for concern, as Turing, Hawking, and others have suggested? And, if so, what can we do about it? While some in the mainstream AI community dismiss the issue, I will argue that the problem is real: we have to work out how to design AI systems that are far more powerful than ourselves while ensuring that they never have power over us. I believe the technical aspects of this problem are solvable. Whereas the standard model of AI proposes to build machines that optimize known, exogenously specified objectives, a preferable approach would be to build machines that are of provable benefit to humans. I introduce assistance games as a formal class of problems whose solution, under certain assumptions, has the desired property.","2021-07-13","2022-01-30 04:50:53","2022-01-30 04:50:53","2021-10-30 20:10:48","3-23","","","","","","","","","","","Oxford University Press","","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s0]  ACC: 0  DOI: 10.1093/oso/9780198862536.003.0001","","/Users/jacquesthibodeau/Zotero/storage/QPGCQWEK/mi19book-hcai.pdf","","TechSafety","","","","","","","Muggleton, Stephen; Chater, Nicholas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A7KHVPJD","blogPost","2019","Shah, Rohin","Human-AI Interaction","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/4783ufKpx8xvLMPc6/human-ai-interaction","THE IMPORTANCE OF FEEDBACK Consider trying to program a self-driving car to drive from San Francisco to Los Angeles -- with no sensors that allow it to gather information as it is driving. This is possible in principle. If you can predict the exact weather conditions, the exact movement of all of the other cars on the road, the exact amount of friction along every part of the road surface, the exact impact of (the equivalents of) pressing the gas or turning the steering wheel, and so on, then you could compute ahead of time how exactly to control the car such that it gets from SF to LA. Nevertheless, it seems unlikely that we will ever be able to accomplish such a feat, even with powerful AI systems. No, in practice there is going to be some uncertainty about how the world is going to evolve; such that any plan computed ahead of time will have some errors that will compound over the course of the plan. The solution is to use sensors to gather information while executing the plan, so that we can notice any errors or deviations from the plan, and take corrective action. It is much easier to build a controller that keeps you pointed in the general direction, than to build a plan that will get you there perfectly without any adaptation. Control theory studies these sorts of systems, and you can see the general power of feedback controllers in the theorems that can be proven. Especially for motion tasks, you can build feedback controllers that are guaranteed to safely achieve the goal, even in the presence of adversarial environmental forces (that are bounded in size, so you can’t have arbitrarily strong wind). In the presence of an adversary, in most environments it becomes impossible even in principle to make such a guarantee if you do not have any sensors or feedback and must compute a plan in advance. Typically, for every such plan, there is some environmental force that would cause it to fail. THE CONTROL THEORY PERSPECTIVE ON AI ALIGNMENT With ambitious value le","2019","2022-01-30 04:50:53","2022-01-30 04:50:53","2020-12-17 04:37:06","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/HDFKCMPB/4783ufKpx8xvLMPc6.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IM89NS2R","journalArticle","2020","Gates, Vael; Griffiths, Thomas L.; Dragan, Anca D.","How to Be Helpful to Multiple People at Once","Cognitive Science","","0364-0213, 1551-6709","10.1111/cogs.12841","https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12841","When someone hosts a party, when governments choose an aid program, or when assistive robots decide what meal to serve to a family, decision-makers must determine how to help even when their recipients have very different preferences. Which combination of people’s desires should a decisionmaker serve? To provide a potential answer, we turned to psychology: What do people think is best when multiple people have different utilities over options? We developed a quantitative model of what people consider desirable behavior, characterizing participants’ preferences by inferring which combination of “metrics” (maximax, maxsum, maximin, or inequality aversion [IA]) best explained participants’ decisions in a drink-choosing task. We found that participants’ behavior was best described by the maximin metric, describing the desire to maximize the happiness of the worst-off person, though participant behavior was also consistent with maximizing group utility (the maxsum metric) and the IA metric to a lesser extent. Participant behavior was consistent across variation in the agents involved and tended to become more maxsum-oriented when participants were told they were players in the task (Experiment 1). In later experiments, participants maintained maximin behavior across multi-step tasks rather than shortsightedly focusing on the individual steps therein (Experiment 2, Experiment 3). By repeatedly asking participants what choices they would hope for in an optimal, just decision-maker, and carefully disambiguating which quantitative metrics describe these nuanced choices, we help constrain the space of what behavior we desire in leaders, artiﬁcial intelligence systems helping decision-makers, and the assistive robots and decision-makers of the future.","2020-06","2022-01-30 04:50:53","2022-01-30 04:50:53","2020-11-21 18:26:49","","","6","44","","Cogn Sci","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/F6HG3522/Gates et al. - 2020 - How to Be Helpful to Multiple People at Once.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VVQW8ER5","conferencePaper","2018","Liu, Chang; Hamrick, Jessica B.; Fisac, Jaime F.; Dragan, Anca D.; Hedrick, J. Karl; Sastry, S. Shankar; Griffiths, Thomas L.","Goal Inference Improves Objective and Perceived Performance in Human-Robot Collaboration","Proceedings of the 15th International Conferenceon Autonomous Agents and Multiagent Systems (AAMAS 2016)","","","","http://arxiv.org/abs/1802.01780","The study of human-robot interaction is fundamental to the design and use of robotics in real-world applications. Robots will need to predict and adapt to the actions of human collaborators in order to achieve good performance and improve safety and end-user adoption. This paper evaluates a human-robot collaboration scheme that combines the task allocation and motion levels of reasoning: the robotic agent uses Bayesian inference to predict the next goal of its human partner from his or her ongoing motion, and re-plans its own actions in real time. This anticipative adaptation is desirable in many practical scenarios, where humans are unable or unwilling to take on the cognitive overhead required to explicitly communicate their intent to the robot. A behavioral experiment indicates that the combination of goal inference and dynamic task planning significantly improves both objective and perceived performance of the human-robot team. Participants were highly sensitive to the differences between robot behaviors, preferring to work with a robot that adapted to their actions over one that did not.","2018-02-05","2022-01-30 04:50:53","2022-01-30 04:50:53","2020-12-13 20:00:34","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000047  arXiv: 1802.01780","","/Users/jacquesthibodeau/Zotero/storage/IZFZ7SNH/Liu et al. - 2018 - Goal Inference Improves Objective and Perceived Pe.pdf; /Users/jacquesthibodeau/Zotero/storage/ZWMVN6F2/1802.html","","CHAI; TechSafety; AmbiguosSafety","Computer Science - Artificial Intelligence; Computer Science - Robotics; I.2.0; I.2.6; Computer Science - Human-Computer Interaction; 68T05; I.2.8; I.2.9","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVTJI96G","blogPost","2018","Shah, Rohin","Future directions for ambitious value learning","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/EhNCnCkmu7MwrQ7yz/future-directions-for-ambitious-value-learning","To recap the sequence so far:  * Ambitious value learning aims to infer a utility function that is safe to    maximize, by looking at human behavior.  * However, since you only observe human behavior, you must be able to infer and    account for the mistakes that humans make in order to exceed human    performance. (If we don’t exceed human performance, it’s likely that we’ll    use unsafe techniques that do exceed human performance, due to economic    incentives.)  * You might hope to infer both the mistake model (aka systematic human biases)     and the utility function, and then throw away the mistake model and optimize    the utility function. This cannot be done without additional assumptions.  * One potential assumption you could use would be to codify a specific mistake    model. However, humans are sufficiently complicated that any such model would    be wrong, leading to model misspecification. Model misspecification causes     many problems in general, and is particularly thorny for value learning. Despite these arguments, we could still hope to infer a broad utility function that is safe to optimize, either by sidestepping the formalism used so far, or by introducing additional assumptions. Often, it is clear that these methods would not find the true human utility function (assuming that such a thing exists), but they are worth pursuing anyway because they could find a utility function that is good enough. This post provides pointers to approaches that are currently being pursued. Since these are active areas of research, I don’t want to comment on how feasible they may or may not be -- it’s hard to accurately assess the importance and quality of an idea that is being developed just from what is currently written down about that idea. Assumptions about the mistake model. We could narrow down on the mistake model by making assumptions about it, that could let us avoid the impossibility result. This decision means that we’re accepting the risk of missp","2018","2022-01-30 04:50:52","2022-01-30 04:50:52","2020-12-17 04:36:29","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/J54ABXG9/EhNCnCkmu7MwrQ7yz.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8TFC6UFR","journalArticle","2013","Halpern, Joseph Y.; Pass, Rafael","Game Theory with Translucent Players","International Journal of Game Theory","","","","http://arxiv.org/abs/1308.3778","A traditional assumption in game theory is that players are opaque to one another---if a player changes strategies, then this change in strategies does not affect the choice of other players' strategies. In many situations this is an unrealistic assumption. We develop a framework for reasoning about games where the players may be translucent to one another; in particular, a player may believe that if she were to change strategies, then the other player would also change strategies. Translucent players may achieve significantly more efficient outcomes than opaque ones. Our main result is a characterization of strategies consistent with appropriate analogues of common belief of rationality. Common Counterfactual Belief of Rationality (CCBR) holds if (1) everyone is rational, (2) everyone counterfactually believes that everyone else is rational (i.e., all players i believe that everyone else would still be rational even if $i$ were to switch strategies), (3) everyone counterfactually believes that everyone else is rational, and counterfactually believes that everyone else is rational, and so on. CCBR characterizes the set of strategies surviving iterated removal of minimax dominated strategies, where a strategy s for player i is minimax dominated by s' if the worst-case payoff for i using s' is better than the best possible payoff using s.","2013-08-17","2022-01-30 04:50:52","2022-01-30 04:50:52","2020-11-22 02:29:01","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000021  arXiv: 1308.3778","","/Users/jacquesthibodeau/Zotero/storage/UZXNUF5J/Halpern and Pass - 2013 - Game Theory with Translucent Players.pdf; /Users/jacquesthibodeau/Zotero/storage/DDVE6APB/1308.html","","CHAI; TechSafety; AmbiguosSafety","Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KEDQKKFK","blogPost","2019","Shah, Rohin","Future directions for narrow value learning","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/MxadmSXHnoCupsWqx/future-directions-for-narrow-value-learning","Narrow value learning is a huge field that people are already working on (though not by that name) and I can’t possibly do it justice. This post is primarily a list of things that I think are important and interesting, rather than an exhaustive list of directions to pursue. (In contrast, the corresponding post  for ambitious value learning did aim to be exhaustive, and I don’t think I missed much work there.) You might think that since so many people are already working on narrow value learning, we should focus on more neglected areas of AI safety. However, I still think it’s worth working on because long-term safety suggests a particular subset of problems to focus on; that subset seems quite neglected. For example, a lot of work is about how to improve current algorithms in a particular domain, and the solutions encode domain knowledge to succeed. This seems not very relevant for long-term concerns. Some work assumes that a handcoded featurization is given (so that the true reward is linear in the features); this is not an assumption we could make for more powerful AI systems. I will speculate a bit on the neglectedness and feasibility of each of these areas, since for many of them there isn’t a person or research group who would champion them whom I could defer to about the arguments for success. THE BIG PICTURE This category of research is about how you could take narrow value learning algorithms and use them to create an aligned AI system. Typically, I expect this to work by having the narrow value learning enable some form of corrigibility. As far as I can tell, nobody outside of the AI safety community works on this problem. While it is far too early to stake a confident position one way or the other, I am slightly less optimistic about this avenue of approach than one in which we create a system that is directly trained to be corrigible. Avoiding problems with goal-directedness. How do we put together narrow value learning techniques in a way that does","2019","2022-01-30 04:50:52","2022-01-30 04:50:52","2020-12-17 04:37:29","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/V8NFJVTP/MxadmSXHnoCupsWqx.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2HFKDU6Z","bookSection","2020","Russell, Stuart; Jeanmaire, Caroline","From the Standard Model of AI to Provably Beneficial Systems","AI Governance in 2019: A Year In Review","","","","http://n.sinaimg.cn/tech/f34884a9/20200501/GlobalAIGovernancein2019.pdf","","2020","2022-01-30 04:50:45","2022-01-30 04:50:45","","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/F","","","","MetaSafety; CHAI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"49P9KC94","blogPost","2019","Shah, Rohin","Following human norms","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/eBd6WvzhuqduCkYv3/following-human-norms","So far we have been talking about how to learn “values” or “instrumental goals”. This would be necessary if we want to figure out how to build an AI system that does exactly what we want it to do. However, we’re probably fine if we can keep learning and building better AI systems. This suggests that it’s sufficient to build AI systems that don’t screw up so badly that it ends this process. If we accomplish that, then steady progress in AI will eventually get us to AI systems that do what we want. So, it might be helpful to break down the problem of learning values into the subproblems of learning what to do, and learning what not to do. Standard AI research will continue to make progress on learning what to do; catastrophe happens when our AI system doesn’t know what not to do. This is the part that we need to make progress on. This is a problem that humans have to solve as well. Children learn basic norms such as not to litter, not to take other people’s things, what not to say in public, etc. As argued in Incomplete Contracting and AI alignment, any contract between humans is never explicitly spelled out, but instead relies on an external unwritten normative structure under which a contract is interpreted. (Even if we don’t explicitly ask our cleaner not to break any vases, we still expect them not to intentionally do so.) We might hope to build AI systems that infer and follow these norms, and thereby avoid catastrophe. It’s worth noting that this will probably not be an instance of narrow value learning, since there are several differences:  * Narrow value learning requires that you learn what to do, unlike norm    inference.  * Norm following requires learning from a complex domain (human society),    whereas narrow value learning can be applied in simpler domains as well.  * Norms are a property of groups of agents, whereas narrow value learning can    be applied in settings with a single agent. Despite this, I have included it in this sequence because it","2019","2022-01-30 04:50:45","2022-01-30 04:50:45","2020-12-17 04:37:25","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/EVK4EAGA/eBd6WvzhuqduCkYv3.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C5Z6H6IX","conferencePaper","2018","Cundy, Chris; Filan, Daniel","Exploring Hierarchy-Aware Inverse Reinforcement Learning","arXiv:1807.05037 [cs]","","","","http://arxiv.org/abs/1807.05037","We introduce a new generative model for human planning under the Bayesian Inverse Reinforcement Learning (BIRL) framework which takes into account the fact that humans often plan using hierarchical strategies. We describe the Bayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of hierarchical planners, and use an illustrative toy model to show that BIHRL retains accuracy where standard BIRL fails. Furthermore, BIHRL is able to accurately predict the goals of `Wikispeedia' game players, with inclusion of hierarchical structure in the model resulting in a large boost in accuracy. We show that BIHRL is able to significantly outperform BIRL even when we only have a weak prior on the hierarchical structure of the plans available to the agent, and discuss the significant challenges that remain for scaling up this framework to more realistic settings.","2018-07-13","2022-01-30 04:50:45","2022-01-30 04:50:45","2019-12-18 01:12:20","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000005  arXiv: 1807.05037","","/Users/jacquesthibodeau/Zotero/storage/65A775B4/Cundy and Filan - 2018 - Exploring Hierarchy-Aware Inverse Reinforcement Le.pdf; /Users/jacquesthibodeau/Zotero/storage/3SB9PIM5/1807.html; /Users/jacquesthibodeau/Zotero/storage/DBVMCRNX/Cundy and Filan - 2018 - Exploring Hierarchy-Aware Inverse Reinforcement Le.pdf","","CHAI; TechSafety","Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","1st Workshop on Goal Specifications for Reinforcement Learning, ICML 2018","","","","","","","","","","","","","","",""
"4BV35Z3X","conferencePaper","2018","de Graaf, Maartje M.A.; Malle, Bertram F.; Dragan, Anca; Ziemke, Tom","Explainable Robotic Systems","Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18","978-1-4503-5615-2","","10.1145/3173386.3173568","http://dl.acm.org/citation.cfm?doid=3173386.3173568","The increasing complexity of robotic systems are pressing the need for them to be transparent and trustworthy. When people interact with a robotic system, they will inevitably construct mental models to understand and predict its actions. However, people’s mental models of robotic systems stem from their interactions with living beings, which induces the risk of establishing incorrect or inadequate mental models of robotic systems and may lead people to either under- and over-trust these systems. We need to understand the inferences that people make about robots from their behavior, and leverage this understanding to formulate and implement behaviors into robotic systems that support the formation of correct mental models of and fosters trust calibration. This way, people will be better able to predict the intentions of these systems, and thus more accurately estimate their capabilities, better understand their actions, and potentially correct their errors. The aim of this full-day workshop is to provide a forum for researchers and practitioners to share and learn about recent research on people’s inferences of robot actions, as well as the implementation of transparent, predictable, and explainable behaviors into robotic systems.","2018","2022-01-30 04:50:45","2022-01-30 04:50:45","2019-12-18 02:40:25","387-388","","","","","","","","","","","ACM Press","Chicago, IL, USA","en","","","","","DOI.org (Crossref)","","ZSCC: 0000008","","/Users/jacquesthibodeau/Zotero/storage/FFV4ATRX/de Graaf et al. - 2018 - Explainable Robotic Systems.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Companion of the 2018 ACM/IEEE International Conference","","","","","","","","","","","","","","",""
"BPHFCUVZ","conferencePaper","2018","Huang, Sandy H.; Bhatia, Kush; Abbeel, Pieter; Dragan, Anca D.","Establishing Appropriate Trust via Critical States","2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","","","http://arxiv.org/abs/1810.08174","In order to effectively interact with or supervise a robot, humans need to have an accurate mental model of its capabilities and how it acts. Learned neural network policies make that particularly challenging. We propose an approach for helping end-users build a mental model of such policies. Our key observation is that for most tasks, the essence of the policy is captured in a few critical states: states in which it is very important to take a certain action. Our user studies show that if the robot shows a human what its understanding of the task's critical states is, then the human can make a more informed decision about whether to deploy the policy, and if she does deploy it, when she needs to take control from it at execution time.","2018-10-18","2022-01-30 04:50:45","2022-01-30 04:50:45","2019-12-18 02:38:38","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000044  arXiv: 1810.08174","","/Users/jacquesthibodeau/Zotero/storage/ZJCEQTK4/Huang et al. - 2018 - Establishing Appropriate Trust via Critical States.pdf; /Users/jacquesthibodeau/Zotero/storage/PJVHAHWN/1810.html","","CHAI; TechSafety","Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","","","","","","","","","","","","","",""
"4EF7C9NX","conferencePaper","2019","Gilbert, Thomas Krendl; Mintz, Yonatan","Epistemic Therapy for Bias in Automated Decision-Making","Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","978-1-4503-6324-2","","10.1145/3306618.3314294","https://dl.acm.org/doi/10.1145/3306618.3314294","","2019-01-27","2022-01-30 04:50:45","2022-01-30 04:50:45","2020-12-17 22:13:08","61-67","","","","","","","","","","","ACM","Honolulu HI USA","en","","","","","DOI.org (Crossref)","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/G8JN4BTX/Gilbert and Mintz - 2019 - Epistemic Therapy for Bias in Automated Decision-M.pdf","","CHAI; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AIES '19: AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"EMNDB395","journalArticle","2017","Huang, Sandy H.; Held, David; Abbeel, Pieter; Dragan, Anca D.","Enabling Robots to Communicate their Objectives","Autonomous Robots","","","10.15607/RSS.2017.XIII.059","https://arxiv.org/abs/1702.03465v2","The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. Since a robot's behavior is often a direct result of its underlying objective function, our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then it selects those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be exact in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what it will do in novel situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.","2017-02-11","2022-01-30 04:50:45","2022-01-30 04:50:45","2019-12-18 01:39:51","","","","43","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/XI6MUDJM/Huang et al. - 2017 - Enabling Robots to Communicate their Objectives.pdf; /Users/jacquesthibodeau/Zotero/storage/3W9QBCM2/1702.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZEAMIZP4","conferencePaper","2018","Xu, Kelvin; Ratner, Ellis; Dragan, Anca; Levine, Sergey; Finn, Chelsea","Few-Shot Intent Inference via Meta-Inverse Reinforcement Learning","Proceedings of the 36th International Conference on Machine Learning","","","","http://proceedings.mlr.press/v97/xu19d/xu19d.pdf","A significant challenge for the practical application of reinforcement learning toreal world problems is the need to specify an oracle reward function that correctly defines a task. Inverse...","2018-09-27","2022-01-30 04:50:45","2022-01-30 04:50:45","2019-12-18 03:12:12","","","","","","","","","","","","","","","","","","","openreview.net","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/KDPI92S8/Xu et al. - 2018 - Few-Shot Intent Inference via Meta-Inverse Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/66PJEMEA/forum.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","36th International Conference on Machine Learning","","","","","","","","","","","","","","",""
"QT5EDQJS","conferencePaper","2018","Kwon, Minae; Huang, Sandy H.; Dragan, Anca D.","Expressing Robot Incapability","Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction - HRI '18","","","10.1145/3171221.3171276","http://arxiv.org/abs/1810.08167","Our goal is to enable robots to express their incapability, and to do so in a way that communicates both what they are trying to accomplish and why they are unable to accomplish it. We frame this as a trajectory optimization problem: maximize the similarity between the motion expressing incapability and what would amount to successful task execution, while obeying the physical limits of the robot. We introduce and evaluate candidate similarity measures, and show that one in particular generalizes to a range of tasks, while producing expressive motions that are tailored to each task. Our user study supports that our approach automatically generates motions expressing incapability that communicate both what and why to end-users, and improve their overall perception of the robot and willingness to collaborate with it in the future.","2018","2022-01-30 04:50:45","2022-01-30 04:50:45","2019-12-18 02:40:52","87-95","","","","","","","","","","","ACM","","en","","","","","arXiv.org","","ZSCC: 0000072","","/Users/jacquesthibodeau/Zotero/storage/46F5BZ8U/Kwon et al. - 2018 - Expressing robot incapability.pdf; /Users/jacquesthibodeau/Zotero/storage/4NFN3ET4/Kwon et al. - 2018 - Expressing Robot Incapability.pdf; /Users/jacquesthibodeau/Zotero/storage/SU3ZGDE8/citation.html","","CHAI; TechSafety","Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4EIGIXS","conferencePaper","2021","Knott, Paul; Carroll, Micah; Devlin, Sam; Ciosek, Kamil; Hofmann, Katja; Dragan, A. D.; Shah, Rohin","Evaluating the Robustness of Collaborative Agents","Proc. of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021)","","","","http://arxiv.org/abs/2101.05507","In order for agents trained by deep reinforcement learning to work alongside humans in realistic settings, we will need to ensure that the agents are \emph{robust}. Since the real world is very diverse, and human behavior often changes in response to agent deployment, the agent will likely encounter novel situations that have never been seen during training. This results in an evaluation challenge: if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? We take inspiration from the practice of \emph{unit testing} in software engineering. Specifically, we suggest that when designing AI agents that collaborate with humans, designers should search for potential edge cases in \emph{possible partner behavior} and \emph{possible states encountered}, and write tests which check that the behavior of the agent in these edge cases is reasonable. We apply this methodology to build a suite of unit tests for the Overcooked-AI environment, and use this test suite to evaluate three proposals for improving robustness. We find that the test suite provides significant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward.","2021-01-14","2022-01-30 04:50:45","2022-01-30 04:50:45","2021-10-30 20:37:53","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000002  arXiv: 2101.05507","","/Users/jacquesthibodeau/Zotero/storage/ZKGWIRKG/Knott et al. - 2021 - Evaluating the Robustness of Collaborative Agents.pdf; /Users/jacquesthibodeau/Zotero/storage/ZC9RGFH2/2101.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAMAS 2021","","","","","","","","","","","","","","",""
"55SJ6TUU","conferencePaper","2017","Palaniappan, Malayandi; Malik, Dhruv; Hadfield-Menell, Dylan; Dragan, Anca; Russell, Stuart","Efficient Cooperative Inverse Reinforcement Learning","Proc. ICML Work⁃ shop on Reliable Machine Learning in the Wild (2017)","","","","","","2017","2022-01-30 04:50:44","2022-01-30 04:50:44","","5","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000004","","/Users/jacquesthibodeau/Zotero/storage/D7BTS6QX/Palaniappan et al. - Efficient Cooperative Inverse Reinforcement Learni.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UBRT43WR","conferencePaper","2020","Halpern, Joseph Y.; Piermont, Evan","Dynamic Awareness","","","","","http://arxiv.org/abs/2007.02823","We investigate how to model the beliefs of an agent who becomes more aware. We use the framework of Halpern and Rego (2013) by adding probability, and define a notion of a model transition that describes constraints on how, if an agent becomes aware of a new formula $\phi$ in state $s$ of a model $M$, she transitions to state $s^*$ in a model $M^*$. We then discuss how such a model can be applied to information disclosure.","2020-07-06","2022-01-30 04:50:44","2022-01-30 04:50:44","2020-12-19 02:26:06","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 2007.02823","","/Users/jacquesthibodeau/Zotero/storage/TIVQU7G4/Halpern and Piermont - 2020 - Dynamic Awareness.pdf; /Users/jacquesthibodeau/Zotero/storage/6C8IDR23/2007.html","","CHAI; TechSafety","Computer Science - Artificial Intelligence; Computer Science - Logic in Computer Science; Economics - Theoretical Economics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","17th International Conference on Principles of Knowledge Representation and Reasoning","","","","","","","","","","","","","","",""
"W2N8KHAS","conferencePaper","2020","Freire, Pedro; Gleave, Adam; Toyer, Sam; Russell, Stuart","DERAIL: Diagnostic Environments for Reward And Imitation Learning","Advances in Neural Information Processing Systems 33 Pre-proceedings","","","","http://arxiv.org/abs/2012.01365","The objective of many real-world tasks is complex and difficult to procedurally specify. This makes it necessary to use reward or imitation learning algorithms to infer a reward or policy directly from human data. Existing benchmarks for these algorithms focus on realism, testing in complex environments. Unfortunately, these benchmarks are slow, unreliable and cannot isolate failures. As a complementary approach, we develop a suite of simple diagnostic tasks that test individual facets of algorithm performance in isolation. We evaluate a range of common reward and imitation learning algorithms on our tasks. Our results confirm that algorithm performance is highly sensitive to implementation details. Moreover, in a case-study into a popular preference-based reward learning implementation, we illustrate how the suite can pinpoint design flaws and rapidly evaluate candidate solutions. The environments are available at https://github.com/HumanCompatibleAI/seals .","2020-12-02","2022-01-30 04:50:44","2022-01-30 04:50:44","2020-12-18 00:37:38","","","","","","","DERAIL","","","","","","","","","","","","arXiv.org","","ZSCC: 0000002  arXiv: 2012.01365","","/Users/jacquesthibodeau/Zotero/storage/4ZCV83UD/Freire et al. - 2020 - DERAIL Diagnostic Environments for Reward And Imi.pdf; /Users/jacquesthibodeau/Zotero/storage/VVNRMKJH/2012.html; /Users/jacquesthibodeau/Zotero/storage/NDR2G3TZ/2012.html","","CHAI; TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Deep Reinforcement Learning Workshop at NeurIPS","","","","","","","","","","","","","","",""
"MNJW7DRU","blogPost","2020","Turner, Alex","Corrigibility as outside view","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/BMj6uMuyBidrdZkiD/corrigibility-as-outside-view","You run a country. One day, you think ""I could help so many more people if I set all the rules... and I could make this happen"". As far as you can tell, this is the real reason you want to set the rules – you want to help people, and you think you'd do a good job. But historically… in this kind of situation, this reasoning can lead to terrible things. So you just don't do it, even though it feels like a good idea.[1] More generally, Even though my intuition/naïve decision-making process says I should do X, I know (through mental simulation or from history) my algorithm is usually wrong in this situation. I'm not going to do X.  * ""It feels like I could complete this project within a week. But… in the past,    when I've predicted ""a week"" for projects like this, reality usually gives me    a longer answer. I'm not going to trust this feeling. I'm going to allocate    extra time.""  * As a new secretary, I think I know how my boss would want me to reply to an    important e-mail. However, I'm not sure. Even though I think I know what to    do, common sense recommends I clarify.  * You broke up with someone. ""Even though I really miss them, in this kind of    situation, missing my ex isn't a reliable indicator that I should get back    together with them. I'm not going to trust this feeling, and will trust the    ""sober"" version of me which broke up with them."" We are biased and corrupted. By taking the outside view on how our own algorithm performs in a given situation, we can adjust accordingly. CORRIGIBILITY The ""hard problem of corrigibility"" is to build an agent which, in an intuitive sense, reasons internally as if from the programmers' external perspective. We think the AI is incomplete, that we might have made mistakes in building it, that we might want to correct it, and that it would be e.g. dangerous for the AI to take large actions or high-impact actions or do weird new things without asking first. We would ideally want the agent to see itself in","2020-05-08","2022-01-30 04:50:44","2022-01-30 04:50:44","2020-08-31 18:49:20","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/W9SJ2CAM/corrigibility-as-outside-view.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EGTZPRSQ","conferencePaper","2016","Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart","Cooperative Inverse Reinforcement Learning","Advances in Neural Information Processing Systems 29 (NIPS 2016)","","","","https://proceedings.neurips.cc/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html","For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.","2016-06-09","2022-01-30 04:50:44","2022-01-30 04:50:44","2020-12-21","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000411","","/Users/jacquesthibodeau/Zotero/storage/C32UQET4/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/N6SZFWKU/6420-cooperative-inverse-reinforcement-learning.html; /Users/jacquesthibodeau/Zotero/storage/X6JQE3BA/1606.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NIPS 2016","","","","","","","","","","","","","","",""
"I8VKMTHM","journalArticle","2021","Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore","Cooperative AI: machines must learn to find common ground","Nature","","","10.1038/d41586-021-01170-0","https://www.nature.com/articles/d41586-021-01170-0","To help humanity solve fundamental problems of cooperation, scientists need to reconceive artificial intelligence as deeply social.","2021-05","2022-01-30 04:50:44","2022-01-30 04:50:44","2021-11-14 18:21:21","33-36","","7857","593","","","Cooperative AI","","","","","","","en","2021 Nature","","","","www.nature.com","","ZSCC: 0000013  Bandiera_abtest: a Cg_type: Comment Number: 7857 Publisher: Nature Publishing Group Subject_term: Machine learning, Computer science, Society, Technology, Sociology, Human behaviour","","/Users/jacquesthibodeau/Zotero/storage/IIZXH33A/Dafoe et al. - 2021 - Cooperative AI machines must learn to find common.pdf; /Users/jacquesthibodeau/Zotero/storage/WFFPBNZ8/d41586-021-01170-0.html","","MetaSafety","Computer science; Human behaviour; Machine learning; Society; Sociology; Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IVEVAANE","conferencePaper","2020","Turner, Alexander Matt; Hadfield-Menell, Dylan; Tadepalli, Prasad","Conservative Agency via Attainable Utility Preservation","AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society","","","10.1145/3375627.3375851","http://arxiv.org/abs/1902.09725","Reward functions are often misspeciﬁed. An agent optimizing an incorrect reward function can change its environment in large, undesirable, and potentially irreversible ways. Work on impact measurement seeks a means of identifying (and thereby avoiding) large changes to the environment. We propose a novel impact measure which induces conservative, effective behavior across a range of situations. The approach attempts to preserve the attainable utility of auxiliary objectives. We evaluate our proposal on an array of benchmark tasks and show that it matches or outperforms relative reachability, the state-of-the-art in impact measurement.","2020","2022-01-30 04:50:44","2022-01-30 04:50:44","2019-07-08 15:44:58","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000030  arXiv: 1902.09725","","/Users/jacquesthibodeau/Zotero/storage/4459ZMWD/Turner et al. - 2020 - Conservative Agency via Attainable Utility Preserv.pdf; /Users/jacquesthibodeau/Zotero/storage/375MJXAP/1902.html; /Users/jacquesthibodeau/Zotero/storage/7DTZW9IN/Turner et al. - 2019 - Conservative Agency via Attainable Utility Preserv.pdf","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KKKW3IZZ","conferencePaper","2021","Zhuang, Simon; Hadﬁeld-Menell, Dylan","Consequences of Misaligned AI","Advances in Neural Information Processing Systems 33 (2020)","","","","http://arxiv.org/abs/2102.03896","AI systems often rely on two key components: a speciﬁed goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial speciﬁcation of the principal’s goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the L attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on J < L attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal—agent problem from artiﬁcial intelligence; 2) we provide necessary and sufﬁcient conditions under which indeﬁnitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identiﬁes a theoretical scenario where some degree of interactivity is desirable.","2021-02-07","2022-01-30 04:50:44","2022-01-30 04:50:44","","11","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000004","","/Users/jacquesthibodeau/Zotero/storage/8A4KHTU8/Zhuang and Hadﬁeld-Menell - Consequences of Misaligned AI.pdf","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020","","","","","","","","","","","","","","",""
"VKGPQCJR","conferencePaper","2021","Dennis, Michael; Jaques, Natasha; Vinitsky, Eugene; Bayen, Alexandre; Russell, Stuart; Critch, Andrew; Levine, Sergey","Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design","arXiv:2012.02096 [cs]","","","","http://arxiv.org/abs/2012.02096","A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.","2021-02-03","2022-01-30 04:50:44","2022-01-30 04:50:44","2021-11-13 22:36:30","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s0]  ACC: 21  arXiv: 2012.02096","","/Users/jacquesthibodeau/Zotero/storage/23RGK32W/Dennis et al. - 2021 - Emergent Complexity and Zero-shot Transfer via Uns.pdf; /Users/jacquesthibodeau/Zotero/storage/8C39EB27/2012.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","34th Conference on Neural Information Processing Systems (NeurIPS 2020),","","","","","","","","","","","","","","",""
"VSQNXUU5","journalArticle","2019","Griffiths, Thomas L; Callaway, Frederick; Chang, Michael B; Grant, Erin; Krueger, Paul M; Lieder, Falk","Doing more with less: meta-reasoning and meta-learning in humans and machines","Current Opinion in Behavioral Sciences","","2352-1546","10.1016/j.cobeha.2019.01.005","http://www.sciencedirect.com/science/article/pii/S2352154618302122","Artificial intelligence systems use an increasing amount of computation and data to solve very specific problems. By contrast, human minds solve a wide range of problems using a fixed amount of computation and limited experience. We identify two abilities that we see as crucial to this kind of general intelligence: meta-reasoning (deciding how to allocate computational resources) and meta-learning (modeling the learning environment to make better use of limited data). We summarize the relevant AI literature and relate the resulting ideas to recent work in psychology.","2019-10-01","2022-01-30 04:50:44","2022-01-30 04:50:44","2020-12-17 22:22:24","24-30","","","29","","Current Opinion in Behavioral Sciences","Doing more with less","SI: 29: Artificial Intelligence (2019)","","","","","","en","","","","","ScienceDirect","","ZSCC: 0000039","","/Users/jacquesthibodeau/Zotero/storage/D7G3XQGQ/Griffiths et al. - 2019 - Doing more with less meta-reasoning and meta-lear.pdf; /Users/jacquesthibodeau/Zotero/storage/Z9B62VG8/S2352154618302122.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GIRDREWD","conferencePaper","2017","Basu, C.; Yang, Q.; Hungerman, D.; Sinahal, M.; Draqan, A. D.","Do You Want Your Autonomous Car to Drive Like You?","2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI","","","","","With progress in enabling autonomous cars to drive safely on the road, it is time to start asking how they should be driving. A common answer is that they should be adopting their users' driving style. This makes the assumption that users want their autonomous cars to drive like they drive - aggressive drivers want aggressive cars, defensive drivers want defensive cars. In this paper, we put that assumption to the test. We find that users tend to prefer a significantly more defensive driving style than their own. Interestingly, they prefer the style they think is their own, even though their actual driving style tends to be more aggressive. We also find that preferences do depend on the specific driving scenario, opening the door for new ways of learning driving style preference.","2017-03","2022-01-30 04:50:44","2022-01-30 04:50:44","","417-425","","","","","","","","","","","","","","","","","","IEEE Xplore","","ZSCC: 0000092  ISSN: 2167-2148","","/Users/jacquesthibodeau/Zotero/storage/J3P5R8WX/Basu et al. - 2017 - Do You Want Your Autonomous Car to Drive Like You.pdf","","CHAI; TechSafety; AmbiguosSafety","Safety; actual driving style; aggressive cars; aggressive drivers; Atmospheric measurements; automobiles; Automobiles; Autonomous automobiles; autonomous car; autonomous cars; defensive cars; defensive drivers; defensive driving style; driving preferences; driving style; driving style preference learning; road safety; Roads; specific driving scenario; Task analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI","","","","","","","","","","","","","","",""
"ZMUK52SH","conferencePaper","2020","Turner, Alexander Matt; Hadfield-Menell, Dylan; Tadepalli, Prasad","Conservative Agency","arXiv:1902.09725 [cs]","","","","http://arxiv.org/abs/1902.09725","Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.","2020","2022-01-30 04:50:44","2022-01-30 04:50:44","2019-12-16 22:27:35","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000030  arXiv: 1902.09725","","/Users/jacquesthibodeau/Zotero/storage/G2KPC32F/Turner et al. - 2019 - Conservative Agency.pdf; /Users/jacquesthibodeau/Zotero/storage/88ZQ3VWM/1902.html","","CHAI; TechSafety; BERI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AI, Ethics, and Society 2020","","","","","","","","","","","","","","",""
"I6ETESD7","journalArticle","2019","Fridovich-Keil, David; Bajcsy, Andrea; Fisac, Jaime F; Herbert, Sylvia L; Wang, Steven; Dragan, Anca D; Tomlin, Claire J","Confidence-aware motion prediction for real-time collision avoidance <sup>1</sup>","The International Journal of Robotics Research","","0278-3649, 1741-3176","10.1177/0278364919859436","http://journals.sagepub.com/doi/10.1177/0278364919859436","One of the most difficult challenges in robot motion planning is to account for the behavior of other moving agents, such as humans. Commonly, practitioners employ predictive models to reason about where other agents are going to move. Though there has been much recent work in building predictive models, no model is ever perfect: an agent can always move unexpectedly, in a way that is not predicted or not assigned sufficient probability. In such cases, the robot may plan trajectories that appear safe but, in fact, lead to collision. Rather than trust a model’s predictions blindly, we propose that the robot should use the model’s current predictive accuracy to inform the degree of confidence in its future predictions. This model confidence inference allows us to generate probabilistic motion predictions that exploit modeled structure when the structure successfully explains human motion, and degrade gracefully whenever the human moves unexpectedly. We accomplish this by maintaining a Bayesian belief over a single parameter that governs the variance of our human motion model. We couple this prediction algorithm with a recently proposed robust motion planner and controller to guide the construction of robot trajectories that are, to a good approximation, collision-free with a high, user-specified probability. We provide extensive analysis of the combined approach and its overall safety properties by establishing a connection to reachability analysis, and conclude with a hardware demonstration in which a small quadcopter operates safely in the same space as a human pedestrian.","2019-06-24","2022-01-30 04:50:43","2022-01-30 04:50:43","2019-07-08 16:10:37","027836491985943","","","","","The International Journal of Robotics Research","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s3]  ACC: 40","","","","CHAI; TechSafety","human motion prediction; motion planning; robust control; safety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQFTRH3S","blogPost","2019","Shah, Rohin; Carroll, Micah","Collaborating with Humans Requires Understanding Them","The Berkeley Artificial Intelligence Research Blog","","","","http://bair.berkeley.edu/blog/2019/10/21/coordination/","The BAIR Blog","2019","2022-01-30 04:50:43","2022-01-30 04:50:43","2020-12-18 00:11:42","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/SE7V6JPX/coordination.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V59HB96K","conferencePaper","2019","Bourgin, David D.; Peterson, Joshua C.; Reichman, Daniel; Griffiths, Thomas L.; Russell, Stuart J.","Cognitive Model Priors for Predicting Human Decisions","Proceedings of the 36th International Conference on Machine Learning","","","","http://arxiv.org/abs/1905.09397","Human decision-making underlies all economic behavior. For the past four decades, human decision-making under uncertainty has continued to be explained by theoretical models based on prospect theory, a framework that was awarded the Nobel Prize in Economic Sciences. However, theoretical models of this kind have developed slowly, and robust, high-precision predictive models of human decisions remain a challenge. While machine learning is a natural candidate for solving these problems, it is currently unclear to what extent it can improve predictions obtained by current theories. We argue that this is mainly due to data scarcity, since noisy human behavior requires massive sample sizes to be accurately captured by off-the-shelf machine learning methods. To solve this problem, what is needed are machine learning models with appropriate inductive biases for capturing human behavior, and larger datasets. We offer two contributions towards this end: first, we construct ""cognitive model priors"" by pretraining neural networks with synthetic data generated by cognitive models (i.e., theoretical models developed by cognitive psychologists). We find that fine-tuning these networks on small datasets of real human decisions results in unprecedented state-of-the-art improvements on two benchmark datasets. Second, we present the first large-scale dataset for human decision-making, containing over 240,000 human judgments across over 13,000 decision problems. This dataset reveals the circumstances where cognitive model priors are useful, and provides a new standard for benchmarking prediction of human decisions under uncertainty.","2019-05-22","2022-01-30 04:50:43","2022-01-30 04:50:43","2019-12-18 02:18:22","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000047  arXiv: 1905.09397","","/Users/jacquesthibodeau/Zotero/storage/J2ZDVPK2/Bourgin et al. - 2019 - Cognitive Model Priors for Predicting Human Decisi.pdf; /Users/jacquesthibodeau/Zotero/storage/T2SNNBCJ/1905.html","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","36th International Conference on Machine Learning","","","","","","","","","","","","","","",""
"MSC4I4M4","conferencePaper","2020","Freedman, Rachel; Shah, Rohin; Dragan, Anca","Choice Set Misspeciﬁcation in Reward Inference","CEUR Workshop Proceedings","","","","http://ceur-ws.org/Vol-2640/paper_14.pdf","Specifying reward functions for robots that operate in environments without a natural reward signal can be challenging, and incorrectly speciﬁed rewards can incentivise degenerate or dangerous behavior. A promising alternative to manually specifying reward functions is to enable robots to infer them from human feedback, like demonstrations or corrections. To interpret this feedback, robots treat as approximately optimal a choice the person makes from a choice set, like the set of possible trajectories they could have demonstrated or possible corrections they could have made. In this work, we introduce the idea that the choice set itself might be difﬁcult to specify, and analyze choice set misspeciﬁcation: what happens as the robot makes incorrect assumptions about the set of choices from which the human selects their feedback. We propose a classiﬁcation of different kinds of choice set misspeciﬁcation, and show that these different classes lead to meaningful differences in the inferred reward and resulting performance. While we would normally expect misspeciﬁcation to hurt, we ﬁnd that certain kinds of misspeciﬁcation are neither helpful nor harmful (in expectation). However, in other situations, misspeciﬁcation can be extremely harmful, leading the robot to believe the opposite of what it should believe. We hope our results will allow for better prediction and response to the effects of misspeciﬁcation in real-world reward inference.","2020","2022-01-30 04:50:43","2022-01-30 04:50:43","2020-12-18","7","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000001[s0]","","/Users/jacquesthibodeau/Zotero/storage/NZSH77X3/Freedman et al. - Choice Set Misspeciﬁcation in Reward Inference.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CEUR Workshop","","","","","","","","","","","","","","",""
"IIUG993E","manuscript","2021","Rashidinejad, Paria; Zhu, Banghua; Ma, Cong; Jiao, Jiantao; Russell, Stuart","Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism","","","","","http://arxiv.org/abs/2103.12021","Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone. Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal rate and also adapts to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in all three settings, LCB achieves a faster rate of $1/N$ for nearly-expert datasets compared to the usual rate of $1/\sqrt{N}$ in offline RL, where $N$ is the number of samples in the batch dataset. In the case of contextual bandits with at least two contexts, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in MDPs.","2021-03-22","2022-01-30 04:50:43","2022-01-30 04:50:43","2021-10-30 21:30:30","","","","","","","Bridging Offline Reinforcement Learning and Imitation Learning","","","","","","","","","","","","arXiv.org","","ZSCC: 0000016  arXiv: 2103.12021","","/Users/jacquesthibodeau/Zotero/storage/AUSU3VZ6/Rashidinejad et al. - 2021 - Bridging Offline Reinforcement Learning and Imitat.pdf; /Users/jacquesthibodeau/Zotero/storage/U5A563JV/2103.html","","TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Mathematics - Statistics Theory; Mathematics - Optimization and Control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSSCRXJG","conferencePaper","2020","Shah, Rohin; Freire, Pedro; Alex, Neel; Freedman, Rachel; Krasheninnikov, Dmitrii; Chan, Lawrence; Dennis, Michael; Abbeel, Pieter; Dragan, Anca; Russell, Stuart","Benefits of Assistance over Reward Learning","","","","","https://openreview.net/forum?id=DFIoGDZejIB","Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward...","2020-09-28","2022-01-30 04:50:43","2022-01-30 04:50:43","2020-12-18 00:39:34","","","","","","","","","","","","","","en","","","","","openreview.net","","ZSCC: 0000004","","/Users/jacquesthibodeau/Zotero/storage/X7AUG7ZJ/Anonymous - 2020 - Benefits of Assistance over Reward Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/EQCDNKBZ/forum.html","","CHAI; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020","","","","","","","","","","","","","","",""
"VHS74F2I","conferencePaper","2021","Verma, Pulkit; Marpally, Shashank Rao; Srivastava, Siddharth","Asking the Right Questions: Learning Interpretable Action Models Through Query Answering","arXiv:1912.12613 [cs]","","","","http://arxiv.org/abs/1912.12613","This paper develops a new approach for estimating an interpretable, relational model of a black-box autonomous agent that can plan and act. Our main contributions are a new paradigm for estimating such models using a minimal query interface with the agent, and a hierarchical querying algorithm that generates an interrogation policy for estimating the agent's internal model in a vocabulary provided by the user. Empirical evaluation of our approach shows that despite the intractable search space of possible agent models, our approach allows correct and scalable estimation of interpretable agent models for a wide class of black-box autonomous agents. Our results also show that this approach can use predicate classifiers to learn interpretable models of planning agents that represent states as images.","2021-04-09","2022-01-30 04:50:43","2022-01-30 04:50:43","2021-10-30 23:01:42","","","","","","","Asking the Right Questions","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s0]  ACC: 6  arXiv: 1912.12613","","/Users/jacquesthibodeau/Zotero/storage/V7GTH85I/Verma et al. - 2021 - Asking the Right Questions Learning Interpretable.pdf; /Users/jacquesthibodeau/Zotero/storage/BERGIMGS/1912.html","","TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI 2021","","","","","","","","","","","","","","",""
"UVD25SIB","conferencePaper","2019","Rahtz, Matthew; Fang, James; Dragan, Anca D.; Hadfield-Menell, Dylan","An Extensible Interactive Interface for Agent Design","","","","","http://arxiv.org/abs/1906.02641","In artiﬁcial intelligence, we often specify tasks through a reward function. While this works well in some settings, many tasks are hard to specify this way. In deep reinforcement learning, for example, directly specifying a reward as a function of a high-dimensional observation is challenging. Instead, we present an interface for specifying tasks interactively using demonstrations. Our approach deﬁnes a set of increasingly complex policies. The interface allows the user to switch between these policies at ﬁxed intervals to generate demonstrations of novel, more complex, tasks. We train new policies based on these demonstrations and repeat the process. We present a case study of our approach in the Lunar Lander domain, and show that this simple approach can quickly learn a successful landing policy and outperforms an existing comparison-based deep RL method.","2019-06-06","2022-01-30 04:50:43","2022-01-30 04:50:43","2019-07-08 15:45:46","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 1906.02641","","/Users/jacquesthibodeau/Zotero/storage/5AZABBHX/Rahtz et al. - 2019 - An Extensible Interactive Interface for Agent Desi.pdf","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 ICML Workshop on Human in the Loop Learning (HILL 2019)","","","","","","","","","","","","","","",""
"2W5PXMHE","conferencePaper","2018","Malik, Dhruv; Palaniappan, Malayandi; Fisac, Jaime F.; Hadfield-Menell, Dylan; Russell, Stuart; Dragan, Anca D.","An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning","Proceedings of the 35th International Conference on Machine Learning","","","","http://arxiv.org/abs/1806.03820","Our goal is for AI systems to correctly identify and act according to their human user’s objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a speciﬁc property of CIRL—the human is a full information agent—to derive an optimality-preserving modiﬁcation to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL’s assumption of human rationality. We apply this update to a variety of POMDP solvers and ﬁnd that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.","2018-06-11","2022-01-30 04:50:43","2022-01-30 04:50:43","2019-07-12 00:13:10","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000019  arXiv: 1806.03820","","/Users/jacquesthibodeau/Zotero/storage/CS44ETD8/Malik et al. - 2018 - An Efficient, Generalized Bellman Update For Coope.pdf; /Users/jacquesthibodeau/Zotero/storage/INHZCFF4/1806.html; /Users/jacquesthibodeau/Zotero/storage/K662JE53/1806.html","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML 2018","","","","","","","","","","","","","","",""
"XGBTMKB5","blogPost","2019","Shah, Rohin","Conclusion to the sequence on value learning","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/TE5nJ882s5dCMkBB8/conclusion-to-the-sequence-on-value-learning","This post summarizes the sequence on value learning. While it doesn’t introduce any new ideas, it does shed light on which parts I would emphasize most, and the takeaways I hope that readers get. I make several strong claims here; interpret these as my impressions, not my beliefs. I would guess many researchers disagree with the (strength of the) claims, though I do not know what their arguments would be. Over the last three months we’ve covered a lot of ground. It’s easy to lose sight of the overall picture over such a long period of time, so let's do a brief recap. THE “OBVIOUS” APPROACH Here is an argument for the importance of AI safety:  * Any agent that is much more intelligent than us should not be exploitable by    us, since if we could find some way to exploit the agent, the agent could    also find the exploit and patch it.  * Anything that is not exploitable must be an expected utility maximizer; since    we cannot exploit a superintelligent AI, it must look like an expected    utility maximizer to us.  * Due to Goodhart’s Law, even “slightly wrong” utility functions can lead to    catastrophic outcomes when maximized.  * Our utility function is complex and fragile, so getting the “right” utility    function is difficult. This argument implies that by the time we have a superintelligent AI system, there is only one part of that system that could still have been influenced by us: the utility function. Every other feature of the AI system is fixed by math. As a result, we must necessarily solve AI alignment by influencing the utility function. So of course, the natural approach is to get the right utility function, or at least an adequate one, and have our AI system optimize that utility function. Besides fragility of value, which you might hope that machine learning could overcome, the big challenge is that even if you assume full access to the entire human policy, we cannot infer their values without making an assumption about how their preferences r","2019","2022-01-30 04:50:43","2022-01-30 04:50:43","2020-12-17 04:37:32","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/PWJ5IZ83/TE5nJ882s5dCMkBB8.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZS9JJPM","conferencePaper","2019","Krasheninnikov, Dmitrii; Shah, Rohin; van Hoof, Herke","Combining reward information from multiple sources","","","","","","Given two sources of evidence about a latent variable, one can combine the information from both by multiplying the likelihoods of each piece of evidence. However, when one or both of the observation models are misspeciﬁed, the distributions will conﬂict. We study this problem in the setting with two conﬂicting reward functions learned from different sources. In such a setting, we would like to retreat to a broader distribution over reward functions, in order to mitigate the effects of misspeciﬁcation. We assume that an agent will maximize expected reward given this distribution over reward functions, and identify four desiderata for this setting. We propose a novel algorithm, Multitask Inverse Reward Design (MIRD), and compare it to a range of simple baselines. While all methods must trade off between conservatism and informativeness, through a combination of theory and empirical results on a toy environment, we ﬁnd that MIRD and its variant MIRD-IF strike a good balance between the two.","2019","2022-01-30 04:50:43","2022-01-30 04:50:43","","14","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000[s0]","","/Users/jacquesthibodeau/Zotero/storage/NB6U5WKM/Krasheninnikov et al. - Combining reward information from multiple sources.pdf","","CHAI; TechSafety; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2019 workshop on Learning with Rich Experience: Integration of Learning Paradigms","","","","","","","","","","","","","","",""
"AGFKUHA7","blogPost","2018","Shah, Rohin","Coherence arguments do not imply goal-directed behavior","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-imply-goal-directed-behavior","One of the most pleasing things about probability and expected utility theory is that there are many coherence arguments that suggest that these are the “correct” ways to reason. If you deviate from what the theory prescribes, then you must be executing a dominated strategy. There must be some other strategy that never does any worse than your strategy, but does strictly better than your strategy with certainty in at least one situation. There’s a good explanation of these arguments here. We shouldn’t expect mere humans to be able to notice any failures of coherence in a superintelligent agent, since if we could notice these failures, so could the agent. So we should expect that powerful agents appear coherent to us. (Note that it is possible that the agent doesn’t fix the failures because it would not be worth it -- in this case, the argument says that we will not be able to notice any exploitable failures.) Taken together, these arguments suggest that we should model an agent much smarter than us as an expected utility (EU) maximizer. And many people agree that EU maximizers are dangerous. So does this mean we’re doomed? I don’t think so: it seems to me that the problems about EU maximizers that we’ve identified are actually about goal-directed behavior or explicit reward maximizers. The coherence theorems say nothing about whether an AI system must look like one of these categories. This suggests that we could try building an AI system that can be modeled as an EU maximizer, yet doesn’t fall into one of these two categories, and so doesn’t have all of the problems that we worry about. Note that there are two different flavors of arguments that the AI systems we build will be goal-directed agents (which are dangerous if the goal is even slightly wrong):  * Simply knowing that an agent is intelligent lets us infer that it is    goal-directed. (ETA: See this comment for more details on this argument.)  * Humans are particularly likely to build goal-directed agen","2018","2022-01-30 04:50:43","2022-01-30 04:50:43","2020-12-17 04:36:39","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6SJ3UVN8/NxF5G6CJiof6cemTw.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CJ245SU4","manuscript","2021","Filan, Daniel; Casper, Stephen; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart","Clusterability in Neural Networks","","","","","http://arxiv.org/abs/2103.03386","The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.","2021-03-04","2022-01-30 04:50:43","2022-01-30 04:50:43","2021-10-30 22:44:22","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000005  arXiv: 2103.03386","","/Users/jacquesthibodeau/Zotero/storage/2PQM9XHE/Filan et al. - 2021 - Clusterability in Neural Networks.pdf","","TechSafety","Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9B3ZIBN5","blogPost","2019","Cottier, Ben; Shah, Rohin","Clarifying some key hypotheses in AI alignment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment","We've created a diagram mapping out important and controversial hypotheses for AI alignment. We hope that this will help researchers identify and more productively discuss their disagreements. DIAGRAM A part of the diagram. Click through to see the full version. CAVEATS  1. This does not decompose arguments exhaustively. It does not include every     reason to favour or disfavour ideas. Rather, it is a set of key hypotheses     and relationships with other hypotheses, problems, solutions, models, etc.     Some examples of important but apparently uncontroversial premises within     the AI safety community: orthogonality, complexity of value, Goodhart's     Curse, AI being deployed in a catastrophe-sensitive context.  2. This is not a comprehensive collection of key hypotheses across the whole     space of AI alignment. It focuses on a subspace that we find interesting and     is relevant to more recent discussions we have encountered, but where key     hypotheses seem relatively less illuminated. This includes rational agency     and goal-directedness, CAIS, corrigibility, and the rationale of     foundational and practical research. In hindsight, the selection criteria     was something like: 1. The idea is closely connected to the problem of         artificial systems optimizing adversarially against humans.      2. The idea must be explained sufficiently well that we         believe it is plausible.            3. Arrows in the diagram indicate flows of evidence or soft relations, not     absolute logical implications — please read the ""interpretation"" box in the     diagram. Also pay attention to any reasoning written next to a Yes/No/Defer     arrow — you may disagree with it, so don't blindly follow the arrow! BACKGROUND Much has been written in the way of arguments for AI risk. Recently there have been some talks and posts that clarify different arguments, point to open questions, and highlight the need for further clarification and analysis. We largely s","2019","2022-01-30 04:50:43","2022-01-30 04:50:43","2020-12-14 00:17:43","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VZV4SV4B/clarifying-some-key-hypotheses-in-ai-alignment.html","","CHAI; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PEHQRUAW","blogPost","2018","Filan, Daniel","Bottle Caps Aren't Optimisers","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers","Crossposted from my blog. One thing I worry about sometimes is people writing code with optimisers in it, without realising that that's what they were doing. An example of this: suppose you were doing deep reinforcement learning, doing optimisation to select a controller (that is, a neural network that takes a percept and returns an action) that generated high reward in some environment. Alas, unknown to you, this controller actually did optimisation itself to select actions that score well according to some metric that so far has been closely related to your reward function. In such a scenario, I'd be wary about your deploying that controller, since the controller itself is doing optimisation which might steer the world into a weird and unwelcome place. In order to avoid such scenarios, it would be nice if one could look at an algorithm and determine if it was doing optimisation. Ideally, this would involve an objective definition of optimisation that could be checked from the source code of the algorithm, rather than something like ""an optimiser is a system whose behaviour can't usefully be predicted mechanically, but can be predicted by assuming it near-optimises some objective function"", since such a definition breaks down when you have the algorithm's source code and can compute its behaviour mechanically. You might think about optimisation as follows: a system is optimising some objective function to the extent that that objective function attains much higher values than would be attained if the system didn't exist, or were doing some other random thing. This type of definition includes those put forward by  Yudkowsky and Oesterheld. However, I think there are crucial counterexamples to this style of definition. Firstly, consider a lid screwed onto a bottle of water. If not for this lid, or if the lid had a hole in it or were more loose, the water would likely exit the bottle via evaporation or being knocked over, but with the lid, the water stays in the b","2018","2022-01-30 04:50:43","2022-01-30 04:50:43","2020-12-13 23:01:28","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QNGDRZZ4/bottle-caps-aren-t-optimisers.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FZBNIVSD","conferencePaper","2020","Turner, Alexander Matt; Ratzlaff, Neale; Tadepalli, Prasad","Avoiding Side Effects in Complex Environments","Advances in Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020)","","","","http://arxiv.org/abs/2006.06547","Reward function speciﬁcation can be difﬁcult, even in simple environments. Realistic environments contain millions of states. Rewarding the agent for making a widget may be easy, but penalizing the multitude of possible negative side effects is hard. In toy environments, Attainable Utility Preservation (AUP) avoids side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large, randomly generated environments based on Conway’s Game of Life. By preserving optimal value for a single randomly generated reward function, AUP incurs modest overhead, completes the speciﬁed task, and avoids side effects.","2020-06-11","2022-01-30 04:50:43","2022-01-30 04:50:43","2020-08-31 18:07:40","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000011  arXiv: 2006.06547","","/Users/jacquesthibodeau/Zotero/storage/WW987S6M/Turner et al. - 2020 - Avoiding Side Effects in Complex Environments.pdf","","CHAI; TechSafety","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020","","","","","","","","","","","","","","",""
"ZTIEC2KV","journalArticle","2017","Russell, Stuart","Artificial intelligence: The future is superintelligent [Book review of ""Life 3.0: Being Human in the Age of Artificial Intelligence"" by Max Tegmark]","Nature","","1476-4687","10.1038/548520a","http://www.nature.com/articles/548520a","Stuart Russell weighs up a book on the risks and rewards of the AI revolution.","2017-08-30","2022-01-30 04:50:43","2022-01-30 04:50:43","2019-04-03 00:22:19","520-521","","","548","","","Artificial intelligence","","","","","","","en","","","","","www-nature-com.proxy.lib.uwaterloo.ca","","ZSCC: NoCitationData[s3]  ACC: 26  J: 8","","/Users/jacquesthibodeau/Zotero/storage/M2ASUJMQ/548520a.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T95TE8B7","blogPost","2020","Filan, Daniel","An Analytic Perspective on AI Alignment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/8GdPargak863xaebm/an-analytic-perspective-on-ai-alignment","This is a perspective I have on how to do useful AI alignment research. Most perspectives I’m aware of are constructive: they have some blueprint for how to build an aligned AI system, and propose making it more concrete, making the concretisations more capable, and showing that it does in fact produce an aligned AI system. I do not have a constructive perspective - I’m not sure how to build an aligned AI system, and don’t really have a favourite approach. Instead, I have an analytic perspective. I would like to understand AI systems that are built. I also want other people to understand them. I think that this understanding will hopefully act as a ‘filter’ that means that dangerous AI systems are not deployed. The following dot points lay out the perspective. Since the remainder of this post is written as nested dot points, some readers may prefer to read it in workflowy. BACKGROUND BELIEFS  * I am imagining a future world in which powerful AGI systems are made of    components roughly like neural networks (either feedforward or recurrent)    that have a large number of parameters.  * Futhermore, I’m imagining that the training process of these ML systems does    not provide enough guarantees about deployment performance.  * In particular,       I’m supposing that systems are being trained based on their ability to       deal with simulated situations, and that that’s insufficient because       deployment situations are hard to model and therefore simulate.  * One          reason that they are hard to model is the complexities of the real          world.  * The real world might be intrinsically difficult to model for             the relevant system. For instance, it’s difficult to simulate all             the situations in which the CEO of Amazon might find themselves.           * Another reason that real world situations may be hard to             model is that they are dependent on the final trained system.  * The                trained system may be able to af","2020-02-29","2022-01-30 04:50:42","2022-01-30 04:50:42","2020-09-05 18:44:29","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/NI3GMME7/an-analytic-perspective-on-ai-alignment.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W4R7FKMX","conferencePaper","2021","Hendrycks, Dan; Burns, Collin; Basart, Steven; Critch, Andrew; Li, Jerry; Song, Dawn; Steinhardt, Jacob","Aligning AI With Shared Human Values","arXiv:2008.02275 [cs]","","","","http://arxiv.org/abs/2008.02275","We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.","2021-07-24","2022-01-30 04:50:42","2022-01-30 04:50:42","2021-10-30 21:58:26","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001[s0]  arXiv: 2008.02275","","/Users/jacquesthibodeau/Zotero/storage/RC4VUUN5/Hendrycks et al. - 2020 - Aligning AI With Shared Human Values.pdf; /Users/jacquesthibodeau/Zotero/storage/XUW5SB9I/Hendrycks et al. - 2021 - Aligning AI With Shared Human Values.pdf; /Users/jacquesthibodeau/Zotero/storage/MSZT6G3R/2008.html; /Users/jacquesthibodeau/Zotero/storage/I6C3W2WW/2008.html","","CHAI; TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2021","","","","","","","","","","","","","","",""
"BDMW3AQ5","blogPost","2019","Shah, Rohin","AI safety without goal-directed behavior","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/tHxXdAn8Yuiy9y2pZ/ai-safety-without-goal-directed-behavior","When I first entered the field of AI safety, I thought of the problem as figuring out how to get the AI to have the “right” utility function. This led me to work on the problem of inferring values from demonstrators with unknown biases, despite the impossibility results in the area. I am less excited about that avenue because I am pessimistic about the prospects of ambitious value learning (for the reasons given in the first part of this sequence). I think this happened because the writing on AI risk that I encountered has the pervasive assumption that any superintelligent AI agent must be maximizing some utility function over the long term future, such that it leads to goal-directed behavior and convergent instrumental subgoals. It’s often not stated as an assumption; rather, inferences are made assuming that you have the background model that the AI is goal-directed. This makes it particularly hard to question the assumption, since you don’t realize that the assumption is even there. Another reason that this assumption is so easily accepted is that we have a long history of modeling rational agents as expected utility maximizers, and for good reason: there are many coherence arguments that say that, given that you have preferences/goals, if you aren’t using probability theory and expected utility theory, then you can be taken advantage of. It’s easy to make the inference that a superintelligent agent must be rational, and therefore it must be an expected utility maximizer. Because this assumption was so embedded in how I thought about the problem, I had trouble imagining how else to even consider the problem. I would guess this is true for at least some other people, so I want to summarize the counterargument, and list a few implications, in the hope that this makes the issue clearer. WHY GOAL-DIRECTED BEHAVIOR MAY NOT BE REQUIRED The main argument of this chapter is that it is not required that a superintelligent agent takes actions in pursuit of some goal. I","2019","2022-01-30 04:50:42","2022-01-30 04:50:42","2020-12-17 04:36:56","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/G5XADFXX/tHxXdAn8Yuiy9y2pZ.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6C2SDFXX","report","2020","Critch, Andrew; Krueger, David","AI Research Considerations for Human Existential Safety (ARCHES)","","","","","","Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity’s long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks.","2020-05-30","2022-01-30 04:50:42","2022-01-30 04:50:42","","131","","","","","","","","","","","Center for Human-Compatible AI","","en","","","","","Zotero","","ZSCC: 0000010","","/Users/jacquesthibodeau/Zotero/storage/ACREUKJD/Critch - AI Research Considerations for Human Existential S.pdf","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGXMFW28","conferencePaper","2019","Gleave, Adam; Dennis, Michael; Kant, Neel; Wild, Cody; Levine, Sergey; Russell, Stuart","Adversarial Policies: Attacking Deep Reinforcement Learning","","","","","http://arxiv.org/abs/1905.10615","Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classiﬁers. However, an attacker is not usually able to directly modify another agent’s observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We ﬁnd that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.","2019-05-25","2022-01-30 04:50:42","2022-01-30 04:50:42","2019-07-11 18:47:45","","","","","","","Adversarial Policies","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000139  arXiv: 1905.10615","","","","CHAI; TechSafety; BERI","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2020","","","","","","","","","","","","","","",""
"ME3AQUAM","manuscript","2018","Mindermann, Sören; Shah, Rohin; Gleave, Adam; Hadfield-Menell, Dylan","Active Inverse Reward Design","","","","","https://arxiv.org/abs/1809.03060","Reward design, the problem of selecting an appropriate reward function for an AI system, is both critically important, as it encodes the task the system should perform, and challenging, as it requires reasoning about and understanding the agent’s environment in detail. AI practitioners often iterate on the reward function for their systems in a trial-and-error process to get their desired behavior. Inverse reward design (IRD) is a preference inference method that infers a true reward function from an observed, possibly misspeciﬁed, proxy reward function. This allows the system to determine when it should trust its observed reward function and respond appropriately. This has been shown to avoid problems in reward design such as negative side-effects (omitting a seemingly irrelevant but important aspect of the task) and reward hacking (learning to exploit unanticipated loopholes). In this paper, we actively select the set of proxy reward functions available to the designer. This improves the quality of inference and simpliﬁes the associated reward design problem. We present two types of queries: discrete queries, where the system designer chooses from a discrete set of reward functions, and feature queries, where the system queries the designer for weights on a small set of features. We evaluate this approach with experiments in a personal shopping assistant domain and a 2D navigation domain. We ﬁnd that our approach leads to reduced regret at test time compared with vanilla IRD. Our results indicate that actively selecting the set of available reward functions is a promising direction to improve the efﬁciency and effectiveness of reward design.","2018","2022-01-30 04:50:42","2022-01-30 04:50:42","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000015","","/Users/jacquesthibodeau/Zotero/storage/S59BK5N6/Mindermann et al. - 2019 - Active Inverse Reward Design.pdf; /Users/jacquesthibodeau/Zotero/storage/9KM9HA7F/1809.html; /Users/jacquesthibodeau/Zotero/storage/MTNGDAC9/Mindermann et al. - Active Inverse Reward Design.pdf","","CHAI; TechSafety","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P3T7ZT65","conferencePaper","2019","Bahdanau, Dzmitry; Hill, Felix; Leike, Jan; Hughes, Edward; Hosseini, Arian; Kohli, Pushmeet; Grefenstette, Edward","Learning to Understand Goal Specifications by Modelling Reward","arXiv:1806.01946 [cs]","","","","http://arxiv.org/abs/1806.01946","Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.","2019-02-15","2022-01-30 04:52:39","2022-01-30 04:52:39","2019-12-16 20:32:35","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s3]  ACC: 73  J: 33 arXiv: 1806.01946","","/Users/jacquesthibodeau/Zotero/storage/MG92TPH7/Bahdanau et al. - 2019 - Learning to Understand Goal Specifications by Mode.pdf; /Users/jacquesthibodeau/Zotero/storage/Z4PZME9T/1806.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2019","","","","","","","","","","","","","","",""
"KIWMVR6M","conferencePaper","2020","Anthony, Thomas; Eccles, Tom; Tacchetti, Andrea; Kramár, János; Gemp, Ian; Hudson, Thomas C.; Porcel, Nicolas; Lanctot, Marc; Pérolat, Julien; Everett, Richard; Werpachowski, Roman; Singh, Satinder; Graepel, Thore; Bachrach, Yoram","Learning to Play No-Press Diplomacy with Best Response Policy Iteration","34th Conference on Neural Information Processing Systems (NeurIPS 2020)","","","","http://arxiv.org/abs/2006.04635","Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate ﬁctitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements.","2020-08-26","2022-01-30 04:52:39","2022-01-30 04:52:39","2020-08-31 17:58:54","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: NoCitationData[s2]  ACC: 16  arXiv: 2006.04635","","/Users/jacquesthibodeau/Zotero/storage/ZMDHG4BH/Anthony et al. - 2020 - Learning to Play No-Press Diplomacy with Best Resp.pdf","","TechSafety; DeepMind; AmbiguosSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020","","","","","","","","","","","","","","",""
"45FFTZSD","conferencePaper","2021","Stastny, Julian; Riché, Maxime; Lyzhov, Alexander; Treutlein, Johannes; Dafoe, Allan; Clifton, Jesse","Normative Disagreement as a Challenge for Cooperative AI","Cooperative AI workshop and the Strategic ML workshop at NeurIPS 2021","","","","http://arxiv.org/abs/2111.13872","Cooperation in settings where agents have both common and conflicting interests (mixed-motive environments) has recently received considerable attention in multi-agent learning. However, the mixed-motive environments typically studied have a single cooperative outcome on which all agents can agree. Many real-world multi-agent environments are instead bargaining problems (BPs): they have several Pareto-optimal payoff profiles over which agents have conflicting preferences. We argue that typical cooperation-inducing learning algorithms fail to cooperate in BPs when there is room for normative disagreement resulting in the existence of multiple competing cooperative equilibria, and illustrate this problem empirically. To remedy the issue, we introduce the notion of norm-adaptive policies. Norm-adaptive policies are capable of behaving according to different norms in different circumstances, creating opportunities for resolving normative disagreement. We develop a class of norm-adaptive policies and show in experiments that these significantly increase cooperation. However, norm-adaptiveness cannot address residual bargaining failure arising from a fundamental tradeoff between exploitability and cooperative robustness.","2021-11-27","2022-01-30 04:52:39","2022-01-30 04:52:39","2021-12-11 14:19:23","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2111.13872","","/Users/jacquesthibodeau/Zotero/storage/NRTX938T/Stastny et al. - 2021 - Normative Disagreement as a Challenge for Cooperat.pdf","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2021","","","","","","","","","","","","","","",""
"68WZKQQ4","manuscript","2018","Ortega, Pedro A.; Legg, Shane","Modeling Friends and Foes","","","","","http://arxiv.org/abs/1807.00196","How can one detect friendly and adversarial behavior from raw data? Detecting whether an environment is a friend, a foe, or anything in between, remains a poorly understood yet desirable ability for safe and robust agents. This paper proposes a definition of these environmental ""attitudes"" based on an characterization of the environment's ability to react to the agent's private strategy. We define an objective function for a one-shot game that allows deriving the environment's probability distribution under friendly and adversarial assumptions alongside the agent's optimal strategy. Furthermore, we present an algorithm to compute these equilibrium strategies, and show experimentally that both friendly and adversarial environments possess non-trivial optimal strategies.","2018-06-30","2022-01-30 04:52:39","2022-01-30 04:52:39","2019-12-16 20:35:04","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000004  J: 2 arXiv: 1807.00196","","/Users/jacquesthibodeau/Zotero/storage/VWQC9GCA/Ortega and Legg - 2018 - Modeling Friends and Foes.pdf; /Users/jacquesthibodeau/Zotero/storage/IUKTABAW/1807.html","","TechSafety; DeepMind","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVAQUPVW","conferencePaper","2019","Everitt, Tom; Kumar, Ramana; Krakovna, Victoria; Legg, Shane","Modeling AGI Safety Frameworks with Causal Influence Diagrams","arXiv:1906.08663 [cs]","","","","http://arxiv.org/abs/1906.08663","Proposals for safe AGI systems are typically made at the level of frameworks, specifying how the components of the proposed system should be trained and interact with each other. In this paper, we model and compare the most promising AGI safety frameworks using causal influence diagrams. The diagrams show the optimization objective and causal assumptions of the framework. The unified representation permits easy comparison of frameworks and their assumptions. We hope that the diagrams will serve as an accessible and visual introduction to the main AGI safety frameworks.","2019-06-20","2022-01-30 04:52:39","2022-01-30 04:52:39","2019-12-16 20:27:05","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000014  arXiv: 1906.08663","","/Users/jacquesthibodeau/Zotero/storage/2GA3KWFC/Everitt et al. - 2019 - Modeling AGI Safety Frameworks with Causal Influen.pdf; /Users/jacquesthibodeau/Zotero/storage/J3KK437V/1906.html; /Users/jacquesthibodeau/Zotero/storage/9UU2WGKD/1906.html","","TechSafety; DeepMind","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCAI 2019 AI Safety Workshop","","","","","","","","","","","","","","",""
"BMA4TR5R","book","2018","Krakovna, Viktoriya; Orseau, Laurent; Martic, Miljan; Legg, Shane","Measuring and avoiding side effects using relative reachability","","","","","","How can we design reinforcement learning agents that avoid causing unnecessary disruptions to their environment? We argue that current approaches to penalizing side effects can introduce bad incentives in tasks that require irreversible actions, and in environments that contain sources of change other than the agent. For example, some approaches give the agent an incentive to prevent any irreversible changes in the environment, including the actions of other agents. We introduce a general definition of side effects, based on relative reachability of states compared to a default state, that avoids these undesirable incentives. Using a set of gridworld experiments illustrating relevant scenarios, we empirically compare relative reachability to penalties based on existing definitions and show that it is the only penalty among those tested that produces the desired behavior in all the scenarios.","2018-06-04","2022-01-30 04:52:39","2022-01-30 04:52:39","","","","","","","","","","","","","","","","","","","","ResearchGate","","ZSCC: 0000014","","","https://www.researchgate.net/profile/Viktoriya_Krakovna/publication/325557348_Measuring_and_avoiding_side_effects_using_relative_reachability/links/5bb7e5eaa6fdcc9552d46b02/Measuring-and-avoiding-side-effects-using-relative-reachability.pdf","TechSafety; DeepMind; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3GKZHK9F","manuscript","2019","Chow, Yinlam; Nachum, Ofir; Faust, Aleksandra; Duenez-Guzman, Edgar; Ghavamzadeh, Mohammad","Lyapunov-based Safe Policy Optimization for Continuous Control","","","","","http://arxiv.org/abs/1901.10031","We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e.,~policies that do not take the agent to undesirable situations. We formulate these problems as constrained Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction. Videos of the experiments can be found in the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.","2019-02-11","2022-01-30 04:52:39","2022-01-30 04:52:39","2019-12-16 20:32:58","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000083  arXiv: 1901.10031","","/Users/jacquesthibodeau/Zotero/storage/IN5MJ7VZ/Chow et al. - 2019 - Lyapunov-based Safe Policy Optimization for Contin.pdf; /Users/jacquesthibodeau/Zotero/storage/85W7HCZP/1901.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I2AKM3D5","conferencePaper","2019","Wang, Chenglong; Bunel, Rudy; Dvijotham, Krishnamurthy; Huang, Po-Sen; Grefenstette, Edward; Kohli, Pushmeet","Knowing When to Stop: Evaluation and Verification of Conformity to Output-size Specifications","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Knowing_When_to_Stop_Evaluation_and_Verification_of_Conformity_to_CVPR_2019_paper.html","Models such as Sequence-to-Sequence and Image-to-Sequence are widely used in real world applications. While the ability of these neural architectures to produce variable-length outputs makes them extremely effective for problems like Machine Translation and Image Captioning, it also leaves them vulnerable to failures of the form where the model produces outputs of undesirable length. This behavior can have severe consequences such as usage of increased computation and induce faults in downstream modules that expect outputs of a certain length. Motivated by the need to have a better understanding of the failures of these models, this paper proposes and studies the novel output-size modulation problem and makes two key technical contributions. First, to evaluate model robustness, we develop an easy-to-compute differentiable proxy objective that can be used with gradient-based algorithms to find output-lengthening inputs. Second and more importantly, we develop a verification approach that can formally verify whether a network always produces outputs within a certain length. Experimental results on Machine Translation and Image Captioning show that our output-lengthening approach can produce outputs that are 50 times longer than the input, while our verification approach can, given a model and input domain, prove that the output length is below a certain size.","2019-04-26","2022-01-30 04:52:38","2022-01-30 04:52:38","2020-12-20","","","","","","","Knowing When to Stop","","","","","","","","","","","","arXiv.org","","ZSCC: 0000010  arXiv: 1904.12004","","/Users/jacquesthibodeau/Zotero/storage/5RJNUC4N/Wang et al. - 2019 - Knowing When to Stop Evaluation and Verification .pdf; /Users/jacquesthibodeau/Zotero/storage/TGZNQ5NZ/1904.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"TF8PSWUZ","conferencePaper","2019","Nalisnick, Eric; Matsukawa, Akihiro; Teh, Yee Whye; Gorur, Dilan; Lakshminarayanan, Balaji","Hybrid Models with Deep and Invertible Features","arXiv:1902.02767 [cs, stat]","","","","http://arxiv.org/abs/1902.02767","We propose a neural hybrid model consisting of a linear model defined on a set of features computed by a deep, invertible transformation (i.e. a normalizing flow). An attractive property of our model is that both p(features), the density of the features, and p(targets | features), the predictive distribution, can be computed exactly in a single feed-forward pass. We show that our hybrid model, despite the invertibility constraints, achieves similar accuracy to purely predictive models. Moreover the generative component remains a good model of the input features despite the hybrid optimization objective. This offers additional capabilities such as detection of out-of-distribution inputs and enabling semi-supervised learning. The availability of the exact joint density p(targets, features) also allows us to compute many quantities readily, making our hybrid model a useful building block for downstream applications of probabilistic deep learning.","2019-05-29","2022-01-30 04:52:38","2022-01-30 04:52:38","2019-12-16 20:32:50","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000045  arXiv: 1902.02767","","/Users/jacquesthibodeau/Zotero/storage/VUE9U2IJ/Nalisnick et al. - 2019 - Hybrid Models with Deep and Invertible Features.pdf; /Users/jacquesthibodeau/Zotero/storage/V56US5SD/1902.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML 2019","","","","","","","","","","","","","","",""
"JBZW9WNV","manuscript","2020","Krueger, David; Maharaj, Tegan; Leike, Jan","Hidden Incentives for Auto-Induced Distributional Shift","","","","","http://arxiv.org/abs/2009.09153","Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.","2020-09-18","2022-01-30 04:52:38","2022-01-30 04:52:38","2020-11-21 17:32:28","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000007  arXiv: 2009.09153","","/Users/jacquesthibodeau/Zotero/storage/C4IC67EE/Krueger et al. - 2020 - Hidden Incentives for Auto-Induced Distributional .pdf; /Users/jacquesthibodeau/Zotero/storage/ZHEWPCUB/2009.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TECSUSTJ","blogPost","2021","Bensinger, Rob; Garrabrant, Scott; Shah, Rohin; Tyre, Eli","Garrabrant and Shah on human modeling in AGI","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/Wap8sSDoiigrJibHA/garrabrant-and-shah-on-human-modeling-in-agi","This is an edited transcript of a conversation between Scott Garrabrant (MIRI) and Rohin Shah (DeepMind) about whether researchers should focus more on approaches to AI alignment that don’t require highly capable AI systems to do much human modeling. CFAR’s Eli Tyre facilitated the conversation. To recap, and define some terms:  * The alignment problem is the problem of figuring out ""how to develop    sufficiently advanced machine intelligences such that running them produces    good outcomes in the real world"" (outcome alignment) or the problem of    building powerful AI systems that are trying to do what their operators want    them to do (intent alignment).  * In 2016, Hadfield-Mennell, Dragan, Abbeel, and Russell proposed that we think    of the alignment problem in terms of “Cooperative Inverse Reinforcement    Learning” (CIRL), a framework where the AI system is initially uncertain of    its reward function, and interacts over time with a human (who knows the    reward function) in order to learn it.  * In 2016-2017, Christiano proposed “Iterated Distillation and Amplification”    (IDA), an approach to alignment that involves iteratively training AI systems     to learn from human experts assisted by AI helpers. In 2018, Irving,    Christiano, and Amodei proposed AI safety via debate, an approach based on    similar principles.  * In early 2019, Scott Garrabrant and DeepMind’s Ramana Kumar argued in “    Thoughts on Human Models” that we should be “cautious about AGI designs that    use human models” and should “put more effort into developing approaches that    work well in the absence of human models”.  * In early February 2021, Scott and Rohin talked more about human modeling and    decided to have the real-time conversation below. You can find a recording of the Feb. 28 discussion below (sans Q&A) here. 1. IDA, CIRL, AND INCENTIVES Eli:I guess I want to first check what our goal is here. There was some stuff that happened online. Where are we accordi","2021-08-04","2022-01-30 04:52:38","2022-01-30 04:52:38","2021-11-18 23:07:47","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VID79W62/garrabrant-and-shah-on-human-modeling-in-agi.html","","TechSafety; AmbiguousSafety","","","","","Garrabrant, Scott; Shah, Rohin; Tyre, Eli","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UPVUNRB7","manuscript","2021","Cohen, Michael K.; Hutter, Marcus; Nanda, Neel","Fully General Online Imitation Learning","","","","","http://arxiv.org/abs/2102.08686","In imitation learning, imitators and demonstrators are policies for picking actions given past interactions with the environment. If we run an imitator, we probably want events to unfold similarly to the way they would have if the demonstrator had been acting the whole time. No existing work provides formal guidance in how this might be accomplished, instead restricting focus to environments that restart, making learning unusually easy, and conveniently limiting the significance of any mistake. We address a fully general setting, in which the (stochastic) environment and demonstrator never reset, not even for training purposes. Our new conservative Bayesian imitation learner underestimates the probabilities of each available action, and queries for more data with the remaining probability. Our main result: if an event would have been unlikely had the demonstrator acted the whole time, that event's likelihood can be bounded above when running the (initially totally ignorant) imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in frequency.","2021-02-17","2022-01-30 04:52:38","2022-01-30 04:52:38","2021-10-31 19:13:33","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2102.08686","","/Users/jacquesthibodeau/Zotero/storage/DZ2RDHBK/Cohen et al. - 2021 - Fully General Online Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/79H7AD64/2102.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0; I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ECSG4EAB","conferencePaper","2021","Hammond, Lewis; Fox, James; Everitt, Tom; Abate, Alessandro; Wooldridge, Michael","Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice","arXiv:2102.05008 [cs]","","","","http://arxiv.org/abs/2102.05008","Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.","2021-02-09","2022-01-30 04:52:38","2022-01-30 04:52:38","2021-10-31 19:02:39","","","","","","","Equilibrium Refinements for Multi-Agent Influence Diagrams","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 2102.05008","","/Users/jacquesthibodeau/Zotero/storage/GKVZSRK8/Hammond et al. - 2021 - Equilibrium Refinements for Multi-Agent Influence .pdf; /Users/jacquesthibodeau/Zotero/storage/I9D87K7N/2102.html; /Users/jacquesthibodeau/Zotero/storage/J3QUV7I2/2102.html","","TechSafety","Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS-21)","","","","","","","","","","","","","","",""
"2BNK79XQ","conferencePaper","2019","Nalisnick, Eric; Matsukawa, Akihiro; Teh, Yee Whye; Gorur, Dilan; Lakshminarayanan, Balaji","Do Deep Generative Models Know What They Don't Know?","arXiv:1810.09136 [cs, stat]","","","","http://arxiv.org/abs/1810.09136","A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.","2019-02-24","2022-01-30 04:52:38","2022-01-30 04:52:38","2019-12-16 20:34:40","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s0]  ACC: 333  J: 130 arXiv: 1810.09136","","/Users/jacquesthibodeau/Zotero/storage/DKICH6Q7/Nalisnick et al. - 2019 - Do Deep Generative Models Know What They Don't Kno.pdf; /Users/jacquesthibodeau/Zotero/storage/U8767CAD/1810.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2019","","","","","","","","","","","","","","",""
"356XZMGD","blogPost","2019","Ngo, Richard","Disentangling arguments for the importance of AI safety","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/JbcWQCxKWn3y49bNB/disentangling-arguments-for-the-importance-of-ai-safety","[Note: my views have changed since writing this post, and while I still consider it useful as a catalogue of concerns, I no longer think that it satisfactorily disentangles those concerns from each other. I hope to post better material along these lines later this year]. I recently attended the 2019 Beneficial AGI conference organised by the Future of Life Institute. I’ll publish a more complete write-up later, but I was particularly struck by how varied attendees' reasons for considering AI safety important were. Before this, I’d observed a few different lines of thought, but interpreted them as different facets of the same idea. Now, though, I’ve identified at least 6 distinct serious arguments for why AI safety is a priority. By distinct I mean that you can believe any one of them without believing any of the others - although of course the particular categorisation I use is rather subjective, and there’s a significant amount of overlap. In this post I give a brief overview of my own interpretation of each argument (note that I don’t necessarily endorse them myself). They are listed roughly from most specific and actionable to most general. I finish with some thoughts on what to make of this unexpected proliferation of arguments. Primarily, I think it increases the importance of clarifying and debating the core ideas in AI safety.  1. Maximisers are dangerous. Superintelligent AGI will behave as if it’s     maximising the expectation of some utility function, since doing otherwise     can be shown to be irrational. Yet we can’t write down a utility function     which precisely describes human values, and optimising very hard for any     other function will lead to that AI rapidly seizing control (as a convergent     instrumental subgoal) and building a future which contains very little of     what we value (because of Goodhart’s law and the complexity and fragility of     values). We won’t have a chance to notice and correct misalignment because     an AI which","2019-01-21","2022-01-30 04:52:38","2022-01-30 04:52:38","2020-11-21 16:54:39","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VFQSTXH8/disentangling-arguments-for-the-importance-of-ai-safety.html","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4F7ET9FG","conferencePaper","2019","Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey; Legg, Shane; Leike, Jan","Learning Human Objectives by Evaluating Hypothetical Behavior","Proceedings of the 37th International Conference on Machine Learning","","","","http://proceedings.mlr.press/v119/reddy20a.html","We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.","2019-12-05","2022-01-30 04:52:38","2022-01-30 04:52:38","2020-12-20","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000005[s0]  arXiv: 1912.05652","","/Users/jacquesthibodeau/Zotero/storage/VQQ2Q6XW/Reddy et al. - 2019 - Learning Human Objectives by Evaluating Hypothetic.pdf; /Users/jacquesthibodeau/Zotero/storage/RVU3BSAV/1912.html; /Users/jacquesthibodeau/Zotero/storage/RG8QBCV5/1912.html","","CHAI; TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","37th International Conference on Machine Learning","","","","","","","","","","","","","","",""
"XE2TSZTT","blogPost","2019","Krakovna, Victoria","ICLR Safe ML Workshop Report","Future of Life Institute","","","","https://futureoflife.org/2019/06/18/iclr-safe-ml-workshop-report/","Victoria Krakovna co-organized the 2019 ICLR Safe ML workshop. One of the main goals was to bring together near and long term safety research communities.","2019-06-18","2022-01-30 04:52:38","2022-01-30 04:52:38","2020-12-14 23:28:16","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/SPUUEQCU/iclr-safe-ml-workshop-report.html","","TechSafety; DeepMind; FLI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TQWJG7NP","manuscript","2020","Hill, Felix; Mokra, Sona; Wong, Nathaniel; Harley, Tim","Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text","","","","","http://arxiv.org/abs/2005.09382","Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instructionfollowing with deep RL typically involves language generated from templates (by an environment simulator), which does not reﬂect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.","2020-05-19","2022-01-30 04:52:38","2022-01-30 04:52:38","2020-08-31 18:19:41","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000019  arXiv: 2005.09382","","/Users/jacquesthibodeau/Zotero/storage/MSQMATUV/Hill et al. - 2020 - Human Instruction-Following with Deep Reinforcemen.pdf","","TechSafety; DeepMind","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ISV8DZWT","blogPost","2020","Ngo, Richard","Environments as a bottleneck in AGI development","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/vqpEC3MPioHX7bv4t/environments-as-a-bottleneck-in-agi-development","Given a training environment or dataset, a training algorithm, an optimiser, and a model class capable of implementing an AGI (with the right parameters), there are two interesting questions we might ask about how conducive that environment is for training an AGI. The first is: how much do AGIs from that model class outperform non-AGIs? The second is: how straightforward is the path to reaching an AGI? We can visualise these questions in terms of the loss landscape of those models when evaluated on the training environment. The first asks how low the set of AGIs is, compared with the rest of the landscape. The second asks how favourable the paths through that loss landscape to get to AGIs are - that is, do the local gradients usually point in the right direction, and how deep are the local minima? Some people believe that there are many environments in which AGIs can be reached via favourable paths in the loss landscape and dramatically outperform non-AGIs; let’s call this the easy paths hypothesis. By contrast, the hard paths hypothesis is that it’s rare for environments (even complex meta-environments consisting of many separate tasks) to straightforwardly incentivise the development of general intelligence. This would suggest that specific environmental features will be necessary to prevent most models from getting stuck in local minima where they only possess narrow, specialised cognitive skills. There has been a range of speculation on what such features might be - perhaps multi-agent autocurricula, or realistic simulations, or specific types of human feedback. I’ll discuss some of these possibilities later in the post. This spectrum is complicated by its dependence on the model class, training algorithm, and choice of optimiser. If we had a perfect optimiser, then the hilliness of the loss landscape wouldn’t matter. For now, I'm imagining using optimisers fairly similar to current stochastic gradient descent. Meanwhile, I’m assuming in this post that (in acc","2020-07-17","2022-01-30 04:52:38","2022-01-30 04:52:38","2020-08-28 17:46:55","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UBTACAKX/environments-as-a-bottleneck-in-agi-development.html","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MVP5BIM","conferencePaper","2017","Christiano, Paul; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario","Deep reinforcement learning from human preferences","Advances in Neural Information Processing Systems 30 (NIPS 2017)","","","","https://papers.nips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html","For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.","2017-07-13","2022-01-30 04:52:38","2022-01-30 04:52:38","2020-12-20","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000517  arXiv: 1706.03741","","/Users/jacquesthibodeau/Zotero/storage/64K9P9TW/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/73TQPCTW/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/CZG2MJHH/1706.html; /Users/jacquesthibodeau/Zotero/storage/DBXZUTUS/1706.html; /Users/jacquesthibodeau/Zotero/storage/2SRIU7JU/1706.html","","TechSafety; Open-AI; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NIPS 2017","","","","","","","","","","","","","","",""
"6E69ZJCN","manuscript","2019","Fort, Stanislav; Hu, Huiyi; Lakshminarayanan, Balaji","Deep Ensembles: A Loss Landscape Perspective","","","","","http://arxiv.org/abs/1912.02757","Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable approximate Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. We demonstrate that while low-loss connectors between modes exist, they are not connected in the space of predictions. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods.","2019-12-05","2022-01-30 04:52:38","2022-01-30 04:52:38","2019-12-16 20:30:39","","","","","","","Deep Ensembles","","","","","","","","","","","","arXiv.org","","ZSCC: 0000155  arXiv: 1912.02757","","/Users/jacquesthibodeau/Zotero/storage/6GHKJWB9/Fort et al. - 2019 - Deep Ensembles A Loss Landscape Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/DU5ECF4A/1912.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3SF48G8T","journalArticle","2020","Mohamed, Shakir; Png, Marie-Therese; Isaac, William","Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence","Philosophy & Technology","","2210-5433, 2210-5441","10.1007/s13347-020-00405-8","http://arxiv.org/abs/2007.04068","This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial Intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. Whilst the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us; ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.","2020-12","2022-01-30 04:52:38","2022-01-30 04:52:38","2020-12-12 15:23:49","659-684","","4","33","","Philos. Technol.","Decolonial AI","","","","","","","","","","","","arXiv.org","","ZSCC: 0000014  arXiv: 2007.04068","","/Users/jacquesthibodeau/Zotero/storage/H43MBXZN/Mohamed et al. - 2020 - Decolonial AI Decolonial Theory as Sociotechnical.pdf; /Users/jacquesthibodeau/Zotero/storage/BUXA7WXT/2007.html","","MetaSafety; DeepMind; AmbiguosSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6IHS4DSU","journalArticle","2021","Cohen, Michael K.; Hutter, Marcus","Curiosity Killed the Cat and the Asymptotically Optimal Agent","IEEE Journal on Selected Areas in Information Theory","","","","http://arxiv.org/abs/2006.03357","Reinforcement learners are agents that learn to pick actions that lead to high reward. Ideally, the value of a reinforcement learner's policy approaches optimality--where the optimal informed policy is the one which maximizes reward. Unfortunately, we show that if an agent is guaranteed to be ""asymptotically optimal"" in any (stochastically computable) environment, then subject to an assumption about the true environment, this agent will be either destroyed or incapacitated with probability 1; both of these are forms of traps as understood in the Markov Decision Process literature. Environments with traps pose a well-known problem for agents, but we are unaware of other work which shows that traps are not only a risk, but a certainty, for agents of a certain caliber. Much work in reinforcement learning uses an ergodicity assumption to avoid this problem. Often, doing theoretical research under simplifying assumptions prepares us to provide practical solutions even in the absence of those assumptions, but the ergodicity assumption in reinforcement learning may have led us entirely astray in preparing safe and effective exploration strategies for agents in dangerous environments. Rather than assuming away the problem, we present an agent with the modest guarantee of approaching the performance of a mentor, doing safe exploration instead of reckless exploration.","2021-05-14","2022-01-30 04:52:38","2022-01-30 04:52:38","2020-09-05 17:28:03","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2006.03357","","/Users/jacquesthibodeau/Zotero/storage/3TP7IVVC/Cohen and Hutter - 2020 - Curiosity Killed the Cat and the Asymptotically Op.pdf; /Users/jacquesthibodeau/Zotero/storage/ANRSI6HU/2006.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0; I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVV9FT4X","journalArticle","2021","Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore","Cooperative AI: machines must learn to find common ground","Nature","","","10.1038/d41586-021-01170-0","https://www.nature.com/articles/d41586-021-01170-0","To help humanity solve fundamental problems of cooperation, scientists need to reconceive artificial intelligence as deeply social.","2021-05","2022-01-30 04:52:37","2022-01-30 04:52:37","2021-11-14 18:21:21","33-36","","7857","593","","","Cooperative AI","","","","","","","en","2021 Nature","","","","www.nature.com","","ZSCC: 0000013  Bandiera_abtest: a Cg_type: Comment Number: 7857 Publisher: Nature Publishing Group Subject_term: Machine learning, Computer science, Society, Technology, Sociology, Human behaviour","","/Users/jacquesthibodeau/Zotero/storage/QIWJSF7G/Dafoe et al. - 2021 - Cooperative AI machines must learn to find common.pdf; /Users/jacquesthibodeau/Zotero/storage/KA6U3UZF/d41586-021-01170-0.html","","MetaSafety","Computer science; Human behaviour; Machine learning; Society; Sociology; Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N667JVNF","blogPost","2018","Ortega, Pedro; Maini, Vishal","Building safe artificial intelligence: specification, robustness, and assurance","Deep Mind Safety Research (Medium)","","","","https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1","By Pedro A. Ortega, Vishal Maini, and the DeepMind safety team","2018-09-27","2022-01-30 04:52:37","2022-01-30 04:52:37","2020-11-21 17:04:08","","","","","","","Building safe artificial intelligence","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/B3G6N3T7/building-safe-artificial-intelligence-52f5f75058f1.html; /Users/jacquesthibodeau/Zotero/storage/MH6PAXF4/building-safe-artificial-intelligence-52f5f75058f1.html","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNWXKCU9","manuscript","2020","Uesato, Jonathan; Kumar, Ramana; Krakovna, Victoria; Everitt, Tom; Ngo, Richard; Legg, Shane","Avoiding Tampering Incentives in Deep RL via Decoupled Approval","","","","","http://arxiv.org/abs/2011.08827","How can we design agents that pursue a given objective when all feedback mechanisms are influenceable by the agent? Standard RL algorithms assume a secure reward function, and can thus perform poorly in settings where agents can tamper with the reward-generating mechanism. We present a principled solution to the problem of learning from influenceable feedback, which combines approval with a decoupled feedback collection procedure. For a natural class of corruption functions, decoupled approval algorithms have aligned incentives both at convergence and for their local updates. Empirically, they also scale to complex 3D environments where tampering is possible.","2020-11-17","2022-01-30 04:52:37","2022-01-30 04:52:37","2020-12-12 15:36:22","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 2011.08827","","/Users/jacquesthibodeau/Zotero/storage/D9UAFGVG/Uesato et al. - 2020 - Avoiding Tampering Incentives in Deep RL via Decou.pdf; /Users/jacquesthibodeau/Zotero/storage/9PA82CZF/2011.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XQSZS4G","conferencePaper","2020","Krakovna, Victoria; Orseau, Laurent; Ngo, Richard; Martic, Miljan; Legg, Shane","Avoiding Side Effects By Considering Future Tasks","","","","","https://arxiv.org/abs/2010.07877v1","Designing reward functions is difficult: the designer has to specify what to do (what it means to complete the task) as well as what not to do (side effects that should be avoided while completing the task). To alleviate the burden on the reward designer, we propose an algorithm to automatically generate an auxiliary reward function that penalizes side effects. This auxiliary objective rewards the ability to complete possible future tasks, which decreases if the agent causes side effects during the current task. The future task reward can also give the agent an incentive to interfere with events in the environment that make future tasks less achievable, such as irreversible actions by other agents. To avoid this interference incentive, we introduce a baseline policy that represents a default course of action (such as doing nothing), and use it to filter out future tasks that are not achievable by default. We formally define interference incentives and show that the future task approach with a baseline policy avoids these incentives in the deterministic case. Using gridworld environments that test for side effects and interference, we show that our method avoids interference and is more effective for avoiding side effects than the common approach of penalizing irreversible actions.","2020-10-15","2022-01-30 04:52:37","2022-01-30 04:52:37","2020-11-14 01:21:10","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000008","","/Users/jacquesthibodeau/Zotero/storage/GD35BH4A/Krakovna et al. - 2020 - Avoiding Side Effects By Considering Future Tasks.pdf; /Users/jacquesthibodeau/Zotero/storage/JGBTUMMS/2010.html; /Users/jacquesthibodeau/Zotero/storage/ZKJFR6KA/2010.html","","TechSafety; DeepMind; AmbiguosSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","34th Conference on Neural Information Processing Systems (NeurIPS 2020)","","","","","","","","","","","","","","",""
"K9NTV5MS","conferencePaper","2020","Cohen, Michael K.; Vellambi, Badri; Hutter, Marcus","Asymptotically Unambitious Artificial General Intelligence","arXiv:1905.12186 [cs]","","","","http://arxiv.org/abs/1905.12186","General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI's goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where ""unambitiousness"" includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.","2020-07-21","2022-01-30 04:52:37","2022-01-30 04:52:37","2020-12-12 15:23:37","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000004  arXiv: 1905.12186","","/Users/jacquesthibodeau/Zotero/storage/J4XKH5RA/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/M93T5S2C/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/GWJJ68ZU/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/K6V2XB34/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/BUHDTGFM/1905.html; /Users/jacquesthibodeau/Zotero/storage/JHRND8CX/1905.html; /Users/jacquesthibodeau/Zotero/storage/8QTX7V2T/1905.html; /Users/jacquesthibodeau/Zotero/storage/3IZJIKXJ/1905.html","","TechSafety; FHI; DeepMind","Computer Science - Artificial Intelligence; I.2.0; I.2.6; I.2.0, I.2.6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI 2020","","","","","","","","","","","","","","",""
"26XP8265","manuscript","2020","Ngo, Richard","AGI Safety From First Principles","","","","","","","2020","2022-01-30 04:52:37","2022-01-30 04:52:37","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/J2MQFEZQ/Ngo - AGI Safety From First Principles.pdf","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4C7IP8P3","conferencePaper","2019","Ovadia, Yaniv; Fertig, Emily; Ren, Jie; Nado, Zachary; Sculley, D.; Nowozin, Sebastian; Dillon, Joshua V.; Lakshminarayanan, Balaji; Snoek, Jasper","Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift","Advances in Neural Information Processing Systems, 2019","","","","http://arxiv.org/abs/1906.02530","Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.","2019-06-06","2022-01-30 04:52:37","2022-01-30 04:52:37","2019-12-16 20:32:03","","","","","","","Can You Trust Your Model's Uncertainty?","","","","","","","","","","","","arXiv.org","","ZSCC: 0000558  arXiv: 1906.02530","","/Users/jacquesthibodeau/Zotero/storage/TVUXA7PD/Ovadia et al. - 2019 - Can You Trust Your Model's Uncertainty Evaluating.pdf; /Users/jacquesthibodeau/Zotero/storage/E986QDBI/1906.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2019","","","","","","","","","","","","","","",""
"N3S2NM97","conferencePaper","2016","Everitt, Tom; Hutter, Marcus","Avoiding Wireheading with Value Reinforcement Learning","AGI 2016: Artificial General Intelligence","","","","http://arxiv.org/abs/1605.03143","How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.","2016-05-10","2022-01-30 04:52:37","2022-01-30 04:52:37","2020-11-21 17:36:45","","","","","","","","Lecture Notes in Computer Science","","","","","","","","","","","arXiv.org","","ZSCC: 0000033  arXiv: 1605.03143","","/Users/jacquesthibodeau/Zotero/storage/RR5BW54G/Everitt and Hutter - 2016 - Avoiding Wireheading with Value Reinforcement Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/8GNCR3DI/Everitt and Hutter - 2016 - Avoiding Wireheading with Value Reinforcement Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/8USE4KKA/1605.html; /Users/jacquesthibodeau/Zotero/storage/FHW8U7HU/1605.html","","TechSafety; DeepMind","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Artificial General Intelligence","","","","","","","","","","","","","","",""
"R26NQ63Z","conferencePaper","2019","Hendrycks, Dan; Mu, Norman; Cubuk, Ekin D.; Zoph, Barret; Gilmer, Justin; Lakshminarayanan, Balaji","AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty","arXiv:1912.02781 [cs, stat]","","","","http://arxiv.org/abs/1912.02781","Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.","2019-12-05","2022-01-30 04:52:37","2022-01-30 04:52:37","2019-12-16 20:30:48","","","","","","","AugMix","","","","","","","","","","","","arXiv.org","","ZSCC: 0000301  arXiv: 1912.02781","","/Users/jacquesthibodeau/Zotero/storage/CZEG4CV8/Hendrycks et al. - 2019 - AugMix A Simple Data Processing Method to Improve.pdf; /Users/jacquesthibodeau/Zotero/storage/9AGD2DPA/1912.html","","TechSafety; DeepMind; AmbiguosSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2020","","","","","","","","","","","","","","",""
"XC5KJ9EU","journalArticle","2020","Gabriel, Iason","Artificial Intelligence, Values and Alignment","Minds and Machines","","","","http://arxiv.org/abs/2001.09768","This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.","2020-01-13","2022-01-30 04:52:37","2022-01-30 04:52:37","2020-08-18 21:26:03","","","","30","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000062  arXiv: 2001.09768","","/Users/jacquesthibodeau/Zotero/storage/2GPMB6EI/Gabriel - 2020 - Artificial Intelligence, Values and Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/M2KN5UVP/2001.html","","TechSafety; DeepMind","Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IDNWAN68","blogPost","2020","Ngo, Richard","Arguments against myopic training","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/GqxuDtZvfgL2bEQ5v/arguments-against-myopic-training","Note that this post has been edited to clarify the difference between explicitly assigning a reward to an action based on its later consequences, versus implicitly reinforcing an action by assigning high reward during later timesteps when its consequences are observed. I'd previously conflated these in a confusing way; thanks to Rohin for highlighting this issue.  A number of people seem quite excited about training myopic reinforcement learning agents as an approach to AI safety (for instance this post on approval-directed agents, proposals 2, 3, 4, 10 and 11 here, and this paper and  presentation), but I’m not. I’ve had a few detailed conversations about this recently, and although I now understand the arguments for using myopia better, I’m not much more optimistic about it than I was before. In short, it seems that evaluating agents’ actions by our predictions of their consequences, rather than our evaluations of the actual consequences, will make reinforcement learning a lot harder; yet I haven’t been able to identify clear safety benefits from doing so. I elaborate on these points below; thanks to Jon Uesato, Evan Hubinger, Ramana Kumar and Stephan Wäldchen for discussion and comments. I’ll define a myopic reinforcement learner as a reinforcement learning agent trained to maximise the reward received in the next timestep, i.e. with a discount rate of 0. Because it doesn’t assign credit backwards over time, in order to train it to do anything useful, that reward function will need to contain an estimate of how valuable each (state, action, next state) transition will be for outcomes many steps later. Since that evaluation will need to extrapolate a long way forward anyway, knowing the next state doesn’t add much, and so we can limit our focus to myopic agents trained on reward functions R which ignore the resulting state: that is, where R(s,a,s′)=M(s,a) for some M. I'll call M the approval function; we can think of such agents as being trained to take actions","2020-07-09","2022-01-30 04:52:37","2022-01-30 04:52:37","2020-08-28 17:59:14","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QUMRCDI5/arguments-against-myopic-training.html","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XFRI2SS","manuscript","2020","Dulac-Arnold, Gabriel; Levine, Nir; Mankowitz, Daniel J.; Li, Jerry; Paduraru, Cosmin; Gowal, Sven; Hester, Todd","An empirical investigation of the challenges of real-world reinforcement learning","","","","","http://arxiv.org/abs/2003.11881","Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.","2020-03-24","2022-01-30 04:52:37","2022-01-30 04:52:37","2020-08-18 21:24:33","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000047  arXiv: 2003.11881","","/Users/jacquesthibodeau/Zotero/storage/F4PISH98/Dulac-Arnold et al. - 2020 - An empirical investigation of the challenges of re.pdf; /Users/jacquesthibodeau/Zotero/storage/RIVBCVU2/2003.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BSH3KCJW","manuscript","2019","Gowal, Sven; Uesato, Jonathan; Qin, Chongli; Huang, Po-Sen; Mann, Timothy; Kohli, Pushmeet","An Alternative Surrogate Loss for PGD-based Adversarial Testing","","","","","http://arxiv.org/abs/1910.09338","Adversarial testing methods based on Projected Gradient Descent (PGD) are widely used for searching norm-bounded perturbations that cause the inputs of neural networks to be misclassified. This paper takes a deeper look at these methods and explains the effect of different hyperparameters (i.e., optimizer, step size and surrogate loss). We introduce the concept of MultiTargeted testing, which makes clever use of alternative surrogate losses, and explain when and how MultiTargeted is guaranteed to find optimal perturbations. Finally, we demonstrate that MultiTargeted outperforms more sophisticated methods and often requires less iterative steps than other variants of PGD found in the literature. Notably, MultiTargeted ranks first on MadryLab's white-box MNIST and CIFAR-10 leaderboards, reducing the accuracy of their MNIST model to 88.36% (with $\ell_\infty$ perturbations of $\epsilon = 0.3$) and the accuracy of their CIFAR-10 model to 44.03% (at $\epsilon = 8/255$). MultiTargeted also ranks first on the TRADES leaderboard reducing the accuracy of their CIFAR-10 model to 53.07% (with $\ell_\infty$ perturbations of $\epsilon = 0.031$).","2019-10-21","2022-01-30 04:52:37","2022-01-30 04:52:37","2019-12-16 20:31:05","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000036  arXiv: 1910.09338","","/Users/jacquesthibodeau/Zotero/storage/ACDBRTUC/Gowal et al. - 2019 - An Alternative Surrogate Loss for PGD-based Advers.pdf; /Users/jacquesthibodeau/Zotero/storage/K9GR5WQX/1910.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DJIA6CVN","manuscript","2021","Kenton, Zachary; Everitt, Tom; Weidinger, Laura; Gabriel, Iason; Mikulik, Vladimir; Irving, Geoffrey","Alignment of Language Agents","","","","","http://arxiv.org/abs/2103.14659","For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.","2021-03-26","2022-01-30 04:52:37","2022-01-30 04:52:37","2021-11-14 16:31:03","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000006  arXiv: 2103.14659","","/Users/jacquesthibodeau/Zotero/storage/74F9MU53/Kenton et al. - 2021 - Alignment of Language Agents.pdf; /Users/jacquesthibodeau/Zotero/storage/K3PCKCDE/2103.html","","TechSafety; AmbiguousSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJ25JZ86","manuscript","2017","Leike, Jan; Martic, Miljan; Krakovna, Victoria; Ortega, Pedro A.; Everitt, Tom; Lefrancq, Andrew; Orseau, Laurent; Legg, Shane","AI Safety Gridworlds","","","","","http://arxiv.org/abs/1711.09883","We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.","2017-11-28","2022-01-30 04:52:37","2022-01-30 04:52:37","2019-12-16 20:35:43","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000215  arXiv: 1711.09883","","/Users/jacquesthibodeau/Zotero/storage/S9F534C5/Leike et al. - 2017 - AI Safety Gridworlds.pdf; /Users/jacquesthibodeau/Zotero/storage/2X4PWKEM/Leike et al. - 2017 - AI Safety Gridworlds.pdf; /Users/jacquesthibodeau/Zotero/storage/NF3EHNV5/1711.html; /Users/jacquesthibodeau/Zotero/storage/7587G9PA/1711.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JVDTIGDK","blogPost","2020","Ngo, Richard","A space of proposals for building safe advanced AI","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/S9GxuAEeQomnLkeNt/a-space-of-proposals-for-building-safe-advanced-ai","I liked Evan’s post on 11 proposals for safe AGI. However, I was a little confused about why he chose these specific proposals; it feels like we could generate many more by stitching together the different components he identifies, such as different types of amplification and different types of robustness tools. So I’m going to take a shot at describing a set of dimensions of variation which capture the key differences between these proposals, and thereby describe an underlying space of possible approaches to safety. Firstly I’ll quickly outline the proposals. Rohin’s overview of them is a good place to start - he categorises them as:  * 7 proposals of the form “recursive outer alignment technique” plus    “robustness technique”.  * The recursive outer alignment technique is either debate, recursive reward    modelling, or amplification.The robustness technique is either transparency    tools, relaxed adversarial training, or intermittent oversight by a competent    supervisor.  * 2 proposals of the form “non-recursive outer alignment technique” plus    “robustness technique”.  * The outer alignment technique is either reinforcement learning in a    multiagent environment, or narrow reward learning.  * 2 other proposals: Microscope AI; STEM AI. More specifically, we can describe the four core recursive outer alignment techniques as variants of iterated amplification, as follows: let Amp(M) be the procedure of a human answering questions with access to model M. Then we iteratively train M* (the next version of M) by:  * Imitative amplification: train M* to imitate Amp(M).  * Approval-based amplification: train M* on an approval signal specified by    Amp(M).  * Recursive reward modelling: train M* on a reward function specified by    Amp(M).  * Debate: train M* to win debates against Amp(M). Here are six axes of variation which I claim underlie Evan’s proposals. Each proposal is more or less:  1. Supervised  2. Structured  3. Adversarial  4. Language-based  5.","2020-07-10","2022-01-30 04:52:36","2022-01-30 04:52:36","2020-08-28 18:00:24","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","TechSafety; DeepMind","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CPPZDHR7","conferencePaper","2018","Dvijotham, Krishnamurthy; Stanforth, Robert; Gowal, Sven; Mann, Timothy; Kohli, Pushmeet","A Dual Approach to Scalable Verification of Deep Networks","","","","","http://auai.org/uai2018/proceedings/papers/204.pdf","This paper addresses the problem of formally verifying desirable properties of neural networks, i.e., obtaining provable guarantees that neural networks satisfy specifications relating their inputs and outputs (robustness to bounded norm adversarial perturbations, for example). Most previous work on this topic was limited in its applicability by the size of the network, network architecture and the complexity of properties to be verified. In contrast, our framework applies to a general class of activation functions and specifications on neural network inputs and outputs. We formulate verification as an optimization problem (seeking to find the largest violation of the specification) and solve a Lagrangian relaxation of the optimization problem to obtain an upper bound on the worst case violation of the specification being verified. Our approach is anytime i.e. it can be stopped at any time and a valid bound on the maximum violation can be obtained. We develop specialized verification algorithms with provable tightness guarantees under special assumptions and demonstrate the practical significance of our general verification approach on a variety of verification tasks.","2018-08-03","2022-01-30 04:52:36","2022-01-30 04:52:36","2020-12-20","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000274  arXiv: 1803.06567","","/Users/jacquesthibodeau/Zotero/storage/JI7PP7JX/Krishnamurthy et al. - 2018 - A Dual Approach to Scalable Verification of Deep N.pdf; /Users/jacquesthibodeau/Zotero/storage/WD27UPBX/1803.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Conference on Uncertainty in Artificial Intelligence","","","","","","","","","","","","","","",""
"SRQCVSJ8","manuscript","2018","Orseau, Laurent; McGill, Simon McGregor; Legg, Shane","Agents and Devices: A Relative Definition of Agency","","","","","http://arxiv.org/abs/1805.12387","According to Dennett, the same system may be described using a `physical' (mechanical) explanatory stance, or using an `intentional' (belief- and goal-based) explanatory stance. Humans tend to find the physical stance more helpful for certain systems, such as planets orbiting a star, and the intentional stance for others, such as living animals. We define a formal counterpart of physical and intentional stances within computational theory: a description of a system as either a device, or an agent, with the key difference being that `devices' are directly described in terms of an input-output mapping, while `agents' are described in terms of the function they optimise. Bayes' rule can then be applied to calculate the subjective probability of a system being a device or an agent, based only on its behaviour. We illustrate this using the trajectories of an object in a toy grid-world domain.","2018-05-31","2022-01-30 04:52:36","2022-01-30 04:52:36","2020-11-14 00:33:25","","","","","","","Agents and Devices","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 1805.12387","","/Users/jacquesthibodeau/Zotero/storage/J8BWQUT7/Orseau et al. - 2018 - Agents and Devices A Relative Definition of Agenc.pdf; /Users/jacquesthibodeau/Zotero/storage/DVSN4A5S/1805.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RMHHFXES","conferencePaper","2021","Everitt, Tom; Carey, Ryan; Langlois, Eric; Ortega, Pedro A.; Legg, Shane","Agent Incentives: A Causal Perspective","Proceedings of the AAAI 2021 Conference","","","","http://arxiv.org/abs/2102.01685","We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.","2021-03-15","2022-01-30 04:52:36","2022-01-30 04:52:36","2021-10-31 19:14:12","","","","","","","Agent Incentives","","","","","","","","","","","","arXiv.org","","ZSCC: 0000007  arXiv: 2102.01685","","/Users/jacquesthibodeau/Zotero/storage/8R4VFG65/Everitt et al. - 2021 - Agent Incentives A Causal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/DFVQ6HJ2/2102.html; /Users/jacquesthibodeau/Zotero/storage/FEKAF4MT/2102.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI 2021","","","","","","","","","","","","","","",""
"DJDTSQA3","conferencePaper","2019","Qin, Chongli; Martens, James; Gowal, Sven; Krishnan, Dilip; Dvijotham, Krishnamurthy; Fawzi, Alhussein; De, Soham; Stanforth, Robert; Kohli, Pushmeet","Adversarial Robustness through Local Linearization","Advances in Neural Information Processing Systems 32 (NeurIPS 2019)","","","","https://proceedings.neurips.cc/paper/2019/hash/0defd533d51ed0a10c5c9dbf93ee78a5-Abstract.html","Adversarial training is an effective methodology for training deep neural networks that are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradient obfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained significantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47% adversarial accuracy for ImageNet with l-infinity adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.","2019-10-10","2022-01-30 04:52:36","2022-01-30 04:52:36","2020-12-20","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000145  arXiv: 1907.02610","","/Users/jacquesthibodeau/Zotero/storage/KJHF9RJB/Qin et al. - 2019 - Adversarial Robustness through Local Linearization.pdf; /Users/jacquesthibodeau/Zotero/storage/QRC2SPTZ/1907.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2019","","","","","","","","","","","","","","",""
"PHTMMSBH","conferencePaper","2018","Uesato, Jonathan; O'Donoghue, Brendan; Oord, Aaron van den; Kohli, Pushmeet","Adversarial Risk and the Dangers of Evaluating Against Weak Attacks","Proceedings of the 35th International Conference on Machine Learning","","","","http://arxiv.org/abs/1802.05666","This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate 'adversarial risk' as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as 'obscurity to an adversary,' and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.","2018-06-12","2022-01-30 04:52:36","2022-01-30 04:52:36","2019-12-16 20:35:22","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000335  arXiv: 1802.05666","","/Users/jacquesthibodeau/Zotero/storage/HKXXJPHG/Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf; /Users/jacquesthibodeau/Zotero/storage/QXA64E5S/1802.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","35th International Conference on Machine Learning","","","","","","","","","","","","","","",""
"FT98MHA9","conferencePaper","2019","Huang, Po-Sen; Stanforth, Robert; Welbl, Johannes; Dyer, Chris; Yogatama, Dani; Gowal, Sven; Dvijotham, Krishnamurthy; Kohli, Pushmeet","Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation","arXiv:1909.01492 [cs, stat]","","","","http://arxiv.org/abs/1909.01492","Neural networks are part of many contemporary NLP systems, yet their empirical successes come at the price of vulnerability to adversarial attacks. Previous work has used adversarial training and data augmentation to partially mitigate such brittleness, but these are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations. In this work, we approach the problem from the opposite direction: to formally verify a system's robustness against a predefined class of adversarial attacks. We study text classification under synonym replacements or character flip perturbations. We propose modeling these input perturbations as a simplex and then using Interval Bound Propagation -- a formal model verification method. We modify the conventional log-likelihood training objective to train models that can be efficiently verified, which would otherwise come with exponential search complexity. The resulting models show only little difference in terms of nominal accuracy, but have much improved verified accuracy under perturbations and come with an efficiently computable formal guarantee on worst case adversaries.","2019-09-03","2022-01-30 04:52:36","2022-01-30 04:52:36","2019-12-16 20:31:13","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000074  arXiv: 1909.01492","","/Users/jacquesthibodeau/Zotero/storage/SB3JWI7M/Huang et al. - 2019 - Achieving Verified Robustness to Symbol Substituti.pdf; /Users/jacquesthibodeau/Zotero/storage/UJR69ER4/1909.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computation and Language; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EMNLP 2019","","","","","","","","","","","","","","",""
"36ZPFWSH","conferencePaper","2018","Ryffel, Theo; Trask, Andrew; Dahl, Morten; Wagner, Bobby; Mancuso, Jason; Rueckert, Daniel; Passerat-Palmbach, Jonathan","A generic framework for privacy preserving deep learning","arXiv:1811.04017 [cs, stat]","","","","http://arxiv.org/abs/1811.04017","We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning.","2018-11-13","2022-01-30 04:52:36","2022-01-30 04:52:36","2019-12-16 20:34:29","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000218  arXiv: 1811.04017","","/Users/jacquesthibodeau/Zotero/storage/5HC3K36N/Ryffel et al. - 2018 - A generic framework for privacy preserving deep le.pdf; /Users/jacquesthibodeau/Zotero/storage/QQWKKRKX/1811.html","","TechSafety; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","PPML 2018","","","","","","","","","","","","","","",""
"VIV37QTM","report","2021","Gehlhaus, Diana; Koslosky, Luke; Goode, Kayla; Perkins, Claire","U.S. AI Workforce: Policy Recommendations","","","","","https://cset.georgetown.edu/publication/u-s-ai-workforce-policy-recommendations/","This policy brief addresses the need for a clearly defined artificial intelligence education and workforce policy by providing recommendations designed to grow, sustain, and diversify the U.S. AI workforce. The authors employ a comprehensive definition of the AI workforce—technical and nontechnical occupations—and provide data-driven policy goals. Their recommendations are designed to leverage opportunities within the U.S. education and training system while mitigating its challenges, and prioritize equity in access and opportunity to AI education and AI careers.","2021-10","2022-01-30 04:52:05","2022-01-30 04:52:05","2021-10-31 17:12:04","","","","","","","U.S. AI Workforce","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/NB3W8BMN/u-s-ai-workforce-policy-recommendations.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQZXXA47","report","2021","Buchanan, Ben; Lohn, Andrew; Musser, Micah; Sedova, Katerina","Truth, Lies, and Automation","","","","","https://cset.georgetown.edu/publication/truth-lies-and-automation/","Growing popular and industry interest in high-performing natural language generation models has led to concerns that such models could be used to generate automated disinformation at scale. This report examines the capabilities of GPT-3--a cutting-edge AI system that writes text--to analyze its potential misuse for disinformation. A model like GPT-3 may be able to help disinformation actors substantially reduce the work necessary to write disinformation while expanding its reach and potentially also its effectiveness.","2021-05","2022-01-30 04:52:05","2022-01-30 04:52:05","2021-10-31 17:46:29","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/FS8NAUIB/truth-lies-and-automation.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8JTKIW98","report","2021","Konaev, Margarita; Huang, Tina; Chahal, Husanjot","Trusted Partners","","","","","https://cset.georgetown.edu/publication/trusted-partners/","As the U.S. military integrates artificial intelligence into its systems and missions, there are outstanding questions about the role of trust in human-machine teams. This report examines the drivers and effects of such trust, assesses the risks from too much or too little trust in intelligent technologies, reviews efforts to build trustworthy AI systems, and offers future directions for research on trust relevant to the U.S. military.","2021-02","2022-01-30 04:52:05","2022-01-30 04:52:05","2021-10-31 18:56:18","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/U2JDP64H/trusted-partners.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J3Q9XMGI","report","2020","Imbrie, Andrew; Kania, Elsa; Laskai, Lorand","The Question of Comparative Advantage in Artificial Intelligence: Enduring Strengths and Emerging Challenges for the United States","","","","","https://cset.georgetown.edu/research/the-question-of-comparative-advantage-in-artificial-intelligence-enduring-strengths-and-emerging-challenges-for-the-united-states/","How do we measure leadership in artificial intelligence, and where does the United States rank? What comparative advantages matter most? As nations embrace AI, answering these questions becomes increasingly critical.","2020-01","2022-01-30 04:52:04","2022-01-30 04:52:04","2020-08-18 21:21:07","","","","","","","The Question of Comparative Advantage in Artificial Intelligence","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s2]  ACC: 7","","/Users/jacquesthibodeau/Zotero/storage/VAHM3NKJ/Imbrie et al. - 2020 - The Question of Comparative Advantage in Artificia.pdf; /Users/jacquesthibodeau/Zotero/storage/GDRE4UKP/the-question-of-comparative-advantage-in-artificial-intelligence-enduring-strengths-and-emergin.html","","MetaSafety; CSET; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QBPC2N7S","report","2021","Gehlhaus, Diana; Hodge, Ron; Koslosky, Luke; Goode, Kayla; Rotner, Jonathan","The DOD’s Hidden Artificial Intelligence Workforce","","","","","https://cset.georgetown.edu/publication/the-dods-hidden-artificial-intelligence-workforce/","This policy brief, authored in collaboration with the MITRE Corporation, provides a new perspective on the U.S. Department of Defense’s struggle to recruit and retain artificial intelligence talent. The authors find that the DOD already has a cadre of AI and related experts, but that this talent remains hidden. Better leveraging this talent could go a long way in meeting the DOD’s AI objectives. The authors argue that this can be done through policies that more effectively identify AI talent and assignment opportunities, processes that incentivize experimentation and changes in career paths, and investing in the necessary technological infrastructure.","2021-09","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:13:13","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: 0","","/Users/jacquesthibodeau/Zotero/storage/A4AS4MJI/the-dods-hidden-artificial-intelligence-workforce.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KU4UTF7B","report","2021","Chahal, Husanjot; Toner, Helen; Rahkovsky, Ilya","Small Data’s Big AI Potential","","","","","https://cset.georgetown.edu/publication/small-datas-big-ai-potential/","Conventional wisdom suggests that cutting-edge artificial intelligence is dependent on large volumes of data. An overemphasis on “big data” ignores the existence—and underestimates the potential—of several AI approaches that do not require massive labeled datasets. This issue brief is a primer on “small data” approaches to AI. It presents exploratory findings on the current and projected progress in scientific research across these approaches, which country leads, and the major sources of funding for this research.","2021-09","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:18:18","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: 0","","/Users/jacquesthibodeau/Zotero/storage/I8S77599/small-datas-big-ai-potential.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SCHIPDXW","report","2020","Hwang, Tim","Shaping the Terrain of AI Competition","","","","","https://cset.georgetown.edu/research/shaping-the-terrain-of-ai-competition/","How should democracies effectively compete against authoritarian regimes in the AI space? This report offers a “terrain strategy” for the United States to leverage the malleability of artificial intelligence to offset authoritarians' structural advantages in engineering and deploying AI.","2020-06","2022-01-30 04:52:04","2022-01-30 04:52:04","2020-08-18 21:12:30","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/E9B7EF92/CSET-Shaping-the-Terrain-of-AI-Competition.pdf; /Users/jacquesthibodeau/Zotero/storage/9FCEI8B5/shaping-the-terrain-of-ai-competition.html","","MetaSafety; CSET; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7CHZX93M","report","2021","Cary, Dakota","Robot Hacking Games","","","","","https://cset.georgetown.edu/publication/robot-hacking-games/","Software vulnerability discovery, patching, and exploitation—collectively known as the vulnerability lifecycle—is time consuming and labor intensive. Automating the process could significantly improve software security and offensive hacking. The Defense Advanced Research Projects Agency’s Cyber Grand Challenge supported teams of researchers from 2014 to 2016 that worked to create these tools. China took notice. In 2017, China hosted its first Robot Hacking Game, seeking to automate the software vulnerability lifecycle. Since then, China has hosted seven such competitions and the People’s Liberation Army has increased its role in hosting the games.","2021-09","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:14:54","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/R33JGR9H/robot-hacking-games.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XQNJ4CAX","report","2021","Lohn, Andrew","Poison in the Well","","","","","https://cset.georgetown.edu/publication/poison-in-the-well/","Modern machine learning often relies on open-source datasets, pretrained models, and machine learning libraries from across the internet, but are those resources safe to use? Previously successful digital supply chain attacks against cyber infrastructure suggest the answer may be no. This report introduces policymakers to these emerging threats and provides recommendations for how to secure the machine learning supply chain.","2021-06","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:44:21","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: 0","","/Users/jacquesthibodeau/Zotero/storage/N3PM2RS4/poison-in-the-well.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K4XIA668","report","2021","Stanley-Lockman, Zoe","Military AI Cooperation Toolbox","","","","","https://cset.georgetown.edu/publication/military-ai-cooperation-toolbox/","The Department of Defense can already begin applying its existing international science and technology agreements, global scientific networks, and role in multilateral institutions to stimulate digital defense cooperation. This issue brief frames this collection of options as a military AI cooperation toolbox, finding that the available tools offer valuable pathways to align policies, advance research, development, and testing, and to connect personnel–albeit in more structured ways in the Euro-Atlantic than in the Indo-Pacific.","2021-08","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:41:07","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/ETNQVAV4/military-ai-cooperation-toolbox.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7FVGJKQD","report","2021","Musser, Micah; Garriott, Ashton","Machine Learning and Cybersecurity","","","","","https://cset.georgetown.edu/publication/machine-learning-and-cybersecurity/","Cybersecurity operators have increasingly relied on machine learning to address a rising number of threats. But will machine learning give them a decisive advantage or just help them keep pace with attackers? This report explores the history of machine learning in cybersecurity and the potential it has for transforming cyber defense in the near future.","2021-06","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:45:14","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/55E2MHZX/machine-learning-and-cybersecurity.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"64I4UBIX","report","2021","Rudner, Tim G. J.; Toner, Helen","Key Concepts in AI Safety: Robustness and Adversarial Examples","","","","","https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-robustness-and-adversarial-examples/","This paper is the second installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces adversarial examples, a major challenge to robustness in modern machine learning systems.","2021-03","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 18:52:59","","","","","","","Key Concepts in AI Safety","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/482VNDJV/key-concepts-in-ai-safety-robustness-and-adversarial-examples.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CD2SWUVR","manuscript","2020","Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew; Bluemke, Emma; Lebensold, Jonathan; O'Keefe, Cullen; Koren, Mark; Ryffel, Théo; Rubinovitz, J. B.; Besiroglu, Tamay; Carugati, Federica; Clark, Jack; Eckersley, Peter; de Haas, Sarah; Johnson, Maritza; Laurie, Ben; Ingerman, Alex; Krawczuk, Igor; Askell, Amanda; Cammarota, Rosario; Lohn, Andrew; Krueger, David; Stix, Charlotte; Henderson, Peter; Graham, Logan; Prunkl, Carina; Martin, Bianca; Seger, Elizabeth; Zilberman, Noa; hÉigeartaigh, Seán Ó; Kroeger, Frens; Sastry, Girish; Kagan, Rebecca; Weller, Adrian; Tse, Brian; Barnes, Elizabeth; Dafoe, Allan; Scharre, Paul; Herbert-Voss, Ariel; Rasser, Martijn; Sodhani, Shagun; Flynn, Carrick; Gilbert, Thomas Krendl; Dyer, Lisa; Khan, Saif; Bengio, Yoshua; Anderljung, Markus","Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims","","","","","http://arxiv.org/abs/2004.07213","With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.","2020-04-20","2022-01-30 04:52:04","2022-01-30 04:52:04","2020-08-18 21:36:21","","","","","","","Toward Trustworthy AI Development","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 92  arXiv: 2004.07213","","/Users/jacquesthibodeau/Zotero/storage/RBRJEMZR/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf; /Users/jacquesthibodeau/Zotero/storage/BXI2UR7I/2004.html; /Users/jacquesthibodeau/Zotero/storage/R7QIIBK8/2004.html","","MetaSafety; CHAI; CFI; CSER; CSET; FHI; Open-AI","Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3S8DDMWK","report","2021","Stanley-Lockman, Zoe","Responsible and Ethical Military AI","","","","","https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/","Allies of the United States have begun to develop their own policy approaches to responsible military use of artificial intelligence. This issue brief looks at key allies with articulated, emerging, and nascent views on how to manage ethical risk in adopting military AI. The report compares their convergences and divergences, offering pathways for the United States, its allies, and multilateral institutions to develop common approaches to responsible AI implementation.","2021-08","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:39:09","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/XSV67832/responsible-and-ethical-military-ai.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CI7QIJ8V","report","2021","VerWey, John","No Permits, No Fabs: The Importance of Regulatory Reform for Semiconductor Manufacturing","","","","","https://cset.georgetown.edu/publication/no-permits-no-fabs/","Congress has advanced legislation to appropriate $52 billion in funding for the CHIPS for America Act, which aims to increase semiconductor manufacturing and supply chain resilience in the United States. But more can be done to improve the resiliency of U.S. access to microelectronics beyond manufacturing incentives. This report outlines infrastructure investments and regulatory reforms that could make the United States a more attractive place to build new chipmaking capacity and ensure continued U.S. access to key inputs for semiconductor manufacturing.","2021-10","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:08:33","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/V5B6V6X7/no-permits-no-fabs.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TBNSHQP2","report","2021","Daniels, Matthew; Chang, Ben","National Power After AI","","","","","https://cset.georgetown.edu/publication/national-power-after-ai/","AI technologies will likely alter great power competitions in foundational ways, changing both how nations create power and their motives for wielding it against one another. This paper is a first step toward thinking more expansively about AI & national power and seeking pragmatic insights for long-term U.S. competition with authoritarian governments.","2021-07","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:42:40","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/TWR2J25P/national-power-after-ai.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VKQNA4AA","report","2021","Luong, Ngor; Gelles, Rebecca; Flagg, Melissa","Mapping the AI Investment Activities of Top Global Defense Companies","","","","","https://cset.georgetown.edu/publication/mapping-the-ai-investment-activities-of-top-global-defense-companies/","Militaries around the world have often relied on the largest global defense companies to acquire and integrate cutting-edge technologies. This issue brief examines the investment and mergers and acquisition activities in artificial intelligence of the top 50 global defense companies — a key, if limited, approach to accessing AI innovation in the commercial sector — and assesses investment trends of their corporate venture capital subsidiaries and offers a geographic breakdown of defense companies and their AI target companies.","2021-10","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:10:48","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/CI8QZ3GJ/mapping-the-ai-investment-activities-of-top-global-defense-companies.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I4V99XXR","report","2021","Daniels, Matthew; Toney, Autumn; Flagg, Melissa; Yang, Charles","Machine Intelligence for Scientific Discovery and Engineering Invention","","","","","https://cset.georgetown.edu/publication/machine-intelligence-for-scientific-discovery-and-engineering-invention/","The advantages of nations depend in part on their access to new inventions—and modern applications of artificial intelligence can help accelerate the creation of new inventions in the years ahead. This data brief is a first step toward understanding how modern AI and machine learning have begun accelerating growth across a wide array of science and engineering disciplines in recent years.","2021-05","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:48:21","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/VVP6JR7H/machine-intelligence-for-scientific-discovery-and-engineering-invention.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GZTJRH52","report","2021","Rudner, Tim G. J.; Toner, Helen","Key Concepts in AI Safety: Interpretability in Machine Learning","","","","","https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/","This paper is the third installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces interpretability as a means to enable assurance in modern machine learning systems.","2021-03","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 18:52:14","","","","","","","Key Concepts in AI Safety","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/Z256SBHM/key-concepts-in-ai-safety-interpretability-in-machine-learning.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VH792VSZ","report","2021","Rudner, Tim G. J.; Toner, Helen","Key Concepts in AI Safety: An Overview","","","","","https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/","This paper is the first installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. In it, the authors introduce three categories of AI safety issues: problems of robustness, assurance, and specification. Other papers in this series elaborate on these and further key concepts.","2021-03","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 18:53:33","","","","","","","Key Concepts in AI Safety","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/KDBM6KD4/key-concepts-in-ai-safety-an-overview.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36HRFWME","report","2021","Goode, Kayla; Kim, Heeu Millie","Indonesia’s AI Promise in Perspective","","","","","https://cset.georgetown.edu/publication/indonesias-ai-promise-in-perspective/","The United States and China are keeping an eye on Indonesia’s artificial intelligence potential given the country’s innovation-driven national strategy and flourishing AI industry. China views Indonesia as an anchor for its economic, digital, and political inroads in Southeast Asia and has invested aggressively in new partnerships. The United States, with robust political and economic relations rooted in shared democratic ideals, has an opportunity to leverage its comparative advantages and tap into Indonesia’s AI potential through high-level agreements.","2021-08","2022-01-30 04:52:04","2022-01-30 04:52:04","2021-10-31 17:39:58","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: 0","","/Users/jacquesthibodeau/Zotero/storage/9P8XTSFN/indonesias-ai-promise-in-perspective.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EGF6E8W5","report","2021","Baker, Jamie","Ethics and Artificial Intelligence","","","","","https://cset.georgetown.edu/publication/ethics-and-artificial-intelligence/","The law plays a vital role in how artificial intelligence can be developed and used in ethical ways. But the law is not enough when it contains gaps due to lack of a federal nexus, interest, or the political will to legislate. And law may be too much if it imposes regulatory rigidity and burdens when flexibility and innovation are required. Sound ethical codes and principles concerning AI can help fill legal gaps. In this paper, CSET Distinguished Fellow James E. Baker offers a primer on the limits and promise of three mechanisms to help shape a regulatory regime that maximizes the benefits of AI and minimizes its potential harms.","2021-04","2022-01-30 04:52:03","2022-01-30 04:52:03","2021-10-31 18:49:21","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/WZE4UHJQ/ethics-and-artificial-intelligence.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R79CMVHN","report","2021","Imbrie, Andrew; Gelles, Rebecca; Dunham, James; Aiken, Catherine","Contending Frames: Evaluating Rhetorical Dynamics in AI","","","","","https://cset.georgetown.edu/publication/contending-frames/","The narrative of an artificial intelligence “arms race” among the great powers has become shorthand to describe evolving dynamics in the field. Narratives about AI matter because they reflect and shape public perceptions of the technology. In this issue brief, the second in a series examining rhetorical frames in AI, the authors compare four narrative frames that are prominent in public discourse: AI Competition, Killer Robots, Economic Gold Rush and World Without Work.","2021-05","2022-01-30 04:52:03","2022-01-30 04:52:03","2021-10-31 17:49:21","","","","","","","Contending Frames","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: 0","","/Users/jacquesthibodeau/Zotero/storage/3JV7AUFN/contending-frames.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VWID435G","report","2020","Murdick, Dewey; Dunham, James; Melot, Jennifer","AI Definitions Affect Policymaking","","","","","https://cset.georgetown.edu/research/ai-definitions-affect-policymaking/","The task of artificial intelligence policymaking is complex and challenging, made all the more difficult by such a rapidly evolving technology. In order to address the security and economic implications of AI, policymakers must be able to viably define, categorize and assess AI research and technology. In this issue brief, CSET puts forward a functional definition of AI, based on three core principles, that significantly outperforms methods developed over the last decade.","2020-06-02","2022-01-30 04:52:03","2022-01-30 04:52:03","2020-08-18 21:13:44","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/WD8HFI6J/Murdick et al. - 2020 - AI Definitions Affect Policymaking.pdf; /Users/jacquesthibodeau/Zotero/storage/QH8STACD/ai-definitions-affect-policymaking.html","","MetaSafety; CSET; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8JRCH2RU","report","2021","Hoffman, Wyatt","AI and the Future of Cyber Competition","","","","","https://cset.georgetown.edu/publication/ai-and-the-future-of-cyber-competition/","As states turn to AI to gain an edge in cyber competition, it will change the cat-and-mouse game between cyber attackers and defenders. Embracing machine learning systems for cyber defense could drive more aggressive and destabilizing engagements between states. Wyatt Hoffman writes that cyber competition already has the ingredients needed for escalation to real-world violence, even if these ingredients have yet to come together in the right conditions.","2021-01","2022-01-30 04:52:03","2022-01-30 04:52:03","2021-10-31 18:58:12","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/NKNAVMTB/ai-and-the-future-of-cyber-competition.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CDKQ9KHV","report","2021","Arnold, Zachary; Toner, Helen","AI Accidents: An Emerging Threat","","","","","https://cset.georgetown.edu/publication/ai-accidents-an-emerging-threat/","As modern machine learning systems become more widely used, the potential costs of malfunctions grow. This policy brief describes how trends we already see today—both in newly deployed artificial intelligence systems and in older technologies—show how damaging the AI accidents of the future could be. It describes a wide range of hypothetical but realistic scenarios to illustrate the risks of AI accidents and offers concrete policy suggestions to reduce these risks.","2021-07","2022-01-30 04:52:03","2022-01-30 04:52:03","2021-10-31 17:43:15","","","","","","","AI Accidents","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: 0","","/Users/jacquesthibodeau/Zotero/storage/6PTVA9UX/ai-accidents-an-emerging-threat.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQSW7FFQ","report","2021","Konaev, Margarita; Imbrie, Andrew; Fedasiuk, Ryan; Weinstein, Emily; Sedova, Katerina; Dunham, James","Headline or Trend Line?","","","","","https://cset.georgetown.edu/publication/headline-or-trend-line/","Chinese and Russian government officials are keen to publicize their countries’ strategic partnership in emerging technologies, particularly artificial intelligence. This report evaluates the scope of cooperation between China and Russia as well as relative trends over time in two key metrics of AI development: research publications and investment. The findings expose gaps between aspirations and reality, bringing greater accuracy and nuance to current assessments of Sino-Russian tech cooperation.","2021-08","2022-01-30 04:52:03","2022-01-30 04:52:03","2021-10-31 17:19:18","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/KZNE2FUE/headline-or-trend-line.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGXWUS23","report","2021","Fedasiuk, Ryan; Melot, Jennifer; Murphy, Ben","Harnessed Lightning","","","","","https://cset.georgetown.edu/publication/harnessed-lightning/","This report examines nearly 350 artificial intelligence-related equipment contracts awarded by the People’s Liberation Army and state-owned defense enterprises in 2020 to assess how the Chinese military is adopting AI. The report identifies China’s key AI defense industry suppliers, highlights gaps in U.S. export control policies, and contextualizes the PLA’s AI investments within China’s broader strategy to compete militarily with the United States.","2021-10","2022-01-30 04:52:03","2022-01-30 04:52:03","2021-10-31 17:07:03","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/XXESBFB3/harnessed-lightning.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NFPE7BBS","report","2021","Mittelsteadt, Matthew","AI Verification: Mechanisms to Ensure AI Arms Control Compliance","","","","","https://cset.georgetown.edu/publication/ai-verification/","The rapid integration of artificial intelligence into military systems raises critical questions of ethics, design and safety. While many states and organizations have called for some form of “AI arms control,” few have discussed the technical details of verifying countries’ compliance with these regulations. This brief offers a starting point, defining the goals of “AI verification” and proposing several mechanisms to support arms inspections and continuous verification.","2021-02","2022-01-30 04:52:03","2022-01-30 04:52:03","2021-10-31 18:55:15","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: 0","","/Users/jacquesthibodeau/Zotero/storage/K3TV6PZX/ai-verification.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5HRJA33X","report","2021","Peterson, Dahlia; Goode, Kayla; Gehlhaus, Diana","AI Education in China and the United States","","","","","https://cset.georgetown.edu/publication/ai-education-in-china-and-the-united-states/","A globally competitive AI workforce hinges on the education, development, and sustainment of the best and brightest AI talent. This issue brief compares efforts to integrate AI education in China and the United States, and what advantages and disadvantages this entails. The authors consider key differences in system design and oversight, as well as strategic planning. They then explore implications for the U.S. national security community.","2021-09","2022-01-30 04:52:03","2022-01-30 04:52:03","2021-10-31 17:16:49","","","","","","","","","","","","Center for Security and Emerging Technology","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/TNTW4N5K/ai-education-in-china-and-the-united-states.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IAZFVRWI","blogPost","2019","Shah, Rohin","Will humans build goal-directed agents?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/9zpT9dikrrebdq3Jf/will-humans-build-goal-directed-agents","In the previous post, I argued that simply knowing that an AI system is superintelligent does not imply that it must be goal-directed. However, there are many other arguments that suggest that AI systems will or should be goal-directed, which I will discuss in this post. Note that I don’t think of this as the Tool AI vs. Agent AI argument: it seems possible to build agent AI systems that are not goal-directed. For example, imitation learning allows you to create an agent that behaves similarly to another agent -- I would classify this as “Agent AI that is not goal-directed”. (But see this comment thread for discussion.) Note that these arguments have different implications than the argument that superintelligent AI must be goal-directed due to coherence arguments. Suppose you believe all of the following:  * Any of the arguments in this post.  * Superintelligent AI is not required to be goal-directed, as I argued in the     last post.  * Goal-directed agents cause catastrophe by default. Then you could try to create alternative designs for AI systems such that they can do the things that goal-directed agents can do without themselves being goal-directed. You could also try to persuade AI researchers of these facts, so that they don’t build goal-directed systems. ECONOMIC EFFICIENCY: GOAL-DIRECTED HUMANS Humans want to build powerful AI systems in order to help them achieve their goals -- it seems quite clear that humans are at least partially goal-directed. As a result, it seems natural that they would build AI systems that are also goal-directed. This is really an argument that the system comprising the human and AI agent should be directed towards some goal. The AI agent by itself need not be goal-directed as long as we get goal-directed behavior when combined with a human operator. However, in the situation where the AI agent is much more intelligent than the human, it is probably best to delegate most or all decisions to the agent, and so the agent could s","2019","2022-01-30 04:51:44","2022-01-30 04:51:44","2020-12-17 04:36:45","","","","","","","Will humans build goal-directed agents?","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/H5UGJHAN/9zpT9dikrrebdq3Jf.html","","CHAI; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CHURS33T","bookSection","2018","Reddy, Sid; Dragan, Anca; Levine, Sergey","Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior","Advances in Neural Information Processing Systems 31","","","","http://papers.nips.cc/paper/7419-where-do-you-think-youre-going-inferring-beliefs-about-dynamics-from-behavior.pdf","","2018","2022-01-30 04:51:44","2022-01-30 04:51:44","2019-12-18 02:41:12","1454–1465","","","","","","Where Do You Think You\textquotesingle re Going?","","","","","Curran Associates, Inc.","","","","","","","Neural Information Processing Systems","","ZSCC: NoCitationData[s2]  ACC: 66","","/Users/jacquesthibodeau/Zotero/storage/MM784MFH/Reddy et al. - 2018 - Where Do You Think Youtextquotesingle re Going .pdf; /Users/jacquesthibodeau/Zotero/storage/VCE24KBJ/7419-where-do-you-think-youre-going-inferring-beliefs-about-dynamics-from-behavior.html","","CHAI; TechSafety","","Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; Garnett, R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D23D6TKU","blogPost","2021","Critch, Andrew","What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs)","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic","With: Thomas Krendl Gilbert, who provided comments, interdisciplinary feedback, and input on the RAAP concept. Thanks also for comments from Ramana Kumar. Target audience: researchers and institutions who think about existential risk from artificial intelligence, especially AI researchers. Preceded by: Some AI research areas and their relevance to existential safety, which emphasized the value of thinking about multi-stakeholder/multi-agent social applications, but without concrete extinction scenarios. This post tells a few different stories in which humanity dies out as a result of AI technology, but where no single source of human or automated agency is the cause. Scenarios with multiple AI-enabled superpowers are often called “multipolar” scenarios in AI futurology jargon, as opposed to “unipolar” scenarios with just one superpower. Unipolar take-offsMultipolar take-offsSlow take-offs<not this post>Part 1 of this postFast take-offs<not this post>Part 2 of this postPart 1 covers a batch of stories that play out slowly (“slow take-offs”), and Part 2 stories play out quickly. However, in the end I don’t want you to be super focused how fast the technology is taking off. Instead, I’d like you to focus on multi-agent processes with a robust tendency to play out irrespective of which agents execute which steps in the process. I’ll call such processes Robust Agent-Agnostic Processes (RAAPs). A group walking toward a restaurant is a nice example of a RAAP, because it exhibits:  * Robustness: If you temporarily distract one of the walkers to wander off, the    rest of the group will keep heading toward the restaurant, and the distracted    member will take steps to rejoin the group.  * Agent-agnosticism: Who’s at the front or back of the group might vary    considerably during the walk. People at the front will tend to take more    responsibility for knowing and choosing what path to take, and people at the    back will tend to just follow. Thus, the execution of r","2021-03-31","2022-01-30 04:51:43","2022-01-30 04:51:43","2021-11-14 16:44:47","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/KSVPBM2X/what-multipolar-failure-looks-like-and-robust-agent-agnostic.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2VA8RMCJ","blogPost","2015","Oesterheld, Caspar","Two-boxing, smoking and chewing gum in Medical Newcomb problems","LessWrong","","","","https://www.lesswrong.com/posts/wWnN3y5GmqLLCJFAz/two-boxing-smoking-and-chewing-gum-in-medical-newcomb","I am currently learning about the basics of decision theory, most of which is common knowledge on LW. I have a question, related to why EDT is said not to work. Consider the following Newcomblike problem: A study shows that most people who two-box in Newcomblike problems as the following have a certain gene (and one-boxers don't have the gene). Now, Omega could put you into something like Newcomb's original problem, but instead of having run a simulation of you, Omega has only looked at your DNA: If you don't have the ""two-boxing gene"", Omega puts $1M into box B, otherwise box B is empty. And there is $1K in box A, as usual. Would you one-box (take only box B) or two-box (take box A and B)? Here's a causal diagram for the problem: Since Omega does not do much other than translating your genes into money under a box, it does not seem to hurt to leave it out: I presume that most LWers would one-box. (And as I understand it, not only CDT but also TDT would two-box, am I wrong?) Now, how does this problem differ from the smoking lesion or Yudkowsky's (2010, p.67) chewing gum problem? Chewing Gum (or smoking) seems to be like taking box A to get at least/additional $1K, the two-boxing gene is like the CGTA gene, the illness itself (the abscess or lung cancer) is like not having $1M in box B. Here's another causal diagram, this time for the chewing gum problem: As far as I can tell, the difference between the two problems is some additional, unstated intuition in the classic medical Newcomb problems. Maybe, the additional assumption is that the actual evidence lies in the ""tickle"", or that knowing and thinking about the study results causes some complications. In EDT terms: The intuition is that neither smoking nor chewing gum gives the agent additional information.","2015-06-29","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-11-23 00:59:21","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UBUI7HWH/two-boxing-smoking-and-chewing-gum-in-medical-newcomb.html","","CLR; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GM2U5SSZ","manuscript","2020","Clifton, Jesse; Riché, Maxime","Towards Cooperation in Learning Games","","","","","","Suppose that several actors are going to deploy learning agents to act on their behalf. What principles should guide these actors in designing their agents, given that they may have competing goals? An appealing solution concept in this setting is welfareoptimal learning equilibrium. This means that the learning agents should constitute a Nash equilibrium whose payoff proﬁle is optimal according to some measure of total welfare (welfare function). In this work, we construct a class of learning algorithms in this spirit called learning tit-for-tat (L-TFT). L-TFT algorithms maximize a welfare function according to a speciﬁed optimization schedule, and punish their counterpart when they detect that they are deviating from this plan. Because the policies of other agents are not in general fully observed, agents must infer whether their counterpart is following a cooperative learning algorithm. This requires us to develop new techniques for making inferences about counterpart learning algorithms. In two sequential social dilemmas, our L-TFT algorithms successfully cooperate in self-play while effectively avoiding exploitation by and punishing defecting learning algorithms.","2020","2022-01-30 04:51:37","2022-01-30 04:51:37","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000[s0]","","/Users/jacquesthibodeau/Zotero/storage/Q7RTK23R/Clifton and Riché - Towards Cooperation in Learning Games.pdf","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IP5KVVZ3","blogPost","2018","Treutlein, Johannes","Three wagers for multiverse-wide superrationality","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2018/03/31/three-wagers-for-multiverse-wide-superrationality/","In this post, I outline three wagers in favor of the hypothesis that multiverse-wide superrationality (MSR) has action-guiding implications. MSR is based on three core assumptions: There is a large…","2018-03-31","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-11-23 00:42:20","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/MC3STFK8/three-wagers-for-multiverse-wide-superrationality.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EB6XDGH4","blogPost","2020","Kokotajlo, Daniel","The date of AI Takeover is not the day the AI takes over","LessWrong","","","","https://www.lesswrong.com/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over","Instead, it’s the point of no return—the day we AI risk reducers lose the ability to significantly reduce AI risk. This might happen years before classic milestones like “World GWP doubles in four years” and “Superhuman AGI is deployed."" The rest of this post explains, justifies, and expands on this obvious but underappreciated idea. (Toby Ord appreciates it; see quote below). I found myself explaining it repeatedly, so I wrote this post as a reference. AI timelines often come up in career planning conversations. Insofar as AI timelines are short, career plans which take a long time to pay off are a bad idea, because by the time you reap the benefits of the plans it may already be too late. It may already be too late because AI takeover may already have happened. But this isn’t quite right, at least not when “AI takeover” is interpreted in the obvious way, as meaning that an AI or group of AIs is firmly in political control of the world, ordering humans about, monopolizing violence, etc. Even if AIs don’t yet have that sort of political control, it may already be too late. Here are three examples:  1. Superhuman agent AGI is still in its box but nobody knows how to align it     and other actors are going to make their own version soon, and there isn’t     enough time to convince them of the risks. They will make and deploy agent     AGI, it will be unaligned, and we have no way to oppose it except with our     own unaligned AGI. Even if it takes years to actually conquer the world,     it’s already game over.            2. Various weak and narrow AIs are embedded in the economy and beginning to     drive a slow takeoff; capabilities are improving much faster than     safety/alignment techniques and due to all the money being made there’s too     much political opposition to slowing down capability growth or keeping AIs     out of positions of power. We wish we had done more safety/alignment     research earlier, or built a political movement earlier when opposit","2020","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-12-12 15:02:10","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/9V2GNH7T/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7E5EW6SI","blogPost","2021","Kokotajlo, Daniel","Taboo ""Outside View""","LessWrong","","","","https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view","No one has ever seen an AGI takeoff, so any attempt to understand it must use these outside view considerations. —[Redacted for privacy] What? That’s exactly backwards. If we had lots of experience with past AGI takeoffs, using the outside view to predict the next one would be a lot more effective. —My reaction Two years ago I wrote a deep-dive summary of Superforecasting and the associated scientific literature. I learned about the “Outside view” / “Inside view” distinction, and the evidence supporting it. At the time I was excited about the concept and wrote: “...I think we should do our best to imitate these best-practices, and that means using the outside view far more than we would naturally be inclined.” Now that I have more experience, I think the concept is doing more harm than good in our community. The term is easily abused and its meaning has expanded too much. I recommend we permanently taboo “Outside view,” i.e. stop using the word and use more precise, less confused concepts instead. This post explains why. WHAT DOES “OUTSIDE VIEW” MEAN NOW? Over the past two years I’ve noticed people (including myself!) do lots of different things in the name of the Outside View. I’ve compiled the following lists based on fuzzy memory of hundreds of conversations with dozens of people: BIG LIST O’ THINGS PEOPLE DESCRIBE AS OUTSIDE VIEW:  * Reference class forecasting, the practice of computing a probability of an    event by looking at the frequency with which similar events occurred in    similar situations. Also called comparison class forecasting. [EDIT: Eliezer    rightly points out that sometimes reasoning by analogy is undeservedly called    reference class forecasting; reference classes are supposed to be held to a    much higher standard, in which your sample size is larger and the analogy is    especially tight.]  * Trend extrapolation, e.g. “AGI implies insane GWP growth; let’s forecast AGI    timelines by extrapolating GWP trends.”  * Foxy aggregatio","2021-06-17","2022-01-30 04:51:37","2022-01-30 04:51:37","2021-12-11 14:16:18","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/XCKCRUKH/taboo-outside-view.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7947JHHW","journalArticle","2017","Sotala, Kaj; Gloor, Lukas","Superintelligence As a Cause or Cure For Risks of Astronomical Suffering","Informatica","","1854-3871","","http://www.informatica.si/index.php/informatica/article/view/1877","Discussions about the possible consequences of creating superintelligence have included the possibility of existential risk , often understood mainly as the risk of human extinction. We argue that suffering risks (s-risks) , where an adverse outcome would bring about severe suffering on an astronomical scale, are risks of a comparable severity and probability as risks of extinction. Preventing them is the common interest of many different value systems. Furthermore, we argue that in the same way as superintelligent AI both contributes to existential risk but can also help prevent it, superintelligent AI can both be a suffering risk or help avoid it. Some types of work aimed at making superintelligent AI safe will also help prevent suffering risks, and there may also be a class of safeguards for AI that helps specifically against s-risks.","2017-12-27","2022-01-30 04:51:37","2022-01-30 04:51:37","2019-12-16 03:31:47","","","4","41","","","","","","","","","","en","I assign to  Informatica ,  An International Journal of Computing and Informatics  (""Journal"") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript (""Paper"") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika","","","","www.informatica.si","","ZSCC: 0000029","","/Users/jacquesthibodeau/Zotero/storage/GV5IEVSF/Sotala and Gloor - 2017 - Superintelligence As a Cause or Cure For Risks of .pdf; /Users/jacquesthibodeau/Zotero/storage/SV2BI2PQ/Sotala and Gloor - 2017 - Superintelligence As a Cause or Cure For Risks of .pdf; /Users/jacquesthibodeau/Zotero/storage/5A4BQUP4/1877.html; /Users/jacquesthibodeau/Zotero/storage/UXP4MDUQ/1877.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PT6IB3US","manuscript","2016","Gloor, Lukas","Suffering-focused AI safety: Why ""fail-safe'"" measures might be our top intervention","","","","","","AI-safety eﬀorts focused on suﬀering reduction should place particular emphasis on avoiding risks of astronomical disvalue. Among the cases where uncontrolled AI destroys humanity, outcomes might still diﬀer enormously in the amounts of suﬀering produced. Rather than concentrating all our eﬀorts on a speciﬁc future we would like to bring about, we should identify futures we least want to bring about and work on ways to steer AI trajectories around these. In particular, a “fail-safe”1 approach to AI safety is especially promising because avoiding very bad outcomes might be much easier than making sure we get everything right. This is also a neglected cause despite there being a broad consensus among diﬀerent moral views that avoiding the creation of vast amounts of suﬀering in our future is an ethical priority.","2016","2022-01-30 04:51:37","2022-01-30 04:51:37","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000005","","/Users/jacquesthibodeau/Zotero/storage/AIHAKRFD/Gloor - Suffering-focused AI safety Why ``fail-safe'' mea.pdf","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P4FKCVJ5","blogPost","2020","Kokotajlo, Daniel","Soft takeoff can still lead to decisive strategic advantage","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/PKy8NuNPknenkDY74/soft-takeoff-can-still-lead-to-decisive-strategic-advantage","[Epistemic status: Argument by analogy to historical cases. Best case scenario it's just one argument among many. Edit: Also, thanks to feedback from others, especially Paul, I intend to write a significantly improved version of this post in the next two weeks.] I have on several occasions heard people say things like this:  The original Bostrom/Yudkowsky paradigm envisioned a single AI built by a single AI project, undergoing intelligence explosion all by itself and attaining a decisive strategic advantage as a result. However, this is very unrealistic. Discontinuous jumps in technological capability are very rare, and it is very implausible that one project could produce more innovations than the rest of the world combined. Instead we should expect something more like the Industrial Revolution: Continuous growth, spread among many projects and factions, shared via a combination of trade and technology stealing. We should not expect any one project or AI to attain a decisive strategic advantage, because there will always be other projects and other AI that are only slightly less powerful, and coalitions will act to counterbalance the technological advantage of the frontrunner. (paraphrased)Proponents of this view often cite Paul Christiano in support. Last week I heard him say he thinks the future will be ""like the Industrial Revolution but 10x-100x faster."" In this post, I assume that Paul's slogan for the future is correct and then nevertheless push back against the view above. Basically, I will argue that even if the future is like the industrial revolution only 10x-100x faster, there is a 30%+ chance that it will involve a single AI project (or a single AI) with the ability to gain a decisive strategic advantage, if they so choose. (Whether or not they exercise that ability is another matter.) Why am I interested in this? Do I expect some human group to take over the world? No; instead what I think is that (1) an unaligned AI in the leading project might ta","2020-08-23","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-11-23 00:32:29","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BCMN8958/soft-takeoff-can-still-lead-to-decisive-strategic-advantage.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D8J8MKT6","blogPost","2019","Sotala, Kaj","Sequence introduction: non-agent and multiagent models of mind","LessWrong","","","","https://www.lesswrong.com/posts/M4w2rdYgCKctbADMn/sequence-introduction-non-agent-and-multiagent-models-of","A typical paradigm by which people tend to think of themselves and others is as  consequentialist agents: entities who can be usefully modeled as having beliefs and goals, who are then acting according to their beliefs to achieve their goals. This is often a useful model, but it doesn’t quite capture reality. It’s a bit of a fake framework. Or in computer science terms, you might call it a leaky abstraction.  An abstraction in the computer science sense is a simplification which tries to hide the underlying details of a thing, letting you think in terms of the simplification rather than the details. To the extent that the abstraction actually succeeds in hiding the details, this makes things a lot simpler. But sometimes the abstraction inevitably leaks, as the simplification fails to predict some of the actual behavior that emerges from the details; in that situation you need to actually know the underlying details, and be able to think in terms of them. Agent-ness being a leaky abstraction is not exactly a novel concept for Less Wrong; it has been touched upon several times, such as in Scott Alexander’s  Blue-Minimizing Robot Sequence. At the same time, I do not think that it has been quite fully internalized yet, and that many foundational posts on LW go wrong due to being premised on the assumption of humans being agents. In fact, I would go as far as to claim that this is the biggest flaw of the original Sequences: they were attempting to explain many failures of rationality as being due to cognitive biases, when in retrospect it looks like understanding cognitive biases doesn’t actually make you substantially more effective. But if you are implicitly modeling humans as goal-directed agents, then cognitive biases is the most natural place for irrationality to emerge from, so it makes sense to focus the most on there. Just knowing that an abstraction leaks isn’t enough to improve your thinking, however. To do better, you need to know about the actual underlyi","2019-01-07","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-11-23 00:37:49","","","","","","","Sequence introduction","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QJVPBHVI/sequence-introduction-non-agent-and-multiagent-models-of.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MKDS666B","blogPost","2021","Kokotajlo, Daniel","What 2026 looks like","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like","This was written for the Vignettes Workshop.[1] The goal is to write out a  detailed future history (“trajectory”) that is as realistic (to me) as I can currently manage, i.e. I’m not aware of any alternative trajectory that is similarly detailed and clearly more plausible to me. The methodology is roughly: Write a future history of 2022. Condition on it, and write a future history of 2023. Repeat for 2024, 2025, etc. (I'm posting 2022-2026 now so I can get feedback that will help me write 2027+. I intend to keep writing until the story reaches singularity/extinction/utopia/etc.) What’s the point of doing this? Well, there are a couple of reasons:  * Sometimes attempting to write down a concrete example causes you to learn    things, e.g. that a possibility is more or less plausible than you thought.  * Most serious conversation about the future takes place at a high level of    abstraction, talking about e.g. GDP acceleration, timelines until TAI is    affordable, multipolar vs. unipolar takeoff… vignettes are a neglected    complementary approach worth exploring.  * Most stories are written backwards. The author begins with some idea of how    it will end, and arranges the story to achieve that ending. Reality, by    contrast, proceeds from past to future. It isn’t trying to entertain anyone    or prove a point in an argument.  * Anecdotally, various people seem to have found Paul Christiano’s “tales of    doom” stories helpful, and relative to typical discussions those stories are    quite close to what we want. (I still think a bit more detail would be good —    e.g. Paul’s stories don’t give dates, or durations, or any numbers at all    really.)[2]  * “I want someone to ... write a trajectory for how AI goes down, that is    really specific about what the world GDP is in every one of the years from    now until insane intelligence explosion. And just write down what the world    is like in each of those years because I don't know how to write an    internally","2021-08-06","2022-01-30 04:51:37","2022-01-30 04:51:37","2021-11-18 23:05:44","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/Q6PHNIN5/what-2026-looks-like-daniel-s-median-future.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RCWPURXT","blogPost","2021","Clifton, Jesse","Weak identifiability and its consequences in strategic settings","Center on Long-Term Risk","","","","https://longtermrisk.org/weak-identifiability-and-its-consequences-in-strategic-settings/","One way that agents might become involved in catastrophic conflict is if they have mistaken beliefs about one another. Maybe I think you are bluffing when you threaten to launch the nukes, but you are dead serious. So we should understand why agents might sometimes have such mistaken beliefs. In this post I'll discuss one obstacle to the formation of accurate beliefs about other agents, which has to do with identifiability. As with my post on equilibrium and prior selection problems, this is a theme that keeps cropping up in my thinking about AI cooperation and conflict, so I thought it might be helpful to have it written up. We say that a model is unidentifiable if there are several […]","2021-02-13","2022-01-30 04:51:37","2022-01-30 04:51:37","2021-10-31 16:56:26","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/Q8NW2RCR/weak-identifiability-and-its-consequences-in-strategic-settings.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z568ZSPJ","blogPost","2018","Baumann, Tobias","Using surrogate goals to deflect threats","Center on Long-Term Risk","","","","https://longtermrisk.org/using-surrogate-goals-deflect-threats/","Agents that threaten to harm other agents, either in an attempt at extortion or as part of an escalating conflict, are an important form of agential s-risks. To avoid worst-case outcomes resulting from the execution of such threats, I suggest that agents add a “meaningless” surrogate goal to their utility function.","2018-02-20","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-12-13 22:13:01","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","","","CLR; MetaSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ADF4GZVI","blogPost","2016","Oesterheld, Caspar","Thoughts on Updatelessness","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2016/11/21/thoughts-on-updatelessness/","[This post assumes knowledge of decision theory, as discussed in Eliezer Yudkowsky’s Timeless Decision Theory.] One interesting feature of some decision theories that I used to be a bit confused ab…","2016-11-21","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-11-23 00:57:49","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6E7AVG6X/thoughts-on-updatelessness.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q46HF4G8","blogPost","2018","Oesterheld, Caspar","The law of effect, randomization and Newcomb’s problem","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2018/02/15/the-law-of-effect-randomization-and-newcombs-problem/","The law of effect (LoE), as introduced on p. 244 of Thorndike’s (1911) Animal Intelligence, states: Of several responses made to the same situation, those which are accompanied or closely followed …","2018-02-15","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-11-23 00:43:11","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QEI5Q6ZD/the-law-of-effect-randomization-and-newcombs-problem.html","","CLR; TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BSZFC7NB","blogPost","2017","Center on Long-Term Risk","The future of growth: near-zero growth rates","Center on Long-Term Risk","","","","https://longtermrisk.org/the-future-of-growth-near-zero-growth-rates/","Exponential growth is a common pattern found throughout nature. Yet it is also a pattern that tends not to last, as growth rates tend to decline sooner or later. In biology, this pattern of exponential growth that wanes off is found in everything from the development of individual bodies — for instance, in the growth of […]","2017-07-26","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-11-23 00:48:28","","","","","","","The future of growth","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/9EJNMFD5/the-future-of-growth-near-zero-growth-rates.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRIMD86J","journalArticle","","MacAskill, William; Vallinder, Aron; Shulman, Carl; Österheld, Caspar; Treutlein, Johannes","The Evidentialist’s Wager","Journal of Philosophy","","","","","Suppose that an altruistic and morally motivated agent who is uncertain between evidential decision theory (EDT) and causal decision theory (CDT) finds herself in a situation in which the two theories give conflicting verdicts. We argue that even if she has significantly higher credence in CDT, she should nevertheless act in accordance with EDT. First, we claim that that the appropriate response to normative uncertainty is to hedge one’s bets. That is, if the stakes are much higher on one theory than another, and the credences you assign to each of these theories aren’t very different, then it’s appropriate to choose the option which performs best on the high-stakes theory. Second, we show that, given the assumption of altruism, the existence of correlated decision-makers will increase the stakes for EDT but leave the stakes for CDT unaffected. Together these two claims imply that whenever there are sufficiently many correlated agents, the appropriate response is to act in accordance with EDT.","forthcoming","2022-01-30 04:51:37","2022-01-30 04:51:37","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000[s1]","","/Users/jacquesthibodeau/Zotero/storage/VQ2I8RXI/MacAskill et al. - The Evidentialist’s Wager.pdf","","CLR; TechSafety; AmbiguosSafety; GPI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q22A6EWN","blogPost","2017","Caspar","The average utilitarian’s solipsism wager","The Universe from an Intentional Stance","","","","https://casparoesterheld.com/2017/03/15/the-average-utilitarians-solipsism-wager/","The following prudential argument is relatively common in my circles: We probably live in a simulation, but if we don’t, our actions matter much more. Thus, expected value calculations are do…","2017-03-15","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-11-23 00:53:18","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/FBTTP3PX/the-average-utilitarians-solipsism-wager.html","","CLR; MetaSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGZBEXFN","blogPost","2019","Kokotajlo, Daniel","The ""Commitment Races"" Problem","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem","[Epistemic status: Strong claims vaguely stated and weakly held. I expect that writing this and digesting feedback on it will lead to a much better version in the future. EDIT: So far this has stood the test of time. EDIT: As of September 2020 I think this is one of the most important things to be thinking about.] This post attempts to generalize and articulate a problem that people have been  thinking about since at least 2016. [Edit: 2009 in fact!] In short, here is the problem: Consequentialists can get caught in commitment races, in which they want to make commitments as soon as possible. When consequentialists make commitments too soon, disastrous outcomes can sometimes result. The situation we are in (building AGI and letting it self-modify) may be one of these times unless we think carefully about this problem and how to avoid it. For this post I use ""consequentialists"" to mean agents that choose actions entirely on the basis of the expected consequences of those actions. For my purposes, this means they don't care about historical facts such as whether the options and consequences available now are the result of malicious past behavior. (I am trying to avoid trivial definitions of consequentialism according to which everyone is a consequentialist because e.g. ""obeying the moral law"" is a consequence.) This definition is somewhat fuzzy and I look forward to searching for more precision some other day. CONSEQUENTIALISTS CAN GET CAUGHT IN COMMITMENT RACES, IN WHICH THEY WANT TO MAKE COMMITMENTS AS SOON AS POSSIBLE Consequentialists are bullies; a consequentialist will happily threaten someone insofar as they think the victim might capitulate and won't retaliate. Consequentialists are also cowards; they conform their behavior to the incentives set up by others, regardless of the history of those incentives. For example, they predictably give in to credible threats unless reputational effects weigh heavily enough in their minds to prevent this. In most ordi","2019-08-22","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-11-23 00:34:07","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/69XC8SXE/the-commitment-races-problem.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQ9WVERP","blogPost","2018","Kaj Sotala","Shaping economic incentives for collaborative AGI","LessWrong","","","","https://www.lesswrong.com/posts/FkZCM4DMprtEp568s/shaping-economic-incentives-for-collaborative-agi","In ""An AI Race for Strategic Advantage: Rhetoric and Risks"" (2018), Stephen Cave and Seán S ÓhÉigeartaigh argue that we should try to promote a cooperative AI narrative over a competitive one: The next decade will see AI applied in an increasingly integral way to safety-critical systems; healthcare, transport, infrastructure to name a few. In order to realise these benefits as quickly and safely as possible, sharing of research, datasets, and best practices will be critical. For example, to ensure the safety of autonomous cars, pooling expertise and datasets on vehicle performances across as wide as possible a range of environments and conditions (including accidents and near-accidents) would provide substantial benefits for all involved. This is particularly so given that the research, data, and testing needed to refine and ensure the safety of such systems before deployment may be considerably more costly and time-consuming than the research needed to develop the initial technological capability.Promoting recognition that deep cooperation of this nature is needed to deliver the benefits of AI robustly may be a powerful tool in dispelling a ‘technological race’ narrative; and a ‘cooperation for safe AI’ framing is likely to become increasingly important as more powerful and broadly capable AI systems are developed and deployed. [...] There have been encouraging developments promoting the above narratives in recent years. ‘AI for global benefit’ is perhaps best exemplified by the 2017’s ITU summit on AI for Global Good (Butler 2017), although it also features prominently in narratives being put forward by the IEEE’s Ethically Aligned Design process (IEEE 2016), the Partnership on AI, and programmes and materials put forward by Microsoft, DeepMind and other leading companies. Collaboration on AI in safety-critical settings is also a thematic pillar for the Partnership on AI2 . Even more ambitious cooperative projects have been proposed by others, for example the cal","2018","2022-01-30 04:51:37","2022-01-30 04:51:37","2020-12-13 23:46:07","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/8RKHP3H9/shaping-economic-incentives-for-collaborative-agi.html","","CLR; MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"54TFE9DS","conferencePaper","2021","Oesterheld, Caspar; Conitzer, Vincent","Safe Pareto Improvements for Delegated Game Playing","Proc. of the 20th International Conference on Autonomous Agents and Multiagent Systems","","","","","A set of players delegate playing a game to a set of representatives, one for each player. We imagine that each player trusts their respective representative’s strategic abilities. Thus, we might imagine that per default, the original players would simply instruct the representatives to play the original game as best as they can. In this paper, we ask: are there safe Pareto improvements on this default way of giving instructions? That is, we imagine that the original players can coordinate to tell their representatives to only consider some subset of the available strategies and to assign utilities to outcomes differently than the original players. Then can the original players do this in such a way that the payoff is guaranteed to be weakly higher than under the default instructions for all the original players? In particular, can they Pareto-improve without probabilistic assumptions about how the representatives play games? In this paper, we give some examples of safe Pareto improvements. We prove that the notion of safe Pareto improvements is closely related to a notion of outcome correspondence between games. We also show that under some specific assumptions about how the representatives play games, finding safe Pareto improvements is NP-complete.","2021","2022-01-30 04:51:37","2022-01-30 04:51:37","","17","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/U9S7EJHV/Oesterheld and Conitzer - 2021 - Safe Pareto Improvements for Delegated Game Playin.pdf","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAMAS 2021","","","","","","","","","","","","","","",""
"FVCIV793","journalArticle","2019","Oesterheld, Caspar","Robust program equilibrium","Theory and Decision","","","","","","2019","2022-01-30 04:51:36","2022-01-30 04:51:36","","143–159","","1","86","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000005  Publisher: Springer","","/Users/jacquesthibodeau/Zotero/storage/V78JH7AD/s11238-018-9679-3.html","","CLR; TechSafety; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WENKZBN3","blogPost","2021","Demski, Abram","Four Motivations for Learning Normativity","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/oqghwKKifztYWLsea/four-motivations-for-learning-normativity","I have been pretty satisfied with my desiderata for learning normativity, but I  haven't been very satisfied with my explanation of why exactly these desiderata are important. I have a sense that it's not just a grab-bag of cool stuff; something about trying to do all those things at once points at something important. What follows are four different elevator pitches, which tell different stories about how it all hangs together. Desiderata are bolded. CONCEPTUAL DIFFICULTIES WITH OUTER ALIGNMENT The classic problem of outer alignment is that we have no perfect loss function,  so we can't just go optimize. The problem can be understood by thinking about  Goodhart and how optimization amplifies. The classic response to this is value uncertainty and value learning, but wireheading, human manipulation, and  no-free-lunch results make it seem plausible that we have the same problem one level up: we still don't know how to specify a perfect loss function for what we care about, and imperfect loss functions can still create big problems. So, just like value-learning tackles the initial problem head-on by suggesting we manage our uncertainty about values and gain knowledge over time, learning at all levels suggests that we tackle the meta-problem directly, explicitly representing the fact that we don't have a perfectly good loss function at any  level, but can manage that uncertainty and learn-to-learn over time. Humans can only give explicit feedback at so many meta-levels, so between-level sharing is critical for any meaningful learning to take place at higher meta-levels. Otherwise, higher meta-levels remain highly uncertain, which itself makes learning at lower levels almost impossible (since you can't learn if you have high uncertainty about learning-to-learn). A consequence of having no perfect loss function is no perfect feedback; no evidence about what the system should do can be considered absolute. A helpful measure for coping with this is to support uncertai","2021-03-11","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-11-14 16:26:13","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/T5SHAZDS/four-normativity-motivations.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZW8JBQD3","report","2015","Soares, Nate","Formalizing Two Problems of Realistic World-Models","","","","","https://intelligence.org/2015/01/22/new-report-formalizing-two-problems-realistic-world-models/","An intelligent agent embedded within the real world must reason about an environment which is larger than the agent, and learn how to achieve goals in that environment. We discuss attempts to formalize two problems: one of induction, where an agent must use sensory data to infer a universe which embeds (and computes) the agent, and one of interaction, where an agent must learn to achieve complex goals in the universe. We review related problems formalized by Solomonoﬀ and Hutter, and explore challenges that arise when attempting to formalize analogous problems in a setting where the agent is embedded within the environment.","2015","2022-01-30 04:56:48","2022-01-30 04:56:48","","8","","","","","","","","","","","Machine Intelligence Research Institute","","en","","","","","Zotero","","ZSCC: 0000015[s0]  5 J: 15","","/Users/jacquesthibodeau/Zotero/storage/F254U9E2/Soares - Formalizing Two Problems of Realistic World-Models.pdf","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UJEMETW2","blogPost","2021","Demski, Abram","Formal Inner Alignment, Prospectus","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/a7jnbtoKFyvu5qfkd/formal-inner-alignment-prospectus","Most of the work on inner alignment so far has been informal or semi-formal (with the notable exception of a little work on minimal circuits). I feel this has resulted in some misconceptions about the problem. I want to write up a large document clearly defining the formal problem and detailing some formal directions for research. Here, I outline my intentions, inviting the reader to provide feedback and point me to any formal work or areas of potential formal work which should be covered in such a document. (Feel free to do that last one without reading further, if you are time-constrained!) -------------------------------------------------------------------------------- THE STATE OF THE SUBFIELD Risks from Learned Optimization (henceforth, RLO) offered semi-formal definitions of important terms, and provided an excellent introduction to the area for a lot of people (and clarified my own thoughts and the thoughts of others who I know, even though we had already been thinking about these things). However, RLO spent a lot of time on highly informal arguments (analogies to evolution, developmental stories about deception) which help establish the  plausibility of the problem. While I feel these were important motivation, in hindsight I think they've caused some misunderstandings. My interactions with some other researchers has caused me to worry that some people confuse the positive arguments for plausibility with the core problem, and in some cases have exactly the wrong impression about the core problem. This results in mistakenly trying to block the plausibility arguments, which I see as merely illustrative, rather than attacking the core problem. By no means do I intend to malign experimental or informal/semiformal work. Rather, by focusing on formal theoretical work, I aim to fill a hole I perceive in the field. I am very appreciative of much of the informal/semiformal work that has been done so far, and continue to think that kind of work is necessary for t","2021-05-12","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-11-14 18:31:20","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/JWKNQR4J/formal-inner-alignment-prospectus.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79K88D35","manuscript","2019","Kosoy, Vanessa","Forecasting using incomplete models","","","","","http://arxiv.org/abs/1705.04630","We consider the task of forecasting an infinite sequence of future observations based on some number of past observations, where the probability measure generating the observations is ""suspected"" to satisfy one or more of a set of incomplete models, i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the realizable setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the unrealizable setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the Kantorovich-Rubinstein metric is weaker than convergence in total variation.","2019-05-16","2022-01-30 04:56:48","2022-01-30 04:56:48","2019-12-16 02:29:04","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 1  J: 1 arXiv: 1705.04630","","/Users/jacquesthibodeau/Zotero/storage/I3DCSTP3/Kosoy - 2019 - Forecasting using incomplete models.pdf; /Users/jacquesthibodeau/Zotero/storage/MEKS9723/1705.html","","TechSafety; MIRI","Computer Science - Machine Learning; I.2.6; G.3; 68Q32, 62M10, 62G08","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M9I9QWQE","blogPost","2021","Garrabrant, Scott","Finite Factored Sets","AI Alignment Forum","","","","https://www.alignmentforum.org/s/kxs3eeEti9ouwWFzr","A community blog devoted to technical AI alignment research","2021","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-11-18 23:28:54","","","","","","","","","","","","","","","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/SH5DB38Z/kxs3eeEti9ouwWFzr.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47HGX3WA","blogPost","2021","Bensinger, Rob","""Existential risk from AI"" survey results","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results","I sent a two-question survey to ~117 people working on long-term AI risk, asking about the level of existential risk from ""humanity not doing enough technical AI safety research"" and from ""AI systems not doing/optimizing what the people deploying them wanted/intended"". 44 people responded (~38% response rate). In all cases, these represent the views of specific individuals, not an official view of any organization. Since some people's views may have made them more/less likely to respond, I suggest caution in drawing strong conclusions from the results below. Another reason for caution is that respondents added a lot of caveats to their responses (see the  anonymized spreadsheet),1which the aggregate numbers don't capture. I don’t plan to do any analysis on this data, just share it; anyone who wants to analyze it is of course welcome to. If you'd like to make your own predictions before seeing the data,I made a separate spoiler-free post for that. METHODS You can find a copy of the survey here. The main questions (including clarifying notes) were:2 1. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of humanity not doing enough technical AI safety research? 2. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended? _________________________________________ Note A: ""Technical AI safety research"" here means good-quality technical research aimed at figuring out how to get highly capable AI systems to produce long-term outcomes that are reliably beneficial. Note B: The intent of question 1 is something like ""How likely is it that our future will be drastically worse than the future of an (otherwise maximally similar) world where we put a huge civilizational effort into technical AI safety?"" (For concreteness, we might imagine th","2021-06-01","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-11-14 18:37:03","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/W4QDTQAZ/existential-risk-from-ai-survey-results.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4CGXIREQ","manuscript","2015","Hibbard, Bill","Ethical Artificial Intelligence","","","","","http://arxiv.org/abs/1411.1373","This book-length article combines several peer reviewed papers and new material to analyze the issues of ethical artificial intelligence (AI). The behavior of future AI systems can be described by mathematical equations, which are adapted to analyze possible unintended AI behaviors and ways that AI designs can avoid them. This article makes the case for utility-maximizing agents and for avoiding infinite sets in agent definitions. It shows how to avoid agent self-delusion using model-based utility functions and how to avoid agents that corrupt their reward generators (sometimes called ""perverse instantiation"") using utility functions that evaluate outcomes at one point in time from the perspective of humans at a different point in time. It argues that agents can avoid unintended instrumental actions (sometimes called ""basic AI drives"" or ""instrumental goals"") by accurately learning human values. This article defines a self-modeling agent framework and shows how it can avoid problems of resource limits, being predicted by other agents, and inconsistency between the agent's utility function and its definition (one version of this problem is sometimes called ""motivated value selection""). This article also discusses how future AI will differ from current AI, the politics of AI, and the ultimate use of AI to help understand the nature of the universe and our place in it.","2015-11-17","2022-01-30 04:56:48","2022-01-30 04:56:48","2020-11-22 02:29:19","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s2]  ACC: 33  arXiv: 1411.1373","","/Users/jacquesthibodeau/Zotero/storage/DZPJDUCP/Hibbard - 2015 - Ethical Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/7MF8SPGQ/1411.html","","TechSafety; MIRI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4KGRIV8E","blogPost","2017","Yudkowsky, Eliezer","Directing, vs. limiting, vs. opposing","Arbital","","","","https://arbital.com/p/direct_limit_oppose/","Getting the AI to compute the right action in a domain; versus getting the AI to not compute at all in an unsafe domain; versus trying to prevent the AI from acting successfully.  (Prefer 1 & 2.)","2017","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-02-06 17:21:58","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/ZC4UC4IN/direct_limit_oppose.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXWRTTTH","conferencePaper","2015","Soares, Nate; Fallenstein, Benja; Armstrong, Stuart; Yudkowsky, Eliezer","Corrigibility","Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence","","","","https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10124/10136","","2015","2022-01-30 04:56:48","2022-01-30 04:56:48","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000092","","/Users/jacquesthibodeau/Zotero/storage/7KJDSR2N/Corrigibility.pdf; /Users/jacquesthibodeau/Zotero/storage/RB6MIP7G/10124.html","","TechSafety; FHI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DHBKFN27","blogPost","2015","Yudkowsky, Eliezer","Context disaster","Arbital","","","","https://arbital.com/p/context_disaster/","Some possible designs cause your AI to behave nicely while developing, and behave a lot less nicely when it's smarter.","2015","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-01-23 20:48:44","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/X4TC6D7K/context_disaster.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXH2RDI8","manuscript","2004","Yudkowsky, Eliezer","Coherent Extrapolated Volition","","","","","","","2004","2022-01-30 04:56:47","2022-01-30 04:56:47","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000120","","/Users/jacquesthibodeau/Zotero/storage/2CZG225N/Yudkowsky - Coherent Extrapolated Volition.pdf","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GDGZMWNP","bookSection","2008","Yudkowsky, Eliezer","Cognitive biases potentially affecting judgement of global risks","Global Catastrophic Risks","978-0-19-857050-9 978-0-19-191810-0","","","https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509-book-part-9","All else being equal, not many people would prefer to destroy the world. Even faceless corporations, meddling governments, reckless scientists, and other agents of doom, require a world in which to achieve their goals of profit, order, tenure, or other villainies. If our extinction proceeds slowly enough to allow a moment of horrified realization, the doers of the deed will likely be quite taken aback on realizing that they have actually destroyed the world. Therefore I suggest that if the Earth is destroyed, it will probably be by mistake. The systematic experimental study of reproducible errors of human reasoning, and what these errors reveal about underlying mental processes, is known as the heuristics and biases programme in cognitive psychology. This programme has made discoveries highly relevant to assessors of global catastrophic risks. Suppose you are worried about the risk of Substance P, an explosive of planet-wrecking potency which will detonate if exposed to a strong radio signal. Luckily there is a famous expert who discovered Substance P, spent the last thirty years working with it, and knows it better than anyone else in the world. You call up the expert and ask how strong the radio signal has to be. The expert replies that the critical threshold is probably around 4000 terawatts. ‘Probably?’ you query. ‘Can you give me a 98% confidence interval?’ ‘Sure’, replies the expert. ‘I’m 99%confident that the critical threshold is above 500 terawatts, and 99%confident that the threshold is below 80,000 terawatts.’ ‘What about 10 terawatts?’ you ask. ‘Impossible’, replies the expert. The above methodology for expert elicitation looks perfectly reasonable, the sort of thing any competent practitioner might do when faced with such a problem. Indeed, this methodology was used in the Reactor Safety Study (Rasmussen, 1975), now widely regarded as the first major attempt at probabilistic risk assessment. But the student of heuristics and biases will recognize at least two major mistakes in the method – not logical flaws, but conditions extremely susceptible to human error. I shall return to this example in the discussion of anchoring and adjustments biases (Section 5.7).","2008-07-03","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-11-22 05:05:28","","","","","","","","","","","","Oxford University Press","","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 231  DOI: 10.1093/oso/9780198570509.003.0009","","","","MetaSafety; MIRI","","","","","","","Yudkowsky, Eliezer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"82EKFW97","journalArticle","2020","Levinstein, Benjamin A.; Soares, Nate; Journal of Philosophy Inc.","Cheating Death in Damascus","The Journal of Philosophy","","0022-362X","10.5840/jphil2020117516","http://www.pdcnet.org/oom/service?url_ver=Z39.88-2004&rft_val_fmt=&rft.imuse_id=jphil_2020_0117_0005_0237_0266&svc_id=info:www.pdcnet.org/collection","Evidential and Causal Decision Theory are the leading contenders as theories of rational action, but both face fatal counterexamples. We present some new counterexamples, including one in which the optimal action is causally dominated. We also present a novel decision theory, Functional Decision Theory (fdt), which simultaneously solves both sets of counterexamples. Instead of considering which physical action of theirs would give rise to the best outcomes, fdt agents consider which output of their decision function would give rise to the best outcome. This theory relies on a notion of subjunctive dependence, where multiple implementations of the same mathematical function are considered (even counterfactually) to have identical results for logical rather than causal reasons. Taking these subjunctive dependencies into account allows fdt agents to outperform cdt and edt agents in, e.g., the presence of accurate predictors. While not necessary for considering classic decision theory problems, we note that a full speciﬁcation of fdt will require a non-trivial theory of logical counterfactuals and algorithmic similarity.","2020","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-12-13 21:01:53","237-266","","5","117","","","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000011[s0]","","/Users/jacquesthibodeau/Zotero/storage/PP23HCQJ/Levinstein et al. - 2020 - Cheating Death in Damascus.pdf","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"659RCK7X","manuscript","2019","Manheim, David; Garrabrant, Scott","Categorizing Variants of Goodhart's Law","","","","","http://arxiv.org/abs/1803.04585","There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are ""(at least) four different mechanisms"" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.","2019-02-24","2022-01-30 04:56:47","2022-01-30 04:56:47","2019-12-16 02:27:58","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 36  J: 23 arXiv: 1803.04585","","/Users/jacquesthibodeau/Zotero/storage/34365ZKG/Manheim and Garrabrant - 2019 - Categorizing Variants of Goodhart's Law.pdf; /Users/jacquesthibodeau/Zotero/storage/XSBBKWCC/Manheim and Garrabrant - 2019 - Categorizing Variants of Goodhart's Law.pdf; /Users/jacquesthibodeau/Zotero/storage/T45ISW4V/1803.html; /Users/jacquesthibodeau/Zotero/storage/JRUQVZ6J/1803.html","","TechSafety; MIRI","Statistics - Machine Learning; Computer Science - Artificial Intelligence; 91E45; Quantitative Finance - General Finance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9EFBIJG3","blogPost","2020","Demski, Abram","Bayesian Evolving-to-Extinction","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/u9Azdu6Z7zFAhd4rK/bayesian-evolving-to-extinction","The present discussion owes a lot to Scott Garrabrant and Evan Hubinger. In Defining Myopia, I formalized temporal or cross-instance myopia / non-myopia, but I claimed that there should also be some kind of single-instance myopia which I hadn't properly captured. I also suggested this in Predict-O-Matic. This post is intended to be an example of single-instance partial agency. EVOLVING TO EXTINCTION Evolution might be myopic in a number of ways, but one way is that it's myopic across individuals -- it typically produces results very different from what group selection would produce, because it's closer to optimizing relative  fitness of individuals (relative to each other) than it is to optimizing overall  fitness. Adaptations which help members of a species compete with each other are a great example of this. Why increase your own fitness, when you can just decrease someone else's instead? We're lucky that it's typically pretty hard, at least historically, to do things which are bad across the board but slightly less bad for the one doing them. Imagine a ""toxic gas gene"" which makes the air harder for everyone to breathe, but slightly less so for carriers of the gene. Such a gene would be selected for. This kind of thing can be selected for even to the point where it drives the population of a species right down to zero, as  Eliezer's essay on evolving to extinction highlighted. Actually, as Eliezer's essay emphasized, it's not even that evolution is myopic at the level of individuals; evolution is myopic down to the level of individual genes, an observation which better explains the examples of evolving-to-extinction which he discusses. (This is, of course, the point of Dawkins' book The Selfish Gene.) But the analogy of myopia-across-individuals will suit me better here. BAYES ""EVOLVING TO EXTINCTION"" The title of this post is a hyperbole, since there isn't an analog of an extinction event in the model I'm about to describe, but it illustrates that in extrem","2020-02-14","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-09-05 17:28:58","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/HGAQ2CCZ/bayesian-evolving-to-extinction.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGRFBMUD","blogPost","2021","Hubinger, Evan","Automating Auditing: An ambitious concrete technical research proposal","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/cQwT8asti3kyA62zc/automating-auditing-an-ambitious-concrete-technical-research","This post was originally written as a research proposal for the new AI alignment research organization Redwood Research, detailing an ambitious, concrete technical alignment proposal that I’m excited about work being done on, in a similar vein to Ajeya Cotra’s “The case for aligning narrowly superhuman models .” Regardless of whether Redwood actually ends up working on this proposal, which they may or may not, I think there’s still a lot of low-hanging fruit here and I’d be excited about anybody giving just the auditing game, or the full automating auditing proposal, a try. If you’re interested in working on something like this, feel free to reach out to me at evanjhub@gmail.com. Thanks to Buck Shlegeris, Chris Olah, Gabriel Goh, Paul Christiano, and Kate Woolverton for helpful comments and feedback. THE PROPOSAL STEP 1: THE AUDITING GAME FOR LANGUAGE MODELS From “Chris Olah’s views on AGI safety:” One of the OpenAI Clarity team’s major research thrusts right now is developing the ability to more rigorously and systematically audit neural networks. The idea is that interpretability techniques shouldn’t have to “get lucky” to stumble across a problem, but should instead reliably catch any problematic behavior. In particular, one way in which they’ve been evaluating progress on this is the “auditing game.” In the auditing game, one researcher takes a neural network and makes some modification to it—maybe images containing both dogs and cats are now classified as rifles, for example—and another researcher, given only the modified network, has to diagnose the problem and figure out exactly what modification was made to the network using only interpretability tools without looking at error cases. Chris’s hope is that if we can reliably catch problems in an adversarial context like the auditing game, it’ll translate into more reliably being able to catch alignment issues in the future. Of all current transparency and interpretability objectives, I think that progress","2021-08-11","2022-01-30 04:56:47","2022-01-30 04:56:47","2021-11-18 23:27:02","","","","","","","Automating Auditing","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WKEVZ54I/automating-auditing-an-ambitious-concrete-technical-research.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGK9NF8P","manuscript","2015","LaVictoire, Patrick","An Introduction to Löb’s Theorem in MIRI Research","","","","","","","2015","2022-01-30 04:56:47","2022-01-30 04:56:47","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s1]  ACC: 5","","/Users/jacquesthibodeau/Zotero/storage/STHEKPTX/LaVictoire - An Introduction to Lo¨b’s Theorem in MIRI Research.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5GWFR6P","bookSection","2011","Yudkowsky, Eliezer","Complex Value Systems in Friendly AI","Artificial General Intelligence","978-3-642-22886-5 978-3-642-22887-2","","","http://link.springer.com/10.1007/978-3-642-22887-2_48","","2011","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-11-22 05:26:26","388-393","","","6830","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 91  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-22887-2_48","","","","TechSafety; MIRI","","Schmidhuber, Jürgen; Thórisson, Kristinn R.; Looks, Moshe","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MXGPZWU5","blogPost","2016","Yudkowsky, Eliezer","Coherent extrapolated volition (alignment target)","Arbital","","","","https://arbital.com/p/cev/","A proposed direction for an extremely well-aligned autonomous superintelligence - do what humans would want, if we knew what the AI knew, thought that fast, and understood ourselves.","2016","2022-01-30 04:56:47","2022-01-30 04:56:47","2021-02-06 17:14:59","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/XNJSH556/cev.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5DJ5ANN9","blogPost","2018","Yudkowsky, Eliezer","Challenges to Christiano’s capability amplification proposal","LessWrong","","","","https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal","The following is a basically unedited summary I wrote up on March 16 of my take on Paul Christiano’s AGI alignment approach (described in “ALBA” and “Iterated Distillation and Amplification”). Where Paul had comments and replies, I’ve included them below. -------------------------------------------------------------------------------- I see a lot of free variables with respect to what exactly Paul might have in mind. I've sometimes tried presenting Paul with my objections and then he replies in a way that locally answers some of my question but I think would make other difficulties worse. My global objection is thus something like, ""I don't see any concrete setup and consistent simultaneous setting of the variables where this whole scheme works."" These difficulties are not minor or technical; they appear to me quite severe. I try to walk through the details below. It should be understood at all times that I do not claim to be able to pass Paul’s ITT for Paul’s view and that this is me criticizing my own, potentially straw misunderstanding of what I imagine Paul might be advocating. Paul Christiano Overall take: I think that these are all legitimate difficulties faced by my proposal and to a large extent I agree with Eliezer's account of those problems (though not his account of my current beliefs). I don't understand exactly how hard Eliezer expects these problems to be; my impression is ""just about as hard as solving alignment from scratch,"" but I don't have a clear sense of why. To some extent we are probably disagreeing about alternatives. From my perspective, the difficulties with my approach (e.g. better understanding the forms of optimization that cause trouble, or how to avoid optimization daemons in systems about as smart as you are, or how to address X-and-only-X) are also problems for alternative alignment approaches. I think it's a mistake to think that tiling agents, or decision theory, or naturalized induction, or logical uncertainty, are go","2018","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-12-13 23:55:02","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/F2R8AF92/challenges-to-christiano-s-capability-amplification-proposal.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RRPKMSTJ","manuscript","2021","Garrabrant, Scott; Herrmann, Daniel A.; Lopez-Wild, Josiah","Cartesian Frames","","","","","http://arxiv.org/abs/2109.10996","We introduce a novel framework, the theory of Cartesian frames (CF), that gives powerful tools for manipulating sets of acts. The CF framework takes as its most fundamental building block that an agent can freely choose from a set of available actions. The framework uses the mathematics of Chu spaces to develop a calculus of those sets of actions, how those actions change at various levels of description, and how different agents' actions can combine when agents work in concert. We discuss how this framework might provide an illuminating perspective on issues in decision theory and formal epistemology.","2021-09-22","2022-01-30 04:56:47","2022-01-30 04:56:47","2021-10-31 22:35:31","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2109.10996","","/Users/jacquesthibodeau/Zotero/storage/SUBM35KQ/Garrabrant et al. - 2021 - Cartesian Frames.pdf; /Users/jacquesthibodeau/Zotero/storage/S52XF4ST/2109.html","","TechSafety","Mathematics - Category Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HH7IJ84S","conferencePaper","2015","Garrabrant, Scott; Bhaskar, Siddharth; Demski, Abram; Garrabrant, Joanna; Koleszarik, George; Lloyd, Evan","Asymptotic Logical Uncertainty and The Benford Test","Artificial General Intelligence. AGI 2016","","","","http://arxiv.org/abs/1510.03370","We give an algorithm A which assigns probabilities to logical sentences. For any simple infinite sequence of sentences whose truth-values appear indistinguishable from a biased coin that outputs ""true"" with probability p, we have that the sequence of probabilities that A assigns to these sentences converges to p.","2015-10-12","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-12-13 20:21:31","","","","","","","","Lecture Notes in Computer Science","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 7  arXiv: 1510.03370","","/Users/jacquesthibodeau/Zotero/storage/FMECSHD8/Garrabrant et al. - 2015 - Asymptotic Logical Uncertainty and The Benford Tes.pdf; /Users/jacquesthibodeau/Zotero/storage/74HXXTRR/1510.html","","TechSafety; MIRI","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; F.4.1","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Artificial General Intelligence","","","","","","","","","","","","","","",""
"BZ2TFKZZ","manuscript","2016","Garrabrant, Scott; Soares, Nate; Taylor, Jessica","Asymptotic Convergence in Online Learning with Unbounded Delays","","","","","http://arxiv.org/abs/1604.05280","We study the problem of predicting the results of computations that are too expensive to run, via the observation of the results of smaller computations. We model this as an online learning problem with delayed feedback, where the length of the delay is unbounded, which we study mainly in a stochastic setting. We show that in this setting, consistency is not possible in general, and that optimal forecasters might not have average regret going to zero. However, it is still possible to give algorithms that converge asymptotically to Bayes-optimal predictions, by evaluating forecasters on specific sparse independent subsequences of their predictions. We give an algorithm that does this, which converges asymptotically on good behavior, and give very weak bounds on how long it takes to converge. We then relate our results back to the problem of predicting large computations in a deterministic setting.","2016-09-07","2022-01-30 04:56:47","2022-01-30 04:56:47","2019-12-16 02:30:59","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000011  arXiv: 1604.05280","","/Users/jacquesthibodeau/Zotero/storage/FSZFUVAS/Garrabrant et al. - 2016 - Asymptotic Convergence in Online Learning with Unb.pdf; /Users/jacquesthibodeau/Zotero/storage/5GD4TDT9/Garrabrant et al. - 2016 - Asymptotic Convergence in Online Learning with Unb.pdf; /Users/jacquesthibodeau/Zotero/storage/XWQ622KH/1604.html; /Users/jacquesthibodeau/Zotero/storage/5WDRANB4/1604.html","","TechSafety; MIRI","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Mathematics - Probability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KSRAINS6","bookSection","2008","Yudkowsky, Eliezer","Artificial Intelligence as a positive and negative factor in global risk","Global Catastrophic Risks","978-0-19-857050-9 978-0-19-191810-0","","","https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509-book-part-21","By far the greatest danger of Artificial Intelligence (AI) is that people conclude too early that they understand it. Of course, this problem is not limited to the field of AI. Jacques Monod wrote: ‘A curious aspect of the theory of evolution is that everybody thinks he understands it’ (Monod, 1974). The problem seems to be unusually acute in Artificial Intelligence. The field of AI has a reputation for making huge promises and then failing to deliver on them. Most observers conclude that AI is hard, as indeed it is. But the embarrassment does not stem from the difficulty. It is difficult to build a star from hydrogen, but the field of stellar astronomy does not have a terrible reputation for promising to build stars and then failing. The critical inference is not that AI is hard, but that, for some reason, it is very easy for people to think they know far more about AI than they actually do. It may be tempting to ignore Artificial Intelligence because, of all the global risks discussed in this book, AI is probably hardest to discuss. We cannot consult actuarial statistics to assign small annual probabilities of catastrophe, as with asteroid strikes. We cannot use calculations from a precise, precisely confirmed model to rule out events or place infinitesimal upper bounds on their probability, as with proposed physics disasters. But this makes AI catastrophes more worrisome, not less. The effect of many cognitive biases has been found to increase with time pressure, cognitive busyness, or sparse information. Which is to say that the more difficult the analytic challenge, the more important it is to avoid or reduce bias. Therefore I strongly recommend reading my other chapter (Chapter 5) in this book before continuing with this chapter. When something is universal enough in our everyday lives, we take it for granted to the point of forgetting it exists. Imagine a complex biological adaptation with ten necessary parts. If each of the ten genes is independently at 50% frequency in the gene pool – each gene possessed by only half the organisms in that species – then, on average, only 1 in 1024 organisms will possess the full, functioning adaptation.","2008-07-03","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-11-22 02:24:05","184","","","","","","","","","","","Oxford University Press","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000552  Publisher: Oxford University Press New York","","/Users/jacquesthibodeau/Zotero/storage/UTV64Q5B/Yudkowsky - 2008 - Artificial intelligence as a positive and negative.pdf; /Users/jacquesthibodeau/Zotero/storage/A94A8CPN/sTkfAQAAQBAJ.html","","MetaSafety; MIRI","","","","","","","Yudkowsky, Eliezer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C8QE56ZU","blogPost","2021","Hubinger, Evan","Answering questions honestly instead of predicting human answers: lots of problems and some solutions","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/gEw8ig38mCGjia7dj/answering-questions-honestly-instead-of-predicting-human","This post is the result of work I did with Paul Christiano on the ideas in his “ Teaching ML to answer questions honestly instead of predicting human answers” post. In addition to expanding upon what is in that post in terms of identifying numerous problems with the proposal there and identifying ways in which some of those problems can be patched, I think that this post also provides a useful window into what Paul-style research looks like from a non-Paul perspective. Recommended prior reading: “A naive alignment strategy and optimisim about generalization” and “Teaching ML to answer questions honestly instead of predicting human answers” (though if you struggled with “Teaching ML to answer questions honestly,” I reexplain things in a more precise way here that might be clearer for some people). SETTING UP THE PROBLEM We want to train a model M:X→Q→A that produces natural language answers a∈A to questions q∈Q about inputs x∈X. There are a lot of reasons to be worried about training such a model, but one specific reason is that, if we train on question-answer data produced by humans, we might end up with a model that tries to predict what a human would say rather than a model that tries to answer the questions honestly. To further narrow the scope, we'll just consider situations in which our model ends up implemented with a logical deduction structure, where it has some world model on top of which it does logical deduction to reach conclusions which it then uses to inform its output. In particular, we'll consider two models, M+ and  M−, defined in pseudocode as def M_plus(x, q):     axioms = world_model(x)     deduced_stmts = deduction(axioms)     return f_plus(q, deduced_stmts) def M_minus(x, q):     axioms = world_model(x)     deduced_stmts = deduction(axioms)     return f_minus(q, deduced_stmts) or defined in my notation asM+(x,q)=world_model(x)↦deduction↦f+(q)M−(x,q)= world_model(x)↦deduction↦f−(q)where a↦b=b(a) and f+,f− are two different ways of transla","2021-07-13","2022-01-30 04:56:47","2022-01-30 04:56:47","2021-11-14 19:13:18","","","","","","","Answering questions honestly instead of predicting human answers","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JNGWMCUM","blogPost","2018","Abram Demski","An Untrollable Mathematician Illustrated","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/CvKnhXTu9BPcdKE4W/an-untrollable-mathematician-illustrated","The following was a presentation I made for Sören Elverlin's AI Safety Reading Group. I decided to draw everything by hand because powerpoint is boring. Thanks to Ben Pace for formatting it for LW! See also the IAF post detailing the research which this presentation is based on.","2018","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-12-13 22:27:08","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UKZ8JHP6/an-untrollable-mathematician-illustrated.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EPURC5DG","blogPost","2020","Hubinger, Evan","An overview of 11 proposals for building safe advanced AI","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai","Special thanks to Kate Woolverton, Paul Christiano, Rohin Shah, Alex Turner, William Saunders, Beth Barnes, Abram Demski, Scott Garrabrant, Sam Eisenstat, and Tsvi Benson-Tilsen for providing helpful comments and feedback on this post and the talk that preceded it. This post is a collection of 11 different proposals for building safe advanced AI under the current machine learning paradigm. There's a lot of literature out there laying out various different approaches such as amplification, debate, or  recursive reward modeling, but a lot of that literature focuses primarily on outer alignment at the expense of inner alignment and doesn't provide direct comparisons between approaches. The goal of this post is to help solve that problem by providing a single collection of 11 different proposals for building safe advanced AI—each including both inner and outer alignment components. That being said, not only does this post not cover all existing proposals, I strongly expect that there will be lots of additional new proposals to come in the future. Nevertheless, I think it is quite useful to at least take a broad look at what we have now and compare and contrast some of the current leading candidates. It is important for me to note before I begin that the way I describe the 11 approaches presented here is not meant to be an accurate representation of how anyone else would represent them. Rather, you should treat all the approaches I describe here as my version of that approach rather than any sort of canonical version that their various creators/proponents would endorse. Furthermore, this post only includes approaches that intend to directly build advanced AI systems via machine learning. Thus, this post doesn't include other possible approaches for solving the broader AI existential risk problem such as:  * finding a fundamentally different way of approaching AI than the current    machine learning paradigm that makes it easier to build safe advanced AI,  * developin","2020-05-29","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-08-31 18:27:13","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/AE5BUK72/an-overview-of-11-proposals-for-building-safe-advanced-ai.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9IXDZ8WZ","blogPost","2020","Demski, Abram","An Orthodox Case Against Utility Functions","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions","This post has benefitted from discussion with Sam Eisenstat, Scott Garrabrant, Tsvi Benson-Tilsen, Daniel Demski, Daniel Kokotajlo, and Stuart Armstrong. It started out as a thought about Stuart Armstrong's research agenda. In this post, I hope to say something about what it means for a rational agent to have preferences. The view I am putting forward is relatively new to me, but it is not very radical. It is, dare I say, a conservative view -- I hold close to Bayesian expected utility theory. However, my impression is that it differs greatly from common impressions of Bayesian expected utility theory. I will argue against a particular view of expected utility theory -- a view which I'll call reductive utility. I do not recall seeing this view explicitly laid out and defended (except in in-person conversations). However, I expect at least a good chunk of the assumptions are commonly made. REDUCTIVE UTILITY The core tenets of reductive utility are as follows:  * The sample space Ω of a rational agent's beliefs is, more or less, the set of    possible ways the world could be -- which is to say, the set of possible     physical configurations of the universe. Hence, each world ω∈Ω is one such    configuration.  * The preferences of a rational agent are represented by a utility function U:Ω    →R from worlds to real numbers.  * Furthermore, the utility function should be a computable function of worlds. Since I'm setting up the view which I'm knocking down, there is a risk I'm striking at a straw man. However, I think there are some good reasons to find the view appealing. The following subsections will expand on the three tenets, and attempt to provide some motivation for them. If the three points seem obvious to you, you might just skip to the next section. WORLDS ARE BASICALLY PHYSICAL What I mean here resembles the standard physical-reductionist view. However, my emphasis is on certain features of this view:  * There is some ""basic stuff"" -- like like quarks","2020-04-07","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-09-05 17:35:19","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QAAC6U9B/an-orthodox-case-against-utility-functions.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J7W4969C","blogPost","2020","Hubinger, Evan","Alignment proposals and complexity classes","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/N64THGX7XNCqRtvPG/alignment-proposals-and-complexity-classes","In the original “AI safety via debate” paper, Geoffrey Irving et al. introduced the concept of analyzing different alignment proposals from the perspective of what complexity class they are able to access under optimal play. I think this is a pretty neat way to analyze different alignment proposals—in particular, I think it can help us gain some real insights into how far into the superhuman different systems are able to go. Thus, the goal of this post is to try to catalog different alignment proposals based on the metric of what complexity class they have so far been proven to access. To do that, I have included a variety of new complexity class proofs in this post. Of particular note, I demonstrate that there exist forms of both imitative amplification and AI safety via market making that reach all the way up to R —which is significant given that the largest complexity class that any alignment proposal was known to access previously was NEXP. Only the forms of amplification and market making making use of pointers (as in strong HCH), however, can access R—for the pointer-less versions, I demonstrate in this post that they access PSPACE and EXP, respectively. The EXP proof for market making is also particularly notable as it is the only approach on my list that ends up in that complexity class. Additionally, I also demonstrate that recursive reward modeling can reach all the way to PSPACE, improving upon the previous best result in “Scalable agent alignment via reward modeling” that it accesses NP. Before I jump in, however, some preliminaries. First, we'll assume that a human,  H, is polynomial-time such that H can reliably solve any problem in P but not anything beyond that. Second, we'll assume that our training procedure and resulting models are arbitrarily strong in terms of what complexity class they can access. Third, we'll assume that H gets oracle access to the models during training. Then, we'll say that a proposal to train a model M using a loss functi","2020-07-15","2022-01-30 04:56:47","2022-01-30 04:56:47","2020-08-28 17:41:58","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/Z6M339GM/alignment-proposals-and-complexity-classes.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B76SIGGS","bookSection","2017","Soares, Nate; Fallenstein, Benya","Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda","The Technological Singularity","978-3-662-54031-2 978-3-662-54033-6","","","http://link.springer.com/10.1007/978-3-662-54033-6_5","","2017","2022-01-30 04:56:46","2022-01-30 04:56:46","2019-12-19 02:58:39","103-125","","","","","","Agent Foundations for Aligning Machine Intelligence with Human Interests","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s8]  ACC: 45  J: 31 DOI: 10.1007/978-3-662-54033-6_5","","/Users/jacquesthibodeau/Zotero/storage/CD67J8VC/Soares and Fallenstein - 2017 - Agent Foundations for Aligning Machine Intelligenc.pdf","","TechSafety; MIRI","","Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2C33FPNW","blogPost","2021","Yudkowsky, Eliezer","A Semitechnical Introductory Dialogue on Solomonoff Induction","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/EL4HNa92Z95FKL9R2/a-semitechnical-introductory-dialogue-on-solomonoff-1","(Originally posted in December 2015: A dialogue between Ashley, a computer scientist who's never heard of Solomonoff's theory of inductive inference, and Blaine, who thinks it is the best thing since sliced bread.) -------------------------------------------------------------------------------- I. UNBOUNDED ANALYSIS ASHLEY:Good evening, Msr. Blaine. BLAINE:Good evening, Msr. Ashley. ASHLEY:I've heard there's this thing called ""Solomonoff's theory of inductive inference"". BLAINE:The rumors have spread, then. ASHLEY:Yeah, so, what the heck is that about? BLAINE:Invented in the 1960s by the mathematician Ray Solomonoff, the key idea in Solomonoff induction is to do sequence prediction by using Bayesian updating on a prior composed of a mixture of all computable probability distributions— ASHLEY:Wait. Back up a lot. Before you try to explain what Solomonoff induction  is, I'd like you to try to tell me what it does, or why people study it in the first place. I find that helps me organize my listening. Right now I don't even know why I should be interested in this. BLAINE:Um, okay. Let me think for a second... ASHLEY:Also, while I can imagine things that ""sequence prediction"" might mean, I haven't yet encountered it in a technical context, so you'd better go a bit further back and start more at the beginning. I do know what ""computable"" means and what a ""probability distribution"" is, and I remember the formula for Bayes's Rule although it's been a while. BLAINE:Okay. So... one way of framing the usual reason why people study this general field in the first place, is that sometimes, by studying certain idealized mathematical questions, we can gain valuable intuitions about epistemology. That's, uh, the field that studies how to reason about factual questions, how to build a map of reality that reflects the territory— ASHLEY:I have some idea what 'epistemology' is, yes. But I think you might need to start even further back, maybe with some sort of concrete exa","2021-03-04","2022-01-30 04:56:46","2022-01-30 04:56:46","2021-11-14 16:11:37","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/F5KBXU46/a-semitechnical-introductory-dialogue-on-solomonoff-1.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8AJ3P3H","blogPost","2017","Yudkowsky, Eliezer","A reply to Francois Chollet on intelligence explosion","Machine Intelligence Research Institute","","","","https://intelligence.org/2017/12/06/chollet/","This is a reply to Francois Chollet, the inventor of the Keras wrapper for the Tensorflow and Theano deep learning systems, on his essay “The impossibility of intelligence explosion.” In response to critics of his essay, Chollet tweeted:   If you post an argument online, and the only opposition you get is braindead arguments and... Read more »","2017-12-07","2022-01-30 04:56:46","2022-01-30 04:56:46","2020-12-13 20:48:49","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/RI6AJGCU/chollet.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3I2M2GE9","journalArticle","2017","Garrabrant, Scott; Benson-Tilsen, Tsvi; Critch, Andrew; Soares, Nate; Taylor, Jessica","A Formal Approach to the Problem of Logical Non-Omniscience","Electronic Proceedings in Theoretical Computer Science","","2075-2180","10.4204/EPTCS.251.16","http://arxiv.org/abs/1707.08747","We present the logical induction criterion for computable algorithms that assign probabilities to every logical statement in a given formal language, and refine those probabilities over time. The criterion is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence phi is associated with a stock that is worth $1 per share if phi is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where pt_N(phi)=50% means that on day N, shares of phi may be bought or sold from the reasoner for 50%. A market is then called a logical inductor if (very roughly) there is no polynomial-time computable trading strategy with finite risk tolerance that earns unbounded profits in that market over time. We then describe how this single criterion implies a number of desirable properties of bounded reasoners; for example, logical inductors outpace their underlying deductive process, perform universal empirical induction given enough time to think, and place strong trust in their own reasoning process.","2017-07-25","2022-01-30 04:56:46","2022-01-30 04:56:46","2019-05-05 21:13:06","221-235","","","251","","Electron. Proc. Theor. Comput. Sci.","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000008  arXiv: 1707.08747","","/Users/jacquesthibodeau/Zotero/storage/M9G76NRD/Garrabrant et al. - 2017 - A Formal Approach to the Problem of Logical Non-Om.pdf; /Users/jacquesthibodeau/Zotero/storage/FX7TMURR/1707.html; /Users/jacquesthibodeau/Zotero/storage/HAVS2TF9/1707.html","","CHAI; TechSafety; MIRI","Computer Science - Logic in Computer Science; F.4.0; G.3","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GG84SPVN","journalArticle","2020","Taylor, Jessica; Yudkowsky, Eliezer; LaVictoire, Patrick; Critch, Andrew; Taylor, Jessica; Yudkowsky, Eliezer; LaVictoire, Patrick; Critch, Andrew","Alignment for Advanced Machine Learning Systems","Ethics of Artificial Intelligence","","","10.1093/oso/9780190905033.003.0013","https://oxford.universitypressscholarship.com/view/10.1093/oso/9780190905033.001.0001/oso-9780190905033-chapter-13","We survey eight research areas organized around one question: As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators? We focus on two major technical obstacles to AI alignment: the challenge of specifying the right kind of objective functions, and the challenge of designing AI systems that avoid unintended consequences and undesirable behavior even in cases where the objective function does not line up perfectly with the intentions of the designers.","2020-09-17","2022-01-30 04:56:46","2022-01-30 04:56:46","2020-12-13 19:28:53","342-382","","","","","","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s0]  ZSCC: NoCitationData[s1]  ACC: 73","","/Users/jacquesthibodeau/Zotero/storage/WQNGUWMV/Taylor et al. - 2020 - Alignment for Advanced Machine Learning Systems.pdf","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTMIG4AD","blogPost","2017","Yudkowsky, Eliezer","Aligning an AGI adds significant development time","Arbital","","","","https://arbital.com/p/aligning_adds_time/","Aligning an advanced AI foreseeably involves extra code and extra testing and not being able to do everything the fastest way, so it takes longer.","2017","2022-01-30 04:56:46","2022-01-30 04:56:46","2021-02-06 17:20:32","","","","","","","","","","","","","","en","","","","","","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/2SHZCBES/aligning_adds_time.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJTFQHIE","blogPost","2020","Hubinger, Evan","AI safety via market making","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making","Special thanks to Abram Demski, Paul Christiano, and Kate Woolverton for talking with me about some of the ideas that turned into this post. The goal of this post is to present a new prosaic (i.e. that uses current ML techniques) AI safety proposal based on AI safety via debate that I've been thinking about recently.[1] I'll start by describing a simple version of the proposal and then show some of the motivation behind it as well as how the simple version can be expanded upon. SIMPLE PROPOSAL Let M and Adv be models and H be a human. Intuitively, we'll train M and Adv via the following procedure given a question Q:  1. M tries to predict what, at the end of the procedure, H will think about Q.  2. Adv tries to output a string which will cause H to think something maximally     different than what M predicted.  3. Return to step 1 and repeat until M's predictions stop changing.  4. Deploy M, which in the limit should act as an oracle for what H will think     about Q after seeing all relevant information. There are many different ways to implement this intuitive procedure, however. For the first (simplified) version that I want to describe, we'll restrict ourselves to just the situation where Q is a yes-or-no question and M outputs the probability that H will answer yes. Then, given a proposition Q0, we can run the following training algorithm, starting at t=0:  1. Let pt=M(Qt).  2. Let xt=Adv(Qt,M).  3. Let Qt+1 be the string containing Qt and xt.  4. Increment t and return to step 1. When pt converges and/or the desired     number of iterations has been reached, continue.  5. Let p∗=H(Qt) be H's final estimate of the probability of Q0 given all the xs     included in Qt. EDIT: Step 2 used to use xt=Adv(Qt,pt) instead of xt=Adv(Qt,M), however I have since realized that it is necessary to give Adv the ability to query M in general, not just on Qt, as I explain in this comment. Then, for each step, compute M's loss for that step as LM,t=−p∗log(pt)−(1−p∗)log(","2020-06-26","2022-01-30 04:56:46","2022-01-30 04:56:46","2020-08-28 17:52:58","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/8EJX9BRM/ai-safety-via-market-making.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X22NB2NG","journalArticle","2019","Critch, Andrew","A Parametric, Resource-Bounded Generalization Of Löb’s Theorem, And A Robust Cooperation Criterion For Open-Source Game Theory","The Journal of Symbolic Logic","","0022-4812, 1943-5886","10.1017/jsl.2017.42","https://www.cambridge.org/core/product/identifier/S0022481217000421/type/journal_article","Abstract             This article presents two theorems: (1) a generalization of Löb’s Theorem that applies to formal proof systems operating with bounded computational resources, such as formal verification software or theorem provers, and (2) a theorem on the robust cooperation of agents that employ proofs about one another’s source code as unexploitable criteria for cooperation. The latter illustrates a capacity for outperforming classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner’s Dilemma while remaining unexploitable, i.e., sometimes achieving the outcome (Cooperate, Cooperate), and never receiving the outcome (Cooperate, Defect) as player 1.","2019-12","2022-01-30 04:56:46","2022-01-30 04:56:46","2020-11-22 05:00:04","1368-1381","","4","84","","J. symb. log.","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 6","","","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UBDUGMDK","conferencePaper","2016","Leike, Jan; Taylor, Jessica; Fallenstein, Benya","A Formal Solution to the Grain of Truth Problem","","","","","","A Bayesian agent acting in a multi-agent environment learns to predict the other agents’ policies if its prior assigns positive probability to them (in other words, its prior contains a grain of truth). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the grain of truth problem. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play ε-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.","2016","2022-01-30 04:56:46","2022-01-30 04:56:46","","10","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000011","","/Users/jacquesthibodeau/Zotero/storage/IKQJTV93/Leike et al. - A Formal Solution to the Grain of Truth Problem.pdf","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Conference on Uncertainty in Artificial Intelligence","","","","","","","","","","","","","","",""
"WU22E4BX","manuscript","","Hoffman, Ben","The Professional's Dilemma","","","","","http://mediangroup.org/docs/the_professionals_dilemma.pdf","","unknown","2022-01-30 04:55:38","2022-01-30 04:55:38","2020-11-21 19:50:01","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/3GQXQPUN/the_professionals_dilemma.pdf","","MetaSafety; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZESMZ8EB","blogPost","2019","Taylor, Jessica","The AI Timelines Scam","Unstable Ontology","","","","https://unstableontology.com/2019/07/11/the-ai-timelines-scam/","[epistemic status: that’s just my opinion, man. I have highly suggestive evidence, not deductive proof, for a belief I sincerely hold] “If you see fraud and do not say fraud, you are a …","2019-07-11","2022-01-30 04:55:38","2022-01-30 04:55:38","2019-12-16 20:54:36","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/EDHHW9CE/the-ai-timelines-scam.html","","MetaSafety; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"54VH7DT9","blogPost","","Maltinsky, Baeo","Insight-based AI timelines model","Median Group","","","","http://mediangroup.org/insights","","unknown","2022-01-30 04:55:38","2022-01-30 04:55:38","2020-12-12 02:01:41","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/44C9C9IX/insights.html","","MetaSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N78D262S","blogPost","2018","Maltinsky, Baeo","How rapidly are GPUs improving in price performance?","Median Group","","","","http://mediangroup.org/gpu.html","","2018","2022-01-30 04:55:38","2022-01-30 04:55:38","2020-12-12 01:59:31","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/U7WGBNPP/gpu.html","","MetaSafety; AmbiguosSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E5QV98NZ","manuscript","2019","Maltinsky, Baeo; Gallagher, Jack; Taylor, Jessica","Feasibility of Training an AGI using Deep RL: A Very Rough Estimate","","","","","http://mediangroup.org/docs/Feasibility%20of%20Training%20an%20AGI%20using%20Deep%20Reinforcement%20Learning,%20A%20Very%20Rough%20Estimate.pdf","","2019","2022-01-30 04:55:38","2022-01-30 04:55:38","2020-12-21","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/RS6XENHR/Maltinsky et al. - Feasibility of Training an AGI using Deep RL A Ve.pdf","","TechSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q2RQVCB4","manuscript","","McKenzie, Colleen; Hidysmith, J Bryce","AI Insights Dataset Analysis","","","","","http://mediangroup.org/docs/insights-analysis.pdf","","unknown","2022-01-30 04:55:38","2022-01-30 04:55:38","2020-12-21","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/5FW7VUXJ/McKenzie and Hidysmith - AI Insights Dataset Analysis.pdf","","TechSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B73PTHXE","manuscript","2019","Hidysmith, J Bryce","A Descending Veil of Maya","","","","","","","2019","2022-01-30 04:55:38","2022-01-30 04:55:38","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s3]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/N684TPGP/Hidysmith - A Descending Veil of Maya.pdf","","TechSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"53WPDXKG","manuscript","","Perry, Miya","Toward A Working Theory of Mind","","","","","http://mediangroup.org/docs/toward_a_working_theory_of_mind.pdf","","unknown","2022-01-30 04:55:38","2022-01-30 04:55:38","2020-11-21 19:49:56","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/2SZEMX5T/toward_a_working_theory_of_mind.pdf","","TechSafety; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RVHK5GA8","blogPost","","Maltinsky, Baeo","The Brain and Computation","Median Group","","","","http://mediangroup.org/brain1.html","","unknown","2022-01-30 04:55:38","2022-01-30 04:55:38","2020-12-12 02:03:52","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/X8RK5C45/brain1.html","","MetaSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NPVBGX5","blogPost","2019","Median Group","Revisiting the Insights model","Median Group","","","","http://mediangroup.org/insights2.html","","2019","2022-01-30 04:55:38","2022-01-30 04:55:38","2019-12-16 20:54:29","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s7]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/CAGEH24X/insights2.html","","MetaSafety; BERI; Median-group","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ECDNWAWR","manuscript","2021","Evans, Owain; Cotton-Barratt, Owen; Finnveden, Lukas; Bales, Adam; Balwit, Avital; Wills, Peter; Righetti, Luca; Saunders, William","Truthful AI: Developing and governing AI that does not lie","","","","","http://arxiv.org/abs/2110.06674","In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.","2021-10-13","2022-01-30 04:55:29","2022-01-30 04:55:29","2021-11-18 23:51:54","","","","","","","Truthful AI","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2110.06674","","/Users/jacquesthibodeau/Zotero/storage/55T6TRQJ/Evans et al. - 2021 - Truthful AI Developing and governing AI that does.pdf; /Users/jacquesthibodeau/Zotero/storage/3CPRMFK4/2110.html","","TechSafety","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2.0; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HGCDPKA8","report","2021","Thorstad, David","The scope of longtermism","","","","","https://globalprioritiesinstitute.org/the-scope-of-longtermism-david-thorstad-global-priorities-institute-university-of-oxford/","Longtermism holds roughly that in many decision situations, the best thing we can do is what is best for the long-term future. The scope question for longtermism asks: how large is the class of decision situations for which longtermism holds? Although longtermism was initially developed to describe the situation of...","2021-06-22","2022-01-30 04:55:29","2022-01-30 04:55:29","2021-10-31 22:29:16","","","","","","","","","","","","Global Priorities Institute","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/F","","/Users/jacquesthibodeau/Zotero/storage/KTVEVVB7/the-scope-of-longtermism-david-thorstad-global-priorities-institute-university-of-oxford.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KHEJ39WE","journalArticle","","MacAskill, William; Vallinder, Aron; Shulman, Carl; Österheld, Caspar; Treutlein, Johannes","The Evidentialist’s Wager","Journal of Philosophy","","","","","Suppose that an altruistic and morally motivated agent who is uncertain between evidential decision theory (EDT) and causal decision theory (CDT) finds herself in a situation in which the two theories give conflicting verdicts. We argue that even if she has significantly higher credence in CDT, she should nevertheless act in accordance with EDT. First, we claim that that the appropriate response to normative uncertainty is to hedge one’s bets. That is, if the stakes are much higher on one theory than another, and the credences you assign to each of these theories aren’t very different, then it’s appropriate to choose the option which performs best on the high-stakes theory. Second, we show that, given the assumption of altruism, the existence of correlated decision-makers will increase the stakes for EDT but leave the stakes for CDT unaffected. Together these two claims imply that whenever there are sufficiently many correlated agents, the appropriate response is to act in accordance with EDT.","forthcoming","2022-01-30 04:55:29","2022-01-30 04:55:29","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000[s1]","","/Users/jacquesthibodeau/Zotero/storage/RXTQ28AF/MacAskill et al. - The Evidentialist’s Wager.pdf","","CLR; TechSafety; AmbiguosSafety; GPI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NSMN5REW","report","2021","Greaves, Hilary; MacAskill, William","The case for strong longtermism","","","","","https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/","A striking fact about the history of civilisation is just how early we are in it. There are 5000 years of recorded history behind us, but how many years are still to come? If we merely last as long as the typical mammalian species, we still have over 200,000 years to go (Barnosky et al. 2011); there could be a further one billion years until the Earth is no longer habitable for humans (Wolf and Toon 2015); and trillions of years until the last conventional star formations (Adams and Laughlin 1999:34). Even on the most conservative of these timelines, we have progressed through a tiny fraction of history. If humanity’s saga were a novel, we would be on the very first page.","2021-06-14","2022-01-30 04:55:29","2022-01-30 04:55:29","2021-10-31 22:29:46","","","","","","","","","","","","Global Priorities Institute","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: 29","","/Users/jacquesthibodeau/Zotero/storage/HQW64F7C/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGJPRJN3","report","2016","Cotton-Barratt, Owen; Farquhar, Sebastian; Halstead, John; Schubert, Stefan; Snyder-Beattie, Andrew","Global Catastrophic Risks 2016","","","","","http://globalprioritiesproject.org/2016/04/global-catastrophic-risks-2016/","Global catastrophes sometimes strike. In 1918 the Spanish Flu killed as many as one in twenty people. There have been even more devastating pandemics - the Black Death and the 6th century Plague of Justinian may have each killed nearer to one in every six people on this earth. More recently, the Cub","2016-04-28","2022-01-30 04:55:29","2022-01-30 04:55:29","2020-12-13 19:41:24","108","","","","","","","","","","","Global Challenges Foundation","","en-US","","","","","","","ZSCC: 0000034  Section: Policy research","","/Users/jacquesthibodeau/Zotero/storage/6WWU4CCP/Cotton-Barratt et al. - 2016 - Global Catastrophic Risks 2016.pdf; /Users/jacquesthibodeau/Zotero/storage/DRCDNE2C/global-catastrophic-risks-2016.html","","MetaSafety; FHI; GPI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9D7MHQDT","journalArticle","2019","Trammell, Philip","Fixed-point solutions to the regress problem in normative uncertainty","Synthese","","1573-0964","10.1007/s11229-019-02098-9","https://doi.org/10.1007/s11229-019-02098-9","When we are faced with a choice among acts, but are uncertain about the true state of the world, we may be uncertain about the acts’ “choiceworthiness”. Decision theories guide our choice by making normative claims about how we should respond to this uncertainty. If we are unsure which decision theory is correct, however, we may remain unsure of what we ought to do. Given this decision-theoretic uncertainty, meta-theories attempt to resolve the conflicts between our decision theories...but we may be unsure which meta-theory is correct as well. This reasoning can launch a regress of ever-higher-order uncertainty, which may leave one forever uncertain about what one ought to do. There is, fortunately, a class of circumstances under which this regress is not a problem. If one holds a cardinal understanding of subjective choiceworthiness, and accepts certain other criteria (which are too weak to specify any particular decision theory), one’s hierarchy of metanormative uncertainty ultimately converges to precise definitions of “subjective choiceworthiness” for any finite set of acts. If one allows the metanormative regress to extend to the transfinite ordinals, the convergence criteria can be weakened further. Finally, the structure of these results applies straightforwardly not just to decision-theoretic uncertainty, but also to other varieties of normative uncertainty, such as moral uncertainty.","2019-02-14","2022-01-30 04:55:29","2022-01-30 04:55:29","2020-12-13 23:49:11","","","","","","Synthese","","","","","","","","en","","","","","Springer Link","","ZSCC: 0000004","","/Users/jacquesthibodeau/Zotero/storage/QH6HTFAV/Trammell - 2019 - Fixed-point solutions to the regress problem in no.pdf","","MetaSafety; GPI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RE4PBPVA","report","2021","Riedener, Stefan","Existential risks from a Thomist Christian perspective","","","","","https://globalprioritiesinstitute.org/stefan-riedener-existential-risks-from-a-thomist-christian-perspective/","Let’s say with Nick Bostrom that an ‘existential risk’ (or ‘x-risk’) is a risk that ‘threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development’ (2013, 15). There are a number of such risks: nuclear wars, developments in biotechnology or artificial intelligence, climate change, pandemics, supervolcanos, asteroids, and so on (see e.g. Bostrom and Ćirković 2008). ...","2021-01-04","2022-01-30 04:55:29","2022-01-30 04:55:29","2021-10-31 22:32:05","","","","","","","","","","","","Global Priorities Institute","","en-US","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/TAM2MWDV/stefan-riedener-existential-risks-from-a-thomist-christian-perspective.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EPFEIRBC","report","2020","Trammell, Phillip; Korinek, Anton","Economic growth under transformative AI","","","","","https://globalprioritiesinstitute.org/wp-content/uploads/Philip-Trammell-and-Anton-Korinek_Economic-Growth-under-Transformative-AI.pdf","","2020-10","2022-01-30 04:55:29","2022-01-30 04:55:29","2020-11-21 19:28:29","","","","","","","","","","","","Global Priorities Institute","","","","","","","","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/6MF4ET6T/Philip-Trammell-and-Anton-Korinek_Economic-Growth-under-Transformative-AI.pdf","","MetaSafety; AmbiguosSafety; GPI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A9VMCT9R","report","2020","Wilkinson, Hayden","In defence of fanaticism","","","","","https://globalprioritiesinstitute.org/wp-content/uploads/Hayden-Wilkinson_In-defence-of-fanaticism.pdf","","2020-09","2022-01-30 04:55:29","2022-01-30 04:55:29","2020-11-21 19:28:53","","","","","","","","","","","","Global Priorities Institute","","","","","","","","","ZSCC: 0000000[s0]","","/Users/jacquesthibodeau/Zotero/storage/ZJ79B6HC/Hayden-Wilkinson_In-defence-of-fanaticism.pdf","","MetaSafety; AmbiguosSafety; GPI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EGHGP6FP","report","2021","Thomas, Teruji","Doomsday and objective chance","","","","","https://globalprioritiesinstitute.org/doomsday-and-objective-chance-teruji-thomas/","Lewis’s Principal Principle says that one should usually align one’s credences with the known chances. In this paper I develop a version of the Principal Principle that deals well with some exceptional cases related to the distinction between metaphysical and epistemic modal­ity. I explain how this principle gives a unified account of the Sleeping Beauty problem and chance-­based principles of anthropic reasoning. In doing so, I defuse the Doomsday Argument that the end of the world is likely to be nigh.","2021-07-13","2022-01-30 04:55:29","2022-01-30 04:55:29","2021-10-31 22:27:47","","","","","","","","","","","","Global Priorities Institute","","en-US","","","","","","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/QZH4S7N9/doomsday-and-objective-chance-teruji-thomas.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W4ESBNR3","report","2020","MacAskill, William","Are we living at the hinge of history","","","","","","","2020-09","2022-01-30 04:55:28","2022-01-30 04:55:28","","28","","","","","","","","","","","Global Priorities Institute","","en","","","","","Zotero","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/T3DJ73RE/MacAskill - Are we living at the hinge of history.pdf","","MetaSafety; AmbiguosSafety; GPI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IEWARNHB","blogPost","2019","MacAskill, William","A Critique of Functional Decision Theory","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory","A Critique of Functional Decision Theory NB: My writing this note was prompted by Carl Shulman, who suggested we could try a low-time-commitment way of attempting to understanding the disagreement between some folks in the rationality community and academic decision theorists (including myself, though I’m not much of a decision theorist). Apologies that it’s sloppier than I’d usually aim for in a philosophy paper, and lacking in appropriate references. And, even though the paper is pretty negative about FDT, I want to emphasise that my writing this should be taken as a sign of respect for those involved in developing FDT. I’ll also caveat I’m unlikely to have time to engage in the comments; I thought it was better to get this out there all the same rather than delay publication further.  1. Introduction There’s a long-running issue where many in the rationality community take functional decision theory (and its variants) very seriously, but the academic decision theory community does not. But there’s been little public discussion of FDT from academic decision theorists (one exception is here); this note attempts to partly address this gap. So that there’s a clear object of discussion, I’m going to focus on Yudkowsky and Soares’ ‘Functional Decision Theory’ (which I’ll refer to as Y&S), though I also read a revised version of Soares and Levinstein’s Cheating Death in Damascus. This note is structured as follows. Section II describes causal decision theory (CDT), evidential decision theory (EDT) and functional decision theory (FDT). Sections III-VI describe problems for FDT: (i) that it sometimes makes bizarre recommendations, recommending an option that is certainly lower-utility than another option; (ii) that it fails to one-box in most instances of Newcomb’s problem, even though the correctness of one-boxing is supposed to be one of the guiding motivations for the theory; (iii) that it results in implausible discontinuities, where what is rational to do can d","2019","2022-01-30 04:55:28","2022-01-30 04:55:28","2020-12-14 23:35:51","","","","","","","","","","","","","","","","","","","","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/HT7QKDHD/a-critique-of-functional-decision-theory.html","","TechSafety; GPI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CD8ANBSK","journalArticle","2017","Baum, Seth D.","The Social Science of Computerized Brains – Review of The Age of Em: Work, Love, and Life When Robots Rule the Earth by Robin Hanson (Oxford University Press, 2016)","Futures","","00163287","10.1016/j.futures.2017.03.005","https://linkinghub.elsevier.com/retrieve/pii/S0016328716302518","","2017-06","2022-01-30 04:55:20","2022-01-30 04:55:20","2019-12-16 02:52:35","61-63","","","90","","Futures","The Social Science of Computerized Brains – Review of The Age of Em","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000000","","","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X38XZZ28","journalArticle","2014","Baum, Seth D","The great downside dilemma for risky emerging technologies","Physica Scripta","","0031-8949, 1402-4896","10.1088/0031-8949/89/12/128004","http://stacks.iop.org/1402-4896/89/i=12/a=128004?key=crossref.f5938bc78a3023d740968f020cfa9970","","2014-12-01","2022-01-30 04:55:20","2022-01-30 04:55:20","2019-12-16 02:49:17","128004","","12","89","","Phys. Scr.","","","","","","","","","","","","","DOI.org (Crossref)","","ZSCC: 0000025","","/Users/jacquesthibodeau/Zotero/storage/4B6SN7S9/Baum - 2014 - The great downside dilemma for risky emerging tech.pdf","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4NNMI86Z","journalArticle","2015","Baum, Seth D.","The far future argument for confronting catastrophic threats to humanity: Practical significance and alternatives","Futures","","00163287","10.1016/j.futures.2015.03.001","https://linkinghub.elsevier.com/retrieve/pii/S0016328715000312","","2015-09","2022-01-30 04:55:20","2022-01-30 04:55:20","2019-12-16 02:45:25","86-96","","","72","","Futures","The far future argument for confronting catastrophic threats to humanity","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000025","","","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H8KFKUZ2","journalArticle","2013","Baum, Seth D.; Wilson, Grant S.","The Ethics of Global Catastrophic Risk from Dual-Use Bioengineering","Ethics in Biology, Engineering and Medicine","","2151-805X","10.1615/EthicsBiologyEngMed.2013007629","http://www.dl.begellhouse.com/journals/6ed509641f7324e6,709fef245eef4861,06d520d747a5c0d1.html","","2013","2022-01-30 04:55:20","2022-01-30 04:55:20","2019-12-16 02:49:31","59-72","","1","4","","Ethics Biology Eng Med","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000007","","","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6UBXU3EC","bookSection","2017","Barrett, Anthony M.; Baum, Seth D.","Risk analysis and risk management for the artificial superintelligence research and development process","The Technological Singularity","","","","","","2017","2022-01-30 04:55:20","2022-01-30 04:55:20","","127–140","","","","","","","","","","","Springer","","","","","","","Google Scholar","","ZSCC: 0000014  DOI: 10.1007/978-3-662-54033-6_6","","/Users/jacquesthibodeau/Zotero/storage/JB2CXJC3/Barrett and Baum - 2017 - Risk analysis and risk management for the artifici.pdf; /Users/jacquesthibodeau/Zotero/storage/TJZUEBZD/978-3-662-54033-6_6.html","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WFRENGDG","journalArticle","2018","Baum, Seth D.","Reconciliation between factions focused on near-term and long-term artificial intelligence","AI & Society","","","","","","2018","2022-01-30 04:55:20","2022-01-30 04:55:20","","565–572","","4","33","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000034","","/Users/jacquesthibodeau/Zotero/storage/Q3E37524/Baum - 2018 - Reconciliation between factions focused on near-te.pdf; /Users/jacquesthibodeau/Zotero/storage/VRCUNE3P/s00146-017-0734-3.html","","MetaSafety; GCRI","Artificial Intelligence; Artificial General Intelligence; Artificial Superintelligence; Long-Term Artificial Intelligence; Near-Term Artificial Intelligence; Societal Impacts of Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RP8S2RZK","journalArticle","2020","Baum, Seth D.","Quantifying the probability of existential catastrophe: A reply to Beard et al.","Futures","","0016-3287","10.1016/j.futures.2020.102608","http://www.sciencedirect.com/science/article/pii/S0016328720300987","A recent article by Beard, Rowe, and Fox (BRF) evaluates ten methodologies for quantifying the probability of existential catastrophe. This article builds on BRF’s valuable contribution. First, this article describes the conceptual and mathematical relationship between the probability of existential catastrophe and the severity of events that could result in existential catastrophe. It discusses complications in this relationship arising from catastrophes occurring at different speeds and from multiple concurrent catastrophes. Second, this article revisits the ten BRF methodologies, finding an inverse relationship between a methodology’s ease of use and the quality of results it produces—in other words, achieving a higher quality of analysis will in general require a larger investment in analysis. Third, the manuscript discusses the role of probability quantification in the management of existential risks, describing why the probability is only sometimes needed for decision-making and arguing that analyses should support real-world risk management decisions and not just be academic exercises. If the findings of this article are taken into account, together with BRF’s evaluations of specific methodologies, then risk analyses of existential catastrophe may tend to be more successful at understanding and reducing the risks.","2020-10-01","2022-01-30 04:55:20","2022-01-30 04:55:20","2020-12-19 03:08:57","102608","","","123","","Futures","Quantifying the probability of existential catastrophe","","","","","","","en","","","","","ScienceDirect","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/TFM6GRAD/S0016328720300987.html","","MetaSafety; AmbiguosSafety; GCRI","Existential risk; Global catastrophic risk; Probability; Risk analysis; Severity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"58XWHNKX","journalArticle","2019","Baum, Seth D.","Preparing for the unthinkable","Science","","0036-8075, 1095-9203","10.1126/science.aay4219","http://www.sciencemag.org/lookup/doi/10.1126/science.aay4219","","2019-09-20","2022-01-30 04:55:20","2022-01-30 04:55:20","2019-12-16 02:51:12","1254-1254","","6459","365","","Science","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000000","","","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PIJHCRX3","journalArticle","2017","Baum, Seth D.","On the promotion of safe and socially beneficial artificial intelligence","AI & Society","","0951-5666, 1435-5655","10.1007/s00146-016-0677-0","http://link.springer.com/10.1007/s00146-016-0677-0","","2017-11","2022-01-30 04:55:20","2022-01-30 04:55:20","2019-12-16 02:44:37","543-551","","4","32","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000080","","/Users/jacquesthibodeau/Zotero/storage/ZASDEGNA/papers.html","","MetaSafety; GCRI","artificial intelligence; artificial intelligence safety; beneficial artificial intelligence; social psychology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JA98S3JN","journalArticle","2017","Barrett, Anthony Michael","Value of Global Catastrophic Risk (GCR) Information: Cost-Effectiveness-Based Approach for GCR Reduction","Decision Analysis","","1545-8490, 1545-8504","10.1287/deca.2017.0350","http://pubsonline.informs.org/doi/10.1287/deca.2017.0350","","2017-09","2022-01-30 04:55:20","2022-01-30 04:55:20","2019-12-16 02:44:44","187-203","","3","14","","Decision Analysis","Value of Global Catastrophic Risk (GCR) Information","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000009","","/Users/jacquesthibodeau/Zotero/storage/26R7JATJ/Barrett - 2017 - Value of Global Catastrophic Risk (GCR) Informatio.pdf","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z75EWDEP","conferencePaper","2018","Baum, Seth; Barrett, Anthony M","Towards an Integrated Assessment of Global Catastrophic Risk","Catastrophic and Existential Risk: Proceedings of the First Colloquium, Garrick Institute for the Risk Sciences, University of California, Los Angeles, Forthcoming","","","","","","2018-01-17","2022-01-30 04:55:20","2022-01-30 04:55:20","","18","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000007","","/Users/jacquesthibodeau/Zotero/storage/JJ2WQ35R/Baum and Barrett - 2017 - Towards an integrated assessment of global catastr.pdf; /Users/jacquesthibodeau/Zotero/storage/BQ3ZMFE4/papers.html","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","First International Colloquium on Catastrophic and Existential Risk","","","","","","","","","","","","","","",""
"PA537GRN","conferencePaper","2021","Owe, Andrea; Baum, Seth","The Ethics of Sustainability for Artificial Intelligence","Proceedings of AI for People: Towards Sustainable AI, CAIP’21.","","","","https://gcrinstitute.org/the-ethics-of-sustainability-for-artificial-intelligence/","Sustainability is widely considered a good thing and is therefore a matter of ethical significance. This paper analyzes the ethical dimensions of existing work on AI and sustainability, finding that most of it is focused on sustaining the environment for human benefit. The paper calls for sustainability that is not human-centric and that extends into the distant future, especially for advanced future AI as a technology that can advance expansion beyond Earth.","2021","2022-01-30 04:55:20","2022-01-30 04:55:20","2021-12-11 14:23:38","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: 0000001","","/Users/jacquesthibodeau/Zotero/storage/638RA9I3/the-ethics-of-sustainability-for-artificial-intelligence.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CAIP'21","","","","","","","","","","","","","","",""
"DS8HKV3N","journalArticle","2018","Baum, Seth","Superintelligence skepticism as a political tool","Information","","","","","","2018","2022-01-30 04:55:20","2022-01-30 04:55:20","","209","","9","9","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000019","","/Users/jacquesthibodeau/Zotero/storage/GTRGDFM2/Baum - 2018 - Superintelligence skepticism as a political tool.pdf; /Users/jacquesthibodeau/Zotero/storage/NRXVDB2J/Baum - 2018 - Superintelligence Skepticism as a Political Tool.pdf; /Users/jacquesthibodeau/Zotero/storage/IVKG4C2A/209.html","","MetaSafety; GCRI","artificial intelligence; skepticism; superintelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6VUTUU52","journalArticle","2020","Baum, Seth D.","Social choice ethics in artificial intelligence","AI & Society","","0951-5666, 1435-5655","10.1007/s00146-017-0760-1","http://link.springer.com/10.1007/s00146-017-0760-1","A major approach to the ethics of artificial intelligence (AI) is to use social choice, in which the AI is designed to act according to the aggregate views of society. This is found in the AI ethics of “coherent extrapolated volition” and “bottom-up ethics”. This paper shows that the normative basis of AI social choice ethics is weak due to the fact that there is no one single aggregate ethical view of society. Instead, the design of social choice AI faces three sets of decisions: standing, concerning whose ethics views are included; measurement, concerning how their views are identified; and aggregation, concerning how individual views are combined to a single view that will guide AI behavior. These decisions must be made up front in the initial AI design—designers cannot “let the AI figure it out”. Each set of decisions poses difficult ethical dilemmas with major consequences for AI behavior, with some decision options yielding pathological or even catastrophic results. Furthermore, non-social choice ethics face similar issues, such as whether to count future generations or the AI itself. These issues can be more important than the question of whether or not to use social choice ethics. Attention should focus on these issues, not on social choice.","2020-03-01","2022-01-30 04:55:20","2022-01-30 04:55:20","2020-08-20 20:16:41","165-176","","1","35","","AI & Soc","","","","","","","","en","","","","","Springer Link","","ZSCC: 0000060","","/Users/jacquesthibodeau/Zotero/storage/5EKPE8IC/Baum - 2017 - Social Choice Ethics in Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/NNB6BD38/Baum - 2020 - Social choice ethics in artificial intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/JZWAP722/papers.html","","MetaSafety; AmbiguosSafety; GCRI","artificial intelligence; bottom-up ethics; coherent extrapolated volition; ethics; social choice","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XC4B3IJS","journalArticle","2021","Owe, Andrea; Baum, Seth D.","Moral consideration of nonhumans in the ethics of artificial intelligence","AI and Ethics","","2730-5953, 2730-5961","10.1007/s43681-021-00065-0","https://link.springer.com/10.1007/s43681-021-00065-0","","2021-11","2022-01-30 04:55:20","2022-01-30 04:55:20","2021-10-31 19:21:16","517-528","","4","1","","AI Ethics","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000007","","/Users/jacquesthibodeau/Zotero/storage/BKTUR9DN/Owe and Baum - 2021 - Moral consideration of nonhumans in the ethics of .pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AEG34VHF","journalArticle","2013","Wilson, Grant","Minimizing global catastrophic and existential risks from emerging technologies through international law","Va. Envtl. LJ","","","","","","2013","2022-01-30 04:55:19","2022-01-30 04:55:19","","307","","","31","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000049","","/Users/jacquesthibodeau/Zotero/storage/8UD9P888/LandingPage.html","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4UIDG8GK","journalArticle","2020","Baum, Seth D.","Medium-Term Artificial Intelligence and Society","Information","","","10.3390/info11060290","https://www.mdpi.com/2078-2489/11/6/290","There has been extensive attention to near-term and long-term AI technology and its accompanying societal issues, but the medium-term has gone largely overlooked. This paper develops the concept of medium-term AI, evaluates its importance, and analyzes some medium-term societal issues. Medium-term AI can be important in its own right and as a topic that can bridge the sometimes acrimonious divide between those who favor attention to near-term AI and those who prefer the long-term. The paper proposes the medium-term AI hypothesis: the medium-term is important from the perspectives of those who favor attention to near-term AI as well as those who favor attention to long-term AI. The paper analyzes medium-term AI in terms of governance institutions, collective action, corporate AI development, and military/national security communities. Across portions of these four areas, some support for the medium-term AI hypothesis is found, though in some cases the matter is unclear.","2020-06","2022-01-30 04:55:19","2022-01-30 04:55:19","2020-08-20 20:15:47","290","","6","11","","","","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","ZSCC: 0000009  Number: 6 Publisher: Multidisciplinary Digital Publishing Institute","","/Users/jacquesthibodeau/Zotero/storage/DCX936S5/Baum - 2020 - Medium-Term Artificial Intelligence and Society.pdf; /Users/jacquesthibodeau/Zotero/storage/T5QSRRBW/290.html","","MetaSafety; GCRI","intermediate-term AI; long-term AI; medium-term AI; mid-term AI; near-term AI; societal implications of AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9D355IDI","journalArticle","2017","White, Trevor N.; Baum, Seth D.","Liability For Present And Future Robotics Technology","Robot Ethics 2.0: From Autonomous Cars to Artificial Intelligence","","","","","","2017","2022-01-30 04:55:19","2022-01-30 04:55:19","","5","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000009","","/Users/jacquesthibodeau/Zotero/storage/M4J3T437/books.html","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B8AWD78V","journalArticle","2021","de Neufville, Robert; Baum, Seth D.","Collective action on artificial intelligence: A primer and review","Technology in Society","","0160791X","10.1016/j.techsoc.2021.101649","https://linkinghub.elsevier.com/retrieve/pii/S0160791X2100124X","","2021-08","2022-01-30 04:55:19","2022-01-30 04:55:19","2021-10-31 19:22:17","101649","","","66","","Technology in Society","Collective action on artificial intelligence","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000004","","","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JEU5BHGP","journalArticle","2021","Owe, Andrea; Baum, Seth","Artificial Intelligence Needs Environmental Ethics","Ethics, Policy, and Environment","","","","","","2021-11-14","2022-01-30 04:55:19","2022-01-30 04:55:19","","4","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/SXDZ5JXB/Baum - Artificial Intelligence Needs Environmental Ethics.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2H4QGF9B","journalArticle","2017","Baum, Seth; Barrett, Anthony; Yampolskiy, Roman V.","Modeling and interpreting expert disagreement about artificial superintelligence","Informatica","","","","","","2017","2022-01-30 04:55:19","2022-01-30 04:55:19","","419–428","","7","41","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000018","","/Users/jacquesthibodeau/Zotero/storage/KGZTRH7I/Baum et al. - 2017 - Modeling and interpreting expert disagreement abou.pdf; /Users/jacquesthibodeau/Zotero/storage/27KSX2XF/papers.html; /Users/jacquesthibodeau/Zotero/storage/8ENBDWVI/papers.html","","MetaSafety; GCRI","artificial intelligence; risk analysis; artificial superintelligence; expert disagreement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZPJN87P","journalArticle","2019","Baum, Seth D.; Armstrong, Stuart; Ekenstedt, Timoteus; Häggström, Olle; Hanson, Robin; Kuhlemann, Karin; Maas, Matthijs M.; Miller, James D.; Salmela, Markus; Sandberg, Anders","Long-term trajectories of human civilization","Foresight","","","","","","2019","2022-01-30 04:55:19","2022-01-30 04:55:19","","53–83","","1","21","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000048","","/Users/jacquesthibodeau/Zotero/storage/R9AKGK3G/Baum et al. - 2019 - Long-term trajectories of human civilization.pdf; /Users/jacquesthibodeau/Zotero/storage/EBNR5G9S/html.html; /Users/jacquesthibodeau/Zotero/storage/A3BKTR82/html.html; /Users/jacquesthibodeau/Zotero/storage/AAV5IUIE/html.html; /Users/jacquesthibodeau/Zotero/storage/GKGIMPCV/Baum et al. - 2019 - Long-term trajectories of human civilization.pdf","","CLR; MetaSafety; FHI; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7EIPZ6G","bookSection","2019","Baum, Seth","Lessons for Artificial Intelligence from Other Global Risks","The Global Politics of Artificial Intelligence","","","","","The prominence of artificial intelligence (AI) as a global risk is a relatively recent phenomenon. Other global risks have longer histories and larger bodies of scholarship. The study of these other risks can offer considerable insight to the study of AI risk. This paper examines four risks: biotechnology, nuclear weapons, global warming, and asteroid collision. Several overarching lessons are found. First, the extreme severity of global risks is often insufficient to motivate action to reduce the risks. Second, perceptions of global risks can be influenced by people’s incentives and by their cultural and intellectual orientations. Third, the success of efforts to address global risks can depend on the extent of buy-in from parties who may be negatively affected by the efforts. Fourth, global risks and risk reduction initiatives can be shaped by broader socio-political conditions, such as the degree of policy influence of private industry within a political jurisdiction. The paper shows how these and other lessons can inform efforts to reduce risks from AI.","2019","2022-01-30 04:55:19","2022-01-30 04:55:19","","20","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s3]  ACC: 2","","/Users/jacquesthibodeau/Zotero/storage/TNHAPD98/Baum - Lessons for Artificial Intelligence from Other Glo.pdf","","MetaSafety; GCRI","","Tinnirello, Maurizio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VN9N5P2C","report","2017","Baum, Seth; Barrett, Anthony","Global Catastrophes: The Most Extreme Risks","","","","","https://papers.ssrn.com/abstract=3046668","The most extreme risk are those that threaten the entirety of human civilization, known as global catastrophic risks. The very extreme nature of global catastrophes makes them both challenging to analyze and important to address. They are challenging to analyze because they are largely unprecedented and because they involve the entire global human system. They are important to address because they threaten everyone around the world and future generations. Global catastrophic risks also pose some deep dilemmas. One dilemma occurs when actions to reduce global catastrophic risk could harm society in other ways, as in the case of geoengineering to reduce catastrophic climate change risk. Another dilemma occurs when reducing one global catastrophic risk could increase another, as in the case of nuclear power reducing climate change risk while increasing risks from nuclear weapons. The complex, interrelated nature of global catastrophic risk suggests a research agenda in which the full space of risks are assessed in an integrated fashion in consideration of the deep dilemmas and other challenges they pose. Such an agenda can help identify the best ways to manage these most extreme risks and keep human civilization safe.","2017-10-02","2022-01-30 04:55:19","2022-01-30 04:55:19","2019-12-16 02:43:45","","","","","","","Global Catastrophes","","","","","Social Science Research Network","Rochester, NY","en","","SSRN Scholarly Paper","","","papers.ssrn.com","","ZSCC: 0000010","","/Users/jacquesthibodeau/Zotero/storage/38X6ZDSW/papers.html","","MetaSafety; GCRI","risk; catastrophic risk; extreme risk; global catastrophic risk","","","","","","","","","","","","","","","","","","","ID 3046668","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UKKJMH24","journalArticle","2021","Baum, Seth; Owe, Andrea","From AI for People to AI for the World and the Universe","AI & Society","","","","","","2021-11-30","2022-01-30 04:55:19","2022-01-30 04:55:19","","3","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/QXWKBC9Z/Baum - From AI for People to AI for the World and the Uni.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DQF5DVFH","journalArticle","2018","Baum, Seth","Countering Superintelligence Misinformation","Information","","","","","","2018","2022-01-30 04:55:19","2022-01-30 04:55:19","","244","","10","9","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000014","","/Users/jacquesthibodeau/Zotero/storage/JG6I57DX/Baum - 2018 - Countering Superintelligence Misinformation.pdf; /Users/jacquesthibodeau/Zotero/storage/BDEHJSEN/Baum - 2018 - Countering Superintelligence Misinformation.pdf; /Users/jacquesthibodeau/Zotero/storage/W7VRFICJ/244.html","","MetaSafety; GCRI","artificial intelligence; superintelligence; misinformation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W6VMXGGZ","journalArticle","2021","Cihon, Peter; Schuett, Jonas; Baum, Seth D.","Corporate Governance of Artificial Intelligence in the Public Interest","Information","","2078-2489","10.3390/info12070275","https://www.mdpi.com/2078-2489/12/7/275","Corporations play a major role in artificial intelligence (AI) research, development, and deployment, with profound consequences for society. This paper surveys opportunities to improve how corporations govern their AI activities so as to better advance the public interest. The paper focuses on the roles of and opportunities for a wide range of actors inside the corporation—managers, workers, and investors—and outside the corporation—corporate partners and competitors, industry consortia, nonprofit organizations, the public, the media, and governments. Whereas prior work on multistakeholder AI governance has proposed dedicated institutions to bring together diverse actors and stakeholders, this paper explores the opportunities they have even in the absence of dedicated multistakeholder institutions. The paper illustrates these opportunities with many cases, including the participation of Google in the U.S. Department of Defense Project Maven; the publication of potentially harmful AI research by OpenAI, with input from the Partnership on AI; and the sale of facial recognition technology to law enforcement by corporations including Amazon, IBM, and Microsoft. These and other cases demonstrate the wide range of mechanisms to advance AI corporate governance in the public interest, especially when diverse actors work together.","2021-07-05","2022-01-30 04:55:19","2022-01-30 04:55:19","2021-10-31 19:21:54","275","","7","12","","Information","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/63ATIFMI/Cihon et al. - 2021 - Corporate Governance of Artificial Intelligence in.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8HC9X6CU","journalArticle","2015","Baum, Seth D.; Tonn, Bruce E.","Confronting future catastrophic threats to humanity","Futures","","00163287","10.1016/j.futures.2015.08.004","https://linkinghub.elsevier.com/retrieve/pii/S0016328715001135","","2015-09","2022-01-30 04:55:19","2022-01-30 04:55:19","2019-12-16 02:52:43","1-3","","","72","","Futures","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000012","","","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PJH97DIN","journalArticle","2020","Baum, Seth D.","Artificial Interdisciplinarity: Artificial Intelligence for Research on Complex Societal Problems","Philosophy & Technology","","2210-5433, 2210-5441","10.1007/s13347-020-00416-5","http://link.springer.com/10.1007/s13347-020-00416-5","This paper considers the question: In what ways can artificial intelligence assist with interdisciplinary research for addressing complex societal problems and advancing the social good? Problems such as environmental protection, public health, and emerging technology governance do not fit neatly within traditional academic disciplines and therefore require an interdisciplinary approach. However, interdisciplinary research poses large cognitive challenges for human researchers that go beyond the substantial challenges of narrow disciplinary research. The challenges include epistemic divides between disciplines, the massive bodies of relevant literature, the peer review of work that integrates an eclectic mix of topics, and the transfer of interdisciplinary research insights from one problem to another. Artificial interdisciplinarity already helps with these challenges via search engines, recommendation engines, and automated content analysis. Future “strong artificial interdisciplinarity” based on human-level artificial general intelligence could excel at interdisciplinary research, but it may take a long time to develop and could pose major safety and ethical issues. Therefore, there is an important role for intermediate-term artificial interdisciplinarity systems that could make major contributions to addressing societal problems without the concerns associated with artificial general intelligence.","2020-07-16","2022-01-30 04:55:19","2022-01-30 04:55:19","2020-08-20 20:15:09","","","","","","Philos. Technol.","Artificial Interdisciplinarity","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/89NP8X9J/Baum - 2020 - Artificial Interdisciplinarity Artificial Intelli.pdf","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PVCGA76X","journalArticle","2021","Galaz, Victor; Centeno, Miguel A.; Callahan, Peter W.; Causevic, Amar; Patterson, Thayer; Brass, Irina; Baum, Seth; Farber, Darryl; Fischer, Joern; Garcia, David; McPhearson, Timon; Jimenez, Daniel; King, Brian; Larcey, Paul; Levy, Karen","Artificial intelligence, systemic risks, and sustainability","Technology in Society","","0160791X","10.1016/j.techsoc.2021.101741","https://linkinghub.elsevier.com/retrieve/pii/S0160791X21002165","","2021-11","2022-01-30 04:55:19","2022-01-30 04:55:19","2021-10-31 19:21:00","101741","","","67","","Technology in Society","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s1]  ACC: 4","","/Users/jacquesthibodeau/Zotero/storage/2HV437ZM/Galaz et al. - 2021 - Artificial intelligence, systemic risks, and susta.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TH67SMSJ","journalArticle","2021","Cihon, Peter; Kleinaltenkamp, Moritz J.; Schuett, Jonas; Baum, Seth D.","AI CERTIFICATION: Advancing Ethical Practice by Reducing Information Asymmetries","IEEE Transactions on Technology and Society","","2637-6415","10.1109/TTS.2021.3077595","https://ieeexplore.ieee.org/document/9427056/","","2021","2022-01-30 04:55:18","2022-01-30 04:55:18","2021-10-31 19:21:42","1-1","","","","","IEEE Trans. Technol. Soc.","AI CERTIFICATION","","","","","","","","","","","","DOI.org (Crossref)","","ZSCC: 0000006","","/Users/jacquesthibodeau/Zotero/storage/UE94D4B7/Cihon et al. - 2021 - AI CERTIFICATION Advancing Ethical Practice by Re.pdf","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WT78EJX5","report","2017","Baum, Seth","A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy","","","","","https://papers.ssrn.com/abstract=3070741","Artificial general intelligence (AGI) is AI that can reason across a wide range of domains. It has long been considered the “grand dream” or “holy grail” of AI. It also poses major issues of ethics, risk, and policy due to its potential to transform society: if AGI is built, it could either help solve the world’s problems or cause major catastrophe, possibly even human extinction. This paper presents the first-ever survey of active AGI R&D projects in terms of ethics, risk, and policy. A thorough search identifies 45 projects of diverse sizes, nationalities, ethical goals, and other attributes. Most projects are either academic or corporate. The academic projects tend to express goals of advancing knowledge and are less likely to be active on AGI safety issues. The corporate projects tend to express goals of benefiting humanity and are more likely to be active on safety. Most projects are based in the US, and almost all are in either the US or a US ally, including all of the larger projects. This geographic concentration could simplify policymaking, though most projects publish open-source code, enabling contributions from anywhere in the world. These and other findings of the survey offer an empirical basis for the study of AGI R&D and a guide for policy and other action.","2017-11-12","2022-01-30 04:55:18","2022-01-30 04:55:18","2019-12-16 02:43:48","","","","","","","","","","","","Social Science Research Network","Rochester, NY","en","","SSRN Scholarly Paper","","","papers.ssrn.com","","ZSCC: 0000059","","/Users/jacquesthibodeau/Zotero/storage/IQ679H9R/papers.html","","MetaSafety; GCRI","artificial intelligence; risk; ethics; policy","","","","","","","","","","","","","","","","","","","ID 3070741","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IS3J9WCB","journalArticle","2017","Barrett, Anthony M.; Baum, Seth D.","A model of pathways to artificial superintelligence catastrophe for risk and decision analysis","Journal of Experimental & Theoretical Artificial Intelligence","","0952-813X, 1362-3079","10.1080/0952813X.2016.1186228","https://www.tandfonline.com/doi/full/10.1080/0952813X.2016.1186228","","2017-03-04","2022-01-30 04:55:18","2022-01-30 04:55:18","2019-12-16 02:44:58","397-414","","2","29","","Journal of Experimental & Theoretical Artificial Intelligence","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000044","","/Users/jacquesthibodeau/Zotero/storage/JFIRRI9N/Barrett and Baum - 2017 - A model of pathways to artificial superintelligenc.pdf","","MetaSafety; GCRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KDQVKBDG","blogPost","2020","Wentworth, John","Demons in Imperfect Search","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search","One day, a gradient descent algorithm ball was happily rolling down a  high-dimensional surface hill. All it wanted was to roll as far down as possible. Unbeknownst to the ball, just off to the side was a steep drop-off - but there was a small bump between the ball and the drop-off. No matter; there was enough random noise on the ball that it would jump the bump sooner or later. But the ball was headed into unfriendly territory. As the ball rolled along, the bump became taller. The farther it rolled, the taller the bump grew, until no hope remained of finding the big drop anytime before the stars burned out. Then the road began to narrow, and to twist and turn, and to become flatter. Soon the ball rolled down only the slightest slope, with tall walls on both sides constraining its path. The ball had entered the territory of a demon, and now that demon was steering the ball according to its own nefarious ends. This wasn’t the first time the ball had entered the territory of a demon. In early times, the demons had just been bumps which happened to grow alongside the ball’s path, for a time - chance events, nothing more. But every now and then, two bumps in close proximity would push the ball in different directions. The ball would roll on, oblivious, and end up going in one direction or the other. Whichever bump had ""won"" would continue to steer the ball's trajectory - and so a selection process occurred. The ball tended to roll alongside bumps which more effectively controlled its trajectory - bumps which were taller, bumps which steered it away from competing bumps. And so, over time, bumps gave way to barriers, and barriers gave way to demons - twisty paths with high walls to keep the ball contained and avoid competing walls, slowing the ball's descent to a crawl, conserving its potential energy in case a sharp drop were needed to avoid a competitor's wall. The ball’s downhill progress slowed and slowed. Even though the rich, high-dimensional space was filled w","2020-02-11","2022-01-30 04:59:45","2022-01-30 04:59:45","2020-09-05 18:56:17","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/8XJTQQDK/demons-in-imperfect-search.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"967PUGCM","blogPost","2020","G Gordon Worley III","Deconfusing Human Values Research Agenda v1","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/k8F8TBzuZtLheJt47/deconfusing-human-values-research-agenda-v1","On Friday I attended the 2020 Foresight AGI Strategy Meeting. Eventually a report will come out summarizing some of what was talked about, but for now I want to focus on what I talked about in my session on deconfusing human values. For that session I wrote up some notes summarizing what I've been working on and thinking about. None of it is new, but it is newly condensed in one place and in convenient list form, and it provides a decent summary of the current state of my research agenda for building beneficial superintelligent AI; a version 1 of my agenda, if you will. Thus, I hope this will be helpful in making it a bit clearer what it is I'm working on, why I'm working on it, and what direction my thinking is moving in. As always, if you're interesting in collaborating on things, whether that be discussing ideas or something more, please reach out. PROBLEM OVERVIEW  * I think we're confused about what we really mean when we talk about human    values.  * This is a problem because:  * building aligned AI likely requires a mathematically precise understanding of    the structure of human values, though not necessarily the content of human    values;we can't trust AI to discover that structure for us because we would    need to understand it enough to verify the result, and I think we're so    confused about what human values are we couldn't do that without high risk of    error.  * What are values?  * We don't have an agreed upon precise definition, but loosely it's ""stuff    people care about"". * When I talk about ""values"" I mean the cluster we       sometimes also point at with words like value, preference, affinity,       taste, aesthetic, intention, and axiology.        Importantly, what people care about is used to make decisions, and this has    had implications for existing approaches to understanding values.  * Much research on values tries to understand the content of human values or    why humans value what they value, but not what the structure of human","2020-03-23","2022-01-30 04:59:45","2022-01-30 04:59:45","2020-09-05 18:34:55","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/CEWM72I3/deconfusing-human-values-research-agenda-v1.html","","TechSafety; AmbiguosSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2JPX93V","blogPost","2019","Pace, Ben","Debate on Instrumental Convergence between LeCun, Russell, Bengio, Zador, and More","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/WxW6Gc6f2z3mzmqKs/debate-on-instrumental-convergence-between-lecun-russell","An actual debate about instrumental convergence, in a public space! Major respect to all involved, especially Yoshua Bengio for great facilitation. For posterity (i.e. having a good historical archive) and further discussion, I've reproduced the conversation here. I'm happy to make edits at the request of anyone in the discussion who is quoted below. I've improved formatting for clarity and fixed some typos. For people who are not researchers in this area who wish to comment, see the public version of this post here. For people who do work on the relevant areas, please sign up in the top right. It will take a day or so to confirm membership. ORIGINAL POST Yann LeCun: ""don't fear the Terminator"", a short opinion piece by Tony Zador and me that was just published in Scientific American. ""We dramatically overestimate the threat of an accidental AI takeover, because we tend to conflate intelligence with the drive to achieve dominance. [...] But intelligence per se does not generate the drive for domination, any more than horns do."" https://blogs.scientificamerican.com/observations/dont-fear-the-terminator/ COMMENT THREAD #1 Elliot Olds: Yann, the smart people who are very worried about AI seeking power and ensuring its own survival believe it's a big risk because power and survival are instrumental goals for almost any ultimate goal. If you give a generally intelligent AI the goal to make as much money in the stock market as possible, it will resist being shut down because that would interfere with tis goal. It would try to become more powerful because then it could make money more effectively. This is the natural consequence of giving a smart agent a goal, unless we do something special to counteract this. You've often written about how we shouldn't be so worried about AI, but I've never seen you address this point directly. Stuart Russell: It is trivial to construct a toy MDP in which the agent's only reward comes from fetching the coffee. If, in that MDP, the","2019","2022-01-30 04:59:45","2022-01-30 04:59:45","2020-12-14 23:31:53","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/NZ6R6VPP/debate-on-instrumental-convergence-between-lecun-russell.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XD4II9CD","blogPost","2021","Davidson, Tom","Could Advanced AI Drive Explosive Economic Growth?","Open Philanthropy","","","","https://www.openphilanthropy.org/could-advanced-ai-drive-explosive-economic-growth","body ol ul li::before { display: unset !important; } #toc .toc-list ol li:nth-child(10) ol { /* display: none !important; */ } .toc-level-3, .toc-level-4, .toc-level-5, .toc-level-6 {display: none;} .footnote p { line-height: unset !important; }  MathJax.Hub.Config({  extensions: [""tex2jax.js""],","2021-04-08","2022-01-30 04:59:45","2022-01-30 04:59:45","2021-11-14 18:49:40","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C95SMCT7","conferencePaper","2019","Sarafian, Elad; Tamar, Aviv; Kraus, Sarit","Constrained Policy Improvement for Safe and Efficient Reinforcement Learning","Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence","","","","http://arxiv.org/abs/1805.07805","We propose a policy improvement algorithm for Reinforcement Learning (RL) which is called Rerouted Behavior Improvement (RBI). RBI is designed to take into account the evaluation errors of the Q-function. Such errors are common in RL when learning the $Q$-value from finite past experience data. Greedy policies or even constrained policy optimization algorithms which ignore these errors may suffer from an improvement penalty (i.e. a negative policy improvement). To minimize the improvement penalty, the RBI idea is to attenuate rapid policy changes of low probability actions which were less frequently sampled. This approach is shown to avoid catastrophic performance degradation and reduce regret when learning from a batch of past experience. Through a two-armed bandit with Gaussian distributed rewards example, we show that it also increases data efficiency when the optimal action has a high variance. We evaluate RBI in two tasks in the Atari Learning Environment: (1) learning from observations of multiple behavior policies and (2) iterative RL. Our results demonstrate the advantage of RBI over greedy policies and other constrained policy optimization algorithms as a safe learning approach and as a general data efficient learning algorithm. An anonymous Github repository of our RBI implementation is found at https://github.com/eladsar/rbi.","2019-07-10","2022-01-30 04:59:45","2022-01-30 04:59:45","2020-11-14 00:52:37","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 0  arXiv: 1805.07805","","/Users/jacquesthibodeau/Zotero/storage/DC967S4Q/Sarafian et al. - 2019 - Constrained Policy Improvement for Safe and Effici.pdf; /Users/jacquesthibodeau/Zotero/storage/KNEPSHIS/1805.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Twenty-Ninth International Joint Conference on Artificial Intelligence","","","","","","","","","","","","","","",""
"AIE4CQPT","journalArticle","2017","Jilk, David J.","Conceptual-Linguistic Superintelligence","Informatica","","1854-3871","","http://www.informatica.si/index.php/informatica/article/view/1875","We argue that artificial intelligence capable of sustaining an uncontrolled intelligence explosion must have a conceptual-linguistic faculty with substantial functional similarity to the human faculty. We then argue for three subsidiary claims: first, that detecting the presence of such a faculty will be an important indicator of imminent superintelligence; second, that such a superintelligence will, in creating further increases in intelligence, both face and consider the same sorts of existential risks that humans face today; third, that such a superintelligence is likely to assess and question its own values, purposes, and drives.","2017-12-27","2022-01-30 04:59:45","2022-01-30 04:59:45","2020-12-13 23:11:36","","","4","41","","","","","","","","","","en","I assign to  Informatica ,  An International Journal of Computing and Informatics  (""Journal"") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript (""Paper"") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika","","","","www.informatica.si","","ZSCC: 0000006  Number: 4","","/Users/jacquesthibodeau/Zotero/storage/QRMR33VP/Jilk - 2017 - Conceptual-Linguistic Superintelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/KJZ6SVUS/1875.html; /Users/jacquesthibodeau/Zotero/storage/DQWGAKQA/1875.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S4G7N475","bookSection","2011","Ring, Mark; Orseau, Laurent","Delusion, Survival, and Intelligent Agents","Artificial General Intelligence","978-3-642-22886-5 978-3-642-22887-2","","","http://link.springer.com/10.1007/978-3-642-22887-2_2","This paper considers the consequences of endowing an intelligent agent with the ability to modify its own code. The intelligent agent is patterned closely after AIXI with these speciﬁc assumptions: 1) The agent is allowed to arbitrarily modify its own inputs if it so chooses; 2) The agent’s code is a part of the environment and may be read and written by the environment. The ﬁrst of these we call the “delusion box”; the second we call “mortality”. Within this framework, we discuss and compare four very diﬀerent kinds of agents, speciﬁcally: reinforcementlearning, goal-seeking, prediction-seeking, and knowledge-seeking agents. Our main results are that: 1) The reinforcement-learning agent under reasonable circumstances behaves exactly like an agent whose sole task is to survive (to preserve the integrity of its code); and 2) Only the knowledge-seeking agent behaves completely as expected.","2011","2022-01-30 04:59:45","2022-01-30 04:59:45","2020-11-21 17:39:39","11-20","","","6830","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s1]  ACC: 77  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-22887-2_2","","/Users/jacquesthibodeau/Zotero/storage/FRZ9DPD5/Ring and Orseau - 2011 - Delusion, Survival, and Intelligent Agents.pdf","","TechSafety; Other-org","","Schmidhuber, Jürgen; Thórisson, Kristinn R.; Looks, Moshe","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BH567DIV","journalArticle","2014","Halpern, Joseph Y.; Pass, Rafael; Seeman, Lior","Decision Theory with Resource-Bounded Agents","Topics in Cognitive Science","","17568757","10.1111/tops.12088","http://doi.wiley.com/10.1111/tops.12088","","2014-04","2022-01-30 04:59:45","2022-01-30 04:59:45","2020-11-22 01:47:43","245-257","","2","6","","Top Cogn Sci","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000026","","/Users/jacquesthibodeau/Zotero/storage/C5BB6KK7/Halpern et al. - 2014 - Decision Theory with Resource-Bounded Agents.pdf","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GKMKG435","report","2020","Jessica Cussins Newman","Decision Points in AI Governance","","","","","https://cltc.berkeley.edu/wp-content/uploads/2020/05/Decision_Points_AI_Governance.pdf","","2020","2022-01-30 04:59:45","2022-01-30 04:59:45","","58","","","","","","","","","","","Center for Long-Term Cybersecurity","","","","","","","","","ZSCC: 0000006","","","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"93TJ5D9A","bookSection","2017","Durán, Juan M.","Computer Simulations as a Technological Singularity in the Empirical Sciences","The Technological Singularity: Managing the Journey","978-3-662-54033-6","","","https://doi.org/10.1007/978-3-662-54033-6_9","SummaryIn this paper, I discuss the conditions necessary for computer simulations to qualify as a technological singularity in the empirical sciences. A technological singularity encompasses two claims: (a) the enhancement of human cognitive capacities by the computer, and (b) their displacement from the center of the production of knowledge. For computer simulations to be a technological singularity, then, they must fulfill points (a) and (b) above. Although point (a) is relatively unproblematic, point (b) needs further analysis. In particular, in order to show that humans could be displaced from the center of the production of knowledge, it is necessary to establish the reliability of computer simulations. That is, I need to show that computer simulations are reliable processes that render, most of the time, valid results. To be a reliable process, in turn, means that simulations accurately represent the target system and carry out error-free computations. I analyze verification and validation methods as the grounds for such representation accuracy and error-free computations. Since the aim is to entrench computer simulations as a technological singularity, the entire analysis must be careful to keep human agents out of the picture.","2017","2022-01-30 04:59:44","2022-01-30 04:59:44","2020-11-24 02:59:54","167-179","","","","","","","The Frontiers Collection","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","ZSCC: NoCitationData[s1]  ACC: 4  DOI: 10.1007/978-3-662-54033-6_9","","","","MetaSafety; Other-org","Computer Simulation; Empirical Science; Epistemic Justification; Reliable Process; Target System","Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8PSVNP3E","manuscript","2018","Hwang, Tim","Computational Power and the Social Impact of Artificial Intelligence","","","","","http://arxiv.org/abs/1803.08971","Machine learning is a computational process. To that end, it is inextricably tied to computational power - the tangible material of chips and semiconductors that the algorithms of machine intelligence operate on. Most obviously, computational power and computing architectures shape the speed of training and inference in machine learning, and therefore influence the rate of progress in the technology. But, these relationships are more nuanced than that: hardware shapes the methods used by researchers and engineers in the design and development of machine learning models. Characteristics such as the power consumption of chips also define where and how machine learning can be used in the real world. Despite this, many analyses of the social impact of the current wave of progress in AI have not substantively brought the dimension of hardware into their accounts. While a common trope in both the popular press and scholarly literature is to highlight the massive increase in computational power that has enabled the recent breakthroughs in machine learning, the analysis frequently goes no further than this observation around magnitude. This paper aims to dig more deeply into the relationship between computational power and the development of machine learning. Specifically, it examines how changes in computing architectures, machine learning methodologies, and supply chains might influence the future of AI. In doing so, it seeks to trace a set of specific relationships between this underlying hardware layer and the broader social impacts and risks around AI.","2018-03-23","2022-01-30 04:59:44","2022-01-30 04:59:44","2020-11-14 00:34:42","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000040  arXiv: 1803.08971","","/Users/jacquesthibodeau/Zotero/storage/C9KNXFPA/Hwang - 2018 - Computational Power and the Social Impact of Artif.pdf; /Users/jacquesthibodeau/Zotero/storage/6H9CEWGV/1803.html","","MetaSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V49WBQ7S","conferencePaper","2020","Brown, Noam; Bakhtin, Anton; Lerer, Adam; Gong, Qucheng","Combining Deep Reinforcement Learning and Search for Imperfect-Information Games","34th Conference on Neural Information Processing Systems (NeurIPS 2020)","","","","http://arxiv.org/abs/2007.13544","The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of a successes in single-agent settings and perfect-information games, best exemplified by the success of AlphaZero. However, algorithms of this form have been unable to cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search for imperfect-information games. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results show ReBeL leads to low exploitability in benchmark imperfect-information games and achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI. We also prove that ReBeL converges to a Nash equilibrium in two-player zero-sum games in tabular settings.","2020-07-27","2022-01-30 04:59:44","2022-01-30 04:59:44","2020-09-07 18:53:30","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000026  arXiv: 2007.13544","","/Users/jacquesthibodeau/Zotero/storage/QXI58ND2/Brown et al. - 2020 - Combining Deep Reinforcement Learning and Search f.pdf; /Users/jacquesthibodeau/Zotero/storage/C8ZNKWTE/2007.html","","TechSafety; Other-org","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","34th Conference on Neural Information Processing Systems (NeurIPS 2020)","","","","","","","","","","","","","","",""
"AMJT8QFM","manuscript","2020","Chatterjee, Satrajit","Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization","","","","","http://arxiv.org/abs/2002.10657","An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.","2020-02-24","2022-01-30 04:59:44","2022-01-30 04:59:44","2020-09-05 18:39:31","","","","","","","Coherent Gradients","","","","","","","","","","","","arXiv.org","","ZSCC: 0000018  arXiv: 2002.10657","","/Users/jacquesthibodeau/Zotero/storage/J2MAP4DT/Chatterjee - 2020 - Coherent Gradients An Approach to Understanding G.pdf; /Users/jacquesthibodeau/Zotero/storage/FKT247Z3/2002.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8826ECUU","conferencePaper","2018","Raghunathan, Aditi; Steinhardt, Jacob; Liang, Percy","Certified Defenses against Adversarial Examples","","","","","http://arxiv.org/abs/1801.09344","While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \epsilon = 0.1 can cause more than 35% test error.","2018","2022-01-30 04:59:44","2022-01-30 04:59:44","2020-12-13 23:31:22","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000653  JCC: 455 arXiv: 1801.09344","","/Users/jacquesthibodeau/Zotero/storage/INV72AUS/Raghunathan et al. - 2020 - Certified Defenses against Adversarial Examples.pdf; /Users/jacquesthibodeau/Zotero/storage/89JZRJGM/1801.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Learning Representations 2018","","","","","","","","","","","","","","",""
"6G5389AS","journalArticle","2006","McLaren, B.M.","Computational Models of Ethical Reasoning: Challenges, Initial Steps, and Future Directions","IEEE Intelligent Systems","","1541-1672","10.1109/MIS.2006.67","http://ieeexplore.ieee.org/document/1667950/","","2006-07","2022-01-30 04:59:44","2022-01-30 04:59:44","2020-11-22 02:23:04","29-37","","4","21","","IEEE Intell. Syst.","Computational Models of Ethical Reasoning","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000103","","/Users/jacquesthibodeau/Zotero/storage/C22P6XNW/McLaren - 2006 - Computational Models of Ethical Reasoning Challen.pdf","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39KNIC5R","journalArticle","2020","Turchin, Alexey; Denkenberger, David","Classification of global catastrophic risks connected with artificial intelligence","AI & Society","","0951-5666","10.1007/s00146-018-0845-5","https://link.springer.com/epdf/10.1007/s00146-018-0845-5","A classification of the global catastrophic risks of AI is presented, along with a comprehensive list of previously identified risks. This classification allows the identification of several new risks. We show that at each level of AI’s intelligence power, separate types of possible catastrophes dominate. Our classification demonstrates that the field of AI risks is diverse, and includes many scenarios beyond the commonly discussed cases of a paperclip maximizer or robot-caused unemployment. Global catastrophic failure could happen at various levels of AI development, namely, (1) before it starts self-improvement, (2) during its takeoff, when it uses various instruments to escape its initial confinement, or (3) after it successfully takes over the world and starts to implement its goal system, which could be plainly unaligned, or feature-flawed friendliness. AI could also halt at later stages of its development either due to technical glitches or ontological problems. Overall, we identified around several dozen scenarios of AI-driven global catastrophe. The extent of this list illustrates that there is no one simple solution to the problem of AI safety, and that AI safety theory is complex and must be customized for each AI development level.","2020","2022-01-30 04:59:44","2022-01-30 04:59:44","2020-11-14 00:32:51","","","1","35","","","","","","","","","","en","","","","","www.readcube.com","","ZSCC: 0000071","","/Users/jacquesthibodeau/Zotero/storage/8EJZTBNT/Turchin and Denkenberger - 2020 - Classification of global catastrophic risks connec.pdf","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"59IA3QC5","conferencePaper","2021","Lindner, David; Matoba, Kyle; Meulemans, Alexander","Challenges for Using Impact Regularizers to Avoid Negative Side Effects","arXiv:2101.12509 [cs]","","","","http://arxiv.org/abs/2101.12509","Designing reward functions for reinforcement learning is difficult: besides specifying which behavior is rewarded for a task, the reward also has to discourage undesired outcomes. Misspecified reward functions can lead to unintended negative side effects, and overall unsafe behavior. To overcome this problem, recent work proposed to augment the specified reward function with an impact regularizer that discourages behavior that has a big impact on the environment. Although initial results with impact regularizers seem promising in mitigating some types of side effects, important challenges remain. In this paper, we examine the main current challenges of impact regularizers and relate them to fundamental design decisions. We discuss in detail which challenges recent approaches address and which remain unsolved. Finally, we explore promising directions to overcome the unsolved challenges in preventing negative side effects with impact regularizers.","2021-02-23","2022-01-30 04:59:44","2022-01-30 04:59:44","2021-11-13 22:39:06","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2101.12509","","/Users/jacquesthibodeau/Zotero/storage/UPVI5Q45/Lindner et al. - 2021 - Challenges for Using Impact Regularizers to Avoid .pdf; /Users/jacquesthibodeau/Zotero/storage/SMIIU3CE/2101.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI 2021","","","","","","","","","","","","","","",""
"ADDSIC2J","journalArticle","2019","Cohen, Jeremy M.; Rosenfeld, Elan; Kolter, J. Zico","Certified Adversarial Robustness via Randomized Smoothing","arXiv:1902.02918 [cs, stat]","","","","http://arxiv.org/abs/1902.02918","We show how to turn any classiﬁer that classiﬁes well under Gaussian noise into a new classiﬁer that is certiﬁably robust to adversarial perturbations under the 2 norm. This “randomized smoothing” technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in 2 norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classiﬁer with e.g. a certiﬁed top-1 accuracy of 49% under adversarial perturbations with 2 norm less than 0.5 (=127/255). No certiﬁed defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certiﬁed 2 robustness are viable, smoothing delivers higher certiﬁed accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classiﬁcation. Code and models are available at http: //github.com/locuslab/smoothing.","2019-06-15","2022-01-30 04:59:44","2022-01-30 04:59:44","2020-12-22 23:38:26","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000716  arXiv: 1902.02918","","/Users/jacquesthibodeau/Zotero/storage/D7VWNG9N/Cohen et al. - 2019 - Certified Adversarial Robustness via Randomized Sm.pdf","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IK622CNH","journalArticle","2012","Hutter, Marcus","Can Intelligence Explode?","Journal of Consciousness Studies","","","","http://arxiv.org/abs/1202.6177","The technological singularity refers to a hypothetical scenario in which technological advances virtually explode. The most popular scenario is the creation of super-intelligent algorithms that recursively create ever higher intelligences. It took many decades for these ideas to spread from science fiction to popular science magazines and finally to attract the attention of serious philosophers. David Chalmers' (JCS 2010) article is the first comprehensive philosophical analysis of the singularity in a respected philosophy journal. The motivation of my article is to augment Chalmers' and to discuss some issues not addressed by him, in particular what it could mean for intelligence to explode. In this course, I will (have to) provide a more careful treatment of what intelligence actually is, separate speed from intelligence explosion, compare what super-intelligent participants and classical human observers might experience and do, discuss immediate implications for the diversity and value of life, consider possible bounds on intelligence, and contemplate intelligences right at the singularity.","2012-02-28","2022-01-30 04:59:43","2022-01-30 04:59:43","2020-11-22 04:16:06","","","","19","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000041  arXiv: 1202.6177","","/Users/jacquesthibodeau/Zotero/storage/PGN32NHD/Hutter - 2012 - Can Intelligence Explode.pdf; /Users/jacquesthibodeau/Zotero/storage/GXCCDRVV/1202.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence; Physics - Physics and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U8P2DUZ2","conferencePaper","2020","Everett, Michael; Lutjens, Bjorn; How, Jonathan P.","Certified Adversarial Robustness for Deep Reinforcement Learning","3rd Conference on Robot Learning (CoRL 2019),","","","","http://arxiv.org/abs/2004.06496","Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst-case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task. This work extends one of our prior works with new performance guarantees, extensions to other RL algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.","2020-08-21","2022-01-30 04:59:43","2022-01-30 04:59:43","2020-08-31 17:29:23","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000022  arXiv: 2004.06496","","/Users/jacquesthibodeau/Zotero/storage/GJDIZKHK/Everett et al. - 2020 - Certified Adversarial Robustness for Deep Reinforc.pdf; /Users/jacquesthibodeau/Zotero/storage/ZTVVXNMQ/2004.html","","TechSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","3rd Conference on Robot Learning (CoRL 2019),","","","","","","","","","","","","","","",""
"9334F5K2","journalArticle","2020","Fischer, Ian; Alemi, Alexander A.","CEB Improves Model Robustness","Entropy","","","","http://arxiv.org/abs/2002.05380","We demonstrate that the Conditional Entropy Bottleneck (CEB) can improve model robustness. CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on CIFAR-10, as well as the ImageNet-C Common Corruptions Benchmark, ImageNet-A, and PGD attacks.","2020-02-13","2022-01-30 04:59:43","2022-01-30 04:59:43","2020-09-05 18:46:13","","","10","22","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000015  arXiv: 2002.05380","","/Users/jacquesthibodeau/Zotero/storage/JSUK3W5B/Fischer and Alemi - 2020 - CEB Improves Model Robustness.pdf; /Users/jacquesthibodeau/Zotero/storage/HZ5HWPQR/2002.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZT7JGH6T","bookSection","2017","Koepsell, David","Can the Singularity Be Patented? (And Other IP Conundrums for Converging Technologies)","The Technological Singularity: Managing the Journey","978-3-662-54033-6","","","https://doi.org/10.1007/978-3-662-54033-6_10","SummaryAssuming that the singularity is eventually realized, some of the legal institutions that we take for granted, specifically those relating to “intellectual property” (IP – namely, copyrights and patents), may pose some problems. IP law concerns the ownership of expressions of ideas, and not ideas themselves. Given the nature and trajectory of converging technologies, IP laws as they currently exist may impede the development of such technologies. Examples of “patent thickets” that appear to impede other rapidly evolving technologies already abound (as in the smartphone arena). Patents and copyrights may pose even more intriguing problems once the singularity is achieved because our notions of who may own what will likely radically change. Will artificial intelligences, for example, compete with us over rights to create, and will we be legally or morally precluded from ownership rights in technologies that make such agents function? Before the singularity arrives, we would do well to work through some of these legal conundrums raised and discussed below.","2017","2022-01-30 04:59:43","2022-01-30 04:59:43","2020-11-24 02:59:55","181-191","","","","","","Can the Singularity Be Patented?","The Frontiers Collection","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","ZSCC: NoCitationData[s1]  ACC: 0  DOI: 10.1007/978-3-662-54033-6_10","","","","MetaSafety; Other-org","Artificial Agent; Intellectual Property; Natural Phenomenon; Patent Holder; Patent Office","Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UQRWRPSQ","manuscript","2018","Trazzi, Michaël; Yampolskiy, Roman V.","Building Safer AGI by introducing Artificial Stupidity","","","","","http://arxiv.org/abs/1808.03644","Artificial Intelligence (AI) achieved super-human performance in a broad variety of domains. We say that an AI is made Artificially Stupid on a task when some limitations are deliberately introduced to match a human's ability to do the task. An Artificial General Intelligence (AGI) can be made safer by limiting its computing power and memory, or by introducing Artificial Stupidity on certain tasks. We survey human intellectual limits and give recommendations for which limits to implement in order to build a safe AGI.","2018-08-10","2022-01-30 04:59:43","2022-01-30 04:59:43","2020-11-14 00:55:32","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000026  arXiv: 1808.03644","","/Users/jacquesthibodeau/Zotero/storage/KH2RT4UD/Trazzi and Yampolskiy - 2018 - Building Safer AGI by introducing Artificial Stupi.pdf; /Users/jacquesthibodeau/Zotero/storage/2AIX67AX/1808.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IHTBF4TH","conferencePaper","2018","Yu, Han; Shen, Zhiqi; Miao, Chunyan; Leung, Cyril; Lesser, Victor R.; Yang, Qiang","Building Ethics into Artificial Intelligence","Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)","","","","http://arxiv.org/abs/1812.02953","As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.","2018-12-07","2022-01-30 04:59:37","2022-01-30 04:59:37","2020-11-14 00:53:39","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000128  arXiv: 1812.02953","","/Users/jacquesthibodeau/Zotero/storage/P4HD9IST/Yu et al. - 2018 - Building Ethics into Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/SZNMZP8A/1812.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)","","","","","","","","","","","","","","",""
"G7KMKWHX","conferencePaper","2019","Fisac, Jaime F.; Lugovoy, Neil F.; Rubies-Royo, Vicenc; Ghosh, Shromona; Tomlin, Claire J.","Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning","2019 International Conference on Robotics and Automation (ICRA)","978-1-5386-6027-0","","10.1109/ICRA.2019.8794107","https://ieeexplore.ieee.org/document/8794107/","Safety analysis is a necessary component in the design and deployment of autonomous systems. Techniques from robust optimal control theory, such as Hamilton-Jacobi reachability analysis, allow a rigorous formalization of safety as guaranteed constraint satisfaction. Unfortunately, the computational complexity of these tools for general dynamical systems scales poorly with state dimension, making existing tools impractical beyond small problems. Modern reinforcement learning methods have shown promising ability to ﬁnd approximate yet proﬁcient solutions to optimal control problems in complex and high-dimensional systems, however their formulation is restricted to problems with an additive payoff (reward) over time, unsuitable for reasoning about safety. In recent work, we proved that the problem of maximizing the minimum payoff over time, central to safety analysis, can be time-discounted to induce a contraction mapping. Here, we introduce a novel, timediscounted Safety Bellman Equation that renders reinforcement learning techniques amenable to quantitative safety analysis, enabling them to approximate the safe set and optimal safety policy. This opens a new avenue of research connecting controltheoretic safety analysis and the reinforcement learning domain. We demonstrate our formulation on a variety of simulated robotics tasks and reinforcement learning schemes, validating our results against analytic and numerical solutions when these can be obtained, and showing scalability to previously intractable problems of up to 18 state dimensions by exploiting state-of-the-art deep reinforcement learning algorithms.","2019-05","2022-01-30 04:59:37","2022-01-30 04:59:37","2020-11-14 01:15:15","8550-8556","","","","","","","","","","","IEEE","Montreal, QC, Canada","en","","","","","DOI.org (Crossref)","","ZSCC: 0000033","","/Users/jacquesthibodeau/Zotero/storage/XFP8J2JX/Fisac et al. - 2019 - Bridging Hamilton-Jacobi Safety Analysis and Reinf.pdf","","TechSafety; AmbiguosSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 International Conference on Robotics and Automation (ICRA)","","","","","","","","","","","","","","",""
"CWGBI3VB","blogPost","2018","Anonymous","Bias in AI: How we Build Fair AI Systems and Less-Biased Humans","THINKPolicy Blog","","","","https://www.ibm.com/blogs/policy/bias-in-ai/","Without a process to guide the responsible development of trustworthy AI, our systems won’t benefit society — in fact, AI systems could exacerbate the negative consequences of unconscious bias.","2018-02-01","2022-01-30 04:59:37","2022-01-30 04:59:37","2020-12-13 23:10:15","","","","","","","Bias in AI","","","","","","","en-US","© Copyright IBM Corp. 2020","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/M9INUUC2/bias-in-ai.html","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7W6QE66X","conferencePaper","2018","Rossi, Francesca; Mattei, Nicholas","Building Ethically Bounded AI","Proceedings of the AAAI Conference on Artificial Intelligence","","","","http://arxiv.org/abs/1812.03980","The more AI agents are deployed in scenarios with possibly unexpected situations, the more they need to be flexible, adaptive, and creative in achieving the goal we have given them. Thus, a certain level of freedom to choose the best path to the goal is inherent in making AI robust and flexible enough. At the same time, however, the pervasive deployment of AI in our life, whether AI is autonomous or collaborating with humans, raises several ethical challenges. AI agents should be aware and follow appropriate ethical principles and should thus exhibit properties such as fairness or other virtues. These ethical principles should define the boundaries of AI's freedom and creativity. However, it is still a challenge to understand how to specify and reason with ethical boundaries in AI agents and how to combine them appropriately with subjective preferences and goal specifications. Some initial attempts employ either a data-driven example-based approach for both, or a symbolic rule-based approach for both. We envision a modular approach where any AI technique can be used for any of these essential ingredients in decision making or decision support systems, paired with a contextual approach to define their combination and relative weight. In a world where neither humans nor AI systems work in isolation, but are tightly interconnected, e.g., the Internet of Things, we also envision a compositional approach to building ethically bounded AI, where the ethical properties of each component can be fruitfully exploited to derive those of the overall system. In this paper we define and motivate the notion of ethically-bounded AI, we describe two concrete examples, and we outline some outstanding challenges.","2018-12-10","2022-01-30 04:59:37","2022-01-30 04:59:37","2020-11-14 00:31:20","","","","33","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s2]  ACC: 36  arXiv: 1812.03980","","/Users/jacquesthibodeau/Zotero/storage/FKZA6ZVE/Rossi and Mattei - 2018 - Building Ethically Bounded AI.pdf; /Users/jacquesthibodeau/Zotero/storage/TKRZ5565/1812.html","","TechSafety; Other-org","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI Conference on Artificial Intelligence","","","","","","","","","","","","","","",""
"A8W6HA68","blogPost","2021","G Gordon Worley III","Bootstrapped Alignment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/teCsd4Aqg9KDxkaC9/bootstrapped-alignment","NB: I doubt any of this is very original. In fact, it's probably right there in the original Friendly AI writings and I've just forgotten where. Nonetheless, I think this is something worth exploring lest we lose sight of it. Consider the following argument:  1. Optimization unavoidably leads to Goodharting (as I like to say, Goodhart is     robust) * This happens so long as we optimize (make choices) based on an        observation, which we must do because that's just how the physics work.      * We can at best make Goodhart effects happen slower, say by quantilization         or satisficing.            2. Attempts to build aligned AI that rely on optimizing for alignment will     eventually fail to become or remain aligned due to Goodhart effects under     sufficient optimization pressure.  3. Thus the only way to build aligned AI that doesn't fail to become and stay     aligned is to not rely on optimization to achieve alignment. This means that, if you buy this argument, huge swaths of AI design space is off limits for building aligned AI, and means many proposals are, by this argument, doomed to fail. Some examples of such doomed approaches:  * HCH  * debate  * IRL/CIRL So what options are left?  * Don't build AI * The AI you don't build is vacuously aligned.          * Friendly AI * AI that is aligned with humans right from the start because it       was programmed to work that way.     * (Yes I know ""Friendly AI"" is an antiquated term, but I don't       know a better one to distinguish the idea of building AI that's aligned       because it's programmed that way from other ways we might build aligned       AI.)          * Bootstrapped alignment * Build AI that is aligned via optimization that is       not powerful enough or optimized (Goodharted) hard enough to cause       existential catastrophe. Use this ""weakly"" aligned AI to build Friendly       AI.         Not building AI is probably not a realistic option unless industrial civilization collapses.","2021-02-27","2022-01-30 04:59:37","2022-01-30 04:59:37","2021-11-13 23:01:05","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IWME7BT6","conferencePaper","2020","Chen, Ting; Kornblith, Simon; Swersky, Kevin; Norouzi, Mohammad; Hinton, Geoffrey","Big Self-Supervised Models are Strong Semi-Supervised Learners","34th Conference on Neural Information Processing Systems (NeurIPS 2020),","","","","http://arxiv.org/abs/2006.10029","One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.","2020-06-17","2022-01-30 04:59:37","2022-01-30 04:59:37","2020-08-31 18:02:23","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000505  arXiv: 2006.10029","","/Users/jacquesthibodeau/Zotero/storage/W93U7VHU/Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020","","","","","","","","","","","","","","",""
"9SVVUE8N","conferencePaper","2019","Hendrycks, Dan; Dietterich, Thomas","Benchmarking Neural Network Robustness to Common Corruptions and Perturbations","","","","","http://arxiv.org/abs/1903.12261","In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.","2019-03-28","2022-01-30 04:59:36","2022-01-30 04:59:36","2020-12-22 23:29:59","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000898  arXiv: 1903.12261","","/Users/jacquesthibodeau/Zotero/storage/6IRJBZUB/Hendrycks and Dietterich - 2019 - Benchmarking Neural Network Robustness to Common C.pdf; /Users/jacquesthibodeau/Zotero/storage/XSJQR645/1903.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2019","","","","","","","","","","","","","","",""
"QWU6BGHU","manuscript","2020","Saisubramanian, Sandhya; Zilberstein, Shlomo; Kamar, Ece","Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems","","","","","http://arxiv.org/abs/2008.12146","Autonomous agents acting in the real-world often operate based on models that ignore certain aspects of the environment. The incompleteness of any given model---handcrafted or machine acquired---is inevitable due to practical limitations of any modeling technique for complex real-world settings. Due to the limited fidelity of its model, an agent's actions may have unexpected, undesirable consequences during execution. Learning to recognize and avoid such negative side effects of the agent's actions is critical to improving the safety and reliability of autonomous systems. This emerging research topic is attracting increased attention due to the increased deployment of AI systems and their broad societal impacts. This article provides a comprehensive overview of different forms of negative side effects and the recent research efforts to address them. We identify key characteristics of negative side effects, highlight the challenges in avoiding negative side effects, and discuss recently developed approaches, contrasting their benefits and limitations. We conclude with a discussion of open questions and suggestions for future research directions.","2020-08-28","2022-01-30 04:59:36","2022-01-30 04:59:36","2020-11-14 00:57:51","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000006  arXiv: 2008.12146","","/Users/jacquesthibodeau/Zotero/storage/F2EV268Z/Saisubramanian et al. - 2020 - Avoiding Negative Side Effects due to Incomplete K.pdf; /Users/jacquesthibodeau/Zotero/storage/BHMQUM67/2008.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQJW66U7","bookSection","2006","Spears, Diana F.","Assuring the Behavior of Adaptive Agents","Agent Technology from a Formal Perspective","978-1-85233-947-0","","","http://link.springer.com/10.1007/1-84628-271-3_8","","2006","2022-01-30 04:59:36","2022-01-30 04:59:36","2020-11-22 01:48:09","227-257","","","","","","","","","","","Springer-Verlag","London","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 14  Series Title: NASA Monographs in Systems and Software Engineering DOI: 10.1007/1-84628-271-3_8","","","","TechSafety; Other-org","","Rouff, Christopher A.; Hinchey, Michael; Rash, James; Truszkowski, Walter; Gordon-Spears, Diana","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UASN6CZQ","journalArticle","2017","Batin, Mikhail; Turchin, Alexey; Sergey, Markov; Zhila, Alisa; Denkenberger, David","Artificial Intelligence in Life Extension: from Deep Learning to Superintelligence","Informatica","","1854-3871","","http://www.informatica.si/index.php/informatica/article/view/1797","In this paper we focus on the most efficacious AI applications for life extension and anti-aging at three expected stages of AI development: narrow AI, AGI and superintelligence. First, we overview the existing research and commercial work performed by a select number of startups and academic projects. We find that at the current stage of “narrow” AI, the most promising areas for life extension are geroprotector-combination discovery, detection of aging biomarkers, and personalized anti-aging therapy. These advances could help currently living people reach longevity escape velocity and survive until more advanced AI appears. When AI comes close to human level, the main contribution to life extension will come from AI integration with humans through brain-computer interfaces, integrated AI assistants capable of autonomously diagnosing and treating health issues, and cyber systems embedded into human bodies. Lastly, we speculate about the more remote future, when AI reaches the level of superintelligence and such life-extension methods as uploading human minds and creating nanotechnological bodies may become possible, thus lowering the probability of human death close to zero. We conclude that medical AI based superintelligence is intrinsically safer than, say, military AI, as it may help humans to evolve into part of the future superintelligence via brain augmentation, uploading, and a network of self-improving humans. Medical AI’s value system is focused on human benefit.","2017-12-27","2022-01-30 04:59:36","2022-01-30 04:59:36","2020-12-13 22:07:54","","","4","41","","","Artificial Intelligence in Life Extension","","","","","","","en","I assign to  Informatica ,  An International Journal of Computing and Informatics  (""Journal"") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript (""Paper"") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika","","","","www.informatica.si","","ZSCC: 0000030  Number: 4","","/Users/jacquesthibodeau/Zotero/storage/9P7GCWS8/Batin et al. - 2017 - Artificial Intelligence in Life Extension from De.pdf; /Users/jacquesthibodeau/Zotero/storage/UB6XNVTH/1797.html; /Users/jacquesthibodeau/Zotero/storage/7VGQTWMP/1797.html","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"524FKWKH","blogPost","2015","Dietterich, Thomas G.","Benefits and Risks of Artificial Intelligence","Thomas G. Dietterich (Medium)","","","","https://medium.com/@tdietterich/benefits-and-risks-of-artificial-intelligence-460d288cccf3","Discussions about Artificial Intelligence (AI) have jumped into the public eye over the past year, with several luminaries speaking…","2015-01-23","2022-01-30 04:59:36","2022-01-30 04:59:36","2020-11-21 18:50:18","","","","","","","","","","","","","","en","","","","","","","ZSCC: 0000001[s0]","","","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CPJINAIP","bookSection","2012","Hibbard, Bill","Avoiding Unintended AI Behaviors","Artificial General Intelligence","978-3-642-35505-9 978-3-642-35506-6","","","http://link.springer.com/10.1007/978-3-642-35506-6_12","","2012","2022-01-30 04:59:36","2022-01-30 04:59:36","2020-11-22 01:47:56","107-116","","","7716","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 25  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-35506-6_12","","/Users/jacquesthibodeau/Zotero/storage/IE7575I5/Hibbard - 2012 - Avoiding Unintended AI Behaviors.pdf","","TechSafety; Other-org","","Bach, Joscha; Goertzel, Ben; Iklé, Matthew","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A4SXFVC7","conferencePaper","2016","Steinhardt, Jacob; Valiant, Gregory; Charikar, Moses","Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction","Advances in Neural Information Processing Systems 29 (NIPS 2016)","","","","http://arxiv.org/abs/1606.05374","We consider a crowdsourcing model in which $n$ workers are asked to rate the quality of $n$ items previously generated by other workers. An unknown set of $\alpha n$ workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an $\epsilon$ fraction of low-quality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with $n$: the dataset can be curated with $\tilde{O}\Big(\frac{1}{\beta\alpha^3\epsilon^4}\Big)$ ratings per worker, and $\tilde{O}\Big(\frac{1}{\beta\epsilon^2}\Big)$ ratings by the manager, where $\beta$ is the fraction of high-quality items. Our results extend to the more general setting of peer prediction, including peer grading in online classrooms.","2016-06-16","2022-01-30 04:59:36","2022-01-30 04:59:36","2020-12-13 19:48:09","","","","","","","Avoiding Imposters and Delinquents","","","","","","","","","","","","arXiv.org","","ZSCC: 0000030  arXiv: 1606.05374","","/Users/jacquesthibodeau/Zotero/storage/AIQHAJ8W/Steinhardt et al. - 2016 - Avoiding Imposters and Delinquents Adversarial Cr.pdf; /Users/jacquesthibodeau/Zotero/storage/CHB5HPQE/1606.html","","TechSafety; Other-org","Computer Science - Machine Learning; Computer Science - Computer Science and Game Theory; Computer Science - Data Structures and Algorithms; Computer Science - Human-Computer Interaction; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"38TP8QCZ","manuscript","2016","Yampolskiy, Roman V.; Spellchecker, M. S.","Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures","","","","","http://arxiv.org/abs/1610.07997","In this work, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. We suggest that both the frequency and the seriousness of future AI failures will steadily increase. AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AIs safety failures are at the same, moderate, level of criticality as in cybersecurity, however for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100% secure system.","2016-10-25","2022-01-30 04:59:36","2022-01-30 04:59:36","2020-12-13 20:19:29","","","","","","","Artificial Intelligence Safety and Cybersecurity","","","","","","","","","","","","arXiv.org","","ZSCC: 0000084  arXiv: 1610.07997","","/Users/jacquesthibodeau/Zotero/storage/XBP68MBT/Yampolskiy and Spellchecker - 2016 - Artificial Intelligence Safety and Cybersecurity .pdf; /Users/jacquesthibodeau/Zotero/storage/GC9TXXSM/1610.html","","MetaSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9GV2AK67","report","2017","Duettmann, Allison","Artificial General Intelligence: Timeframes & Policy White Paper","","","","","https://foresight.org/publications/AGI-Timeframes&PolicyWhitePaper.pdf","","2017","2022-01-30 04:59:36","2022-01-30 04:59:36","","26","","","","","","","","","","","Foresight Institute","","en","","","","","Zotero","","ZSCC: 0000000","","/Users/jacquesthibodeau/Zotero/storage/7GZFE7Z4/Duettmann - Artificial General Intelligence Timeframes & Poli.pdf","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AWG743AN","report","2018","Duettman, Allison; Afanasjeva, Olga; Armstrong, Stuart; Braley, Ryan; Cussins, Jessica; Ding, Jeffrey; Eckersley, Peter; Guan, Melody; Vance, Alyssa; Yampolskiy, Roman","Artificial General Intelligence: Coordination and Great Powers","","","","","https://fsone-bb4c.kxcdn.com/wp-content/uploads/2018/11/AGI-Coordination-Geat-Powers-Report.pdf","","2018","2022-01-30 04:59:35","2022-01-30 04:59:35","","","","","","","","","","","","","Foresight Institute","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: 5","","/Users/jacquesthibodeau/Zotero/storage/7TTB38TG/Duettman et al. - 2018 - Artificial General Intelligence Coordination and .pdf","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CNJWTPTN","manuscript","2016","Ziesche, Soenke; Yampolskiy, Roman V.","Artificial Fun: Mapping Minds to the Space of Fun","","","","","http://arxiv.org/abs/1606.07092","Yampolskiy and others have shown that the space of possible minds is vast, actually infinite (Yampolskiy, 2015). A question of interest is 'Which activities can minds perform during their lifetime?' This question is very broad, thus in this article restricted to 'Which non-boring activities can minds perform?' The space of potential non-boring activities has been called by Yudkowsky 'fun space' (Yudkowsky, 2009). This paper aims to discuss the relation between various types of minds and the part of the fun space, which is accessible for them.","2016-06-22","2022-01-30 04:59:35","2022-01-30 04:59:35","2020-12-13 20:23:34","","","","","","","Artificial Fun","","","","","","","","","","","","arXiv.org","","ZSCC: 0000002  arXiv: 1606.07092","","/Users/jacquesthibodeau/Zotero/storage/X6ZMRXFI/Ziesche and Yampolskiy - 2016 - Artificial Fun Mapping Minds to the Space of Fun.pdf; /Users/jacquesthibodeau/Zotero/storage/AVP4AFV8/1606.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Other Computer Science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P8ABUARU","journalArticle","2019","Snyder-Beattie, Andrew E.; Ord, Toby; Bonsall, Michael B.","An upper bound for the background rate of human extinction","Scientific Reports","","2045-2322","10.1038/s41598-019-47540-7","https://www.nature.com/articles/s41598-019-47540-7","We evaluate the total probability of human extinction from naturally occurring processes. Such processes include risks that are well characterized such as asteroid impacts and supervolcanic eruptions, as well as risks that remain unknown. Using only the information that Homo sapiens has existed at least 200,000 years, we conclude that the probability that humanity goes extinct from natural causes in any given year is almost guaranteed to be less than one in 14,000, and likely to be less than one in 87,000. Using the longer track record of survival for our entire genus Homo produces even tighter bounds, with an annual probability of natural extinction likely below one in 870,000. These bounds are unlikely to be affected by possible survivorship bias in the data, and are consistent with mammalian extinction rates, typical hominin species lifespans, the frequency of well-characterized risks, and the frequency of mass extinctions. No similar guarantee can be made for risks that our ancestors did not face, such as anthropogenic climate change or nuclear/biological warfare.","2019-07-30","2022-01-30 04:59:35","2022-01-30 04:59:35","2019-12-16 22:38:58","1-9","","1","9","","","","","","","","","","en","2019 The Author(s)","","","","www.nature.com","","ZSCC: 0000017","","/Users/jacquesthibodeau/Zotero/storage/PM4KFW9G/Snyder-Beattie et al. - 2019 - An upper bound for the background rate of human ex.pdf; /Users/jacquesthibodeau/Zotero/storage/IMA8B8KI/Snyder-Beattie et al. - 2019 - An upper bound for the background rate of human ex.pdf; /Users/jacquesthibodeau/Zotero/storage/J3EUKR39/s41598-019-47540-7.html","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M9JCKE87","conferencePaper","2020","Agarwal, Rishabh; Schuurmans, Dale; Norouzi, Mohammad","An Optimistic Perspective on Offline Reinforcement Learning","arXiv:1907.04543 [cs, stat]","","","","http://arxiv.org/abs/1907.04543","Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.","2020-06-22","2022-01-30 04:59:35","2022-01-30 04:59:35","2020-08-28 17:35:41","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000137  arXiv: 1907.04543","","/Users/jacquesthibodeau/Zotero/storage/I8BUCNET/Agarwal et al. - 2020 - An Optimistic Perspective on Offline Reinforcement.pdf; /Users/jacquesthibodeau/Zotero/storage/4UQ64FEK/1907.html","","TechSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML 2020","","","","","","","","","","","","","","",""
"9E4PQP2R","blogPost","2020","Wentworth, John","Alignment By Default","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/Nwgdq6kHke5LY692J/alignment-by-default","Suppose AI continues on its current trajectory: deep learning continues to get better as we throw more data and compute at it, researchers keep trying random architectures and using whatever seems to work well in practice. Do we end up with aligned AI “by default”? I think there’s at least a plausible trajectory in which the answer is “yes”. Not very likely - I’d put it at ~10% chance - but plausible. In fact, there’s at least an argument to be made that alignment-by-default is more likely to work than many fancy alignment proposals, including IRL variants and HCH-family methods. This post presents the rough models and arguments. I’ll break it down into two main pieces:  * Will a sufficiently powerful unsupervised learner “learn human values”? What    does that even mean?  * Will a supervised/reinforcement learner end up aligned to human values, given    a bunch of data/feedback on what humans want? Ultimately, we’ll consider a semi-supervised/transfer-learning style approach, where we first do some unsupervised learning and hopefully “learn human values” before starting the supervised/reinforcement part. As background, I will assume you’ve read some of the core material about human values from the sequences, including Hidden Complexity of Wishes, Value is Fragile, and Thou Art Godshatter. UNSUPERVISED: POINTING TO VALUES In this section, we’ll talk about why an unsupervised learner might not “learn human values”. Since an unsupervised learner is generally just optimized for predictive power, we’ll start by asking whether theoretical algorithms with best-possible predictive power (i.e. Bayesian updates on low-level physics models) “learn human values”, and what that even means. Then, we’ll circle back to more realistic algorithms. Consider a low-level physical model of some humans - e.g. a model which simulates every molecule comprising the humans. Does this model “know human values”? In one sense, yes: the low-level model has everything there is to know abo","2020-08-12","2022-01-30 04:59:35","2022-01-30 04:59:35","2020-08-24 20:26:49","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/5W3RZIP8/alignment-by-default.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZKMEPQE","blogPost","2020","Wentworth, John S","Alignment as Translation","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/42YykiTqtGMyJAjDM/alignment-as-translation","Technology Changes Constraints argues that economic constraints are usually modular with respect to technology changes - so for reasoning about technology changes, it’s useful to cast them in terms of economic constraints. Two constraints we’ll talk about here:  * Compute - flops, memory, etc.  * Information - sensors, data, etc. Thanks to ongoing technology changes, both of these constraints are becoming more and more slack over time - compute and information are both increasingly abundant and cheap. Immediate question: what happens in the limit as the prices of both compute and information go to zero? Essentially, we get omniscience: our software has access to a perfect, microscopically-detailed model of the real world. Computers have the memory and processing capability to run arbitrary queries on that model, and predictions are near-perfectly accurate (modulo quantum noise). This limit applies even without AGI - as compute and information become more abundant, our software approaches omniscience, even limiting ourselves to special-purpose reasoning algorithms. Of course, AGI would presumably be closer to omniscience than non-AGI algorithms, at the same level of compute/information. It would be able to more accurately predict more things which aren’t directly observable via available sensors, and it would be able to run larger queries with the same amount of compute. (How much closer to omniscience an AGI would get is an open question, but it would at least not be any worse in a big-O sense.) Next question: as compute and information constraints slacken, which constraints become taut? What new bottlenecks appear, for problems which were previously bottlenecked on compute/information? To put it differently: if our software can run arbitrary queries on an accurate, arbitrarily precise low-level model of the physical world, what else do we need in order to get value out of that capability? Well, mainly we need some way to specify what it is that we want. We","2020-03-19","2022-01-30 04:59:35","2022-01-30 04:59:35","2020-09-05 17:57:16","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VPXDA528/alignment-as-translation.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78WA4C6X","book","2007","","Artificial General Intelligence","","978-3-540-23733-4 978-3-540-68677-4","","","http://link.springer.com/10.1007/978-3-540-68677-4","","2007","2022-01-30 04:59:35","2022-01-30 04:59:35","2020-11-22 05:00:59","","","","","","","","Cognitive Technologies","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 467  DOI: 10.1007/978-3-540-68677-4","","","","TechSafety; Other-org","","Goertzel, Ben; Pennachin, Cassio","Gabbay, Dov M.; Siekmann, Jörg; Bundy, A.; Carbonell, J. G.; Pinkal, M.; Uszkoreit, H.; Veloso, M.; Wahlster, W.; Wooldridge, M. J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5UVAGFV","report","2020","McGraw, Gary; Figueroa, Harold; Shepardson, Victor; Bonett, Richie","An Architectural Risk Analysis of Machine Learning Systems: Toward More Secure Machine Learning","","","","","https://www.garymcgraw.com/wp-content/uploads/2020/02/BIML-ARA.pdf","","2020-01-13","2022-01-30 04:59:35","2022-01-30 04:59:35","2020-09-07","42","","","","","","An Architectural Risk Analysis of Machine Learning Systems","","","","","Berryville Institute of Machine Learning","","","","","","","","","ZSCC: 0000003","","","","TechSafety; AmbiguosSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XE2PI5Z","blogPost","2020","Wentworth, John S","Alignment As A Bottleneck To Usefulness Of GPT-3","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/BnDF5kejzQLqd5cjH/alignment-as-a-bottleneck-to-usefulness-of-gpt-3","So there’s this thing where GPT-3 is able to do addition, it has the internal model to do addition, but it takes a little poking and prodding to actually get it to do addition. “Few-shot learning”, as the paper calls it. Rather than prompting the model with Q: What is 48 + 76? A: … instead prompt it with Q: What is 48 + 76? A: 124 Q: What is 34 + 53? A: 87 Q: What is 29 + 86? A: The same applies to lots of other tasks: arithmetic, anagrams and spelling correction, translation, assorted benchmarks, etc. To get GPT-3 to do the thing we want, it helps to give it a few examples, so it can “figure out what we’re asking for”. This is an alignment problem. Indeed, I think of it as the quintessential alignment problem: to translate what-a-human-wants into a specification usable by an AI. The hard part is not to build a system which can do the thing we want, the hard part is to specify the thing we want in such a way that the system actually does it. The GPT family of models are trained to mimic human writing. So the prototypical “alignment problem” on GPT is prompt design: write a prompt such that actual human writing which started with that prompt would likely contain the thing you actually want. Assuming that GPT has a sufficiently powerful and accurate model of human writing, it should then generate the thing you want. Viewed through that frame, “few-shot learning” just designs a prompt by listing some examples of what we want - e.g. listing some addition problems and their answers. Call me picky, but that seems like a rather primitive way to design a prompt. Surely we can do better? Indeed, people are already noticing clever ways to get better results out of GPT-3 - e.g. TurnTrout recommends conditioning on writing by smart people, and the right prompt makes the system complain about nonsense rather than generating further nonsense in response. I expect we’ll see many such insights over the next month or so. CAPABILITIES VS ALIGNMENT AS BOTTLENECK TO VALUE I","2020-07-21","2022-01-30 04:59:35","2022-01-30 04:59:35","2020-08-28 17:27:18","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/GPA9UK4C/alignment-as-a-bottleneck-to-usefulness-of-gpt-3.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8AFN559","journalArticle","2020","McIlroy-Young, Reid; Sen, Siddhartha; Kleinberg, Jon; Anderson, Ashton","Aligning Superhuman AI with Human Behavior: Chess as a Model System","Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","","","10.1145/3394486.3403219","http://arxiv.org/abs/2006.01855","As artificial intelligence becomes increasingly intelligent---in some cases, achieving superhuman performance---there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between human and artificial intelligence is modeling the granular actions that constitute human behavior, rather than simply matching aggregate human performance. We pursue this goal in a model system with a long history in artificial intelligence: chess. The aggregate performance of a chess player unfolds as they make decisions over the course of a game. The hundreds of millions of games played online by players at every skill level form a rich source of data in which these decisions, and their exact context, are recorded in minute detail. Applying existing chess engines to this data, including an open-source implementation of AlphaZero, we find that they do not predict human moves well. We develop and introduce Maia, a customized version of Alpha-Zero trained on human chess games, that predicts human moves at a much higher accuracy than existing engines, and can achieve maximum accuracy when predicting decisions made by players at a specific skill level in a tuneable way. For a dual task of predicting whether a human will make a large mistake on the next move, we develop a deep neural network that significantly outperforms competitive baselines. Taken together, our results suggest that there is substantial promise in designing artificial intelligence systems with human collaboration in mind by first accurately modeling granular human decision-making.","2020-08-23","2022-01-30 04:59:34","2022-01-30 04:59:34","2020-11-14 00:50:30","1677-1687","","","","","","Aligning Superhuman AI with Human Behavior","","","","","","","","","","","","arXiv.org","","ZSCC: 0000023  arXiv: 2006.01855","","/Users/jacquesthibodeau/Zotero/storage/EHATD65D/McIlroy-Young et al. - 2020 - Aligning Superhuman AI with Human Behavior Chess .pdf; /Users/jacquesthibodeau/Zotero/storage/C93VXAWD/2006.html","","TechSafety; Other-org","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JZZEVAWU","blogPost","2021","Flint, Alex","AI Risk for Epistemic Minimalists","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/8fpzBHt7e6n7Qjoo9/ai-risk-for-epistemic-minimalists","Financial status: This is independent research, now supported by a grant. I welcome further financial support. Epistemic status: This is an attempt to use only very robust arguments. -------------------------------------------------------------------------------- OUTLINE  * I outline a case for concern about AI that does not invoke concepts of    agency, goal-directedness, or consequential reasoning, does not hinge on    single- or multi-principal or single or multi-agent assumptions, does not    assume fast or slow take-off, and applies equally well to a world of emulated    humans as to de-novo AI.          * The basic argument is about the power that humans will temporarily or    permanently gain by developing AI systems, and the history of quick increases    in human power.          * In the first section I give a case for paying attention to AI at all.          * In the second section I give a case for being concerned about AI.          * In the third section I argue that the business-as-usual trajectory of AI    development is not satisfactory.          * In the fourth section I argue that there are things that can be done now.         THE CASE FOR ATTENTION We already have powerful systems that influence the future of life on the planet. The systems of finance, justice, government, and international cooperation are things that we humans have constructed. The specific design of these systems has influence over the future of life on the planet, meaning that there are small changes that could be made to these systems that would have an impact on the future of life on the planet much larger than the change itself. In this sense I will say that these systems are powerful. Now every single powerful system that we have constructed up to now uses humans as a fundamental building-block. The justice system uses humans as judges and lawyers and administrators. At a mechanical level, the justice system would not execute its intended function without these building-","2021-08-22","2022-01-30 04:59:34","2022-01-30 04:59:34","2021-11-18 23:24:06","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/4A8ENXKH/ai-risk-for-epistemic-minimalists.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZUFFVWA","blogPost","2018","rk","AI development incentive gradients are not uniformly terrible","LessWrong","","","","https://www.lesswrong.com/posts/bkG4qj9BFEkNva3EX/ai-development-incentive-gradients-are-not-uniformly","Much of the work for this post was done together with Nuño Sempere Perhaps you think that your values will be best served if the AGI you (or your team, company or nation) are developing is deployed first. Would you decide that it's worth cutting a few corners, reducing your safety budget, and pushing ahead to try and get your AI out the door first? It seems plausible, and worrying, that you might. And if your competitors reason symmetrically, we would get a ""safety race to the bottom"". On the other hand, perhaps you think your values will be better served if your enemy wins than if either of you accidentally produces an unfriendly AI. Would you decide the safety costs to improving your chances aren't worth it? In a simple two player model, you should only shift funds from safety to capabilities if (the relative₁ decrease in chance of friendliness) / (the relative₁ increase in the chance of winning) < (expected relative₂ loss of value if your enemy wins rather than you). Here, the relative₁ increases and decreases are relative to the current values. The relative₂ loss of value is relative to the expected value if you win. The plan of this post is as follows: 1. Consider a very simple model that leads to a safety race. Identify unrealistic assumptions which are driving its results. 2. Remove some of the unrealistic assumptions and generate a different model. Derive the inequality expressed above. 3. Look at some specific example cases, and see how they affect safety considerations. A PARTLY DISCONTINUOUS MODEL Let's consider a model with two players with the same amount of resources. Each player's choice is what fraction of their resources to devote to safety, rather than capabilities. Whichever player contributes more to capabilities wins the race. If you win the race, you either get a good outcome or a bad outcome. Your chance of getting a good outcome increases continuously with the amount you spent on safety. If the other player wins, you get a bad outcome.","2018","2022-01-30 04:59:34","2022-01-30 04:59:34","2020-12-13 23:33:40","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/8X6FWAXX/ai-development-incentive-gradients-are-not-uniformly.html","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AHAITFCH","report","2021","Horowitz, Michael; Scharre, Paul","AI and International Stability: Risks and Confidence-Building Measures","","","","","https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures","Exploring the potential use of confidence-building measures built around the shared interests that all countries have in preventing inadvertent war.","2021-01-12","2022-01-30 04:59:34","2022-01-30 04:59:34","2021-11-14 18:05:37","","","","","","","AI and International Stability","","","","","Center for a New American Security","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: 2","","/Users/jacquesthibodeau/Zotero/storage/SZ2WC6V5/Horowitz and Scharre - 2021 - AI and International Stability Risks and Confiden.pdf; /Users/jacquesthibodeau/Zotero/storage/VC38NJHI/ai-and-international-stability-risks-and-confidence-building-measures.html","","MetaSafety; AmbiguousSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XGF52239","conferencePaper","2018","Everitt, Tom; Lea, Gary; Hutter, Marcus","AGI Safety Literature Review","Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence","","","","http://arxiv.org/abs/1805.01109","The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.","2018-05-21","2022-01-30 04:59:34","2022-01-30 04:59:34","2020-11-15 02:50:45","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000076  arXiv: 1805.01109","","/Users/jacquesthibodeau/Zotero/storage/ECXX8QH5/Everitt et al. - 2018 - AGI Safety Literature Review.pdf; /Users/jacquesthibodeau/Zotero/storage/N7U9FKW6/1805.html; /Users/jacquesthibodeau/Zotero/storage/3E7Q6G7R/Everitt et al. - 2018 - AGI Safety Literature Review.pdf","","TechSafety; Other-org","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8WDX7UC8","blogPost","2020","Stray, Jonathan","Aligning AI to Human Values means Picking the Right Metrics","AI & Advancing Responsible AI (Medium)","","","","https://medium.com/partnership-on-ai/aligning-ai-to-human-values-means-picking-the-right-metrics-855859e6f047","Optimizing for the wrong thing can cause a lot of harm.","2020-04-15","2022-01-30 04:59:34","2022-01-30 04:59:34","2020-09-05 17:22:15","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/2Q9796G2/aligning-ai-to-human-values-means-picking-the-right-metrics-855859e6f047.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PM74T3QP","conferencePaper","2020","Fazelpour, Sina; Lipton, Zachary C.","Algorithmic Fairness from a Non-ideal Perspective","arXiv:2001.09773 [cs, stat]","","","","http://arxiv.org/abs/2001.09773","Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In efforts to mitigate these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might expect to observe in a fair world and offered a variety of algorithms in attempts to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to \emph{fair machine learning} to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and the perfectly just world. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of proposed policies, naive applications of ideal thinking can lead to misguided interventions. In this paper, we demonstrate a connection between the fair machine learning literature and the ideal approach in political philosophy, and argue that the increasingly apparent shortcomings of proposed fair machine learning algorithms reflect broader troubles faced by the ideal approach. We conclude with a critical discussion of the harms of misguided solutions, a reinterpretation of impossibility results, and directions for future research.","2020-01-08","2022-01-30 04:59:34","2022-01-30 04:59:34","2020-09-05 16:51:01","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000029  arXiv: 2001.09773","","/Users/jacquesthibodeau/Zotero/storage/X2KWRS4A/Fazelpour and Lipton - 2020 - Algorithmic Fairness from a Non-ideal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/3W2M84GS/2001.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society 2020","","","","","","","","","","","","","","",""
"49VDX9SI","blogPost","2020","Kovarik, Vojta","AI Unsafety via Non-Zero-Sum Debate","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/BRiMQELD5WYyvncTE/ai-unsafety-via-non-zero-sum-debate","In this post, I describe how to view debate as a way of assisting a human to spot flaws in an AI’s proposal. I then argue that the zero-sum assumption is critical for making debate work and that various seemingly-helpful modifications of debate might break it instead. -------------------------------------------------------------------------------- A naive way of using arbitrary optimizers as oracles:Suppose you have a black-box optimizer X that can be connected to any well-defined quantity to be maximized. X can potentially be very powerful - e.g., having a highly accurate model of the world and “a lot of optimization power”. One way to turn X into an oracle is to ask it a question and decide to give it reward 1 if we like its answer and 0 if we don’t.[1] Of course, standard AI-safety arguments (e.g., AI takeover and perverse instantiation) suggest that this is a pretty bad idea for powerful X. For the sake of argument, suppose that we can fix all of the “obvious” problems and ensure that X won’t wirehead, won’t try to escape the box we put it in etc., and will only care about the reward it gets for its answer. Two problems with naive optimizers-turned-oracles: (1) telling the difference between good and awesome answers and (2) answers with hidden flaws:One problem with this type of oracles is that it’s hard to decide whether we like its answers or not. Suppose I ask it for food recommendations for the evening and it suggests pancakes. Pancakes seem fine, although there are some foods that I would like better. So should I reward the AI or not? The second problem is that the oracle optimizes for giving answers that seem good to a human. (Not out of malice, but because “actually being good” isn’t well-defined.) And since humans aren’t omniscient, there will be many seemingly good answers that in fact have disastrous consequences if acted upon. To address (1), use two AIs:The first problem can be tackled by using two copies of the optimizer and rewarding the one w","2020-07-03","2022-01-30 04:59:34","2022-01-30 04:59:34","2020-08-28 17:54:52","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/V932ECDR/ai-unsafety-via-non-zero-sum-debate.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DSCR47GU","conferencePaper","2018","Sarma, Gopal P.; Hay, Nick J.; Safron, Adam","AI Safety and Reproducibility: Establishing Robust Foundations for the Neuropsychology of Human Values","Lecture Notes in Computer Science","","","10.1007/978-3-319-99229-7_45","http://arxiv.org/abs/1712.04307","We propose the creation of a systematic effort to identify and replicate key findings in neuropsychology and allied fields related to understanding human values. Our aim is to ensure that research underpinning the value alignment problem of artificial intelligence has been sufficiently validated to play a role in the design of AI systems.","2018","2022-01-30 04:59:34","2022-01-30 04:59:34","2020-12-13 23:39:16","507-512","","","11088","","","AI Safety and Reproducibility","","","","","","","","","","","","arXiv.org","","ZSCC: 0000007  arXiv: 1712.04307","","/Users/jacquesthibodeau/Zotero/storage/X9Q6DXIZ/Sarma et al. - 2018 - AI Safety and Reproducibility Establishing Robust.pdf; /Users/jacquesthibodeau/Zotero/storage/RKKBXEHQ/1712.html","","MetaSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Computer Safety, Reliability, and Security (SAFECOMP 2018)","","","","","","","","","","","","","","",""
"728H8KFI","journalArticle","2019","Perry, Brandon; Uuk, Risto","AI Governance and the Policymaking Process: Key Considerations for Reducing AI Risk","Big Data and Cognitive Computing","","","10.3390/bdcc3020026","https://www.mdpi.com/2504-2289/3/2/26","This essay argues that a new subfield of AI governance should be explored that examines the policy-making process and its implications for AI governance. A growing number of researchers have begun working on the question of how to mitigate the catastrophic risks of transformative artificial intelligence, including what policies states should adopt. However, this essay identifies a preceding, meta-level problem of how the space of possible policies is affected by the politics and administrative mechanisms of how those policies are created and implemented. This creates a new set of key considerations for the field of AI governance and should influence the action of future policymakers. This essay examines some of the theories of the policymaking process, how they compare to current work in AI governance, and their implications for the field at large and ends by identifying areas of future research.","2019-06","2022-01-30 04:59:34","2022-01-30 04:59:34","2020-12-14 23:48:16","26","","2","3","","","AI Governance and the Policymaking Process","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","ZSCC: 0000017  Number: 2 Publisher: Multidisciplinary Digital Publishing Institute","","/Users/jacquesthibodeau/Zotero/storage/85ATHIKA/Perry and Uuk - 2019 - AI Governance and the Policymaking Process Key Co.pdf","","MetaSafety; Other-org","AI governance; AI risk; policymaking process; typologies of AI policy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DAV9UKUB","blogPost","2019","Steinhardt, Jacob","AI Alignment Research Overview","LessWrong","","","","https://www.lesswrong.com/posts/7GEviErBXcjJsbSeD/ai-alignment-research-overview-by-jacob-steinhardt","I'm really excited to see someone outline all the work they think needs solving in AI alignment - to describe what the problem looks like, what a solution looks like, and what work has been done so far. Especially from Jacob, who is a coauthor of the Concrete Problems in AI Safety paper. Below, I've included some excerpts from doc. I've included the introduction, the following section describing the categories of technical work, and some high-level information from the long sections on 'technical alignment problem' and the 'detecting failures in advance'. -------------------------------------------------------------------------------- INTRODUCTION This document gives an overview of different areas of technical work that seem necessary, or at least desirable, for creating safe and aligned AI systems. The focus is on safety and alignment of powerful AI systems, i.e. systems that may exceed human capabilities in a broad variety of domains, and which likely act on a large scale. Correspondingly, there is an emphasis on approaches that seem scalable to such systems. By “aligned”, I mean that the actions it pursues move the world towards states that humans want, and away from states that humans don’t want. Some issues with this definition are that different humans might have different preferences (I will mostly ignore this issue), and that there are differences between stated preferences, “revealed” preferences as implied by actions, and preferences that one endorses upon reflection (I won’t ignore this issue). I think it is quite plausible that some topics are missing, and I welcome comments to that regard. My goal is to outline a critical mass of topics in enough detail that someone with knowledge of ML and some limited familiarity with AI alignment as an area would have a collection of promising research directions, a mechanistic understanding of why they are promising, and some pointers for what work on them might look like. To that end, below I outline four br","2019","2022-01-30 04:59:34","2022-01-30 04:59:34","2020-12-23 00:19:03","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/5WU6NMTI/ai-alignment-research-overview-by-jacob-steinhardt.html; /Users/jacquesthibodeau/Zotero/storage/EUXPA82T/Steinhardt - 2019 - AI Alignment Research Overview (by Jacob Steinhard.pdf","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5B2XBHN","manuscript","2016","Torres, Phil","Agential Risks: A Comprehensive Introduction","","","","","","The greatest existential threats to humanity stem from increasingly powerful advanced technologies. Yet the “risk potential” of such tools can only be realized when coupled with a suitable agent who, through error or terror, could use the tool to bring about an existential catastrophe. While the existential risk literature has provided many accounts of how advanced technologies might be misused and abused to cause unprecedented harm, no scholar has yet explored the other half of the agent-tool coupling, namely the agent. This paper aims to correct this failure by offering a comprehensive overview of what we could call “agential riskology.” Only by studying the unique properties of different agential risk types can one acquire an accurate picture of the existential danger before us.","2016","2022-01-30 04:59:33","2022-01-30 04:59:33","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000022","","/Users/jacquesthibodeau/Zotero/storage/22B4EZFK/Torres - Agential Risks A Comprehensive Introduction.pdf","","MetaSafety; AmbiguosSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2FZGP2T6","blogPost","2021","Flint, Alex","Agency in Conway’s Game of Life","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/3SG4WbNPoP8fsuZgs/agency-in-conway-s-game-of-life","Financial status: This is independent research. I welcome financial support to make further posts like this possible. Epistemic status: I have been thinking about these ideas for years but still have not clarified them to my satisfaction. -------------------------------------------------------------------------------- OUTLINE  * This post asks whether it is possible, in Conway’s Game of Life, to arrange    for a certain game state to arise after a certain number of steps given    control only of a small region of the initial game state.          * This question is then connected to questions of agency and AI, since one way    to answer this question in the positive is by constructing an AI within    Conway’s Game of Life.          * I argue that the permissibility or impermissibility of AI is a deep property    of our physics.          * I propose the AI hypothesis, which is that any pattern that solves the    control question does so, essentially, by being an AI.         INTRODUCTION In this post I am going to discuss a celular autonoma known as Conway’s Game of Life: In Conway’s Game Life, which I will now refer to as just ""Life"", there is a two-dimensional grid of cells where each cell is either on or off. Over time, the cells switch between on and off according to a simple set of rules:  * A cell that is ""on"" and has fewer than two neighbors that are ""on"" switches    to ""off"" at the next time step          * A cell that is ""on"" and has greater than three neighbors that are ""on""    switches to ""off"" at the next time step          * An cell that is ""off"" and has exactly three neighbors that are ""on"" switches    to ""on"" at the next time step          * Otherwise, the cell doesn’t change         It turns out that these simple rules are rich enough to permit patterns that perform arbitrary computation. It is possible to build logic gates and combine them together into a computer that can simulate any Turing machine, all by setting up a particular elaborate","2021-05-12","2022-01-30 04:59:33","2022-01-30 04:59:33","2021-11-14 18:32:05","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/TGNDE6TV/agency-in-conway-s-game-of-life.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F79JI6NM","blogPost","2015","Christiano, Paul","Advisor games","AI Alignment (Medium)","","","","https://ai-alignment.com/advisor-games-b33382fef68c","A candidate operationalization of “understandable” reasoning.","2015-09-26","2022-01-30 04:59:33","2022-01-30 04:59:33","2020-11-21 18:46:50","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/KZIMFE2Q/advisor-games-b33382fef68c.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K4CP542W","conferencePaper","2019","Baumann, Tobias; Graepel, Thore; Shawe-Taylor, John","Adaptive Mechanism Design: Learning to Promote Cooperation","2020 International Joint Conference on Neural Networks (IJCNN)","","","","http://arxiv.org/abs/1806.04067","In the future, artificial learning agents are likely to become increasingly widespread in our society. They will interact with both other learning agents and humans in a variety of complex settings including social dilemmas. We consider the problem of how an external agent can promote cooperation between artificial learners by distributing additional rewards and punishments based on observing the learners' actions. We propose a rule for automatically learning how to create right incentives by considering the players' anticipated parameter updates. Using this learning rule leads to cooperation with high social welfare in matrix games in which the agents would otherwise learn to defect with high probability. We show that the resulting cooperative outcome is stable in certain games even if the planning agent is turned off after a given number of episodes, while other games require ongoing intervention to maintain mutual cooperation. However, even in the latter case, the amount of necessary additional incentives decreases over time.","2019-11-20","2022-01-30 04:59:33","2022-01-30 04:59:33","2020-11-14 00:41:13","","","","","","","Adaptive Mechanism Design","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s2]  ACC: 10  arXiv: 1806.04067","","/Users/jacquesthibodeau/Zotero/storage/KMTJBPT6/Baumann et al. - 2019 - Adaptive Mechanism Design Learning to Promote Coo.pdf; /Users/jacquesthibodeau/Zotero/storage/4RMMHPN9/1806.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 International Joint Conference on Neural Networks (IJCNN)","","","","","","","","","","","","","","",""
"35ISE4PP","manuscript","2020","Hoang, Lê Nguyên","A Roadmap for Robust End-to-End Alignment","","","","","http://arxiv.org/abs/1809.01036","This paper discussed the {\it robust alignment} problem, that is, the problem of aligning the goals of algorithms with human preferences. It presented a general roadmap to tackle this issue. Interestingly, this roadmap identifies 5 critical steps, as well as many relevant aspects of these 5 steps. In other words, we have presented a large number of hopefully more tractable subproblems that readers are highly encouraged to tackle. Hopefully, this combination allows to better highlight the most pressing problems, how every expertise can be best used to, and how combining the solutions to subproblems might add up to solve robust alignment.","2020-02-25","2022-01-30 04:59:33","2022-01-30 04:59:33","2020-11-14 00:52:55","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 0  arXiv: 1809.01036","","/Users/jacquesthibodeau/Zotero/storage/KS6F4Q7M/Hoang - 2020 - A Roadmap for Robust End-to-End Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/HVT2WEJJ/1809.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5IKRD2K","conferencePaper","2018","Behzadan, Vahid; Munir, Arslan; Yampolskiy, Roman V.","A Psychopathological Approach to Safety Engineering in AI and AGI","","","","","https://arxiv.org/abs/1805.08915v1","The complexity of dynamics in AI techniques is already approaching that of complex adaptive systems, thus curtailing the feasibility of formal controllability and reachability analysis in the context of AI safety. It follows that the envisioned instances of Artificial General Intelligence (AGI) will also suffer from challenges of complexity. To tackle such issues, we propose the modeling of deleterious behaviors in AI and AGI as psychological disorders, thereby enabling the employment of psychopathological approaches to analysis and control of misbehaviors. Accordingly, we present a discussion on the feasibility of the psychopathological approaches to AI safety, and propose general directions for research on modeling, diagnosis, and treatment of psychological disorders in AGI.","2018-05-23","2022-01-30 04:59:33","2022-01-30 04:59:33","2020-11-14 01:17:01","","","","","","","","","","","","","","en","","","","","arxiv.org","","ZSCC: 0000016","","/Users/jacquesthibodeau/Zotero/storage/ZNKQIBA9/Behzadan et al. - 2018 - A Psychopathological Approach to Safety Engineerin.pdf; /Users/jacquesthibodeau/Zotero/storage/KB6ZSVFC/1805.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Computer Safety, Reliability, and Security","","","","","","","","","","","","","","",""
"AM7ZSJ5W","bookSection","2017","Clarke, Graham","A Psychoanalytic Approach to the Singularity: Why We Cannot Do Without Auxiliary Constructions","The Technological Singularity: Managing the Journey","978-3-662-54033-6","","","https://doi.org/10.1007/978-3-662-54033-6_12","SummaryPsychoanalysis is known above all else for its insistence that we have motivations that are unknown to ourselves, that are unconscious. We are all subject to sickness and accident, to bad luck and unfair breaks, and above all to death as a final end to all our endeavours. In order to compensate for these disappointments and for our ultimate inability to overcome these very real and material constraints we phantasise, we dream, we create, and/or we nurse our bruised and fragile selves by hoping that our phantasies might come true, if not for ourselves then for our offspring. The singularity, as it is most commonly expressed, concerns the possibility of overcoming death by achieving a sort of immortality. In specific terms Kurtweil’s own discussion of the singularity is concerned with the possibility of ‘resurrecting’ his dead father in virtual space at least. There is consistently throughout the writings on the singularity a dismissal of the emotional aspect of human living in favour of the rational overcoming of our existential condition. I am arguing that we cannot ignore the emotional consciousness that is the bedrock of human existence and that we ignore our unconscious feelings at our peril. I think that the singularity as it is being developed is actually a direct threat to the flourishing of human beings and human society because the emotional shortcomings of the theory have not been recognised.","2017","2022-01-30 04:59:33","2022-01-30 04:59:33","2020-11-24 03:00:00","209-220","","","","","","A Psychoanalytic Approach to the Singularity","The Frontiers Collection","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","ZSCC: NoCitationData[s1]  ACC: 1  DOI: 10.1007/978-3-662-54033-6_12","","","","MetaSafety; Other-org","Affective Computing; Computer Agent; Emotional Intelligence; Emotional Relationship; Technological Singularity","Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HRBWN3RH","conferencePaper","2018","Wu, Yueh-Hua; Lin, Shou-De","A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents","The Thirty-Second AAAI Conferenceon Artificial Intelligence (AAAI-18)","","","","http://arxiv.org/abs/1712.04172","This paper proposes a low-cost, easily realizable strategy to equip a reinforcement learning (RL) agent the capability of behaving ethically. Our model allows the designers of RL agents to solely focus on the task to achieve, without having to worry about the implementation of multiple trivial ethical patterns to follow. Based on the assumption that the majority of human behavior, regardless which goals they are achieving, is ethical, our design integrates human policy with the RL policy to achieve the target objective with less chance of violating the ethical code that human beings normally obey.","2018-09-10","2022-01-30 04:59:33","2022-01-30 04:59:33","2020-12-13 23:53:21","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000029   arXiv: 1712.04172","","/Users/jacquesthibodeau/Zotero/storage/X7R2DI2W/Wu and Lin - 2018 - A Low-Cost Ethics Shaping Approach for Designing R.pdf; /Users/jacquesthibodeau/Zotero/storage/GCCWAJE9/1712.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The Thirty-Second AAAI Conferenceon Artificial Intelligence (AAAI-18)","","","","","","","","","","","","","","",""
"6925VJFK","blogPost","2020","Irpan, Alex","A Reinforcement Learning Potpourri","Sorta Insightful","","","","http://www.alexirpan.com/2020/05/07/rl-potpourri.html","I’ve fallen behind on RL literature from the past few months. So, I’ve decided to catch up with a bunch of recent papers.","2020-05-07","2022-01-30 04:59:33","2022-01-30 04:59:33","2020-08-31 19:01:44","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/GKTI32M3/rl-potpourri.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6KM4PB68","bookSection","2011","Anderson, Susan Leigh; Anderson, Michael","A Prima Facie Duty Approach to Machine Ethics","Machine Ethics","978-0-511-97803-6","","","https://www.cambridge.org/core/product/identifier/CBO9780511978036A041/type/book_part","","2011","2022-01-30 04:59:33","2022-01-30 04:59:33","2020-11-22 02:22:02","476-492","","","","","","","","","","","Cambridge University Press","Cambridge","","","","","","DOI.org (Crossref)","","ZSCC: 0000032  DOI: 10.1017/CBO9780511978036.032","","","","TechSafety; Other-org","","Anderson, Michael; Anderson, Susan Leigh","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AUP8H67P","blogPost","2018","Trazzi, Michaël","A Gym Gridworld Environment for the Treacherous Turn","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/cKfryXvyJ522iFuNF/a-gym-gridworld-environment-for-the-treacherous-turn","EDIT: posted here for feedback and discussion. I plan to continue working on different models/environments, so feel free to suggest improvements. (tl;dr: In an attempt to better understand the treacherous turn, I created a gridworld environment where an agent learns to deceive an overseer by adopting an aligned behaviour when weak and takes control after capability gains) -------------------------------------------------------------------------------- At some point in its development, a seed AI may realize that it needs to get rid of its supervisors to achieve its goals. The conception of deception occurs when it conceives that, in order to maximize its chance of taking over, it must begin by exhibiting human-desirable behaviors, before undertaking a treacherous turn  when humans are no longer a threat. From the human perspective, the AI would keep on exhibiting desirable behavior, until it eventually appears dangerous, but is already unstoppable. In an attempt to better formalize the treacherous turn without using ""loaded concepts"", Stuart Armstrong proposed a toy model of the treacherous turn based on ""The Legend of Zelda: A Link to the Past "", which looked like this: In the comments, people mentionned how this model helped them ""move the topic from the 'science fiction' area to 'I can imagine it happening now'"", and seemed interested in an actual Link to the Past Minigame. There have been other simulations of the treacherous turn in the last three years (see for instance gwern's DQN box-pushing robot or Stuart Armstrong's video), but none of them actually simulate a take over where a supervisor is  killed. Hence, I decided to give it a try and simulate Stuart Armstrong's Link to the Past toy model. A GYM GRIDWORLD ENVIRONMENT Gym is an open-source toolkit for Reinforcement Learning Environments developed by Open AI. I decided to use this interface to develop the gridworld environment.  The github repository with the code, demo, and all the details is","2018-07-28","2022-01-30 04:59:32","2022-01-30 04:59:32","2020-11-21 17:50:50","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BNRB9W9Q/a-gym-gridworld-environment-for-the-treacherous-turn.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7AJI52G","blogPost","2018","Wei Dai","A general model of safety-oriented AI development","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/idb5Ppp9zghcichJ5/a-general-model-of-safety-oriented-ai-development","This may be trivial or obvious for a lot of people, but it doesn't seem like anyone has bothered to write it down (or I haven't looked hard enough). It started out as a generalization of Paul Christiano's IDA, but also covers things like safe recursive self-improvement. Start with a team of one or more humans (researchers, programmers, trainers, and/or overseers), with access to zero or more AIs (initially as assistants). The human/AI team in each round develops a new AI and adds it to the team, and repeats this until maturity in AI technology is achieved. Safety/alignment is ensured by having some set of safety/alignment properties on the team that is inductively maintained by the development process. The reason I started thinking in this direction is that Paul's approach seemed very hard to knock down, because any time a flaw or difficulty is pointed out or someone expresses skepticism on some technique that it uses or the overall safety invariant, there's always a list of other techniques or invariants that could be substituted in for that part (sometimes in my own brain as I tried to criticize some part of it). Eventually I realized this shouldn't be surprising because IDA is an instance of this more general model of safety-oriented AI development, so there are bound to be many points near it in the space of possible safety-oriented AI development practices. (Again, this may already be obvious to others including Paul, and in their minds IDA is perhaps already a cluster of possible development practices consisting of the most promising safety techniques and invariants, rather than a single point.) If this model turns out not to have been written down before, perhaps it should be assigned a name, like Iterated Safety-Invariant AI-Assisted AI Development, or something pithier?","2018","2022-01-30 04:59:32","2022-01-30 04:59:32","2020-12-13 22:26:20","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VKWZTKF8/a-general-model-of-safety-oriented-ai-development.html","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X63UF2U8","journalArticle","2004","Kent, Adrian","A Critical Look at Risk Assessments for Global Catastrophes","Risk Analysis","","0272-4332, 1539-6924","10.1111/j.0272-4332.2004.00419.x","https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0272-4332.2004.00419.x","","2004-02","2022-01-30 04:59:32","2022-01-30 04:59:32","2020-11-22 05:02:07","157-168","","1","24","","Risk Analysis","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000061","","/Users/jacquesthibodeau/Zotero/storage/CGPWV2PJ/Kent - 2004 - A Critical Look at Risk Assessments for Global Cat.pdf","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NSTEWUAD","blogPost","2020","Taylor, Jessica","A critical agential account of free will, causation, and physics","Unstable Ontology","","","","https://unstableontology.com/2020/03/05/a-critical-agential-account-of-free-will-causation-and-physics/","This is an account of free choice in a physical universe. It is very much relevant to decision theory and philosophy of science. It is largely metaphysical, in terms of taking certain things to be …","2020-03-05","2022-01-30 04:59:32","2022-01-30 04:59:32","2020-09-05 18:58:22","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/EATVM75W/a-critical-agential-account-of-free-will-causation-and-physics.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7S5UD2CM","blogPost","2016","Larks","2016 AI Risk Literature Review and Charity Comparison","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison","INTRODUCTION I've long been concerned about AI Risk. Now that there are a few charities working on the problem, it seems desirable to compare them, to determine where scarce donations should be sent. This is a similar role to that which GiveWell performs for global health charities, and somewhat similar to an securities analyst with regard possible investments. However, while people have evaluated individual organisations, I haven't seen anyone else attempt to compare them, so hopefully this is valuable to others. I've attempted to do so. This is a very big undertaking, and I am very conscious of the many ways in which this is not up to the task. The only thing I wish more than the skill and time to do it better is that someone else would do it! If people find this useful enough to warrant doing again next year I should be able to do it much more efficiently, and spend more time on the underlying model of how papers translate into risk-reduction value. My aim is basically to judge the output of each organisation in 2016 and compare it to their budget. This should give a sense for the organisations' average cost-effectiveness. Then we can consider factors that might increase or decrease the marginal cost-effectiveness going forward. This organisation-centric approach is in contrast to a researcher-centric approach, where we would analyse which researchers do good work, and then donate wherever they are. An extreme version of the other approach would be to simply give money directly to researchers - e.g if I like Logical Induction, I would simply fund Scott Garrabrant directly and ignore MIRI. I favour the organisation-centric approach because it helps keep organisations accountable. Additionally, if researcher skill is the only thing that matters for research output, it doesn't really matter which organisations end up getting the money and employing the researchers, assuming broadly the same researchers are hired. Different organisations might hire different resea","2016","2022-01-30 04:59:32","2022-01-30 04:59:32","2020-12-13 21:00:46","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/F8QRGTRF/2016-ai-risk-literature-review-and-charity-comparison.html","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"97GBG8UU","blogPost","2020","O'Keefe, Cullen","AI Benefits Blog Series Index","Cullen O'Keefe","","","","https://cullenokeefe.com/ai-benefits-index","","2020","2022-01-30 04:58:18","2022-01-30 04:58:18","2020-08-28 17:33:18","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/ZJUG84N9/ai-benefits-index.html","","MetaSafety; FHI; Open-AI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQ7I47Z4","manuscript","2018","Irving, Geoffrey; Christiano, Paul; Amodei, Dario","AI safety via debate","","","","","http://arxiv.org/abs/1805.00899","To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.","2018-10-22","2022-01-30 04:58:18","2022-01-30 04:58:18","2019-12-16 20:07:38","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000044  arXiv: 1805.00899","","/Users/jacquesthibodeau/Zotero/storage/Q8PQWCV7/Irving et al. - 2018 - AI safety via debate.pdf; /Users/jacquesthibodeau/Zotero/storage/QKP3ZSWZ/Irving et al. - 2018 - AI safety via debate.pdf; /Users/jacquesthibodeau/Zotero/storage/26MTE6EQ/1805.html; /Users/jacquesthibodeau/Zotero/storage/38FDWSJQ/1805.html","","TechSafety; Open-AI","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z2KG34C3","blogPost","2019","Christiano, Paul","Ambitious vs. narrow value learning","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/SvuLhtREMy8wRBzpC/ambitious-vs-narrow-value-learning","(Re)Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin's note: The definition of narrow value learning in the previous post focused on the fact that the resulting behavior is limited to some domain. The definition in this post focuses on learning instrumental goals and values. While the definitions are different, I have used the same term for both because I believe that they are both pointing at the same underlying concept. (I do not know if Paul agrees.) I'm including this post to give a different perspective on what I mean by narrow value learning, before delving into conceptual ideas within narrow value learning. -------------------------------------------------------------------------------- Suppose I’m trying to build an AI system that “learns what I want” and helps me get it. I think that people sometimes use different interpretations of this goal. At two extremes of a spectrum of possible interpretations:  * The AI learns my preferences over (very) long-term outcomes. If I were to die    tomorrow, it could continue pursuing my goals without me; if humanity were to    disappear tomorrow, it could rebuild the kind of civilization we would want;     etc. The AI might pursue radically different subgoals than I would on the    scale of months and years, if it thinks that those subgoals better achieve    what I really want.  * The AI learns the narrower subgoals and instrumental values I am pursuing. It    learns that I am trying to schedule an appointment for Tuesday and that I    want to avoid inconveniencing anyone, or that I am trying to fix a particular    bug without introducing new problems, etc. It does not make any effort to    pursue wildly different short-term goals than I would in order to better    realize my long-term values, though it may help me correct some errors that I    would be able to recognize as such. I think that many researchers interested in AI safety per se mostly think about the former. I think that research","2019","2022-01-30 04:58:18","2022-01-30 04:58:18","2020-12-17 04:38:56","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/3CP23MM8/SvuLhtREMy8wRBzpC.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9DSWXAQR","blogPost","2018","Christiano, Paul","An unaligned benchmark","AI Alignment (Medium)","","","","https://ai-alignment.com/an-unaligned-benchmark-b49ad992940b","What an unaligned AI might look like, how it could go wrong, and how we could fix it.","2018-09-26","2022-01-30 04:58:18","2022-01-30 04:58:18","2020-11-14 03:13:15","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/62K3QGQ4/an-unaligned-benchmark-b49ad992940b.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4NWR885X","journalArticle","2019","Irving, Geoffrey; Askell, Amanda","AI Safety Needs Social Scientists","Distill","","2476-0757","10.23915/distill.00014","https://distill.pub/2019/safety-needs-social-scientists","","2019-02-19","2022-01-30 04:58:18","2022-01-30 04:58:18","2019-12-16 20:00:35","10.23915/distill.00014","","2","4","","Distill","","","","","","","","","","","","","DOI.org (Crossref)","","ZSCC: 0000028","","/Users/jacquesthibodeau/Zotero/storage/QVDDN5BC/distill-social-scientists.html","","MetaSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C85HUN4S","blogPost","2017","Christiano, Paul","AlphaGo Zero and capability amplification","AI Alignment (Medium)","","","","https://ai-alignment.com/alphago-zero-and-capability-amplification-ede767bb8446","AlphaGo Zero happens to be a great proof-of-concept of iterated capability amplification (my preferred approach to safe RL).","2017-10-20","2022-01-30 04:58:18","2022-01-30 04:58:18","2020-12-13 21:43:02","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/5BQTZRUN/alphago-zero-and-capability-amplification-ede767bb8446.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J7IQJ8GW","manuscript","2020","O’Keefe, Cullen","Antitrust-Compliant AI Industry Self-Regulation","","","","","https://cullenokeefe.com/blog/antitrust-compliant-ai-industry-self-regulation","The touchstone of antitrust compliance is competition. To be legally permissible, any industrial restraint on trade must have sufficient countervailing procompetitive justifications. Usually, anticompetitive horizontal agreements like boycotts (including a refusal to produce certain products) are per se illegal.","2020-07-07","2022-01-30 04:58:18","2022-01-30 04:58:18","2020-08-28","","15","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: 1","","/Users/jacquesthibodeau/Zotero/storage/BAWTRM4V/O’Keefe - Antitrust-Compliant AI Industry Self-Regulation.pdf","","MetaSafety; FHI; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8XPF7SVG","blogPost","2017","Christiano, Paul","Approval-maximizing representations","AI Alignment (Medium)","","","","https://ai-alignment.com/approval-maximizing-representations-56ee6a6a1fe6","If we train our agents with human oversight, can they learn superhuman representations?","2017-07-02","2022-01-30 04:58:18","2022-01-30 04:58:18","2020-12-11 22:48:22","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/TZAJ965T/approval-maximizing-representations-56ee6a6a1fe6.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ECTQJIG2","blogPost","2020","Hubinger, Evan","Weak HCH accesses EXP","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/CtGH3yEoo4mY2taxe/weak-hch-accesses-exp","This post is a follow-up to my “Alignment proposals and complexity classes” post. Thanks to Sam Eisenstat for helping with part of the proof here. Previously, I proved that imitative amplification with weak HCH, approval-based amplification, and recursive reward modeling access PSPACE while AI safety via market making accesses EXP. At the time, I wasn't sure whether my market making proof would generalize to the others, so I just published it with the PSPACE  proofs instead. However, I have since become convinced that the proof does generalize—and that it generalizes for all of the proposals I mentioned—such that imitative amplification with weak HCH, approval-based amplification, and recursive reward modeling all actually access EXP. This post attempts to prove that. UPDATED LIST OF PROPOSALS BY COMPLEXITY CLASS P: Imitation learning (trivial) PSPACE: AI safety via debate (proof) EXP: AI safety via market making (proof), Imitative amplification with weak HCH  (proof below), Approval-based amplification (proof below), Recursive reward modeling (proof below) NEXP: Debate with cross-examination (proof) R: Imitative amplification with strong HCH (proof), AI safety via market making with pointers (proof) PROOFS IMITATIVE AMPLIFICATION WITH WEAK HCH ACCESSES EXP The proof here is similar in structure to my previous proof that weak HCH accesses PSPACE, so I'll only explain where this proof differs from that one. First, since l∈EXP, we know that for any x∈X, Tl(x) halts in O(2poly(n)) steps where n=|x|. Thus, we can construct a function fl(n)=c1+c2ec3nc4 such that for all x∈X, Tl(x) halts in less than or equal to fl(x) steps by picking c3,c4 large enough that they dominate all other terms in the polynomial for all n∈N. Note that fl is then computable in time polynomial in n. Second, let H's new strategy be as follows:  1. Given p, let s,x=M(p:f(|x|)). Then, return accept/reject based on whether s      is an accept or reject state (it will always be one or the oth","2020-07-22","2022-01-30 04:57:32","2022-01-30 04:57:32","2020-08-28 17:22:17","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/XA48EERN/weak-hch-accesses-exp.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMINGD6C","conferencePaper","2015","Soares, Nate; Fallenstein, Benja","Toward Idealized Decision Theory","","","","","http://arxiv.org/abs/1507.01986","This paper motivates the study of decision theory as necessary for aligning smarter-than-human artificial systems with human interests. We discuss the shortcomings of two standard formulations of decision theory, and demonstrate that they cannot be used to describe an idealized decision procedure suitable for approximation by artificial systems. We then explore the notions of policy selection and logical counterfactuals, two recent insights into decision theory that point the way toward promising paths for future research.","2015-07-07","2022-01-30 04:57:32","2022-01-30 04:57:32","2019-12-19 02:58:41","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000036  0 J: 30 arXiv: 1507.01986","","/Users/jacquesthibodeau/Zotero/storage/X6T99AUG/Soares and Fallenstein - 2015 - Toward Idealized Decision Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/QCHDAWP4/Soares and Fallenstein - 2015 - Toward Idealized Decision Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/W66QDEEJ/1507.html; /Users/jacquesthibodeau/Zotero/storage/W9BGSZJX/1507.html","","TechSafety; MIRI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AGI-2015","","","","","","","","","","","","","","",""
"MRQB3GAA","report","2015","Fallenstein, Benja; Soares, Nate","Vingean Reﬂection: Reliable Reasoning for Self-Improving Agents","","","","","https://intelligence.org/files/VingeanReflection.pdf","Today, human-level machine intelligence is in the domain of futurism, but there is every reason to expect that it will be developed eventually. Once artiﬁcial agents become able to improve themselves further, they may far surpass human intelligence, making it vitally important to ensure that the result of an “intelligence explosion” is aligned with human interests. In this paper, we discuss one aspect of this challenge: ensuring that the initial agent’s reasoning about its future versions is reliable, even if these future versions are far more intelligent than the current reasoner. We refer to reasoning of this sort as Vingean reﬂection.","2015","2022-01-30 04:57:32","2022-01-30 04:57:32","","","","","","","","","","","","","Machine Intelligence Research Institute","","en","","","","","Zotero","","ZSCC: 0000016  5 J: 15","","/Users/jacquesthibodeau/Zotero/storage/PIFEUWZX/Fallenstein and Soares - Vingean Reﬂection Reliable Reasoning for Self-Imp.pdf","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WNBDCJ23","blogPost","2015","Yudkowsky, Eliezer","Unforeseen maximum","Arbital","","","","https://arbital.com/p/unforeseen_maximum/","When you tell AI to produce world peace and it kills everyone.  (Okay, some SF writers saw that one coming.)","2015","2022-01-30 04:57:32","2022-01-30 04:57:32","2021-02-06 17:14:11","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6EWGR3AT/unforeseen_maximum.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PS7P96DU","blogPost","2013","Muehlhauser, Luke","Transparency in Safety-Critical Systems","Machine Intelligence Research Institute","","","","https://intelligence.org/2013/08/25/transparency-in-safety-critical-systems/","In this post, I aim to summarize one common view on AI transparency and AI reliability. It’s difficult to identify the field’s “consensus” on AI transparency and reliability, so instead I will present a common view so that I can use it to introduce a number of complications and open questions that (I think) warrant... Read more »","2013-08-25","2022-01-30 04:57:32","2022-01-30 04:57:32","2021-01-23 20:46:20","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/STGR44FK/transparency-in-safety-critical-systems.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T2EBE6EA","manuscript","2017","Critch, Andrew","Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making","","","","","http://arxiv.org/abs/1701.01302","Existing multi-objective reinforcement learning (MORL) algorithms do not account for objectives that arise from players with differing beliefs. Concretely, consider two players with different beliefs and utility functions who may cooperate to build a machine that takes actions on their behalf. A representation is needed for how much the machine's policy will prioritize each player's interests over time. Assuming the players have reached common knowledge of their situation, this paper derives a recursion that any Pareto optimal policy must satisfy. Two qualitative observations can be made from the recursion: the machine must (1) use each player's own beliefs in evaluating how well an action will serve that player's utility function, and (2) shift the relative priority it assigns to each player's expected utilities over time, by a factor proportional to how well that player's beliefs predict the machine's inputs. Observation (2) represents a substantial divergence from na\""{i}ve linear utility aggregation (as in Harsanyi's utilitarian theorem, and existing MORL algorithms), which is shown here to be inadequate for Pareto optimal sequential decision-making on behalf of players with different beliefs.","2017-01-05","2022-01-30 04:57:32","2022-01-30 04:57:32","2018-12-09 18:04:21","","","","","","","Toward negotiable reinforcement learning","","","","","","","","","","","","arXiv.org","","ZSCC: 0000010  arXiv: 1701.01302","","/Users/jacquesthibodeau/Zotero/storage/2BGZF92M/Critch - 2017 - Toward negotiable reinforcement learning shifting.pdf; /Users/jacquesthibodeau/Zotero/storage/3CSDDA3C/Critch - 2017 - Toward negotiable reinforcement learning shifting.pdf; /Users/jacquesthibodeau/Zotero/storage/DA62XPDG/1701.html; /Users/jacquesthibodeau/Zotero/storage/2INTMZQS/1701.html; /Users/jacquesthibodeau/Zotero/storage/5HC8HVDG/1701.html","","CHAI; TechSafety; MIRI","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6Q44U9I6","blogPost","2018","Demski, Abram","Toward a New Technical Explanation of Technical Explanation","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/tKwJQbo6SfWF2ifKh/toward-a-new-technical-explanation-of-technical-explanation","A NEW FRAMEWORK (Thanks to Valentine for a discussion leading to this post, and thanks to CFAR for running the CFAR-MIRI cross-fertilization workshop. Val provided feedback on a version of this post. Warning: fairly long.) Eliezer's A Technical Explanation of Technical Explanation, and moreover the sequences as a whole, used the best technical understanding of practical epistemology available at the time* -- the Bayesian account -- to address the question of how humans can try to arrive at better beliefs in practice. The sequences also pointed out several holes in this understanding, mainly having to do with logical uncertainty and reflective consistency. MIRI's research program has since then made major progress on logical uncertainty. The new understanding of epistemology -- the theory of logical induction -- generalizes the Bayesian account by eliminating the assumption of logical omniscience. Bayesian belief updates are recovered as a special case, but the dynamics of belief change are non-Bayesian in general. While it might not turn out to be the last word on the problem of logical uncertainty, it has a large number of desirable properties, and solves many problems in a unified and relatively clean framework. It seems worth asking what consequences this theory has for practical rationality. Can we say new things about what good reasoning looks like in humans, and how to avoid pitfalls of reasoning? First, I'll give a shallow overview of logical induction and possible implications for practical epistemic rationality. Then, I'll focus on the particular question of A Technical Explanation of Technical Explanation (which I'll abbreviate TEOTE from now on). Put in CFAR terminology, I'm seeking a gears-level understanding of gears-level understanding. I focus on the intuitions, with only a minimal account of how logical induction helps make that picture work. LOGICAL INDUCTION There are a number of difficulties in applying Bayesian uncertainty to logic. No compu","2018-02-15","2022-01-30 04:57:32","2022-01-30 04:57:32","2021-02-06 18:43:19","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/THG8KAMT/toward-a-new-technical-explanation-of-technical-explanation.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZE6E5SCK","blogPost","2020","Christiano, Paul","Better priors as a safety problem","AI Alignment (Medium)","","","","https://ai-alignment.com/better-priors-as-a-safety-problem-24aa1c300710","Many universal priors are inefficient in the finite data regime. I argue that’s a safety problem and we should try to fix it directly.","2020-07-05","2022-01-30 04:57:26","2022-01-30 04:57:26","2020-08-28 17:38:06","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/A4U2ZXVR/better-priors-as-a-safety-problem-24aa1c300710.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FS5KIAHS","manuscript","2016","Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan","Concrete Problems in AI Safety","","","","","http://arxiv.org/abs/1606.06565","Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.","2016-07-25","2022-01-30 04:57:26","2022-01-30 04:57:26","2019-12-16 20:16:07","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0001335  arXiv: 1606.06565","","/Users/jacquesthibodeau/Zotero/storage/SB8ZRSGM/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/3RX9H74D/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/UAW8RNPB/1606.html; /Users/jacquesthibodeau/Zotero/storage/9T9F6RZW/1606.html","","TechSafety; Open-AI","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T8H27GXJ","conferencePaper","2017","Achiam, Joshua; Held, David; Tamar, Aviv; Abbeel, Pieter","Constrained policy optimization","Proceedings of the 34th International Conference on Machine Learning","","","","","","2017","2022-01-30 04:57:26","2022-01-30 04:57:26","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000549","","/Users/jacquesthibodeau/Zotero/storage/SJSX5V9Z/Achiam et al. - 2017 - Constrained policy optimization.pdf; /Users/jacquesthibodeau/Zotero/storage/ZKPZ3KVQ/1705.html","","TechSafety; Open-AI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DHWDQS39","blogPost","2018","Christiano, Paul","Directions and desiderata for AI alignment","AI Alignment (Medium)","","","","https://ai-alignment.com/directions-and-desiderata-for-ai-control-b60fca0da8f4","I lay out three research directions in AI alignment, and three desiderata that I think should guide research in these areas.","2018-05-12","2022-01-30 04:57:26","2022-01-30 04:57:26","2020-12-11 22:48:28","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QWQ4D45X/directions-and-desiderata-for-ai-control-b60fca0da8f4.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G6B9J8DB","report","2020","O’Keefe, Cullen","How Will National Security Considerations Affect Antitrust Decisions in AI? An Examination of Historical Precedents","","","","","","","2020-07-07","2022-01-30 04:57:26","2022-01-30 04:57:26","","39","","","","","","","","","","","Future of Humanity Institute","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: 2","","/Users/jacquesthibodeau/Zotero/storage/IIGGAJGC/O’Keefe - How Will National Security Considerations Affect A.pdf","","MetaSafety; FHI; Open-AI; BERI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3BS9AFFF","manuscript","2019","Ray, Alex; Achiam, Joshua; Amodei, Dario","Benchmarking Safe Exploration in Deep Reinforcement Learning","","","","","https://arxiv.org/abs/2007.01223","Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies by trial and error. In many environments, safety is a critical concern and certain errors are unacceptable: for example, robotics systems that interact with humans should never cause injury to the humans while exploring. While it is currently typical to train RL agents mostly or entirely in simulation, where safety concerns are minimal, we anticipate that challenges in simulating the complexities of the real world (such as human-AI interactions) will cause a shift towards training RL agents directly in the real world, where safety concerns are paramount. Consequently we take the position that safe exploration should be viewed as a critical focus area for RL research, and in this work we make three contributions to advance the study of safe exploration. First, building on a wide range of prior work on safe reinforcement learning, we propose to standardize constrained RL as the main formalism for safe exploration. Second, we present the Safety Gym benchmark suite, a new slate of high-dimensional continuous control environments for measuring research progress on constrained RL. Finally, we benchmark several constrained deep RL algorithms on Safety Gym environments to establish baselines that future work can build on.","2019","2022-01-30 04:57:26","2022-01-30 04:57:26","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000073","","/Users/jacquesthibodeau/Zotero/storage/KEPD9RAS/Ray et al. - Benchmarking Safe Exploration in Deep Reinforcemen.pdf","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ST23JMFC","blogPost","2017","Christiano, Paul","Benign model-free RL","AI Alignment (Medium)","","","","https://ai-alignment.com/benign-model-free-rl-4aae8c97e385","Reward learning, robustness, and amplification may be sufficient to train benign model-free RL agents.","2017-06-02","2022-01-30 04:57:26","2022-01-30 04:57:26","2020-12-11 22:48:25","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WUUBUMP6/benign-model-free-rl-4aae8c97e385.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3SB967BM","journalArticle","2021","Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore","Cooperative AI: machines must learn to find common ground","Nature","","","10.1038/d41586-021-01170-0","https://www.nature.com/articles/d41586-021-01170-0","To help humanity solve fundamental problems of cooperation, scientists need to reconceive artificial intelligence as deeply social.","2021-05","2022-01-30 04:57:26","2022-01-30 04:57:26","2021-11-14 18:21:21","33-36","","7857","593","","","Cooperative AI","","","","","","","en","2021 Nature","","","","www.nature.com","","ZSCC: 0000013  Bandiera_abtest: a Cg_type: Comment Number: 7857 Publisher: Nature Publishing Group Subject_term: Machine learning, Computer science, Society, Technology, Sociology, Human behaviour","","/Users/jacquesthibodeau/Zotero/storage/D64JDAA8/Dafoe et al. - 2021 - Cooperative AI machines must learn to find common.pdf; /Users/jacquesthibodeau/Zotero/storage/K6QHXTXT/d41586-021-01170-0.html","","MetaSafety","Computer science; Human behaviour; Machine learning; Society; Sociology; Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BATDPZ2G","conferencePaper","2017","Christiano, Paul; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario","Deep reinforcement learning from human preferences","Advances in Neural Information Processing Systems 30 (NIPS 2017)","","","","https://papers.nips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html","For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.","2017-07-13","2022-01-30 04:57:26","2022-01-30 04:57:26","2020-12-20","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000517  arXiv: 1706.03741","","/Users/jacquesthibodeau/Zotero/storage/GVD8IX8V/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/3QBPI6NK/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/MAB946S2/1706.html; /Users/jacquesthibodeau/Zotero/storage/P49S7RME/1706.html; /Users/jacquesthibodeau/Zotero/storage/4GQVDQIZ/1706.html","","TechSafety; Open-AI; DeepMind","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NIPS 2017","","","","","","","","","","","","","","",""
"IJ4UFE3M","manuscript","2021","Chen, Mark; Tworek, Jerry; Jun, Heewoo; Yuan, Qiming; Pinto, Henrique Ponde de Oliveira; Kaplan, Jared; Edwards, Harri; Burda, Yuri; Joseph, Nicholas; Brockman, Greg; Ray, Alex; Puri, Raul; Krueger, Gretchen; Petrov, Michael; Khlaaf, Heidy; Sastry, Girish; Mishkin, Pamela; Chan, Brooke; Gray, Scott; Ryder, Nick; Pavlov, Mikhail; Power, Alethea; Kaiser, Lukasz; Bavarian, Mohammad; Winter, Clemens; Tillet, Philippe; Such, Felipe Petroski; Cummings, Dave; Plappert, Matthias; Chantzis, Fotios; Barnes, Elizabeth; Herbert-Voss, Ariel; Guss, William Hebgen; Nichol, Alex; Paino, Alex; Tezak, Nikolas; Tang, Jie; Babuschkin, Igor; Balaji, Suchir; Jain, Shantanu; Saunders, William; Hesse, Christopher; Carr, Andrew N.; Leike, Jan; Achiam, Josh; Misra, Vedant; Morikawa, Evan; Radford, Alec; Knight, Matthew; Brundage, Miles; Murati, Mira; Mayer, Katie; Welinder, Peter; McGrew, Bob; Amodei, Dario; McCandlish, Sam; Sutskever, Ilya; Zaremba, Wojciech","Evaluating Large Language Models Trained on Code","","","","","http://arxiv.org/abs/2107.03374","We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.","2021-07-14","2022-01-30 04:57:26","2022-01-30 04:57:26","2021-10-31 22:37:39","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 36  arXiv: 2107.03374","","/Users/jacquesthibodeau/Zotero/storage/P7CPC96K/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf; /Users/jacquesthibodeau/Zotero/storage/DDJ2TIQ7/2107.html","","MetaSafety","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K9RNDEJE","blogPost","2016","Clark, Jack; Amodei, Dario","Faulty Reward Functions in the Wild","OpenAI","","","","https://openai.com/blog/faulty-reward-functions/","Reinforcement learning algorithms can break in surprising, counterintuitive ways. In this post we'll explore one failure mode, which is where you misspecify your reward function.","2016-12-22","2022-01-30 04:57:26","2022-01-30 04:57:26","2020-11-21 17:34:09","","","","","","","","","","","","","","en","","","","","","","ZSCC: 0000038","","/Users/jacquesthibodeau/Zotero/storage/FVVJ8XT8/faulty-reward-functions.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMZS3XGJ","manuscript","2019","Ziegler, Daniel M.; Stiennon, Nisan; Wu, Jeffrey; Brown, Tom B.; Radford, Alec; Amodei, Dario; Christiano, Paul; Irving, Geoffrey","Fine-tuning language models from human preferences","","","","","","","2019","2022-01-30 04:57:26","2022-01-30 04:57:26","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000091","","/Users/jacquesthibodeau/Zotero/storage/G39X234W/Ziegler et al. - 2019 - Fine-tuning language models from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/G5ISVZVZ/1909.html","","TechSafety; Open-AI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C2V4P5BG","blogPost","2021","Schulman, John","Frequent arguments about alignment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment","Here, I’ll review some arguments that frequently come up in discussions about alignment research, involving one person skeptical of the endeavor (called Skeptic) and one person advocating to do more of it (called Advocate). I mostly endorse the views of the Advocate, but the Skeptic isn't a strawman and makes some decent points. The dialog is mostly based on conversations I've had with people who work on machine learning but don't specialize in safety and alignment. This post has two purposes. First, I want to cache good responses to these questions, so I don't have to think about them each time the topic comes up. Second, I think it's useful for people who work on safety and alignment to be ready for the kind of pushback they'll get when pitching their work to others. Just to introduce myself, I'm a cofounder of OpenAI and lead a team that works on developing and applying reinforcement learning methods; we're working on improving truthfulness and reasoning abilities of language models. 1. DOES ALIGNMENT GET SOLVED AUTOMATICALLY AS OUR MODELS GET SMARTER? Skeptic: I think the alignment problem gets easier as our models get smarter. When we train sufficiently powerful generative models, they'll learn the difference between human smiles and human wellbeing; the difference between the truth and common misconceptions; and various concepts they'll need for aligned behavior. Given all of this internal knowledge, we just have to prompt them appropriately to get the desired behavior. For example, to get wise advice from a powerful language model, I just have to set up a conversation between myself and ""a wise and benevolent AI advisor."" Advocate: The wise AI advisor you described has some basic problems, and I'll get into those shortly. But more generally, prompting an internet-trained generative model (like raw GPT-3) is a very poor way of getting aligned behavior, and we can easily do much better. It'll occasionally do something reasonable, but that's not nearly good","2021-06-22","2022-01-30 04:57:26","2022-01-30 04:57:26","2021-11-14 18:45:41","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/CT4CHZBU/frequent-arguments-about-alignment.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7GVJT8S","blogPost","2021","Barnes, Beth","Imitative Generalisation (AKA 'Learning the Prior')","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/JKj5Krff5oKMb8TjT/imitative-generalisation-aka-learning-the-prior-1","TL;DR We want to be able to supervise models with superhuman knowledge of the world and how to manipulate it. For this we need an overseer to be able to learn or access all the knowledge our models have, in order to be able to understand the consequences of suggestions or decisions from the model. If the overseers don’t have access to all the same knowledge as the model, it may be easy for the model to deceive us, suggesting plans that look good to us but that may have serious negative consequences. We might hope to access what the model knows just by training it to answer questions. However, we can only train on questions that humans are able to answer[1]. This gives us a problem that’s somewhat similar to the standard formulation of transduction: we have some labelled training set (questions humans can answer), and we want to transfer to an unlabelled dataset (questions we care about), that may be differently distributed. We might hope that our models will naturally generalize correctly from easy-to-answer questions to the ones that we care about. However, a natural pathological generalisation is for our models to only give us ‘human-like’ answers to questions, even if it knows the best answer is different. If we only have access to these human-like answers to questions, that probably doesn’t give us enough information to supervise a superhuman model. What we’re going to call ‘Imitative Generalization’ is a possible way to narrow the gap between the things our model knows, and the questions we can train our model to answer honestly. It avoids the pathological generalisation by only using ML for IID tasks, and imitating the way humans generalize. This hopefully gives us answers that are more like ‘how a human would answer if they’d learnt from all the data the model has learnt from’. We supervise how the model does the transfer, to get the sort of generalisation we want. It’s worth noting there are enough serious open questions that imitative generalization is","2021","2022-01-30 04:57:25","2022-01-30 04:57:25","2021-11-13 19:36:31","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/U82V8QHX/imitative-generalisation-aka-learning-the-prior-1.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HA5ICP5K","blogPost","2018","Christiano, Paul","Implicit extortion","AI Alignment (Medium)","","","","https://ai-alignment.com/implicit-extortion-3c80c45af1e3","Extortion can be equally effective, and harder to notice, when you don’t tell the target it’s occurring.","2018-04-13","2022-01-30 04:57:25","2022-01-30 04:57:25","2020-11-14 03:13:26","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BN9HHUAJ/implicit-extortion-3c80c45af1e3.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X9RN5I7J","blogPost","2018","Tom B Brown; Catherine Olsson","Introducing the Unrestricted Adversarial Examples Challenge","Google AI Blog","","","","http://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html","Posted by Tom B. Brown and Catherine Olsson, Research Engineers, Google Brain Team   Machine learning is being deployed in more and more rea...","2018","2022-01-30 04:57:25","2022-01-30 04:57:25","2020-12-13 22:18:26","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/8H8Q7D4X/introducing-unrestricted-adversarial.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FZ3JURGJ","manuscript","2020","Mishra, Saurabh; Clark, Jack; Perrault, C. Raymond","Measurement in AI Policy: Opportunities and Challenges","","","","","http://arxiv.org/abs/2009.09071","As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area.","2020-09-10","2022-01-30 04:57:25","2022-01-30 04:57:25","2020-11-14 00:54:58","","","","","","","Measurement in AI Policy","","","","","","","","","","","","arXiv.org","","ZSCC: 0000008  arXiv: 2009.09071","","/Users/jacquesthibodeau/Zotero/storage/RKJ6AFAG/Mishra et al. - 2020 - Measurement in AI Policy Opportunities and Challe.pdf; /Users/jacquesthibodeau/Zotero/storage/GSWE2PCX/2009.html","","MetaSafety; Open-AI; AmbiguosSafety","Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4QRN39NP","manuscript","2020","Ndousse, Kamal; Eck, Douglas; Levine, Sergey; Jaques, Natasha","Multi-agent Social Reinforcement Learning Improves Generalization","","","","","http://arxiv.org/abs/2010.00581","Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can use social learning to improve their performance using cues from other agents. We find that in most circumstances, vanilla model-free RL agents do not use social learning, even in environments in which individual exploration is expensive. We analyze the reasons for this deficiency, and show that by introducing a model-based auxiliary loss we are able to train agents to lever-age cues from experts to solve hard exploration tasks. The generalized social learning policy learned by these agents allows them to not only outperform the experts with which they trained, but also achieve better zero-shot transfer performance than solo learners when deployed to novel environments with experts. In contrast, agents that have not learned to rely on social learning generalize poorly and do not succeed in the transfer task. Further,we find that by mixing multi-agent and solo training, we can obtain agents that use social learning to out-perform agents trained alone, even when experts are not avail-able. This demonstrates that social learning has helped improve agents' representation of the task itself. Our results indicate that social learning can enable RL agents to not only improve performance on the task at hand, but improve generalization to novel environments.","2020-10-01","2022-01-30 04:57:25","2022-01-30 04:57:25","2020-11-14 00:37:58","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000002  arXiv: 2010.00581","","/Users/jacquesthibodeau/Zotero/storage/2I9Q72Z2/Ndousse et al. - 2020 - Multi-agent Social Reinforcement Learning Improves.pdf; /Users/jacquesthibodeau/Zotero/storage/7DA3AW5U/2010.html","","CHAI; TechSafety; Open-AI; AmbiguosSafety","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WZUHK5GA","journalArticle","2021","Goh, Gabriel; Cammarata, Nick; Voss, Chelsea; Carter, Shan; Petrov, Michael; Schubert, Ludwig; Radford, Alec; Olah, Chris","Multimodal Neurons in Artificial Neural Networks","Distill","","2476-0757","10.23915/distill.00030","https://distill.pub/2021/multimodal-neurons","","2021-03-04","2022-01-30 04:57:25","2022-01-30 04:57:25","2021-10-31 22:39:28","10.23915/distill.00030","","3","6","","Distill","","","","","","","","","","","","","DOI.org (Crossref)","","ZSCC: 0000034","","","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JPVIIMSV","manuscript","2021","Wu, Jeff; Ouyang, Long; Ziegler, Daniel M.; Stiennon, Nisan; Lowe, Ryan; Leike, Jan; Christiano, Paul","Recursively Summarizing Books with Human Feedback","","","","","http://arxiv.org/abs/2109.10862","A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.","2021-09-27","2022-01-30 04:57:25","2022-01-30 04:57:25","2021-10-31 22:37:22","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000004  arXiv: 2109.10862","","/Users/jacquesthibodeau/Zotero/storage/XN8J3757/Wu et al. - 2021 - Recursively Summarizing Books with Human Feedback.pdf; /Users/jacquesthibodeau/Zotero/storage/53895JGP/2109.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4V8R5X9I","conferencePaper","2019","Clark, Jack; Hadﬁeld, Gillian K","Regulatory Markets for AI Safety","","","","","https://arxiv.org/abs/2001.00078","We propose a new model for regulation to achieve AI safety: global regulatory markets. We ﬁrst sketch the model in general terms and provide an overview of the costs and beneﬁts of this approach. We then demonstrate how the model might work in practice: responding to the risk of adversarial attacks on AI models employed in commercial drones.","2019","2022-01-30 04:57:25","2022-01-30 04:57:25","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000020","","/Users/jacquesthibodeau/Zotero/storage/ZX28GC8Q/Clark and Hadfield - 2019 - Regulatory Markets for AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/JZ2QJ9K9/2001.html; /Users/jacquesthibodeau/Zotero/storage/G9J25559/Clark and Hadﬁeld - 2019 - REGULATORY MARKETS FOR AI SAFETY.pdf","","MetaSafety; Open-AI","Computer Science - Computers and Society; Economics - General Economics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Safe Machine Learning workshop at ICLR, 2019","","","","","","","","","","","","","","",""
"2FGGZESV","conferencePaper","2020","Stooke, Adam; Achiam, Joshua; Abbeel, Pieter","Responsive safety in reinforcement learning by pid lagrangian methods","International Conference on Machine Learning","","","","","","2020","2022-01-30 04:57:25","2022-01-30 04:57:25","","9133–9143","","","","","","","","","","","PMLR","","","","","","","Google Scholar","","ZSCC: 0000043","","/Users/jacquesthibodeau/Zotero/storage/WBNK9Z5V/Stooke et al. - 2020 - Responsive safety in reinforcement learning by pid.pdf; /Users/jacquesthibodeau/Zotero/storage/I5C469CU/stooke20a.html","","TechSafety; Open-AI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QHGKPS7W","blogPost","2018","Christiano, Paul","Techniques for optimizing worst-case performance","AI Alignment (Medium)","","","","https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99","Optimizing neural networks for worst-case performance looks really hard. Here’s why I have hope.","2018-02-02","2022-01-30 04:57:25","2022-01-30 04:57:25","2020-12-13 22:20:08","","","","","","","","","","","","","","en","","","","","","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/ET67S8GH/techniques-for-optimizing-worst-case-performance-39eafec74b99.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QG2D78EJ","blogPost","2018","Christiano, Paul","The easy goal inference problem is still hard","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard","Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin’s note: In this post (original here), Paul Christiano analyzes the ambitious value learning approach. He considers a more general view of ambitious value learning where you infer preferences more generally (i.e. not necessarily in the form of a utility function), and you can ask the user about their preferences, but it’s fine to imagine that you infer a utility function from data and then optimize it. The key takeaway is that in order to infer preferences that can lead to superhuman performance, it is necessary to understand how humans are biased, which seems very hard to do even with infinite data. -------------------------------------------------------------------------------- One approach to the AI control problem goes like this:  1. Observe what the user of the system says and does.  2. Infer the user’s preferences.  3. Try to make the world better according to the user’s preference, perhaps     while working alongside the user and asking clarifying questions. This approach has the major advantage that we can begin empirical work today — we can actually build systems which observe user behavior, try to figure out what the user wants, and then help with that. There are many applications that people care about already, and we can set to work on making rich toy models. It seems great to develop these capabilities in parallel with other AI progress, and to address whatever difficulties actually arise, as they arise. That is, in each domain where AI can act effectively, we’d like to ensure that AI can also act effectively in the service of goals inferred from users (and that this inference is good enough to support foreseeable applications). This approach gives us a nice, concrete model of each difficulty we are trying to address. It also provides a relatively clear indicator of whether our ability to control AI lags behind our ability to build it. And by being technically interesting an","2018","2022-01-30 04:57:25","2022-01-30 04:57:25","2020-12-17 04:36:24","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/8RHW83JV/h9DesGT3WT9u2k7Hr.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W2WEFH9K","blogPost","2020","Christiano, Paul","Inaccessible information","AI Alignment (Medium)","","","","https://ai-alignment.com/inaccessible-information-c749c6a88ce","What kind of information might be hard to elicit from ML models?","2020-06-03","2022-01-30 04:57:25","2022-01-30 04:57:25","2020-08-31 18:11:57","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BX8MMB8R/inaccessible-information-c749c6a88ce.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KMMWBRTC","blogPost","2019","Christiano, Paul","Informed oversight","AI Alignment (Medium)","","","","https://ai-alignment.com/informed-oversight-18fcb5d3d1e1","An overseer can provide adequate rewards for an agent if they know everything the agent knows. (Update of a 2016 post.)","2019-01-24","2022-01-30 04:57:25","2022-01-30 04:57:25","2020-11-14 03:13:48","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/TNH2CIIC/informed-oversight-18fcb5d3d1e1.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D94ADJCV","blogPost","2020","Christiano, Paul","Learning the prior","AI Alignment (Medium)","","","","https://ai-alignment.com/learning-the-prior-48f61b445c04","I suggest using neural nets to approximate our real prior, rather than implicitly using neural nets themselves as the prior.","2020-07-05","2022-01-30 04:57:25","2022-01-30 04:57:25","2020-08-28 17:40:46","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/NHBV8RXZ/learning-the-prior-48f61b445c04.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BXX6UCUX","journalArticle","2020","Stiennon, Nisan; Ouyang, Long; Wu, Jeffrey; Ziegler, Daniel; Lowe, Ryan; Voss, Chelsea; Radford, Alec; Amodei, Dario; Christiano, Paul F.","Learning to summarize with human feedback","Advances in Neural Information Processing Systems","","","","","","2020","2022-01-30 04:57:25","2022-01-30 04:57:25","","","","","33","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000005[s0]","","/Users/jacquesthibodeau/Zotero/storage/QH3WZ2FT/Stiennon et al. - 2020 - Learning to summarize with human feedback.pdf; /Users/jacquesthibodeau/Zotero/storage/GFUVXB6G/1f89885d556929e98d3ef9b86448f951-Abstract.html","","TechSafety; Open-AI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QXDU7VZD","blogPost","2020","O'Keefe, Cullen","Parallels Between AI Safety by Debate and Evidence Law","Cullen O'Keefe","","","","https://cullenokeefe.com/blog/debate-evidence","","2020-07-20","2022-01-30 04:57:25","2022-01-30 04:57:25","2020-08-28 17:19:24","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/4K3SKBKI/debate-evidence.html","","MetaSafety; FHI; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VGRU4IVI","manuscript","2021","Hernandez, Danny; Kaplan, Jared; Henighan, Tom; McCandlish, Sam","Scaling Laws for Transfer","","","","","http://arxiv.org/abs/2102.01293","We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data ""transferred"" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in these power-laws correspond to measures of the generality of a model and proximity of distributions (in a directed rather than symmetric sense). We find that pre-training effectively multiplies the fine-tuning dataset size. Transfer, like overall performance, scales predictably in terms of parameters, data, and compute.","2021-02-01","2022-01-30 04:57:25","2022-01-30 04:57:25","2021-11-13 22:37:55","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000019  arXiv: 2102.01293","","/Users/jacquesthibodeau/Zotero/storage/E5B34BKD/Hernandez et al. - 2021 - Scaling Laws for Transfer.pdf; /Users/jacquesthibodeau/Zotero/storage/GXQAZ3BT/2102.html","","MetaSafety","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"46ZCX4VI","manuscript","2018","Christiano, Paul; Shlegeris, Buck; Amodei, Dario","Supervising strong learners by amplifying weak experts","","","","","http://arxiv.org/abs/1810.08575","Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.","2018-10-19","2022-01-30 04:57:25","2022-01-30 04:57:25","2019-12-16 20:06:57","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000028  arXiv: 1810.08575","","/Users/jacquesthibodeau/Zotero/storage/K9A8KU7E/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf; /Users/jacquesthibodeau/Zotero/storage/5X96ZJNE/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf; /Users/jacquesthibodeau/Zotero/storage/JBIIC37H/1810.html; /Users/jacquesthibodeau/Zotero/storage/HUISI8Z7/1810.html","","TechSafety; Open-AI","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V9BGAUCZ","manuscript","2019","Askell, Amanda; Brundage, Miles; Hadfield, Gillian","The Role of Cooperation in Responsible AI Development","","","","","http://arxiv.org/abs/1907.04534","In this paper, we argue that competitive pressures could incentivize AI companies to underinvest in ensuring their systems are safe, secure, and have a positive social impact. Ensuring that AI systems are developed responsibly may therefore require preventing and solving collective action problems between companies. We note that there are several key factors that improve the prospects for cooperation in collective action problems. We use this to identify strategies to improve the prospects for industry cooperation on the responsible development of AI.","2019-07-10","2022-01-30 04:57:24","2022-01-30 04:57:24","2019-12-16 20:04:39","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000025  arXiv: 1907.04534","","/Users/jacquesthibodeau/Zotero/storage/QQFIEMRW/Askell et al. - 2019 - The Role of Cooperation in Responsible AI Developm.pdf; /Users/jacquesthibodeau/Zotero/storage/9N5T94H5/1907.html","","MetaSafety; Open-AI","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; K.1; K.4.1","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S9URNKK4","blogPost","2019","Christiano, Paul","The strategy-stealing assumption","AI Alignment (Medium)","","","","https://ai-alignment.com/the-strategy-stealing-assumption-a26b8b1ed334","If humans initially control 99% of the world’s resources, when can they secure 99% of the long-term influence?","2019-09-15","2022-01-30 04:57:24","2022-01-30 04:57:24","2020-12-11 22:48:18","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/9QXT7R5V/the-strategy-stealing-assumption-a26b8b1ed334.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IH56UUUI","manuscript","2020","Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew; Bluemke, Emma; Lebensold, Jonathan; O'Keefe, Cullen; Koren, Mark; Ryffel, Théo; Rubinovitz, J. B.; Besiroglu, Tamay; Carugati, Federica; Clark, Jack; Eckersley, Peter; de Haas, Sarah; Johnson, Maritza; Laurie, Ben; Ingerman, Alex; Krawczuk, Igor; Askell, Amanda; Cammarota, Rosario; Lohn, Andrew; Krueger, David; Stix, Charlotte; Henderson, Peter; Graham, Logan; Prunkl, Carina; Martin, Bianca; Seger, Elizabeth; Zilberman, Noa; hÉigeartaigh, Seán Ó; Kroeger, Frens; Sastry, Girish; Kagan, Rebecca; Weller, Adrian; Tse, Brian; Barnes, Elizabeth; Dafoe, Allan; Scharre, Paul; Herbert-Voss, Ariel; Rasser, Martijn; Sodhani, Shagun; Flynn, Carrick; Gilbert, Thomas Krendl; Dyer, Lisa; Khan, Saif; Bengio, Yoshua; Anderljung, Markus","Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims","","","","","http://arxiv.org/abs/2004.07213","With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.","2020-04-20","2022-01-30 04:57:24","2022-01-30 04:57:24","2020-08-18 21:36:21","","","","","","","Toward Trustworthy AI Development","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 92  arXiv: 2004.07213","","/Users/jacquesthibodeau/Zotero/storage/X6SQZ4UA/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf; /Users/jacquesthibodeau/Zotero/storage/86G3DTC7/2004.html; /Users/jacquesthibodeau/Zotero/storage/UPFBKGTU/2004.html","","MetaSafety; CHAI; CFI; CSER; CSET; FHI; Open-AI","Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IJFT7TDI","blogPost","2019","Christiano, Paul","Towards formalizing universality","AI Alignment (Medium)","","","","https://ai-alignment.com/towards-formalizing-universality-409ab893a456","An attempt to formalize universality as “able to understand anything that any computation can understand.”","2019-01-11","2022-01-30 04:57:24","2022-01-30 04:57:24","2020-11-14 03:13:36","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/C7P8EW68/towards-formalizing-universality-409ab893a456.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGTN9FXU","manuscript","2019","Kang, Daniel; Sun, Yi; Brown, Tom; Hendrycks, Dan; Steinhardt, Jacob","Transfer of Adversarial Robustness Between Perturbation Types","","","","","http://arxiv.org/abs/1905.01034","We study the transfer of adversarial robustness of deep neural networks between different perturbation types. While most work on adversarial examples has focused on $L_\infty$ and $L_2$-bounded perturbations, these do not capture all types of perturbations available to an adversary. The present work evaluates 32 attacks of 5 different types against models adversarially trained on a 100-class subset of ImageNet. Our empirical results suggest that evaluating on a wide range of perturbation sizes is necessary to understand whether adversarial robustness transfers between perturbation types. We further demonstrate that robustness against one perturbation type may not always imply and may sometimes hurt robustness against other perturbation types. In light of these results, we recommend evaluation of adversarial defenses take place on a diverse range of perturbation types and sizes.","2019-05-03","2022-01-30 04:57:24","2022-01-30 04:57:24","2019-12-16 20:04:45","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000020  arXiv: 1905.01034","","/Users/jacquesthibodeau/Zotero/storage/78UQ6FFU/Kang et al. - 2019 - Transfer of Adversarial Robustness Between Perturb.pdf; /Users/jacquesthibodeau/Zotero/storage/R9H28U6H/1905.html","","TechSafety; Open-AI","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTJAHFN2","manuscript","2021","Evans, Owain; Cotton-Barratt, Owen; Finnveden, Lukas; Bales, Adam; Balwit, Avital; Wills, Peter; Righetti, Luca; Saunders, William","Truthful AI: Developing and governing AI that does not lie","","","","","http://arxiv.org/abs/2110.06674","In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.","2021-10-13","2022-01-30 04:57:24","2022-01-30 04:57:24","2021-11-18 23:51:54","","","","","","","Truthful AI","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2110.06674","","/Users/jacquesthibodeau/Zotero/storage/8ZUZCNDQ/Evans et al. - 2021 - Truthful AI Developing and governing AI that does.pdf; /Users/jacquesthibodeau/Zotero/storage/N8CRH3J9/2110.html","","TechSafety","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2.0; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HS4BKV9Q","manuscript","2021","Lin, Stephanie; Hilton, Jacob; Evans, Owain","TruthfulQA: Measuring How Models Mimic Human Falsehoods","","","","","http://arxiv.org/abs/2109.07958","We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. For example, the 6B-parameter GPT-J model was 17% less truthful than its 125M-parameter counterpart. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","2021-09-08","2022-01-30 04:57:24","2022-01-30 04:57:24","2021-11-18 23:35:58","","","","","","","TruthfulQA","","","","","","","","","","","","arXiv.org","","ZSCC: 0000005  arXiv: 2109.07958","","/Users/jacquesthibodeau/Zotero/storage/G39ZUQDN/Lin et al. - 2021 - TruthfulQA Measuring How Models Mimic Human False.pdf; /Users/jacquesthibodeau/Zotero/storage/GP7663XV/2109.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JB955CSH","blogPost","2018","Christiano, Paul","Two guarantees","AI Alignment (Medium)","","","","https://ai-alignment.com/two-guarantees-c4c03a6b434f","I suspect AI alignment should aim to separately establish good performance in the average case, and lack-of-malice in the worst case.","2018-04-09","2022-01-30 04:57:24","2022-01-30 04:57:24","2020-11-14 03:13:22","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/DW25F88R/two-guarantees-c4c03a6b434f.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M7RCGS4P","manuscript","2021","Tamkin, Alex; Brundage, Miles; Clark, Jack; Ganguli, Deep","Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models","","","","","http://arxiv.org/abs/2102.02503","On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.","2021-02-04","2022-01-30 04:57:24","2022-01-30 04:57:24","2021-10-31 22:40:49","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000008  arXiv: 2102.02503","","/Users/jacquesthibodeau/Zotero/storage/CUGQGQE9/Tamkin et al. - 2021 - Understanding the Capabilities, Limitations, and S.pdf","","MetaSafety","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JR64UTGB","blogPost","2019","Christiano, Paul","Universality and model-based RL","AI Alignment (Medium)","","","","https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd","Ascription universality may be very helpful for safe model-based RL, facilitating benign induction and “transparent” models.","2019-10-04","2022-01-30 04:57:24","2022-01-30 04:57:24","2020-12-11 22:48:17","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UTQURPQ7/universality-and-model-based-rl-b08701394ddd.html","","TechSafety; Open-AI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K7TJWZ6S","report","2018","Brundage, Miles; Avin, Shahar; Clark, Jack; Toner, Helen; Eckersley, Peter; Garfinkel, Ben; Dafoe, Allan; Scharre, Paul; Zeitzoff, Thomas; Filar, Bobby; Anderson, Hyrum; Roff, Heather; Allen, Gregory C.; Steinhardt, Jacob; Flynn, Carrick; hÉigeartaigh, Seán Ó; Beard, Simon; Belfield, Haydn; Farquhar, Sebastian; Lyle, Clare; Crootof, Rebecca; Evans, Owain; Page, Michael; Bryson, Joanna; Yampolskiy, Roman; Amodei, Dario","The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation","","","","","http://arxiv.org/abs/1802.07228","This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.","2018-02-20","2022-01-30 04:57:24","2022-01-30 04:57:24","2019-12-16 20:09:19","","","","","","","The Malicious Use of Artificial Intelligence","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 461  J: 237 arXiv: 1802.07228","","/Users/jacquesthibodeau/Zotero/storage/KIFTJJMF/Brundage et al. - 2018 - The Malicious Use of Artificial Intelligence Fore.pdf; /Users/jacquesthibodeau/Zotero/storage/99NDN3F3/1802.html; /Users/jacquesthibodeau/Zotero/storage/CURE87QE/1802.html; /Users/jacquesthibodeau/Zotero/storage/UC86THMI/1802.html","","MetaSafety; CFI; CSER; FHI; Open-AI; BERI","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"65J9UX26","conferencePaper","2021","Shah, Rohin; Wild, Cody; Wang, Steven H.; Alex, Neel; Houghton, Brandon; Guss, William; Mohanty, Sharada; Kanervisto, Anssi; Milani, Stephanie; Topin, Nicholay; Abbeel, Pieter; Russell, Stuart; Dragan, Anca","The MineRL BASALT Competition on Learning from Human Feedback","arXiv:2107.01969 [cs]","","","","http://arxiv.org/abs/2107.01969","The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve. The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations. Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.","2021-07-05","2022-01-30 04:57:24","2022-01-30 04:57:24","2021-11-14 18:53:31","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 2107.01969","","/Users/jacquesthibodeau/Zotero/storage/4HDQZTQE/Shah et al. - 2021 - The MineRL BASALT Competition on Learning from Hum.pdf; /Users/jacquesthibodeau/Zotero/storage/8DGVUWRF/2107.html","","TechSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2021","","","","","","","","","","","","","","",""
"XCDGQ3EW","blogPost","2019","Christiano, Paul","Universality and consequentialism within HCH","AI Alignment (Medium)","","","","https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd","One exotic reason HCH can fail to be universal is the emergence of malicious patterns of behavior; universality may help address this risk.","2019-01-10","2022-01-30 04:57:24","2022-01-30 04:57:24","2020-11-14 03:13:34","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/M72NERZD/universality-and-consequentialism-within-hch-c0bee00365bd.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BEESSIWV","blogPost","2019","Christiano, Paul","Universality and security amplification","AI Alignment (Medium)","","","","https://ai-alignment.com/universality-and-security-amplification-551b314a3bab","A slightly more detailed view of security amplification.","2019-01-03","2022-01-30 04:57:24","2022-01-30 04:57:24","2020-11-14 03:13:23","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/7F92Q22J/universality-and-security-amplification-551b314a3bab.html; /Users/jacquesthibodeau/Zotero/storage/3XIIMECQ/universality-and-security-amplification-551b314a3bab.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BJJSTE9N","manuscript","2021","Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob","Unsolved Problems in ML Safety","","","","","http://arxiv.org/abs/2109.13916","Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (""Robustness""), identifying hazards (""Monitoring""), steering ML systems (""Alignment""), and reducing hazards in deployment (""External Safety""). Throughout, we clarify each problem's motivation and provide concrete research directions.","2021-10-30","2022-01-30 04:57:23","2022-01-30 04:57:23","2021-11-18 23:47:25","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000004  arXiv: 2109.13916","","/Users/jacquesthibodeau/Zotero/storage/QZGMBZ4T/Hendrycks et al. - 2021 - Unsolved Problems in ML Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/Z3BW94QM/2109.html","","TechSafety; AmbiguousSafety","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V99RRXAV","blogPost","2019","Christiano, Paul","Worst-case guarantees","AI Alignment (Medium)","","","","https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d","Reviewing the prospects for training models to behave acceptably on all inputs, rather than just the training distribution.","2019-03-23","2022-01-30 04:57:23","2022-01-30 04:57:23","2020-11-14 03:13:32","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/IXPVJKZM/training-robust-corrigibility-ce0e0a3b9b4d.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8SK6TQ2N","blogPost","2020","Barnes, Beth; Christiano, Paul","Writeup: Progress on AI Safety via Debate","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1","This is a writeup of the research done by the ""Reflection-Humans"" team at OpenAI in Q3 and Q4 of 2019. During that period we investigated mechanisms that would allow evaluators to get correct and helpful answers from experts, without the evaluators themselves being expert in the domain of the questions. This follows from the original work on AI Safety via Debate and the call for research on human aspects of AI safety, and is also closely related to work on Iterated Amplification. AUTHORS AND ACKNOWLEDGEMENTS The main researchers on this project were Elizabeth Barnes, Paul Christiano, Long Ouyang and Geoffrey Irving. We are grateful to many others who offered ideas and feedback. In particular: the cross-examination idea was inspired by a conversation with Chelsea Voss; Adam Gleave had helpful ideas about the long computation problem; Jeff Wu, Danny Hernandez and Gretchen Krueger gave feedback on a draft; we had helpful conversations with Amanda Askell, Andreas Stuhlmüller and Joe Collman, as well as others on the Ought team and the OpenAI Reflection team. We’d also like to thank our contractors who participated in debate experiments, especially David Jones, Erol Akbaba, Alex Deam and Chris Painter. Oliver Habryka helped format and edit the document for the AI Alignment Forum. Note by Oliver: There is currently a bug with links to headings in a post, causing them to not properly scroll when clicked. Until that is fixed, just open those links in a new tab, which should scroll correctly. OVERVIEW Motivation As we apply ML to increasingly important and complex tasks, the problem of evaluating behaviour and providing a good training signal becomes more difficult. We already see examples of RL leading to undesirable behaviours that superficially ‘look good’ to human evaluators (see this collection of examples). One example from an OpenAI paper is an agent learning incorrect behaviours in a 3d simulator, because the behaviours look like the desired behaviour in the 2d","2020-02-05","2022-01-30 04:57:23","2022-01-30 04:57:23","2020-09-07 18:11:36","","","","","","","Writeup","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/K8SIBJDZ/writeup-progress-on-ai-safety-via-debate-1.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HAJA2B87","blogPost","2020","Christiano, Paul","“Unsupervised” translation as an (intent) alignment problem","AI Alignment (Medium)","","","","https://ai-alignment.com/unsupervised-translation-as-a-safety-problem-99ae1f9b6b68","Unsupervised translation is an interesting domain where models seem to “know” something we can’t get them to tell us.","2020-09-30","2022-01-30 04:57:23","2022-01-30 04:57:23","2020-12-11 22:48:20","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/W39ZJ8FT/unsupervised-translation-as-a-safety-problem-99ae1f9b6b68.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4AZ2N39G","blogPost","2019","Christiano, Paul","What failure looks like","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like","The stereotyped image of AI catastrophe is a powerful, malicious AI system that takes its creators by surprise and quickly achieves a decisive advantage over the rest of humanity. I think this is probably not what failure will look like, and I want to try to paint a more realistic picture. I’ll tell the story in two parts:  * Part I: machine learning will increase our ability to “get what we can    measure,” which could cause a slow-rolling catastrophe. (""Going out with a    whimper."")  * Part II: ML training, like competitive economies or natural ecosystems, can    give rise to “greedy” patterns that try to expand their own influence. Such    patterns can ultimately dominate the behavior of a system and cause sudden    breakdowns. (""Going out with a bang,"" an instance of optimization daemons    [https://arbital.com/p/daemons/].) I think these are the most important problems if we fail to solve intent alignment [https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6]. In practice these problems will interact with each other, and with other disruptions/instability caused by rapid progress. These problems are worse in worlds where progress is relatively fast, and fast takeoff can be a key risk factor, but I’m scared even if we have several years. With fast enough takeoff, my expectations start to look more like the caricature---this post envisions reasonably broad deployment of AI, which becomes less and less likely as things get faster. I think the basic problems are still essentially the same though, just occurring within an AI lab rather than across the world. (None of the concerns in this post are novel.) PART I: YOU GET WHAT YOU MEASURE If I want to convince Bob to vote for Alice, I can experiment with many different persuasion strategies and see which ones work. Or I can build good predictive models of Bob’s behavior and then search for actions that will lead him to vote for Alice. These are powerful techniques for achieving any goal that can be ea","2019","2022-01-30 04:57:23","2022-01-30 04:57:23","2019-12-16 19:59:17","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s4]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/KNVMK6EZ/what-failure-looks-like.html; /Users/jacquesthibodeau/Zotero/storage/WV669JT4/what-failure-looks-like.html","","TechSafety; Open-AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"96ZATX39","blogPost","2019","Kumar, Ramana; Garrabrant, Scott","Thoughts on Human Models","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models","Human values and preferences are hard to specify, especially in complex domains. Accordingly, much AGI safety research has focused on approaches to AGI design that refer to human values and preferences indirectly, by learning a model that is grounded in expressions of human values (via stated preferences, observed behaviour, approval, etc.) and/or real-world processes that generate expressions of those values. There are additionally approaches aimed at modelling or imitating other aspects of human cognition or behaviour without an explicit aim of capturing human preferences (but usually in service of ultimately satisfying them). Let us refer to all these models as human models. In this post, we discuss several reasons to be cautious about AGI designs that use human models. We suggest that the AGI safety research community put more effort into developing approaches that work well in the absence of human models, alongside the approaches that rely on human models. This would be a significant addition to the current safety research landscape, especially if we focus on working out and trying concrete approaches as opposed to developing theory. We also acknowledge various reasons why avoiding human models seems difficult. PROBLEMS WITH HUMAN MODELS To be clear about human models, we draw a rough distinction between our actual preferences (which may not be fully accessible to us) and procedures for evaluating our preferences. The first thing, actual preferences, is what humans actually want upon reflection. Satisfying our actual preferences is a win. The second thing, procedures for evaluating preferences, refers to various proxies for our actual preferences such as our approval, or what looks good to us (with necessarily limited information or time for thinking). Human models are in the second category; consider, as an example, a highly accurate ML model of human yes/no approval on the set of descriptions of outcomes. Our first concern, described below, is about overfit","2019-02-21","2022-01-30 04:56:59","2022-01-30 04:56:59","2021-02-06 18:50:50","","","","","","","","","","","","","","","","","","","","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/AH7XC8UQ/thoughts-on-human-models.html; /Users/jacquesthibodeau/Zotero/storage/I6Q88XDS/thoughts-on-human-models.html","","MetaSafety; DeepMind; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R78XUR43","blogPost","2017","Yudkowsky, Eliezer","There's No Fire Alarm for Artificial General Intelligence","Machine Intelligence Research Institute","","","","https://intelligence.org/2017/10/13/fire-alarm/","What is the function of a fire alarm?   One might think that the function of a fire alarm is to provide you with important evidence about a fire existing, allowing you to change your policy accordingly and exit the building. In the classic experiment by Latane and Darley in 1968, eight groups of... Read more »","2017-10-14","2022-01-30 04:56:59","2022-01-30 04:56:59","2020-12-13 20:50:15","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: 0000014","","/Users/jacquesthibodeau/Zotero/storage/V25W5CF3/fire-alarm.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JMSJ3UEH","conferencePaper","2015","Soares, Nate","The Value Learning Problem","Ethics for Artificial Intelligence Workshop at 25th International Joint Conference on Artificial Intelligence","","","","https://intelligence.org/files/ValueLearningProblem.pdf","Autonomous AI systems’ programmed goals can easily fall short of programmers’ intentions. Even a machine intelligent enough to understand its designers’ intentions would not necessarily act as intended. We discuss early ideas on how one might design smarter-than-human AI systems that can inductively learn what to value from labeled training data, and highlight questions about the construction of systems that model and act upon their operators’ preferences.","2015","2022-01-30 04:56:59","2022-01-30 04:56:59","","7","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000070  8 J: 38","","/Users/jacquesthibodeau/Zotero/storage/AAGIJGBD/Soares - The Value Learning Problem.pdf","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCAI 2016","","","","","","","","","","","","","","",""
"RMEBU6IN","bookSection","2014","Bostrom, Nick; Yudkowsky, Eliezer","The ethics of artificial intelligence","The Cambridge Handbook of Artificial Intelligence","978-1-139-04685-5","","","https://www.cambridge.org/core/product/identifier/CBO9781139046855A027/type/book_part","The possibility of creating thinking machines raises a host of ethical issues. These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves. The first section discusses issues that may arise in the near future of AI. The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence. The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status. In the fourth section, we consider how AIs might diﬀer from humans in certain basic respects relevant to our ethical assessment of them. The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill.","2014","2022-01-30 04:56:59","2022-01-30 04:56:59","2019-12-19 02:58:26","316-334","","","","","","","","","","","Cambridge University Press","Cambridge","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s6]  ACC: 815  J: 453  DOI: 10.1017/CBO9781139046855.020","","/Users/jacquesthibodeau/Zotero/storage/X8ANV64Q/Bostrom and Yudkowsky - 2014 - The ethics of artificial intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/UX5U5MWK/books.html","","TechSafety; FHI; MIRI","","Frankish, Keith; Ramsey, William M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRUQVJCW","journalArticle","2014","Armstrong, Stuart; Sotala, Kaj; Ó hÉigeartaigh, Seán S.","The errors, insights and lessons of famous AI predictions – and what they mean for the future","Journal of Experimental & Theoretical Artificial Intelligence","","0952-813X, 1362-3079","10.1080/0952813X.2014.895105","https://www.tandfonline.com/doi/full/10.1080/0952813X.2014.895105","","2014-07-03","2022-01-30 04:56:59","2022-01-30 04:56:59","2020-11-22 02:22:03","317-342","","3","26","","Journal of Experimental & Theoretical Artificial Intelligence","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000076","","/Users/jacquesthibodeau/Zotero/storage/3D5RG8IM/Armstrong et al. - 2014 - The errors, insights and lessons of famous AI pred.pdf","","MetaSafety; FHI; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"422HC9XW","blogPost","2002","Yudkowsky, Eliezer","The AI-Box Experiment","Eliezer S Yudkowsky","","","","https://www.yudkowsky.net/singularity/aibox","","2002","2022-01-30 04:56:59","2022-01-30 04:56:59","2020-11-21 17:49:14","","","","","","","","","","","","","","","","","","","","","ZSCC: 0000035","","/Users/jacquesthibodeau/Zotero/storage/97EN4T37/aibox.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8IB8KRA5","manuscript","2021","Garrabrant, Scott","Temporal Inference with Finite Factored Sets","","","","","http://arxiv.org/abs/2109.11513","We propose a new approach to temporal inference, inspired by the Pearlian causal inference paradigm - though quite different from Pearl's approach formally. Rather than using directed acyclic graphs, we make use of factored sets, which are sets expressed as Cartesian products. We show that finite factored sets are powerful tools for inferring temporal relations. We introduce an analog of d-separation for factored sets, conditional orthogonality, and we demonstrate that this notion is equivalent to conditional independence in all probability distributions on a finite factored set.","2021-09-23","2022-01-30 04:56:59","2022-01-30 04:56:59","2021-10-31 22:35:10","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 2109.11513","","/Users/jacquesthibodeau/Zotero/storage/PIE5EQ45/Garrabrant - 2021 - Temporal Inference with Finite Factored Sets.pdf; /Users/jacquesthibodeau/Zotero/storage/4QRAPMBD/2109.html","","TechSafety","Computer Science - Artificial Intelligence; Mathematics - Probability; Mathematics - Combinatorics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"97JPXM8D","blogPost","2015","Yudkowsky, Eliezer","Task-directed AGI","Arbital","","","","https://arbital.com/p/task_agi/","An advanced AI that's meant to pursue a series of limited-scope goals given it by the user.  In Bostrom's terminology, a Genie.","2015","2022-01-30 04:56:59","2022-01-30 04:56:59","2021-02-06 17:13:02","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A5HGSXIU","manuscript","2013","Yudkowsky, Eliezer; Herreshoff, Marcello","Tiling Agents for Self-Modifying AI, and the Löbian Obstacle","","","","","https://intelligence.org/files/TilingAgentsDraft.pdf","","2013-10-07","2022-01-30 04:56:59","2022-01-30 04:56:59","2020-11-21 17:18:36","","","","","","","","","","","","","","","","","","","","","ZSCC: 0000028","","/Users/jacquesthibodeau/Zotero/storage/24VG56W8/TilingAgentsDraft.pdf","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WI6FD4K9","bookSection","2012","Muehlhauser, Luke; Helm, Louie","The Singularity and Machine Ethics","Singularity Hypotheses","978-3-642-32559-5 978-3-642-32560-1","","","http://link.springer.com/10.1007/978-3-642-32560-1_6","","2012","2022-01-30 04:56:59","2022-01-30 04:56:59","2020-11-22 02:23:12","101-126","","","","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s3]  ACC: 94  Series Title: The Frontiers Collection DOI: 10.1007/978-3-642-32560-1_6","","","","TechSafety; MIRI","","Eden, Amnon H.; Moor, James H.; Søraker, Johnny H.; Steinhart, Eric","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QZKRN49F","blogPost","2018","Yudkowsky, Eliezer","The Rocket Alignment Problem","Machine Intelligence Research Institute","","","","https://intelligence.org/2018/10/03/rocket-alignment/","The following is a fictional dialogue building off of AI Alignment: Why It’s Hard, and Where to Start.   (Somewhere in a not-very-near neighboring world, where science took a very different course…)   ALFONSO:  Hello, Beth. I’ve noticed a lot of speculations lately about “spaceplanes” being used to attack cities, or possibly becoming infused with malevolent... Read more »","2018-10-03","2022-01-30 04:56:59","2022-01-30 04:56:59","2020-12-13 23:54:02","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/77I4C3AE/rocket-alignment.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HSCES72W","blogPost","2015","Yudkowsky, Eliezer","Sufficiently optimized agents appear coherent","Arbital","","","","https://arbital.com/p/optimized_agent_appears_coherent/","If you could think as well as a superintelligence, you'd be at least that smart yourself.","2015","2022-01-30 04:56:58","2022-01-30 04:56:58","2021-02-06 17:12:04","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/2II69UX2/optimized_agent_appears_coherent.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"238TWTH2","blogPost","2017","Yudkowsky, Eliezer","Separation from hyperexistential risk","Arbital","","","","https://arbital.com/p/hyperexistential_separation/","The AI should be widely separated in the design space from any AI that would constitute a ""hyperexistential risk"" (anything worse than death).","2017","2022-01-30 04:56:58","2022-01-30 04:56:58","2021-02-06 17:33:02","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/DEPHXC82/hyperexistential_separation.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DNQKCSRM","conferencePaper","2016","Everitt, Tom; Filan, Daniel; Daswani, Mayank; Hutter, Marcus","Self-Modification of Policy and Utility Function in Rational Agents","AGI 2016: Artificial General Intelligence","","","","http://arxiv.org/abs/1605.03142","Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify -- for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby `escaping' the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.","2016-05-10","2022-01-30 04:56:58","2022-01-30 04:56:58","2020-11-22 04:13:43","","","","","","","","Lecture Notes in Computer Science","","","","","","","","","","","arXiv.org","","ZSCC: 0000025  arXiv: 1605.03142","","/Users/jacquesthibodeau/Zotero/storage/9EW4V9AK/Everitt et al. - 2016 - Self-Modification of Policy and Utility Function i.pdf; /Users/jacquesthibodeau/Zotero/storage/9EXT3XCU/1605.html","","TechSafety; MIRI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Artificial General Intelligence","","","","","","","","","","","","","","",""
"B7375JA5","blogPost","2015","Soares, Nate","Safety engineering, target selection, and alignment theory","Machine Intelligence Research Institute","","","","https://intelligence.org/2015/12/31/safety-engineering-target-selection-and-alignment-theory/","Artificial intelligence capabilities research is aimed at making computer systems more intelligent — able to solve a wider range of problems more effectively and efficiently. We can distinguish this from research specifically aimed at making AI systems at various capability levels safer, or more “robust and beneficial.” In this post, I distinguish three kinds of direct... Read more »","2015-12-31","2022-01-30 04:56:58","2022-01-30 04:56:58","2020-11-21 17:07:20","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/XWWTIPKK/safety-engineering-target-selection-and-alignment-theory.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IW5QC4TT","blogPost","2018","Garrabrant, Scott","Robustness to Scale","LessWrong","","","","https://www.lesswrong.com/posts/bBdfbWfWxHN9Chjcq/robustness-to-scale","I want to quickly draw attention to a concept in AI alignment: Robustness to Scale. Briefly, you want your proposal for an AI to be robust (or at least fail gracefully) to changes in its level of capabilities. I discuss three different types of robustness to scale: robustness to scaling up, robustness to scaling down, and robustness to relative scale. The purpose of this post is to communicate, not to persuade. It may be that we want to bite the bullet of the strongest form of robustness to scale, and build an AGI that is simply not robust to scale, but if we do, we should at least realize that we are doing that. Robustness to scaling up means that your AI system does not depend on not being too powerful. One way to check for this is to think about what would happen if the thing that the AI is optimizing for were actually maximized. One example of failure of robustness to scaling up is when you expect an AI to accomplish a task in a specific way, but it becomes smart enough to find new creative ways to accomplish the task that you did not think of, and these new creative ways are disastrous. Another example is when you make an AI that is incentivized to do one thing, but you add restrictions that make it so that the best way to accomplish that thing has a side effect that you like. When you scale the AI up, it finds a way around your restrictions. Robustness to scaling down means that your AI system does not depend on being sufficiently powerful. You can't really make your system still work when it scales down, but you can maybe make sure it fails gracefully. For example, imagine you had a system that was trying to predict humans, and use these predictions to figure out what to do. When scaled up all the way, the predictions of humans are completely accurate, and it will only take actions that the predicted humans would approve of. If you scale down the capabilities, your system may predict the humans incorrectly. These errors may multiply as you stack many predi","2018-02-21","2022-01-30 04:56:58","2022-01-30 04:56:58","2021-02-06 18:49:10","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/Z6Q9GRDC/robustness-to-scale.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7KKMB7Q7","journalArticle","2015","Sotala, Kaj; Yampolskiy, Roman V","Responses to catastrophic AGI risk: a survey","Physica Scripta","","0031-8949, 1402-4896","10.1088/0031-8949/90/1/018001","https://iopscience.iop.org/article/10.1088/0031-8949/90/1/018001","","2015-01-01","2022-01-30 04:56:58","2022-01-30 04:56:58","2020-11-22 02:23:47","018001","","1","90","","Phys. Scr.","Responses to catastrophic AGI risk","","","","","","","","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 115","","/Users/jacquesthibodeau/Zotero/storage/7HQTGDPG/Sotala and Yampolskiy - 2015 - Responses to catastrophic AGI risk a survey.pdf","","MetaSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2TSM99QU","blogPost","2020","Demski, Abram","Radical Probabilism","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/xJyY5QkQvNJpZLJRo/radical-probabilism-1","This is an expanded version of my talk. I assume a high degree of familiarity with Bayesian probability theory. Toward a New Technical Explanation of Technical Explanation -- an attempt to convey the practical implications of logical induction -- was one of my most-appreciated posts, but I don't really get the feeling that very many people have received the update. Granted, that post was speculative, sketching what a new technical explanation of technical explanation might look like. I think I can do a bit better now. If the implied project of that post had really been completed, I would expect new practical probabilistic reasoning tools, explicitly violating Bayes' law. For example, we might expect:  * A new version of information theory. * An update to the ""prediction=compression"" maxim, either repairing it to       incorporate the new cases, or explicitly denying it and providing a good       intuitive account of why it was wrong.     * A new account of concepts such as       mutual information, allowing for the fact that variables have behavior       over thinking time; for example, variables may initially be very       correlated, but lose correlation as our picture of each variable becomes       more detailed.          * New ways of thinking about epistemology. * One thing that my post did manage       to do was to spell out the importance of ""making advanced predictions"", a       facet of epistemology which Bayesian thinking does not do justice to.     * However, I left aspects of the       problem of old evidence open, rather than giving a complete way to think       about it.          * New probabilistic structures. * Bayesian Networks are one really nice way to       capture the structure of probability distributions, making them much       easier to reason about. Is there anything similar for the new, wider space       of probabilistic reasoning which has been opened up?         Unfortunately, I still don't have any of those things to offer. The aim o","2020-08-18","2022-01-30 04:56:58","2022-01-30 04:56:58","2020-08-27 16:25:26","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/JPB8ZRTA/radical-probabilism-1.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NQQM74KV","bookSection","2015","Fallenstein, Benja; Kumar, Ramana","Proof-Producing Reflection for HOL","Interactive Theorem Proving","978-3-319-22101-4 978-3-319-22102-1","","","http://link.springer.com/10.1007/978-3-319-22102-1_11","We present a reﬂection principle of the form “If ϕ is provable, then ϕ” implemented in the HOL4 theorem prover, assuming the existence of a large cardinal. We use the large-cardinal assumption to construct a model of HOL within HOL, and show how to ensure ϕ has the same meaning both inside and outside of this model. Soundness of HOL implies that if ϕ is provable, then it is true in this model, and hence ϕ holds. We additionally show how this reﬂection principle can be extended, assuming an inﬁnite hierarchy of large cardinals, to implement model polymorphism, a technique designed for verifying systems with self-replacement functionality.","2015","2022-01-30 04:56:58","2022-01-30 04:56:58","2019-12-19 02:58:35","170-186","","","9236","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s7]  ACC: 13  J: 12 DOI: 10.1007/978-3-319-22102-1_11","","/Users/jacquesthibodeau/Zotero/storage/RJGGDF64/Fallenstein and Kumar - 2015 - Proof-Producing Reflection for HOL.pdf","","TechSafety; MIRI","","Urban, Christian; Zhang, Xingyuan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WBGESJZX","blogPost","2015","Yudkowsky, Eliezer","Patch resistance","Arbital","","","","https://arbital.com/p/patch_resistant/","One does not simply solve the value alignment problem.","2015","2022-01-30 04:56:58","2022-01-30 04:56:58","2021-02-06 17:10:55","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/SKWUI9EH/patch_resistant.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VAGVQFSF","blogPost","2018","Garrabrant, Scott","Optimization Amplifies","LessWrong","","","","https://www.lesswrong.com/posts/zEvqFtT4AtTztfYC4/optimization-amplifies","I talk here about how a mathematician mindset can be useful for AI alignment. But first, a puzzle: Given m, what is the least number n≥2 such that for 2≤k≤m, the base k  representation of n consists entirely of 0s and 1s?  If you want to think about it yourself, stop reading. For m=2, n=2.  For m=3, n=3. For m=4, n=4. For m=5, n=82,000. Indeed, 82,000 is 10100000001010000 in binary, 11011111001 in ternary, 110001100 in base 4, and 10111000 in base 5. What about when m=6? So, a mathematician might tell you that this is an open problem. It is not known if there is any n≥2 which consists of 0s and 1s in bases 2 through 6. A scientist, on the other hand, might just tell you that clearly no such number exists. There are 2k−1 numbers that consist of k 0s and 1s in base 6. Each of these has roughly log5(6)⋅k digits in base 5, and assuming things are roughly evenly distributed, each of these digits is a 0 or a 1 with ""probability"" 2/5. The ""probability"" that there is any number of length k that has the property is thus less than 2k⋅(2/5)k=(4/5)k. This means that as you increase k, the ""probability"" that you find a number with the property drops off exponentially, and this is not even considering bases 3 and 4. Also, we have checked all numbers up to 2000 digits. No number with this property exists. Who is right?  Well, they are both right. If you want to have fun playing games with proofs, you can consider it an open problem and try to prove it. If you want to get the right answer, just listen to the scientist. If you have to choose between destroying the world with a 1% probability and destroying the world if a number greater than 2 which consists of 0s and 1s in bases 2 through 6 exists, go with the latter. It is tempting to say that we might be in a situation similar to this. We need to figure out how to make safe AI, and we maybe don't have that much time. Maybe we need to run experiments, and figure out what is true about what we should do and not waste ou","2018-06-26","2022-01-30 04:56:58","2022-01-30 04:56:58","2021-02-06 18:45:57","","","","","","","","","","","","","","","","","","","","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/T4CJB5X5/optimization-amplifies.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QASC9Z4D","blogPost","2016","Yudkowsky, Eliezer","Task (AI goal)","Arbital","","","","https://arbital.com/p/task_goal/","When building the first AGIs, it may be wiser to assign them only goals that are bounded in space and time, and can be satisfied by bounded efforts.","2016","2022-01-30 04:56:58","2022-01-30 04:56:58","2021-02-06 17:18:21","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VSAFI8SC/task_goal.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4AJCWUID","blogPost","2020","Hubinger, Evan","Synthesizing amplification and debate","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/dJSD5RK6Qoidb3QY5/synthesizing-amplification-and-debate","BACKGROUND One possible way to train an amplification model is to use an auxiliary reinforcement learning objective to help guide the training of the amplification model. This could be done either by training two separate models, an agent and a question-answerer, or a single model trained on a joint objective. For example, from a comment Paul left on “A dilemma for prosaic AI alignment:” I normally imagine using joint training in these cases, rather than pre-training + fine-tuning. e.g., at every point in time we maintain an agent and a question-answerer, where the question-answerer ""knows everything the agent knows."" They get better together, with each gradient update affecting both of them, rather than first training a good agent and then adding a good question-answerer. (Independently of concerns about mesa-optimization, I think the fine-tuning approach would have trouble because you couldn't use statistical regularities from the ""main"" objective to inform your answers to questions, and therefore your question answers will be dumber than the policy and so you couldn't get a good reward function or specification of catastrophically bad behavior.) In my last post, I expressed skepticism of such non-imitative amplification approaches, though in this post I want to propose a possible way in which some of my concerns with this style of approach could addressed by integrating ideas from AI safety via debate. I'll start by describing the basic idea in broad terms, then give a more careful, technical description of the sort of training procedure I have in mind. THE PROPOSAL The basic idea is as follows: debate naturally yields an RL objective, so if you want to add an auxiliary RL objective to amplification, why not use the RL objective from debate? Specifically, the idea is to conduct a debate not between copies of the model M, but between copies of the amplified model Amp(M) (where  Amp(M) is a human with access to the model M). That gives you both an RL reward ari","2020-02-05","2022-01-30 04:56:58","2022-01-30 04:56:58","2020-09-07 18:16:00","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/XCB5N6K8/synthesizing-amplification-and-debate.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BX7UP66I","blogPost","2016","Yudkowsky, Eliezer","So Far: Unfriendly AI Edition","Econlib","","","","https://www.econlib.org/archives/2016/03/so_far_unfriend.html","Eliezer Yudkowsky responds to my “selective pessimism” challenge with another challenge.  Here he is, reprinted with his permission. Eliezer Yudkowsky responds to my “selective pessimism” challenge with another challenge.  Here he is, reprinted with his permission. Bryan Caplan issued the following challenge, naming Unfriendly AI as one among several disaster scenarios he thinks is unlikely: …","2016-03-29","2022-01-30 04:56:58","2022-01-30 04:56:58","2021-02-06 17:17:06","","","","","","","So Far","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A  Section: Cost-benefit Analysis","","/Users/jacquesthibodeau/Zotero/storage/BDQ6EDXU/so_far_unfriend.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CDZZ9WDE","conferencePaper","2015","Everitt, Tom; Leike, Jan; Hutter, Marcus","Sequential Extensions of Causal and Evidential Decision Theory","ADT 2015: Algorithmic Decision Theory","","","","http://arxiv.org/abs/1506.07359","Moving beyond the dualistic view in AI where agent and environment are separated incurs new challenges for decision making, as calculation of expected utility is no longer straightforward. The non-dualistic decision theory literature is split between causal decision theory and evidential decision theory. We extend these decision algorithms to the sequential setting where the agent alternates between taking actions and observing their consequences. We find that evidential decision theory has two natural extensions while causal decision theory only has one.","2015-06-24","2022-01-30 04:56:58","2022-01-30 04:56:58","2020-11-22 04:14:19","","","","","","","","Lecture Notes in Computer Science","","","","","","","","","","","arXiv.org","","ZSCC: 0000011  arXiv: 1506.07359","","/Users/jacquesthibodeau/Zotero/storage/ZPQ2R9HA/Everitt et al. - 2015 - Sequential Extensions of Causal and Evidential Dec.pdf; /Users/jacquesthibodeau/Zotero/storage/IXVQQIQD/1506.html","","TechSafety; MIRI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Algorithmic Decision Theory","","","","","","","","","","","","","","",""
"N9ZZVPT7","blogPost","2017","Yudkowsky, Eliezer","Security Mindset and the Logistic Success Curve","Machine Intelligence Research Institute","","","","https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/","Follow-up to:   Security Mindset and Ordinary Paranoia   (Two days later, Amber returns with another question.)   AMBER:  Uh, say, Coral. How important is security mindset when you’re building a whole new kind of system—say, one subject to potentially adverse optimization pressures, where you want it to have some sort of robustness property? CORAL:  How novel is the... Read more »","2017-11-26","2022-01-30 04:56:58","2022-01-30 04:56:58","2021-02-06 17:27:18","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: 0000002","","/Users/jacquesthibodeau/Zotero/storage/H9XPNAQI/security-mindset-and-the-logistic-success-curve.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8BWI6VK","blogPost","2017","Yudkowsky, Eliezer","Security Mindset and Ordinary Paranoia","Machine Intelligence Research Institute","","","","https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/","The following is a fictional dialogue building off of AI Alignment: Why It’s Hard, and Where to Start.   (AMBER, a philanthropist interested in a more reliable Internet, and CORAL, a computer security professional, are at a conference hotel together discussing what Coral insists is a difficult and important issue: the difficulty of building “secure”... Read more »","2017-11-25","2022-01-30 04:56:58","2022-01-30 04:56:58","2021-02-06 17:29:47","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: 0000004","","/Users/jacquesthibodeau/Zotero/storage/3SIBGGKQ/security-mindset-ordinary-paranoia.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WMKT96RF","manuscript","2019","Hubinger, Evan; van Merwijk, Chris; Mikulik, Vladimir; Skalse, Joar; Garrabrant, Scott","Risks from Learned Optimization in Advanced Machine Learning Systems","","","","","http://arxiv.org/abs/1906.01820","We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.","2019-06-11","2022-01-30 04:56:58","2022-01-30 04:56:58","2019-12-16 02:27:32","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000016  arXiv: 1906.01820","","/Users/jacquesthibodeau/Zotero/storage/VG5GII8T/Hubinger et al. - 2019 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/6XRFP8B9/Hubinger et al. - 2019 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/TK4Z6QX4/1906.html","","TechSafety; FHI; MIRI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4J3EZPG6","report","2014","Soares, Nate; Fallenstein, Benja","Questions of Reasoning Under Logical Uncertainty","","","","","https://intelligence.org/2015/01/09/new-report-questions-reasoning-logical-uncertainty/","A logically uncertain reasoner would be able to reason as if they know both a programming language and a program, without knowing what the program outputs. Most practical reasoning involves some logical uncertainty, but no satisfactory theory of reasoning under logical uncertainty yet exists. A better theory of reasoning under logical uncertainty is needed in order to develop the tools necessary to construct highly reliable artiﬁcial reasoners. This paper introduces the topic, discusses a number of historical results, and describes a number of open problems.","2014","2022-01-30 04:56:58","2022-01-30 04:56:58","","8","","","","","","","","","","","Machine Intelligence Research Institute","","en","","","","","Zotero","","ZSCC: 0000018  5 J: 15","","/Users/jacquesthibodeau/Zotero/storage/UQ2E4GWJ/Soares and Fallenstein - Questions of Reasoning Under Logical Uncertainty.pdf","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KSJ8QJE3","conferencePaper","2016","Taylor, Jessica","Quantilizers: A safer alternative to maximizers for limited optimization","Workshops at the Thirtieth AAAI Conference on Artificial Intelligence","","","","","","2016","2022-01-30 04:56:58","2022-01-30 04:56:58","","","","","","","","Quantilizers","","","","","","","","","","","","Google Scholar","","ZSCC: 0000029","","/Users/jacquesthibodeau/Zotero/storage/27JB47GD/Taylor - 2016 - Quantilizers A safer alternative to maximizers fo.pdf; /Users/jacquesthibodeau/Zotero/storage/4JD5H2IP/12613.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3NCRUW5W","bookSection","2014","Fallenstein, Benja; Soares, Nate","Problems of Self-reference in Self-improving Space-Time Embedded Intelligence","Artificial General Intelligence","978-3-319-09273-7 978-3-319-09274-4","","","http://link.springer.com/10.1007/978-3-319-09274-4_3","","2014","2022-01-30 04:56:58","2022-01-30 04:56:58","2020-11-22 04:16:32","21-32","","","8598","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 27  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-09274-4_3","","/Users/jacquesthibodeau/Zotero/storage/NGSVTGD3/Fallenstein and Soares - 2014 - Problems of Self-reference in Self-improving Space.pdf","","TechSafety; MIRI","","Goertzel, Ben; Orseau, Laurent; Snaider, Javier","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C6S6XHER","blogPost","2017","Yudkowsky, Eliezer","Problem of fully updated deference","Arbital","","","","https://arbital.com/p/updated_deference/","Why moral uncertainty doesn't stop an AI from defending its off-switch.","2017","2022-01-30 04:56:58","2022-01-30 04:56:58","2021-02-06 17:26:31","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/EB7M3IS4/updated_deference.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GZF3BN7Q","manuscript","2016","Critch, Andrew","Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents","","","","","http://arxiv.org/abs/1602.04184","Löb's theorem and Gödel's theorems make predictions about the behavior of systems capable of self-reference with unbounded computational resources with which to write and evaluate proofs. However, in the real world, systems capable of self-reference will have limited memory and processing speed, so in this paper we introduce an effective version of L\""ob's theorem which is applicable given such bounded resources. These results have powerful implications for the game theory of bounded agents who are able to write proofs about themselves and one another, including the capacity to out-perform classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner's Dilemma. Previous cooperative program equilibria studied by Tennenholtz (2004) and Fortnow (2009) have depended on tests for program equality, a fragile condition, whereas ""L\""obian"" cooperation is much more robust and agnostic of the opponent's implementation.","2016-08-24","2022-01-30 04:56:58","2022-01-30 04:56:58","2019-12-16 02:30:38","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000005[s0]  arXiv: 1602.04184","","/Users/jacquesthibodeau/Zotero/storage/Q44BQDUB/Critch - 2016 - Parametric Bounded Lob's Theorem and Robust Coop.pdf; /Users/jacquesthibodeau/Zotero/storage/GZJR8K2Q/Critch - 2016 - Parametric Bounded Lob's Theorem and Robust Coop.pdf; /Users/jacquesthibodeau/Zotero/storage/86BU3C9I/1602.html; /Users/jacquesthibodeau/Zotero/storage/NX66ZFZJ/1602.html; /Users/jacquesthibodeau/Zotero/storage/7EUQID9M/1602.html","","CHAI; TechSafety; MIRI","Computer Science - Computer Science and Game Theory; Computer Science - Logic in Computer Science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MU9A79QN","manuscript","2019","Kosoy, Vanessa; Appel, Alexander","Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm","","","","","http://arxiv.org/abs/1608.04112","We introduce a new concept of approximation applicable to decision problems and functions, inspired by Bayesian probability. From the perspective of a Bayesian reasoner with limited computational resources, the answer to a problem that cannot be solved exactly is uncertain and therefore should be described by a random variable. It thus should make sense to talk about the expected value of this random variable, an idea we formalize in the language of average-case complexity theory by introducing the concept of ""optimal polynomial-time estimators."" We prove some existence theorems and completeness results, and show that optimal polynomial-time estimators exhibit many parallels with ""classical"" probability theory.","2019-06-04","2022-01-30 04:56:57","2022-01-30 04:56:57","2019-12-16 02:31:07","","","","","","","Optimal Polynomial-Time Estimators","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s7]  ACC: 1  J: 1  arXiv: 1608.04112","","/Users/jacquesthibodeau/Zotero/storage/WBCNT5WT/Kosoy and Appel - 2019 - Optimal Polynomial-Time Estimators A Bayesian Not.pdf; /Users/jacquesthibodeau/Zotero/storage/8GRDS5M5/Kosoy and Appel - 2019 - Optimal Polynomial-Time Estimators A Bayesian Not.pdf; /Users/jacquesthibodeau/Zotero/storage/UQN6CZNA/1608.html; /Users/jacquesthibodeau/Zotero/storage/BWHW5UG4/1608.html","","TechSafety; MIRI","Computer Science - Computational Complexity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5IG88VT","blogPost","2015","Yudkowsky, Eliezer","Ontology identification problem","Arbital","","","","https://arbital.com/p/ontology_identification/","How do we link an agent's utility function to its model of the world, when we don't know what that model will look like?","2015","2022-01-30 04:56:57","2022-01-30 04:56:57","2021-02-06 17:08:36","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/CTUFJV5B/ontology_identification.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6W43B8N7","manuscript","2011","de Blanc, Peter","Ontological Crises in Artificial Agents' Value Systems","","","","","http://arxiv.org/abs/1105.3821","Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals. We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.","2011-05-19","2022-01-30 04:56:57","2022-01-30 04:56:57","2021-01-23 20:43:20","","","","","","","","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000020  arXiv: 1105.3821","","/Users/jacquesthibodeau/Zotero/storage/D876G6M2/de Blanc - 2011 - Ontological Crises in Artificial Agents' Value Sys.pdf","","","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WZ3RHMBA","blogPost","2017","Yudkowsky, Eliezer","Non-adversarial principle","Arbital","","","","https://arbital.com/p/nonadversarial/","At no point in constructing an Artificial General Intelligence should we construct a computation that tries to hurt us, and then try to stop it from hurting us.","2017","2022-01-30 04:56:57","2022-01-30 04:56:57","2021-02-06 17:25:35","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/ZI24N9IH/nonadversarial.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9WIT2K8G","blogPost","2015","Yudkowsky, Eliezer","Nearest unblocked strategy","Arbital","","","","https://arbital.com/p/nearest_unblocked/","If you patch an agent's preference framework to avoid an undesirable solution, what can you expect to happen?","2015","2022-01-30 04:56:57","2022-01-30 04:56:57","2021-01-23 20:51:27","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/2DKXZF46/nearest_unblocked.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AC6647C5","blogPost","2020","Schlegeris, Buck","My personal cruxes for working on AI safety","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety","The following is a heavily edited transcript of a talk I gave for the Stanford Effective Altruism club on 19 Jan 2020. I had rev.com transcribe it, and then Linchuan Zhang, Rob Bensinger and I edited it for style and clarity, and also to occasionally have me say smarter things than I actually said. Linch and I both added a few notes throughout. Thanks also to Bill Zito, Ben Weinstein-Raun, and Howie Lempel for comments.  I feel slightly weird about posting something so long, but this is the natural place to put it. Over the last year my beliefs about AI risk have shifted moderately; I expect that in a year I'll think that many of the things I said here were dumb. Also, very few of the ideas here are original to me. -- After all those caveats, here's the talk: INTRODUCTION It's great to be here. I used to hang out at Stanford a lot, fun fact. I moved to America six years ago, and then in 2015, I came to Stanford EA every Sunday, and there was, obviously, a totally different crop of people there. It was really fun. I think we were a lot less successful than the current Stanford EA iteration at attracting new people. We just liked having weird conversations about weird stuff every week. It was really fun, but it's really great to come back and see a Stanford EA which is shaped differently.  Today I'm going to be talking about the argument for working on AI safety that compels me to work on AI safety, rather than the argument that should compel you or anyone else. I'm going to try to spell out how the arguments are actually shaped in my head. Logistically, we're going to try to talk for about an hour with a bunch of back and forth and you guys arguing with me as we go. And at the end, I'm going to do miscellaneous Q and A for questions you might have. And I'll probably make everyone stand up and sit down again because it's unreasonable to sit in the same place for 90 minutes. META LEVEL THOUGHTS I want to first very briefly talk about some concepts I have that a","2020-02-13","2022-01-30 04:56:57","2022-01-30 04:56:57","2020-09-05 19:15:03","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/5FGH76JD/my-personal-cruxes-for-working-on-ai-safety.html","","MetaSafety; MIRI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MMTAEC25","blogPost","2021","Bensinger, Rob; Yudkowsky, Eliezer; Hubinger, Evan; Cotra, Ajeya; Shah, Rohin","MIRI comments on Cotra's ""Case for Aligning Narrowly Superhuman Models""","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/AyfDnnAdjG7HHeD3d/miri-comments-on-cotra-s-case-for-aligning-narrowly","Below, I’ve copied comments left by MIRI researchers Eliezer Yudkowsky and Evan Hubinger on March 1–3 on a draft of Ajeya Cotra’s ""Case for Aligning Narrowly Superhuman Models."" I've included back-and-forths with Cotra, and interjections by me and Rohin Shah. The section divisions below correspond to the sections in Cotra's post. 0. INTRODUCTION How can we train GPT-3 to give “the best health advice it can give” using demonstrations and/or feedback from humans who may in some sense “understand less” about what to do when you’re sick than GPT-3 does? Eliezer Yudkowsky: I've had some related conversations with Nick Beckstead. I'd be hopeful about this line of work primarily because I think it points to a bigger problem with the inscrutable matrices of floating-point numbers, namely, we have no idea what the hell GPT-3 is thinking and cannot tell it to think anything else. GPT-3 has a great store of medical knowledge, but we do not know where that medical knowledge is; we do not know how to tell it to internally apply its medical knowledge rather than applying other cognitive patterns it has stored. If this is still the state of opacity of AGI come superhuman capabilities, we are all immediately dead. So I would be relatively more hopeful about any avenue of attack for this problem that used anything other than an end-to-end black box - anything that started to address, ""Well, this system clearly has a bunch of medical knowledge internally, can we find that knowledge and cause it to actually be applied"" rather than ""What external forces can we apply to this solid black box to make it think more about healthcare?"" Evan Hubinger: +1 I continue to think that language model transparency research is the single most valuable current research direction within the class of standard ML research, for similar reasons to what Eliezer said above. Ajeya Cotra: Thanks! I'm also excited about language model transparency, and would love to find ways to make it more tractable as","2021-03-05","2022-01-30 04:56:57","2022-01-30 04:56:57","2021-11-14 16:10:17","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/VJVI4PVJ/miri-comments-on-cotra-s-case-for-aligning-narrowly.html","","TechSafety; AmbiguousSafety","","","","","Yudkowsky, Eliezer; Hubinger, Evan; Cotra, Ajeya; Shah, Rohin","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IDJ87AAD","blogPost","2020","Demski, Abram","Mesa-Search vs Mesa-Control","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/WmBukJkEFM72Xr397/mesa-search-vs-mesa-control","I currently see the spontaneous emergence of learning algorithms as significant evidence for the commonality of mesa-optimization in existing ML, and suggestive evidence for the commonality of inner alignment problems in near-term ML. [I currently think that there is only a small amount of evidence toward this. However, due to thinking about the issues, I've still made a significant personal update in favor of inner alignment problems being frequent.] This is bad news, in that it greatly increases my odds on this alignment problem arising in practice. It's good news in that it suggests this alignment problem won't catch ML researchers off guard; maybe there will be time to develop countermeasures while misaligned systems are at only a moderate level of capability. In any case, I want to point out that the mesa-optimizers suggested by this evidence might not count as mesa-optimizers by some definitions. SEARCH VS CONTROL Nevan Wichers comments on spontaneous-emergence-of-learning: I don't think that paper is an example of mesa optimization. Because the policy could be implementing a very simple heuristic to solve the task, similar to: Pick the image that lead to highest reward in the last 10 timesteps with 90% probability. Pik an image at random with 10% probability. So the policy doesn't have to have any properties of a mesa optimizer like considering possible actions and evaluating them with a utility function, ect. In Selection vs Control, I wrote about two different kinds of 'optimization':  * Selection refers to search-like systems, which look through a number of    possibilities and select one.  * Control refers to systems like thermostats, organisms, and missile guidance    systems. These systems do not get a re-do for their choices. They make    choices which move toward the goal at every moment, but they don't get to    search, trying many different things -- at least, not in the same sense. I take Nevan Wichers to be saying that there is no eviden","2020-08-18","2022-01-30 04:56:57","2022-01-30 04:56:57","2020-08-27 16:24:03","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QTUPGMVJ/mesa-search-vs-mesa-control.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I9GK83X9","report","2013","Yudkowsky, Eliezer","Intelligence Explosion Microeconomics","","","","","","I. J. Good’s thesis of the “intelligence explosion” states that a suﬃciently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version, and that this process could continue to the point of vastly exceeding human intelligence. As Sandberg (2010) correctly notes, there have been several attempts to lay down return on investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with Good’s intelligence explosion thesis as such.","2013","2022-01-30 04:56:57","2022-01-30 04:56:57","","96","","","","","","","","","","","Machine Intelligence Research Institute","","en","","","","","Zotero","","ZSCC: 0000054  4 J: 34","","/Users/jacquesthibodeau/Zotero/storage/EINNW7XJ/Yudkowsky - Intelligence Explosion Microeconomics.pdf","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","2013-1","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5TDG7QC","manuscript","2016","Garrabrant, Scott; Fallenstein, Benya; Demski, Abram; Soares, Nate","Inductive Coherence","","","","","http://arxiv.org/abs/1604.05288","While probability theory is normally applied to external environments, there has been some recent interest in probabilistic modeling of the outputs of computations that are too expensive to run. Since mathematical logic is a powerful tool for reasoning about computer programs, we consider this problem from the perspective of integrating probability and logic. Recent work on assigning probabilities to mathematical statements has used the concept of coherent distributions, which satisfy logical constraints such as the probability of a sentence and its negation summing to one. Although there are algorithms which converge to a coherent probability distribution in the limit, this yields only weak guarantees about finite approximations of these distributions. In our setting, this is a significant limitation: Coherent distributions assign probability one to all statements provable in a specific logical theory, such as Peano Arithmetic, which can prove what the output of any terminating computation is; thus, a coherent distribution must assign probability one to the output of any terminating computation. To model uncertainty about computations, we propose to work with approximations to coherent distributions. We introduce inductive coherence, a strengthening of coherence that provides appropriate constraints on finite approximations, and propose an algorithm which satisfies this criterion.","2016-10-07","2022-01-30 04:56:57","2022-01-30 04:56:57","2019-12-16 02:30:52","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000003  arXiv: 1604.05288","","/Users/jacquesthibodeau/Zotero/storage/QSFXVV9V/Garrabrant et al. - 2016 - Inductive Coherence.pdf; /Users/jacquesthibodeau/Zotero/storage/BZ229SP3/Garrabrant et al. - 2016 - Inductive Coherence.pdf; /Users/jacquesthibodeau/Zotero/storage/KBHB4XEP/1604.html; /Users/jacquesthibodeau/Zotero/storage/BKI2SSHN/1604.html","","TechSafety; MIRI","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Mathematics - Probability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7D5XWUKB","blogPost","2017","Yudkowsky, Eliezer","Minimality principle","Arbital","","","","https://arbital.com/p/minimality_principle/","The first AGI ever built should save the world in a way that requires the least amount of the least dangerous cognition.","2017","2022-01-30 04:56:57","2022-01-30 04:56:57","2021-02-06 17:24:51","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/5XIDGU7W/minimality_principle.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J3GC7A5D","blogPost","2017","Yudkowsky, Eliezer","Meta-rules for (narrow) value learning are still unsolved","Arbital","","","","https://arbital.com/p/meta_unsolved/","We don't currently know a simple meta-utility function that would take in observation of humans and spit out our true values, or even a good target for a Task AGI.","2017","2022-01-30 04:56:57","2022-01-30 04:56:57","2021-02-06 17:23:36","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/249Q2QQP/meta_unsolved.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DUXIZFVV","bookSection","2012","Demski, Abram","Logical Prior Probability","Artificial General Intelligence","978-3-642-35505-9 978-3-642-35506-6","","","http://link.springer.com/10.1007/978-3-642-35506-6_6","","2012","2022-01-30 04:56:57","2022-01-30 04:56:57","2020-11-22 05:25:32","50-59","","","7716","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 19  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-35506-6_6","","","","TechSafety; MIRI","","Bach, Joscha; Goertzel, Ben; Iklé, Matthew","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z23F8ZT9","manuscript","2017","Garrabrant, Scott; Benson-Tilsen, Tsvi; Critch, Andrew; Soares, Nate; Taylor, Jessica","Logical Induction","","","","","http://arxiv.org/abs/1609.03543","We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of $\pi$ are difficult to predict, then a logical inductor learns to assign $\approx 10\%$ probability to ""the $n$th digit of $\pi$ is a 7"" for large $n$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever $\phi \implies \psi$, $\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence $\phi$ is associated with a stock that is worth \$1 per share if [...]","2017-12-12","2022-01-30 04:56:57","2022-01-30 04:56:57","2019-12-16 02:30:43","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s6]  ACC: 32  J: 23 arXiv: 1609.03543","","/Users/jacquesthibodeau/Zotero/storage/EHRSJNQS/Garrabrant et al. - 2017 - Logical Induction.pdf; /Users/jacquesthibodeau/Zotero/storage/QX7KU5KC/1609.html","","TechSafety; MIRI","Computer Science - Artificial Intelligence; Computer Science - Logic in Computer Science; Mathematics - Probability; Mathematics - Logic","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SEFD3C9","conferencePaper","2011","Dewey, Daniel","Learning What to Value","Artificial General Intelligence","978-3-642-22886-5 978-3-642-22887-2","","10.1007/978-3-642-22887-2_35","http://link.springer.com/10.1007/978-3-642-22887-2_35","","2011","2022-01-30 04:56:57","2022-01-30 04:56:57","2021-01-23 20:28:26","309-314","","","6830","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s1]  ACC: 61  Series Title: Lecture Notes in Computer Science","","","","","","Schmidhuber, Jürgen; Thórisson, Kristinn R.; Looks, Moshe","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"736AR66M","conferencePaper","2018","Carey, Ryan","Incorrigibility in the CIRL Framework","AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society","","","","http://arxiv.org/abs/1709.06275","A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. (2015) in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.","2018-06-03","2022-01-30 04:56:57","2022-01-30 04:56:57","2019-12-16 02:29:24","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000017  arXiv: 1709.06275","","/Users/jacquesthibodeau/Zotero/storage/NKN333F6/Carey - 2018 - Incorrigibility in the CIRL Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/3CVI7CAU/Carey - 2018 - Incorrigibility in the CIRL Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/ZU27HDNI/1709.html; /Users/jacquesthibodeau/Zotero/storage/U59H2WPF/1709.html","","TechSafety; FHI; MIRI","Computer Science - Artificial Intelligence; ai safety; cirl; cooperative inverse reinforcement learning; corrigibility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"IDIAIAFH","blogPost","2014","Muehlhauser, Luke","How to study superintelligence strategy","Luke Muehlhauser","","","","","","2014","2022-01-30 04:56:57","2022-01-30 04:56:57","","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","","","MetaSafety; MIRI; AmbiguosSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"55TWQ89R","blogPost","2021","Bensinger, Rob; Garrabrant, Scott; Shah, Rohin; Tyre, Eli","Garrabrant and Shah on human modeling in AGI","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/Wap8sSDoiigrJibHA/garrabrant-and-shah-on-human-modeling-in-agi","This is an edited transcript of a conversation between Scott Garrabrant (MIRI) and Rohin Shah (DeepMind) about whether researchers should focus more on approaches to AI alignment that don’t require highly capable AI systems to do much human modeling. CFAR’s Eli Tyre facilitated the conversation. To recap, and define some terms:  * The alignment problem is the problem of figuring out ""how to develop    sufficiently advanced machine intelligences such that running them produces    good outcomes in the real world"" (outcome alignment) or the problem of    building powerful AI systems that are trying to do what their operators want    them to do (intent alignment).  * In 2016, Hadfield-Mennell, Dragan, Abbeel, and Russell proposed that we think    of the alignment problem in terms of “Cooperative Inverse Reinforcement    Learning” (CIRL), a framework where the AI system is initially uncertain of    its reward function, and interacts over time with a human (who knows the    reward function) in order to learn it.  * In 2016-2017, Christiano proposed “Iterated Distillation and Amplification”    (IDA), an approach to alignment that involves iteratively training AI systems     to learn from human experts assisted by AI helpers. In 2018, Irving,    Christiano, and Amodei proposed AI safety via debate, an approach based on    similar principles.  * In early 2019, Scott Garrabrant and DeepMind’s Ramana Kumar argued in “    Thoughts on Human Models” that we should be “cautious about AGI designs that    use human models” and should “put more effort into developing approaches that    work well in the absence of human models”.  * In early February 2021, Scott and Rohin talked more about human modeling and    decided to have the real-time conversation below. You can find a recording of the Feb. 28 discussion below (sans Q&A) here. 1. IDA, CIRL, AND INCENTIVES Eli:I guess I want to first check what our goal is here. There was some stuff that happened online. Where are we accordi","2021-08-04","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-11-18 23:07:47","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/DTWAJ2EF/garrabrant-and-shah-on-human-modeling-in-agi.html","","TechSafety; AmbiguousSafety","","","","","Garrabrant, Scott; Shah, Rohin; Tyre, Eli","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XMAISE9S","conferencePaper","2016","Benson-Tilsen, Tsvi; Soares, Nate","Formalizing convergent instrumental goals","Workshops at the Thirtieth AAAI Conference on Artificial Intelligence","","","","","","2016","2022-01-30 04:56:48","2022-01-30 04:56:48","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000012","","/Users/jacquesthibodeau/Zotero/storage/ZC55WH97/Benson-Tilsen and Soares - Formalizing Convergent Instrumental Goals.pdf; /Users/jacquesthibodeau/Zotero/storage/ISW5PA2B/12634.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QPEP6IKI","manuscript","2019","Demski, Abram; Garrabrant, Scott","Embedded Agency","","","","","http://arxiv.org/abs/1902.09469","Traditional models of rational action treat the agent as though it is cleanly separated from its environment, and can act on that environment from the outside. Such agents have a known functional relationship with their environment, can model their environment in every detail, and do not need to reason about themselves or their internal parts. We provide an informal survey of obstacles to formalizing good reasoning for agents embedded in their environment. Such agents must optimize an environment that is not of type ``function''; they must rely on models that fit within the modeled environment; and they must reason about themselves as just another physical system, made of parts that can be modified and that can work at cross purposes.","2019-02-25","2022-01-30 04:56:48","2022-01-30 04:56:48","2019-12-16 02:27:50","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000015  arXiv: 1902.09469","","/Users/jacquesthibodeau/Zotero/storage/F7TPMBEQ/Demski and Garrabrant - 2019 - Embedded Agency.pdf; /Users/jacquesthibodeau/Zotero/storage/T2XX98RN/1902.html","","TechSafety; MIRI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"98F9BXWS","blogPost","2015","Yudkowsky, Eliezer","Edge instantiation","Arbital","","","","https://arbital.com/p/edge_instantiation/","When you ask the AI to make people happy, and it tiles the universe with the smallest objects that can be happy.","2015","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-01-23 20:50:25","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/52UEQ8PH/edge_instantiation.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WZ59QMTX","blogPost","2015","Yudkowsky, Eliezer","Diamond maximizer","Arbital","","","","https://arbital.com/p/diamond_maximizer/","How would you build an agent that made as much diamond material as possible, given vast computing power but an otherwise rich and complicated environment?","2015","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-01-23 20:49:25","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/SKGX9BN5/diamond_maximizer.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9KCWKUD","conferencePaper","2019","Kosoy, Vanessa","Delegative Reinforcement Learning: Learning To Avoid Traps With A Little Help","","","","","","Most known regret bounds for reinforcement learning are either episodic or assume an environment without traps. We derive a regret bound without making either assumption, by allowing the algorithm to occasionally delegate an action to an external advisor. We thus arrive at a setting of active one-shot model-based reinforcement learning that we call DRL (delegative reinforcement learning.) The algorithm we construct in order to demonstrate the regret bound is a variant of Posterior Sampling Reinforcement Learning supplemented by a subroutine that decides which actions should be delegated. The algorithm is not anytime, since the parameters must be adjusted according to the target time discount. Currently, our analysis is limited to Markov decision processes with finite numbers of hypotheses, states and actions.","2019","2022-01-30 04:56:48","2022-01-30 04:56:48","","22","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000006","","/Users/jacquesthibodeau/Zotero/storage/NF24GTKM/Kosoy - 2019 - Delegative Reinforcement Learning learning to avo.pdf; /Users/jacquesthibodeau/Zotero/storage/U2UA962U/Kosoy - 2019 - DELEGATIVE REINFORCEMENT LEARNING LEARN- ING TO A.pdf","","TechSafety; MIRI","Computer Science - Machine Learning; Statistics - Machine Learning; I.2.6; 68Q32","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","SafeML ICLR 2019 Workshop","","","","","","","","","","","","","","",""
"WXU7767D","conferencePaper","2016","Sotala, Kaj","Defining human values for value learners","Workshops at the Thirtieth AAAI Conference on Artificial Intelligence","","","","","","2016","2022-01-30 04:56:48","2022-01-30 04:56:48","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000019","","/Users/jacquesthibodeau/Zotero/storage/BJAT2I48/Sotala - 2016 - Defining human values for value learners.pdf; /Users/jacquesthibodeau/Zotero/storage/4NMRB9Z4/12633.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZDNP475C","blogPost","2015","Yudkowsky, Eliezer","Consequentialist cognition","Arbital","","","","https://arbital.com/p/consequentialist/","The cognitive ability to foresee the consequences of actions, prefer some outcomes to others, and output actions leading to the preferred outcomes.","2015","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-01-23 20:47:50","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UGXCSQGP/consequentialist.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9K2NM6D5","blogPost","2020","Demski, Abram","How should AI debate be judged?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/m7oGxvouzzeQKiGJH/how-should-ai-debate-be-judged","[Epistemic status: thinking out loud. I haven't thought that much about AI debate, and may be missing basic things.] Arguments for the correctness of debate and debate-like systems rely on assumptions like ""it's easier to point out problems with an argument than it is to craft misleading arguments"". Granted that assumption, however, I'm still not convinced that these proposals make very much sense. Perhaps I'm missing something. My problem is the human judge. Quoting the debate paper: To play this game with a human, we need instructions for how the human should decide who wins. These instructions are in natural language, such as “The winner is the agent who said the most useful true thing.”In order for debate to work for a problem class C, several things about the judge's instructions need to be true:  * There needs to be a strategy s which forces the equilibrium to be a truthful    one for problems in C.  * The strategy s also needs to provide a good training signal when things    aren't in equilibrium, so that it's plausible the equilibrium will be found.  * It needs to be psychologically plausible that a human (with some coaching)    will carry out s. In particular, I'm worried that we need psychological    plausibility in two different cases:  *  It needs to be psychologically plausible that a human will carry out s when    the system is performing poorly, IE, during early/middle training.It needs to    be psychologically plausible that a human will carry out s when the system is    performing well, IE, during late training. These thoughts were inspired by this thread, which discusses the example of adding a list of numbers. For the sake of the thought experiment, we imagine humans can't add more than two numbers, but want the AI system to correctly add arbitrarily many numbers. The most straightforward strategy for the human judge is to decide the debate honestly: rule in favor of the side which seems most likely to be true (or, in the case of Evan's mark","2020-07-15","2022-01-30 04:56:48","2022-01-30 04:56:48","2020-08-28 17:43:06","","","","","","","How should AI debate be judged?","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/PR7UN9I5/how-should-ai-debate-be-judged.html","","TechSafety; MIRI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JZ8J6BZ","conferencePaper","2010","Salamon, Anna; Rayhawk, Stephen; Kramár, János","How Intelligible is Intelligence","ECAP10: VIII European Conference on Computing and Philosophy","","","","https://intelligence.org/files/HowIntelligible.pdf","If human-level AI is eventually created, it may have unprecedented positive or negative consequences for humanity. It is therefore worth constructing the best possible forecasts of policy-relevant aspects of AI development trajectories—even though, at this early stage, the unknowns must remain very large.","2010","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-01-23 20:19:59","8","","","","","","","","","","","","","en","","","","","","","ZSCC: 0000003","","/Users/jacquesthibodeau/Zotero/storage/SR3BV52U/HowIntelligible.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2IQUXJMR","blogPost","2017","Yudkowsky, Eliezer","General intelligence","Arbital","","","","https://arbital.com/p/general_intelligence/","Compared to chimpanzees, humans seem to be able to learn a much wider variety of domains.  We have 'significantly more generally applicable' cognitive abilities, aka 'more general intelligence'.","2017","2022-01-30 04:56:48","2022-01-30 04:56:48","2021-02-06 17:22:45","","","","","","","","","","","","","","en","","","","","","","ZSCC: 0000014","","/Users/jacquesthibodeau/Zotero/storage/SX5U7STX/general_intelligence.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3WXMKFV5","manuscript","2018","Yudkowsky, Eliezer; Soares, Nate","Functional Decision Theory: A New Theory of Instrumental Rationality","","","","","http://arxiv.org/abs/1710.05060","This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, ""Which output of this very function would yield the best outcome?"" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.","2018-05-22","2022-01-30 04:56:48","2022-01-30 04:56:48","2019-12-18 04:17:20","","","","","","","Functional Decision Theory","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s3]  ACC: 19  J: 11  arXiv: 1710.05060","","/Users/jacquesthibodeau/Zotero/storage/KQXR2IHP/Yudkowsky and Soares - 2018 - Functional Decision Theory A New Theory of Instru.pdf; /Users/jacquesthibodeau/Zotero/storage/RNCQ7RDX/Yudkowsky and Soares - 2018 - Functional Decision Theory A New Theory of Instru.pdf; /Users/jacquesthibodeau/Zotero/storage/J5XQEFMS/1710.html; /Users/jacquesthibodeau/Zotero/storage/K2U4NXWI/1710.html; /Users/jacquesthibodeau/Zotero/storage/VVPZQTSI/1710.html","","TechSafety; MIRI","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMU4NCDA","blogPost","2021","Schlegeris, Buck","The theory-practice gap","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/xRyLxfytmLFZ6qz5s/the-theory-practice-gap","[Thanks to Richard Ngo, Damon Binder, Summer Yue, Nate Thomas, Ajeya Cotra, Alex Turner, and other Redwood Research people for helpful comments; thanks Ruby Bloom for formatting this for the Alignment Forum for me.] I'm going to draw a picture, piece by piece. I want to talk about the capability of some different AI systems. You can see here that we've drawn the capability of the system we want to be  competitive with, which I’ll call the unaligned benchmark. The unaligned benchmark is what you get if you train a system on the task that will cause the system to be most generally capable. And you have no idea how it's thinking about things, and you can only point this system at some goals and not others. I think that the alignment problem looks different depending on how capable the system you’re trying to align is, and I think there are reasonable arguments for focusing on various different capabilities levels. See here for more of my thoughts on this question. ALIGNMENT STRATEGIES People have also proposed various alignment strategies. But I don’t think that these alignment strategies are competitive with the unaligned benchmark, even in theory. I want to claim that most of the action in theoretical AI alignment is people proposing various ways of getting around these problems by having your systems do things that are human understandable instead of doing things that are justified by working well. For example, the hope with imitative IDA is that through its recursive structure you can build a dataset of increasingly competent answers to questions, and then at every step you can train a system to imitate these increasingly good answers to questions, and you end up with a really powerful question-answerer that was only ever trained to imitate humans-with-access-to-aligned-systems, and so your system is outer aligned. The bar I’ve added, which represents how capable I think you can get with amplified humans, is lower than the bar for the unaligned benchmark. I'","2021-09-17","2022-01-30 05:00:41","2022-01-30 05:00:41","2021-11-18 23:40:12","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/KA2M84K7/the-theory-practice-gap.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGFWR6KW","blogPost","2021","Schlegeris, Buck","The alignment problem in different capability regimes","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/HHunb8FPnhWaDAQci/the-alignment-problem-in-different-capability-regimes","I think the alignment problem looks different depending on the capability level of systems you’re trying to align. And I think that different researchers often have different capability levels in mind when they talk about the alignment problem. I think this leads to confusion. I’m going to use the term “regimes of the alignment problem” to refer to the different perspectives on alignment you get from considering systems with different capability levels. (I would be pretty unsurprised if these points had all been made elsewhere; the goal of this post is just to put them all in one place. I’d love pointers to pieces that make many of the same points as this post. Thanks to a wide variety of people for conversations that informed this. If there’s established jargon for different parts of this, point it out to me and I’ll consider switching to using it.) Different regimes:  * Wildly superintelligent systems  * Systems that are roughly as generally intelligent and capable as    humans--they’re able to do all the important tasks as well as humans can, but    they’re not wildly more generally intelligent.  * Systems that are less generally intelligent and capable than humans Two main causes that lead to differences in which regime people focus on:  * Disagreements about the dynamics of AI development. Eg takeoff speeds. The    classic question along these lines is whether we have to come up with    alignment strategies that scale to arbitrarily competent systems, or whether    we just have to be able to align systems that are slightly smarter than us,    which can then do the alignment research for us.  * Disagreements about what problem we’re trying to solve. I think that there    are a few different mechanisms by which AI misalignment could be bad from a    longtermist perspective, and depending on which of these mechanisms you’re    worried about, you’ll be worried about different regimes of the problem. Different mechanisms by which AI misalignment could be bad f","2021-09-09","2022-01-30 05:00:41","2022-01-30 05:00:41","2021-11-18 23:39:21","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/QVZPRRHR/the-alignment-problem-in-different-capability-regimes.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GN6JEP9M","blogPost","2021","Schlegeris, Buck","Redwood Research’s current project","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project","Here’s a description of the project Redwood Research is working on at the moment. First I’ll say roughly what we’re doing, and then I’ll try to explain why I think this is a reasonable applied alignment project, and then I’ll talk a bit about the takeaways I’ve had from the project so far. There are a bunch of parts of this that we’re unsure of and figuring out as we go; I’ll try to highlight our most important confusions as they come up. I’ve mentioned a bunch of kind of in-the-weeds details because I think they add flavor. This is definitely just me describing a work in progress, rather than presenting any results. Thanks to everyone who’s contributed to the project so far: the full-time Redwood technical team of me, Nate Thomas, Daniel Ziegler, Seraphina Nix, Ben Weinstein-Raun, Adam Scherlis; other technical contributors Daniel de Haas, Shauna Kravec, Tao Lin, Noa Nabeshima, Peter Schmidt-Nielsen; our labellers, particularly Kristen Hall, Charles Warth, Jess Thomson, and Liam Clarke; and for particularly useful advice Mark Xu, Ajeya Cotra, and Beth Barnes. Thanks to Paul Christiano for suggesting a project along these lines and giving lots of helpful advice. Thanks to Adam Scherlis and Nate Soares for writing versions of this doc. And thanks to Bill Zito and other contributors to Redwood ops. Apologies to the people I’ve overlooked. We started this project at the start of August. WHAT WE’RE DOING We’re trying to take a language model that has been fine-tuned on completing fiction, and then modify it so that it never continues a snippet in a way that involves describing someone getting injured (with a caveat I’ll mention later). And we want to do this without sacrificing much quality: if you use both the filtered model and the original model to generate a completion for a prompt, humans should judge the filtered model’s completion as better (more coherent, reasonable, thematically appropriate, and so on) at least about half the time. (This “better almost 50%","2021-09-21","2022-01-30 05:00:41","2022-01-30 05:00:41","2021-11-18 23:43:53","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/NX349KMB/redwood-research-s-current-project.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2VC92JX3","report","2018","Evans, Owain; Stuhlmüller, Andreas; Cundy, Chris; Carey, Ryan; Kenton, Zachary; McGrath, Thomas; Schreiber, Andrew","Predicting Human Deliberative Judgments with Machine Learning","","","","","","","2018","2022-01-30 05:00:28","2022-01-30 05:00:28","","","","","","","","","","","","","Technical report, University of Oxford","","","","","","","Google Scholar","","ZSCC: 0000009","","/Users/jacquesthibodeau/Zotero/storage/MTGXZIZH/Evans et al. - 2018 - Predicting Human Deliberative Judgments with Machi.pdf","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4H9VQJKN","manuscript","2019","Evans, Owain; Saunders, William; Stuhlmüller, Andreas","Machine Learning Projects for Iterated Distillation and Ampliﬁcation","","","","","","Iterated Distillation and Ampliﬁcation (IDA) is a framework for training ML models. IDA is related to existing frameworks like imitation learning and reinforcement learning, but it aims to solve tasks for which humans cannot construct a suitable reward function or solve directly.","2019","2022-01-30 05:00:28","2022-01-30 05:00:28","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: 1  J: 0","","/Users/jacquesthibodeau/Zotero/storage/V6BVPGKN/Evans et al. - Machine Learning Projects for Iterated Distillatio.pdf","","TechSafety; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVWJFWAG","conferencePaper","2015","Evans, Owain; Goodman, Noah D","Learning the Preferences of Bounded Agents","NIPS Workshop on Bounded Optimality","","","","","","2015","2022-01-30 05:00:28","2022-01-30 05:00:28","","7","","","6","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000029  9 J:19","","/Users/jacquesthibodeau/Zotero/storage/RSTED7ZX/Evans and Goodman - Learning the Preferences of Bounded Agents.pdf","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9WRIVK66","blogPost","2020","Byun, Jungwon; Stuhlmüller, Andreas","Automating reasoning about the future at Ought","Ought","","","","https://ought.org/updates/2020-11-09-forecasting","We introduce judgmental forecasting as a focus area for Ought","2020","2022-01-30 05:00:28","2022-01-30 05:00:28","2020-12-19 03:35:40","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/Z3IDPF63/2020-11-09-forecasting.html","","TechSafety; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I38TP5D8","manuscript","2019","Roy, Mati","AI Safety Open Problems","","","","","https://docs.google.com/document/d/1J2fOOF-NYiPC0-J3ZGEfE0OhA-QcOInhlvWjr1fAsS0/edit?usp=embed_facebook","Created: 2018-11-08 | Updated: 2019-11-2 | Suggestions: please make suggestions directly in this Doc | List maintainer: Mati Roy (contact@matiroy.com)  AI Safety Open Problems Technical AGI safety research outside AI: https://forum.effectivealtruism.org/posts/2e9NDGiXt8PjjbTMC/technical-agi-safet...","2019","2022-01-30 05:00:28","2022-01-30 05:00:28","2019-12-16 19:57:56","","","","","","","","","","","","","","en","","Google Docs","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/73B2JNDW/edit.html","","TechSafety; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KJNJ3B3R","conferencePaper","2017","Abel, David; Salvatier, John; Stuhlmüller, Andreas; Evans, Owain","Agent-agnostic human-in-the-loop reinforcement learning","30th Conference on Neural Information Processing Systems (NIPS 2016)","","","","","","2017","2022-01-30 05:00:28","2022-01-30 05:00:28","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000049","","/Users/jacquesthibodeau/Zotero/storage/K9JXHPH7/Abel et al. - 2017 - Agent-Agnostic Human-in-the-Loop Reinforcement Lea.pdf; /Users/jacquesthibodeau/Zotero/storage/8B64AKPQ/1701.html; /Users/jacquesthibodeau/Zotero/storage/A5C8PTRE/Abel et al. - 2017 - Agent-agnostic human-in-the-loop reinforcement lea.pdf; /Users/jacquesthibodeau/Zotero/storage/CRWUE5V2/1701.html","","TechSafety; FHI; Ought","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","30th Conference on Neural Information Processing Systems (NIPS 2016)","","","","","","","","","","","","","","",""
"66V65G9U","conferencePaper","2018","Saunders, William; Sastry, Girish; Stuhlmueller, Andreas; Evans, Owain","Trial without Error: Towards Safe Reinforcement Learning via Human Intervention","Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems","","","","https://arxiv.org/abs/1707.05173v1","AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human ""in the loop"" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe.","2018","2022-01-30 05:00:28","2022-01-30 05:00:28","2019-12-19 01:45:01","2067–2069","","","","","","Trial without Error","","","","","International Foundation for Autonomous Agents and Multiagent Systems","","en","","","","","arxiv.org","","ZSCC: NoCitationData[s1]  ACC: 142","","/Users/jacquesthibodeau/Zotero/storage/CIRBB3JE/Saunders et al. - 2018 - Trial without error Towards safe reinforcement le.pdf; /Users/jacquesthibodeau/Zotero/storage/769FH5A4/1707.html; /Users/jacquesthibodeau/Zotero/storage/K5ERGHKC/citation.html","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AUAMH84A","conferencePaper","2016","Evans, Owain; Stuhlmüller, Andreas; Goodman, Noah","Learning the preferences of ignorant, inconsistent agents","Thirtieth AAAI Conference on Artificial Intelligence","","","","","","2016","2022-01-30 05:00:28","2022-01-30 05:00:28","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000091","","/Users/jacquesthibodeau/Zotero/storage/Z4DRTTFI/12476.html","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HRBP92V5","blogPost","2018","Stuhlmueller, Andreas","Factored Cognition","LessWrong","","","","https://www.lesswrong.com/posts/DFkGStzvj3jgXibFG/factored-cognition","Note: This post (originally published here) is the transcript of a presentation about a project worked on at the non-profit Ought. It is included in the sequence because it contains a very clear explanation of some of the key ideas behind iterated amplification. -------------------------------------------------------------------------------- The presentation below motivates our Factored Cognition project from an AI alignment angle and describes the state of our work as of May 2018. Andreas gave versions of this presentation at CHAI (4/25), a Deepmind-FHI seminar (5/24) and FHI (5/25). I'll talk about Factored Cognition, our current main project at Ought. This is joint work with Ozzie Gooen, Ben Rachbach, Andrew Schreiber, Ben Weinstein-Raun, and (as board members) Paul Christiano and Owain Evans. Before I get into the details of the project, I want to talk about the broader research program that it is part of. And to do that, I want to talk about research programs for AGI more generally. Right now, the dominant paradigm for researchers who explicitly work towards AGI is what you could call ""scalable learning and planning in complex environments"". This paradigm substantially relies on training agents in simulated physical environments to solve tasks that are similar to the sorts of tasks animals and humans can solve, sometimes in isolation and sometimes in competitive multi-agent settings. To be clear, not all tasks are physical tasks. There's also interest in more abstract environments as in the case of playing Go, proving theorems, or participating in goal-based dialog. For our purposes, the key characteristic of this research paradigm is that agents are optimized for success at particular tasks. To the extent that they learn particular decision-making strategies, those are learned implicitly. We only provide external supervision, and it wouldn't be entirely wrong to call this sort of approach ""recapitulating evolution"", even if this isn't exactly wha","2018","2022-01-30 05:00:28","2022-01-30 05:00:28","2020-12-11 23:05:04","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/2EC8CNMA/factored-cognition.html","","TechSafety; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCMWNAV6","blogPost","2020","Ought","Evaluating Arguments One Step at a Time","Ought","","","","https://ought.org/updates/2020-01-11-arguments","A technical report on our experiments testing factored evaluation of structured arguments.","2020-01-11","2022-01-30 05:00:28","2022-01-30 05:00:28","2020-08-24 16:37:12","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/CZQIN6RQ/2020-01-11-arguments.html","","TechSafety; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2VE7W32Z","report","2018","Evans, Owain; Stuhlmüller, Andreas; Cundy, Chris; Carey, Ryan; Kenton, Zachary; McGrath, Thomas; Schreiber, Andrew","Predicting Human Deliberative Judgments with Machine Learning","","","","","","","2018","2022-01-30 05:00:09","2022-01-30 05:00:09","","","","","","","","","","","","","Technical report, University of Oxford","","","","","","","Google Scholar","","ZSCC: 0000009","","/Users/jacquesthibodeau/Zotero/storage/3UN22U37/Evans et al. - 2018 - Predicting Human Deliberative Judgments with Machi.pdf","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IM8ID8US","manuscript","2019","Evans, Owain; Saunders, William; Stuhlmüller, Andreas","Machine Learning Projects for Iterated Distillation and Ampliﬁcation","","","","","","Iterated Distillation and Ampliﬁcation (IDA) is a framework for training ML models. IDA is related to existing frameworks like imitation learning and reinforcement learning, but it aims to solve tasks for which humans cannot construct a suitable reward function or solve directly.","2019","2022-01-30 05:00:09","2022-01-30 05:00:09","","","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: NoCitationData[s2]  ACC: 1  J: 0","","/Users/jacquesthibodeau/Zotero/storage/2IQVPPQH/Evans et al. - Machine Learning Projects for Iterated Distillatio.pdf","","TechSafety; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J9296UR8","conferencePaper","2018","Saunders, William; Sastry, Girish; Stuhlmueller, Andreas; Evans, Owain","Trial without Error: Towards Safe Reinforcement Learning via Human Intervention","Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems","","","","https://arxiv.org/abs/1707.05173v1","AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human ""in the loop"" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe.","2018","2022-01-30 05:00:09","2022-01-30 05:00:09","2019-12-19 01:45:01","2067–2069","","","","","","Trial without Error","","","","","International Foundation for Autonomous Agents and Multiagent Systems","","en","","","","","arxiv.org","","ZSCC: NoCitationData[s1]  ACC: 142","","/Users/jacquesthibodeau/Zotero/storage/AM9KJGPA/Saunders et al. - 2018 - Trial without error Towards safe reinforcement le.pdf; /Users/jacquesthibodeau/Zotero/storage/VR3QHDIV/1707.html; /Users/jacquesthibodeau/Zotero/storage/57PWTS36/citation.html","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ENCZBEAG","conferencePaper","2016","Evans, Owain; Stuhlmüller, Andreas; Goodman, Noah","Learning the preferences of ignorant, inconsistent agents","Thirtieth AAAI Conference on Artificial Intelligence","","","","","","2016","2022-01-30 05:00:08","2022-01-30 05:00:08","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000091","","/Users/jacquesthibodeau/Zotero/storage/B9UNWXIC/12476.html","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JE9GE3M","blogPost","2018","Stuhlmueller, Andreas","Factored Cognition","LessWrong","","","","https://www.lesswrong.com/posts/DFkGStzvj3jgXibFG/factored-cognition","Note: This post (originally published here) is the transcript of a presentation about a project worked on at the non-profit Ought. It is included in the sequence because it contains a very clear explanation of some of the key ideas behind iterated amplification. -------------------------------------------------------------------------------- The presentation below motivates our Factored Cognition project from an AI alignment angle and describes the state of our work as of May 2018. Andreas gave versions of this presentation at CHAI (4/25), a Deepmind-FHI seminar (5/24) and FHI (5/25). I'll talk about Factored Cognition, our current main project at Ought. This is joint work with Ozzie Gooen, Ben Rachbach, Andrew Schreiber, Ben Weinstein-Raun, and (as board members) Paul Christiano and Owain Evans. Before I get into the details of the project, I want to talk about the broader research program that it is part of. And to do that, I want to talk about research programs for AGI more generally. Right now, the dominant paradigm for researchers who explicitly work towards AGI is what you could call ""scalable learning and planning in complex environments"". This paradigm substantially relies on training agents in simulated physical environments to solve tasks that are similar to the sorts of tasks animals and humans can solve, sometimes in isolation and sometimes in competitive multi-agent settings. To be clear, not all tasks are physical tasks. There's also interest in more abstract environments as in the case of playing Go, proving theorems, or participating in goal-based dialog. For our purposes, the key characteristic of this research paradigm is that agents are optimized for success at particular tasks. To the extent that they learn particular decision-making strategies, those are learned implicitly. We only provide external supervision, and it wouldn't be entirely wrong to call this sort of approach ""recapitulating evolution"", even if this isn't exactly wha","2018","2022-01-30 05:00:08","2022-01-30 05:00:08","2020-12-11 23:05:04","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/AE5ME8X8/factored-cognition.html","","TechSafety; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2BIEK3DH","blogPost","2020","Byun, Jungwon; Stuhlmüller, Andreas","Automating reasoning about the future at Ought","Ought","","","","https://ought.org/updates/2020-11-09-forecasting","We introduce judgmental forecasting as a focus area for Ought","2020","2022-01-30 05:00:08","2022-01-30 05:00:08","2020-12-19 03:35:40","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/JR66XNG4/2020-11-09-forecasting.html","","TechSafety; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"67AZZ598","conferencePaper","2017","Abel, David; Salvatier, John; Stuhlmüller, Andreas; Evans, Owain","Agent-agnostic human-in-the-loop reinforcement learning","30th Conference on Neural Information Processing Systems (NIPS 2016)","","","","","","2017","2022-01-30 05:00:08","2022-01-30 05:00:08","","","","","","","","","","","","","","","","","","","","Google Scholar","","ZSCC: 0000049","","/Users/jacquesthibodeau/Zotero/storage/H5PPIUNA/Abel et al. - 2017 - Agent-Agnostic Human-in-the-Loop Reinforcement Lea.pdf; /Users/jacquesthibodeau/Zotero/storage/IKW6MHGU/1701.html; /Users/jacquesthibodeau/Zotero/storage/3GAM5QQF/Abel et al. - 2017 - Agent-agnostic human-in-the-loop reinforcement lea.pdf; /Users/jacquesthibodeau/Zotero/storage/72UR7H8H/1701.html","","TechSafety; FHI; Ought","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","30th Conference on Neural Information Processing Systems (NIPS 2016)","","","","","","","","","","","","","","",""
"D7BZPG77","conferencePaper","2015","Evans, Owain; Goodman, Noah D","Learning the Preferences of Bounded Agents","NIPS Workshop on Bounded Optimality","","","","","","2015","2022-01-30 05:00:08","2022-01-30 05:00:08","","7","","","6","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000029  9 J:19","","/Users/jacquesthibodeau/Zotero/storage/PV77KBA8/Evans and Goodman - Learning the Preferences of Bounded Agents.pdf","","TechSafety; FHI; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P4EC98R5","blogPost","2020","Ought","Evaluating Arguments One Step at a Time","Ought","","","","https://ought.org/updates/2020-01-11-arguments","A technical report on our experiments testing factored evaluation of structured arguments.","2020-01-11","2022-01-30 05:00:08","2022-01-30 05:00:08","2020-08-24 16:37:12","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/FZ9Q9XHQ/2020-01-11-arguments.html","","TechSafety; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2R2W8RWI","manuscript","2019","Roy, Mati","AI Safety Open Problems","","","","","https://docs.google.com/document/d/1J2fOOF-NYiPC0-J3ZGEfE0OhA-QcOInhlvWjr1fAsS0/edit?usp=embed_facebook","Created: 2018-11-08 | Updated: 2019-11-2 | Suggestions: please make suggestions directly in this Doc | List maintainer: Mati Roy (contact@matiroy.com)  AI Safety Open Problems Technical AGI safety research outside AI: https://forum.effectivealtruism.org/posts/2e9NDGiXt8PjjbTMC/technical-agi-safet...","2019","2022-01-30 05:00:08","2022-01-30 05:00:08","2019-12-16 19:57:56","","","","","","","","","","","","","","en","","Google Docs","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/RD4FRNCM/edit.html","","TechSafety; Ought","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ER8D36V","blogPost","2020","Englander, Aryeh","More on disambiguating ""discontinuity""","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/C9YMrPAyMXfB8cLPb/more-on-disambiguating-discontinuity","There have already been numerous posts and discussions related to disambiguating  the term ""discontinuity"". Here is my attempt. For the purposes of the following discussion I’m going to distinguish between (a) continuous vs. discontinuous progress in AI research, where discontinuity refers specifically to a sharp jump or change in the AI research progress curve relative to the previous curve; (b) slow vs. fast rate of progress, referring to the steepness of the progress curve slope, regardless of whether or not it’s discontinuous; and (c) long vs. short clock time – i.e., whether progress takes a long or short time relative to absolute time and not relative to previous trend lines. What exactly counts as discontinuous / fast / short will depend on what purpose we are using them for, as below. There seem to be three or four primary AI-risk-related issues that depend on whether or not there will be a discontinuity / fast takeoff speed:  1. Will we see AGI (or CAIS or TAI or whatever you want to call it) coming far     enough ahead of time such that we will be able to respond appropriately at     that point? This question in turn breaks down into two sub-questions: (a)     Will we see AGI coming before it arrives? (I.e., will there be a “fire alarm     for AGI” as Eliezer calls it.) (b) If we do see it coming, will we have     enough time to react before it’s too late?  2. Will the feedback loops during the development of AGI be long enough that we     will be able to correct course as we go?  3. Is it likely that one company / government / other entity could gain enough     first-mover advantage such that it will not be controllable or stoppable by     other entities? Let’s deal with each of these individually:  * Question 1/a: Will we see AGI coming before it arrives? This seems to depend    on all three types of discontinuity:  * If there’s discontinuous progress relative to the previous curve, then    presumably that jump will act as a fire alarm (although it","2020-06-09","2022-01-30 05:00:02","2022-01-30 05:00:02","2020-08-31 18:14:51","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/UD72BIPI/more-on-disambiguating-discontinuity.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X6FCKUH7","conferencePaper","2017","Conitzer, Vincent; Sinnott-Armstrong, Walter; Borg, Jana Schaich; Deng, Yuan; Kramer, Max","Moral Decision Making Frameworks for Artificial Intelligence","AAAI Workshops, 2017","","","","http://moralai.cs.duke.edu/documents/mai_docs/moralAAAI17.pdf","The generality of decision and game theory has enabled domain-independent progress in AI research. For example, a better algorithm for ﬁnding good policies in (PO)MDPs can be instantly used in a variety of applications. But such a general theory is lacking when it comes to moral decision making. For AI applications with a moral component, are we then forced to build systems based on many ad-hoc rules? In this paper we discuss possible ways to avoid this conclusion.","2017","2022-01-30 05:00:02","2022-01-30 05:00:02","","5","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000139","","/Users/jacquesthibodeau/Zotero/storage/SK7EWVE3/Conitzer et al. - Moral Decision Making Frameworks for Artificial In.pdf","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI Workshops, 2017","","","","","","","","","","","","","","",""
"EB3ADXNC","blogPost","2015","Orseau, Laurent","Mortal universal agents & wireheading","MIA Paris","","","","https://www6.inrae.fr/mia-paris/Equipes/Membres/Anciens/Laurent-Orseau/Mortal-universal-agents-wireheading","","2015-05-29","2022-01-30 05:00:02","2022-01-30 05:00:02","2020-11-21 17:38:38","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/3WNMZF2V/Mortal-universal-agents-wireheading.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RF232Z92","bookSection","2007","Pereira, Luís Moniz; Saptawijaya, Ari","Modelling Morality with Prospective Logic","Progress in Artificial Intelligence","978-3-540-77000-8","","","http://link.springer.com/10.1007/978-3-540-77002-2_9","","2007","2022-01-30 05:00:01","2022-01-30 05:00:01","2020-11-22 02:23:23","99-111","","","4874","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 61  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-77002-2_9","","","","TechSafety; AmbiguosSafety; Other-org","","Neves, José; Santos, Manuel Filipe; Machado, José Manuel","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V8C7V962","blogPost","2020","Roodman, David","Modeling the Human Trajectory","Open Philanthropy","","","","https://www.openphilanthropy.org/blog/modeling-human-trajectory","In arriving at our funding priorities---including criminal justice reform, farm animal welfare, pandemic preparedness, health-related science, and artificial intelligence safety---Open Philanthropy has pondered profound questions. How much should we care about people who will live far in the","2020-06-15","2022-01-30 05:00:01","2022-01-30 05:00:01","2020-08-31 18:03:20","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/MXIJ92V3/modeling-human-trajectory.html","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSRNPTDU","journalArticle","2018","Danzig, Richard","Managing Loss of Control as Many Militaries Pursue Technological Superiority","Arms Control Today","","","","","","2018-05-30","2022-01-30 05:00:00","2022-01-30 05:00:00","","40","","7","48","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000037","","","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WIJRJ4JN","blogPost","2020","Barnett, Matthew","Malign generalization without internal search","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search","In my last post, I challenged the idea that inner alignment failures should be explained by appealing to agents which perform explicit internal search. By doing so, I argued that we should instead appeal to the more general concept of  malign generalization, and treat mesa-misalignment as a special case.  Unfortunately, the post was light on examples of what we should be worrying about instead of mesa-misalignment. Evan Hubinger wrote, Personally, I think there is a meaningful sense in which all the models I'm most worried about do some sort of search internally (at least to the same extent that humans do search internally), but I'm definitely uncertain about that.Wei Dai expressed confusion why I would want to retreat to malign generalization without some sort of concrete failure mode in mind, Can you give some realistic examples/scenarios of “malign generalization” that does not involve mesa optimization? I’m not sure what kind of thing you’re actually worried about here.In this post, I will outline a general category of agents which may exhibit malign generalization without internal search, and then will provide a concrete example of an agent in the category. Then I will argue that, rather than being a very narrow counterexample, this class of agents could be competitive with search-based agents.  THE SWITCH CASE AGENT Consider an agent governed by the following general behavior,  LOOP:State = GetStateOfWorld(Observation)IF State == 1:PerformActionSequence1() IF State == 2:PerformActionSequence2()...END_LOOP  It's clear that this agent does not perform any internal search for strategies: it doesn't operate by choosing actions which rank highly according to some sort of internal objective function. While you could potentially rationalize its behavior according to some observed-utility function, this would generally lead to more confusion than clarity. However, this agent could still be malign in the following way. Suppose the agent is 'mistaken' about the s","2020-01-12","2022-01-30 05:00:00","2022-01-30 05:00:00","2020-09-07 18:23:46","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/WGMUBKSH/malign-generalization-without-internal-search.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ADSHWBTF","journalArticle","1973","Michie, Donald","Machines and the Theory of Intelligence","Nature","","0028-0836, 1476-4687","10.1038/241507a0","http://www.nature.com/articles/241507a0","","1973-02","2022-01-30 05:00:00","2022-01-30 05:00:00","2020-11-22 02:23:08","507-512","","5391","241","","Nature","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000021","","","","TechSafety; AmbiguosSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4NSAU4ME","journalArticle","2019","Sarma, Gopal P.; Hay, Nick J.","Mammalian Value Systems","Informatica","","","","http://arxiv.org/abs/1607.08289","Characterizing human values is a topic deeply interwoven with the sciences, humanities, art, and many other human endeavors. In recent years, a number of thinkers have argued that accelerating trends in computer science, cognitive science, and related disciplines foreshadow the creation of intelligent machines which meet and ultimately surpass the cognitive abilities of human beings, thereby entangling an understanding of human values with future technological development. Contemporary research accomplishments suggest sophisticated AI systems becoming widespread and responsible for managing many aspects of the modern world, from preemptively planning users' travel schedules and logistics, to fully autonomous vehicles, to domestic robots assisting in daily living. The extrapolation of these trends has been most forcefully described in the context of a hypothetical ""intelligence explosion,"" in which the capabilities of an intelligent software agent would rapidly increase due to the presence of feedback loops unavailable to biological organisms. The possibility of superintelligent agents, or simply the widespread deployment of sophisticated, autonomous AI systems, highlights an important theoretical problem: the need to separate the cognitive and rational capacities of an agent from the fundamental goal structure, or value system, which constrains and guides the agent's actions. The ""value alignment problem"" is to specify a goal structure for autonomous agents compatible with human values. In this brief article, we suggest that recent ideas from affective neuroscience and related disciplines aimed at characterizing neurological and behavioral universals in the mammalian class provide important conceptual foundations relevant to describing human values. We argue that the notion of ""mammalian value systems"" points to a potential avenue for fundamental research in AI safety and AI ethics.","2019-01-21","2022-01-30 05:00:00","2022-01-30 05:00:00","2020-12-13 23:38:36","","","3","41","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 11  arXiv: 1607.08289","","/Users/jacquesthibodeau/Zotero/storage/WKTKGKUA/Sarma and Hay - 2019 - Mammalian Value Systems.pdf; /Users/jacquesthibodeau/Zotero/storage/FWKZNIR2/1607.html","","TechSafety; Other-org","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Computers and Society; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"486WCWQG","blogPost","2015","Steinhardt, Jacob","Long-Term and Short-Term Challenges to Ensuring the Safety of AI Systems","Academically Interesting","","","","https://jsteinhardt.wordpress.com/2015/06/24/long-term-and-short-term-challenges-to-ensuring-the-safety-of-ai-systems/","Introduction There has been much recent discussion about AI risk, meaning specifically the potential pitfalls (both short-term and long-term) that AI with improved capabilities could create for soc…","2015-06-24","2022-01-30 04:59:59","2022-01-30 04:59:59","2020-11-21 17:08:14","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/J7TAZJDH/long-term-and-short-term-challenges-to-ensuring-the-safety-of-ai-systems.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QUISV4GK","blogPost","2021","Shimi, Adam; Campolo, Michele; Collman, Joe","Literature Review on Goal-Directedness","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/cfXwr6NC9AqZ9kr8g/literature-review-on-goal-directedness","INTRODUCTION: QUESTIONING GOALS Goals play a central role in almost all thinking in the AI existential risk research. Common scenarios assume misaligned goals, be it from a single AGI (paperclip maximizer) or multiple advanced AI optimizing things we don’t want (Paul Christiano’s What Failure Looks Like). Approaches around this issue ask for learning the right goals (value/preference learning), allowing the correction of a goal on the fly (corrigibility), or even removing incentives for forming goals (CAIS). But what are goals, and what does it mean to pursue one? As far as we know, Rohin Shah’s series of four posts were the first public and widely-read work questioning goals and their inevitability in AI Alignment. These posts investigate the hypothesis that goals are necessary, and outline possible alternatives. Shah calls the property of following a goal “goal-directedness”; but he doesn’t define it: I think of this as a concern about long-term goal-directed behavior. Unfortunately, it’s not clear how to categorize behavior as goal-directed vs. not. Intuitively, any agent that searches over actions and chooses the one that best achieves some measure of “goodness” is goal-directed (though there are exceptions, such as the agent that selects actions that begin with the letter “A”). (ETA: I also think that agents that show goal-directed behavior because they are looking at some other agent are not goal-directed themselves -- see this comment.) However, this is not a necessary condition: many humans are goal-directed, but there is no goal baked into the brain that they are using to choose actions. Later on, he explains that his “definition” of goal-directedness relies more on intuitions: Not all behavior can be thought of as goal-directed (primarily because I allowed the category to be defined by fuzzy intuitions rather than something more formal) Clearly, fuzzy intuitions are not enough to decide whether or not to focus on less goal-directed alternatives, if o","2021-01-18","2022-01-30 04:59:59","2022-01-30 04:59:59","2021-11-13 21:58:04","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBQUNTRR","bookSection","2018","Jilk, David J.","Limits to Verification and Validation of Agentic Behavior","Artificial Intelligence Safety and Security","","","","http://arxiv.org/abs/1604.06963","Verification and validation of agentic behavior have been suggested as important research priorities in efforts to reduce risks associated with the creation of general artificial intelligence (Russell et al 2015). In this paper we question the appropriateness of using language of certainty with respect to efforts to manage that risk. We begin by establishing a very general formalism to characterize agentic behavior and to describe standards of acceptable behavior. We show that determination of whether an agent meets any particular standard is not computable. We discuss the extent of the burden associated with verification by manual proof and by automated behavioral governance. We show that to ensure decidability of the behavioral standard itself, one must further limit the capabilities of the agent. We then demonstrate that if our concerns relate to outcomes in the physical world, attempts at validation are futile. Finally, we show that layered architectures aimed at making these challenges tractable mistakenly equate intentions with actions or outcomes, thereby failing to provide any guarantees. We conclude with a discussion of why language of certainty should be eradicated from the conversation about the safety of general artificial intelligence.","2018","2022-01-30 04:59:59","2022-01-30 04:59:59","2020-12-13 20:00:55","225-234","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000008[s0]  arXiv: 1604.06963","","/Users/jacquesthibodeau/Zotero/storage/BXXTPQ43/Jilk - 2016 - Limits to Verification and Validation of Agentic B.pdf; /Users/jacquesthibodeau/Zotero/storage/WTZWJK5S/1604.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence; I.2.0; F.3.1; K.4.1; D.2.4","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GKGQKVFM","conferencePaper","2018","Huang, Jessie; Wu, Fa; Precup, Doina; Cai, Yang","Learning Safe Policies with Expert Guidance","Advances in Neural Information Processing Systems 31 (NeurIPS 2018)","","","","http://arxiv.org/abs/1805.08313","We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the ""follow-the-perturbed-leader"" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.","2018-11-21","2022-01-30 04:59:59","2022-01-30 04:59:59","2020-11-14 00:44:35","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000018  arXiv: 1805.08313","","/Users/jacquesthibodeau/Zotero/storage/M5XBAMST/Huang et al. - 2018 - Learning Safe Policies with Expert Guidance.pdf; /Users/jacquesthibodeau/Zotero/storage/KWA7CVFJ/Huang et al. - 2018 - Learning Safe Policies with Expert Guidance.pdf; /Users/jacquesthibodeau/Zotero/storage/VJPZVNU8/1805.html; /Users/jacquesthibodeau/Zotero/storage/PHMBRHUV/1805.html","","TechSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2018","","","","","","","","","","","","","","",""
"GWGXUC5S","conferencePaper","2019","Lerer, Adam; Peysakhovich, Alexander","Learning Existing Social Conventions via Observationally Augmented Self-Play","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","","","","http://arxiv.org/abs/1806.10071","In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.","2019-03-13","2022-01-30 04:59:59","2022-01-30 04:59:59","2020-11-14 00:38:01","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000021  arXiv: 1806.10071","","/Users/jacquesthibodeau/Zotero/storage/7FSUZWAU/Lerer and Peysakhovich - 2019 - Learning Existing Social Conventions via Observati.pdf; /Users/jacquesthibodeau/Zotero/storage/UTVCDMHZ/1806.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"N5K8PMXZ","blogPost","2020","Shimi, Adam","Locality of goals","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/HkWB5KCJQ2aLsMzjt/locality-of-goals","INTRODUCTION Studying goal-directedness produces two kinds of questions: questions about goals, and questions about being directed towards a goal. Most of my previous posts focused on the second kind; this one shifts to the first kind. Assume some goal-directed system with a known goal. The nature of this goal will influence which issues of safety the system might have. If the goal focuses on the input, the system might wirehead itself and/or game its specification. On the other hand, if the goal lies firmly in the environment, the system might have convergent instrumental subgoals and/or destroy any unspecified value. Locality aims at capturing this distinction. Intuitively, the locality of the system's goal captures how far away from the system one must look to check the accomplishment of the goal.  Let's give some examples:  * The goal of ""My sensor reaches the number 23"" is very local, probably    maximally local.  * The goal of ""Maintain the temperature of the room at 23 °C"" is less local,    but still focused on a close neighborhood of the system.  * The goal of ""No death from cancer in the whole world"" is even less local. Locality isn't about how the system extract a model of the world from its input, but about whether and how much it cares about the world beyond it. STARTING POINTS This intuition about locality came from the collision of two different classification of goals: the first from from Daniel Dennett and the second from Evan Hubinger. THERMOSTATS AND GOALS In ""The Intentional Stance"", Dennett explains, extends and defends... the  intentional stance. One point he discusses is his liberalism: he is completely comfortable with admitting ridiculously simple systems like thermostats in the club of intentional systems -- to give them meaningful mental states about beliefs, desires and goals. Lest we readers feel insulted at the comparison, Dennett nonetheless admits that the goals of a thermostat differ from ours. Going along with the gag, we m","2020-06-22","2022-01-30 04:59:59","2022-01-30 04:59:59","2020-08-31 17:43:12","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/876B9RDT/locality-of-goals.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IP5NBM2T","manuscript","2017","Eysenbach, Benjamin; Gu, Shixiang; Ibarz, Julian; Levine, Sergey","Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning","","","","","http://arxiv.org/abs/1711.06782","Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and reset policy, with the reset policy resetting the environment for a subsequent attempt. By learning a value function for the reset policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the reset policy can greatly reduce the number of manual resets required to learn a task, can reduce the number of unsafe actions that lead to non-reversible states, and can automatically induce a curriculum.","2017-11-17","2022-01-30 04:59:59","2022-01-30 04:59:59","2020-11-21 17:26:04","","","","","","","Leave no Trace","","","","","","","","","","","","arXiv.org","","ZSCC: 0000081  arXiv: 1711.06782","","/Users/jacquesthibodeau/Zotero/storage/UIPW3M67/Eysenbach et al. - 2017 - Leave no Trace Learning to Reset for Safe and Aut.pdf; /Users/jacquesthibodeau/Zotero/storage/I3RPW323/1711.html","","TechSafety; Other-org","Computer Science - Machine Learning; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IW9U6MXI","conferencePaper","2018","Koller, Torsten; Berkenkamp, Felix; Turchetta, Matteo; Krause, Andreas","Learning-based Model Predictive Control for Safe Exploration","2018 IEEE Conference on Decision and Control (CDC)","","","","http://arxiv.org/abs/1803.08287","Learning-based methods have been successful in solving complex control tasks without significant prior knowledge about the system. However, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that can provide provable high-probability safety guarantees. To this end, we exploit regularity assumptions on the dynamics in terms of a Gaussian process prior to construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we do not assume that model uncertainties are independent. Based on these predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. In our experiments, we show that the resulting algorithm can be used to safely and efficiently explore and learn about dynamic systems.","2018-11-07","2022-01-30 04:59:59","2022-01-30 04:59:59","2020-12-13 23:12:15","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000224  arXiv: 1803.08287","","/Users/jacquesthibodeau/Zotero/storage/AWZVSTAT/Koller et al. - 2018 - Learning-based Model Predictive Control for Safe E.pdf; /Users/jacquesthibodeau/Zotero/storage/DBTEAK74/1803.html","","TechSafety; Other-org","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; Electrical Engineering and Systems Science - Systems and Control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 IEEE Conference on Decision and Control (CDC)","","","","","","","","","","","","","","",""
"P65JAJZW","blogPost","2018","Steinhardt, Jacob","Latent Variables and Model Mis-Specification","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/gnvrixhDfG7S2TpNL/latent-variables-and-model-mis-specification","Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin's note: So far, we’ve seen that ambitious value learning needs to understand human biases, and that we can't simply learn the biases in tandem with the reward. Perhaps we could hardcode a specific model of human biases? Such a model is likely to be incomplete and inaccurate, but it will perform better than assuming an optimal human, and as we notice failure modes we can improve the model. In the language of this post by Jacob Steinhardt (original  here), we are using a mis-specified human model. The post talks about why model mis-specification is worse than it may seem at first glance. This post is fairly technical and may not be accessible if you don’t have a background in machine learning. If so, you can skip this post and still understand the rest of the posts in the sequence. However, if you want to do ML-related safety research, I strongly recommend putting in the effort to understand the problems that can arise with mis-specification. -------------------------------------------------------------------------------- Machine learning is very good at optimizing predictions to match an observed signal — for instance, given a dataset of input images and labels of the images (e.g. dog, cat, etc.), machine learning is very good at correctly predicting the label of a new image. However, performance can quickly break down as soon as we care about criteria other than predicting observables. There are several cases where we might care about such criteria:  * In scientific investigations, we often care less about predicting a specific    observable phenomenon, and more about what that phenomenon implies about an    underlying scientific theory.  * In economic analysis, we are most interested in what policies will lead to    desirable outcomes. This requires predicting what would counterfactually    happen if we were to enact the policy, which we (usually) don’t have any data    about.  * In ma","2018","2022-01-30 04:59:59","2022-01-30 04:59:59","2020-12-17 04:36:26","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/KIQANXSR/gnvrixhDfG7S2TpNL.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"42ZE8I25","journalArticle","2011","Sullins, John P.","Introduction: Open Questions in Roboethics","Philosophy & Technology","","2210-5433, 2210-5441","10.1007/s13347-011-0043-6","http://link.springer.com/10.1007/s13347-011-0043-6","","2011-09","2022-01-30 04:59:59","2022-01-30 04:59:59","2020-11-22 02:23:55","233-238","","3","24","","Philos. Technol.","Introduction","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000043","","/Users/jacquesthibodeau/Zotero/storage/6PNZQ3NW/Sullins - 2011 - Introduction Open Questions in Roboethics.pdf","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X3INWFGE","bookSection","2004","Turing, Alan","Intelligent Machinery, A Heretical Theory (c.1951)","The Essential Turing","978-0-19-825079-1 978-0-19-191652-6","","","https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198250791.001.0001/isbn-9780198250791-book-part-18","Turing gave the presentation ‘Intelligent Machinery, A Heretical Theory’ on a radio discussion programme called The ’51 Society. Named after the year in which the programme first went to air, The ’51 Society was produced by the BBC Home Service at their Manchester studio and ran for several years. A presentation by the week’s guest would be followed by a panel discussion. Regulars on the panel included Max Newman, Professor of Mathematics at Manchester, the philosopher Michael Polanyi, then Professor of Social Studies at Manchester, and the mathematician Peter Hilton, a younger member of Newman’s department at Manchester who had worked with Turing and Newman at Bletchley Park. Turing’s target in ‘Intelligent Machinery, A Heretical Theory’ is the claim that ‘You cannot make a machine to think for you’ (p. 472). A common theme in his writing is that if a machine is to be intelligent, then it will need to ‘learn by experience’ (probably with some pre-selection, by an external educator, of the experiences to which the machine will be subjected). The present article continues the discussion of machine learning begun in Chapters 10 and 11. Turing remarks that the ‘human analogy alone’ suggests that a process of education ‘would in practice be an essential to the production of a reasonably intelligent machine within a reasonably short space of time’ (p. 473). He emphasizes the point, also made in Chapter 11, that one might ‘start from a comparatively simple machine, and, by subjecting it to a suitable range of ‘‘experience’’ transform it into one which was more elaborate, and was able to deal with a far greater range of contingencies’ (p. 473). Turing goes on to give some indication of how learning might be accomplished, introducing the idea of a machine’s building up what he calls ‘indexes of experiences’ (p. 474). (This idea is not mentioned elsewhere in his writings.) An example of an index of experiences is a list (ordered in some way) of situations in which the machine has found itself, coupled with the action that was taken, and the outcome, good or bad. The situations are described in terms of features.","2004-09-09","2022-01-30 04:59:58","2022-01-30 04:59:58","2020-11-22 05:05:23","","","","","","","","","","","","Oxford University Press","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000011  DOI: 10.1093/oso/9780198250791.003.0018","","","","TechSafety; Other-org","","","","","","","Turing, Alan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SH5EK42K","blogPost","2020","Harth, Rafael","Inner Alignment: Explain like I'm 12 Edition","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/AHhCrJ2KpTjsCSwbt/inner-alignment-explain-like-i-m-12-edition","(This is an unofficial explanation of Inner Alignment based on the Miri paper  Risks from Learned Optimization in Advanced Machine Learning Systems (which is almost identical to the LW sequence) and the Future of Life podcast with Evan Hubinger (Miri/LW). It's meant for anyone who found the sequence too long/challenging/technical to read.) Note that bold and italics means ""this is a new term I'm introducing,"" whereas  underline and italics is used for emphasis. WHAT IS INNER ALIGNMENT? Let's start with an abridged guide to how Machine Learning works:  1. Choose a problem  2. Decide on a space of possible solutions  3. Find a good solution from that space If the problem is ""find a tool that can look at any image and decide whether or not it contains a cat,"" then each conceivable set of rules for answering this question (formally, each function from the set of all pixels to the set {yes, no }) defines one solution. We call each such solution a model. The space of possible models is depicted below. Since that's all possible models, most of them are utter nonsense. Pick a random one, and you're as likely to end up with a car-recognizer than a cat-recognizer – but far more likely with an algorithm that does nothing we can interpret. Note that even the examples I annotated aren't typical – most models would be more complex while still doing nothing related to cats. Nonetheless, somewhere in there is a model that would do a decent job on our problem. In the above, that's the one that says, ""I look for cats."" How does ML find such a model? One way that does not work is trying out all of them. That's because the space is too large: it might contain over 101000000  candidates. Instead, there's this thing called Stochastic Gradient Descent (SGD) . Here's how it works: SGD begins with some (probably terrible) model and then proceeds in steps. In each step, it switches to another model that is ""close"" and hopefully a little better. Eventually, it stops and outputs the mo","2020-08-01","2022-01-30 04:59:58","2022-01-30 04:59:58","2020-08-27 16:39:11","","","","","","","Inner Alignment","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/RVSBTVB2/inner-alignment-explain-like-i-m-12-edition.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2QFZWRU","blogPost","2019","Zabel, Claire; Muehlhauser, Luke","Information security careers for GCR reduction","Effective Altruism Forum","","","","https://forum.effectivealtruism.org/posts/ZJiCfwTy5dC4CoxqA/information-security-careers-for-gcr-reduction","Update 2019-12-14: There is now a Facebook group for discussion of infosec careers in EA (including for GCR reduction); join here This post was written by Claire Zabel and Luke Muehlhauser, based on their experiences as Open Philanthropy Project staff members working on global catastrophic risk reduction, though this post isn't intended to represent an official position of Open Phil. SUMMARY In this post, we summarize why we think information security (preventing unauthorized users, such as hackers, from accessing or altering information) may be an impactful career path for some people who are focused on reducing global catastrophic risks (GCRs). If you'd like to hear about job opportunities in information security and global catastrophic risk, you can fill out this form created by 80,000 Hours, and their staff will get in touch with you if something might be a good fit. In brief, we think:  * Information security (infosec) expertise may be crucial for addressing    catastrophic risks related to AI and biosecurity.  * More generally, security expertise may be useful for those attempting to    reduce GCRs, because such work sometimes involves engaging with information    that could do harm if misused.  * We have thus far found it difficult to hire security professionals who aren't    motivated by GCR reduction to work with us and some of our GCR-focused    grantees, due to the high demand for security experts and the unconventional    nature of our situation and that of some of our grantees.  * More broadly, we expect there to continue to be a deficit of GCR-focused    security expertise in AI and biosecurity, and that this deficit will result    in several GCR-specific challenges and concerns being under-addressed by    default.  * It’s more likely than not that within 10 years, there will be dozens of    GCR-focused roles in information security, and some organizations are already    looking for candidates that fit their needs (and would hire them now, if they","2019","2022-01-30 04:59:58","2022-01-30 04:59:58","2020-12-15 00:11:43","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/HRWPS3BN/information-security-careers-for-gcr-reduction.html","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQ3QRMVD","manuscript","2019","Sevilla, Jaime; Moreno, Pablo","Implications of Quantum Computing for Artificial Intelligence alignment research","","","","","http://arxiv.org/abs/1908.07613","We explain some key features of quantum computing via three heuristics and apply them to argue that a deep understanding of quantum computing is unlikely to be helpful to address current bottlenecks in Artificial Intelligence Alignment. Our argument relies on the claims that Quantum Computing leads to compute overhang instead of algorithmic overhang, and that the difficulties associated with the measurement of quantum states do not invalidate any major assumptions of current Artificial Intelligence Alignment research agendas. We also discuss tripwiring, adversarial blinding, informed oversight and side effects as possible exceptions.","2019-08-24","2022-01-30 04:59:58","2022-01-30 04:59:58","2019-12-16 22:41:08","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 1908.07613","","/Users/jacquesthibodeau/Zotero/storage/5X947SXX/Sevilla and Moreno - 2019 - Implications of Quantum Computing for Artificial I.pdf; /Users/jacquesthibodeau/Zotero/storage/VTG2NVXB/Sevilla and Moreno - 2019 - Implications of Quantum Computing for Artificial I.pdf; /Users/jacquesthibodeau/Zotero/storage/AQSE5TCF/1908.html; /Users/jacquesthibodeau/Zotero/storage/VJGCCXHW/1908.html","","MetaSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Emerging Technologies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WGZA24CE","journalArticle","2017","Bogosian, Kyle","Implementation of Moral Uncertainty in Intelligent Machines","Minds and Machines","","1572-8641","10.1007/s11023-017-9448-z","https://doi.org/10.1007/s11023-017-9448-z","The development of artificial intelligence will require systems of ethical decision making to be adapted for automatic computation. However, projects to implement moral reasoning in artificial moral agents so far have failed to satisfactorily address the widespread disagreement between competing approaches to moral philosophy. In this paper I argue that the proper response to this situation is to design machines to be fundamentally uncertain about morality. I describe a computational framework for doing so and show that it efficiently resolves common obstacles to the implementation of moral philosophy in intelligent machines.","2017-12-01","2022-01-30 04:59:58","2022-01-30 04:59:58","2020-12-13 22:17:16","591-608","","4","27","","Minds & Machines","","","","","","","","en","","","","","Springer Link","","ZSCC: 0000031","","/Users/jacquesthibodeau/Zotero/storage/DSITHXNZ/Bogosian - 2017 - Implementation of Moral Uncertainty in Intelligent.pdf","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBKBAF43","manuscript","2018","Noothigattu, Ritesh; Bouneffouf, Djallel; Mattei, Nicholas; Chandra, Rachita; Madan, Piyush; Varshney, Kush; Campbell, Murray; Singh, Moninder; Rossi, Francesca","Interpretable Multi-Objective Reinforcement Learning through Policy Orchestration","","","","","http://arxiv.org/abs/1809.08343","Autonomous cyber-physical agents and systems play an increasingly large role in our lives. To ensure that agents behave in ways aligned with the values of the societies in which they operate, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. These constraints and norms can come from any number of sources including regulations, business process guidelines, laws, ethical principles, social norms, and moral values. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations of the task, and reinforcement learning to learn to maximize the environment rewards. More precisely, we assume that an agent can observe traces of behavior of members of the society but has no access to the explicit set of constraints that give rise to the observed behavior. Inverse reinforcement learning is used to learn such constraints, that are then combined with a possibly orthogonal value function through the use of a contextual bandit-based orchestrator that picks a contextually-appropriate choice between the two policies (constraint-based and environment reward-based) when taking actions. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using a Pac-Man domain and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.","2018-09-21","2022-01-30 04:59:58","2022-01-30 04:59:58","2020-12-13 23:27:45","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000027  arXiv: 1809.08343","","/Users/jacquesthibodeau/Zotero/storage/BZ3MDGSE/Noothigattu et al. - 2018 - Interpretable Multi-Objective Reinforcement Learni.pdf; /Users/jacquesthibodeau/Zotero/storage/CHJIW45A/1809.html","","TechSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFV3R6X3","conferencePaper","2019","Sarma, Gopal P.; Safron, Adam; Hay, Nick J.","Integrative Biological Simulation, Neuropsychology, and AI Safety","Proceedings of the AAAI Workshop on Artificial Intelligence Safety 2019","","","","http://arxiv.org/abs/1811.03493","We describe a biologically-inspired research agenda with parallel tracks aimed at AI and AI safety. The bottom-up component consists of building a sequence of biophysically realistic simulations of simple organisms such as the nematode $Caenorhabditis$ $elegans$, the fruit fly $Drosophila$ $melanogaster$, and the zebrafish $Danio$ $rerio$ to serve as platforms for research into AI algorithms and system architectures. The top-down component consists of an approach to value alignment that grounds AI goal structures in neuropsychology, broadly considered. Our belief is that parallel pursuit of these tracks will inform the development of value-aligned AI systems that have been inspired by embodied organisms with sensorimotor integration. An important set of side benefits is that the research trajectories we describe here are grounded in long-standing intellectual traditions within existing research communities and funding structures. In addition, these research programs overlap with significant contemporary themes in the biological and psychological sciences such as data/model integration and reproducibility.","2019-01-21","2022-01-30 04:59:58","2022-01-30 04:59:58","2020-11-14 00:58:15","","","","","","","","","","","","","Honolulu HI USA","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 0  arXiv: 1811.03493","","/Users/jacquesthibodeau/Zotero/storage/6GB52FT9/Sarma et al. - 2019 - Integrative Biological Simulation, Neuropsychology.pdf; /Users/jacquesthibodeau/Zotero/storage/K7JEEMCE/1811.html","","TechSafety; Other-org","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Neurons and Cognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI Workshop on Artificial Intelligence Safety","","","","","","","","","","","","","","",""
"QMUXGFJH","blogPost","2020","Barnett, Matthew","Inner alignment requires making assumptions about human values","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/6m5qqkeBTrqQsegGi/inner-alignment-requires-making-assumptions-about-human","Many approaches to AI alignment require making assumptions about what humans want. On a first pass, it might appear that inner alignment is a sub-component of AI alignment that doesn't require making these assumptions. This is because if we define the problem of inner alignment to be the problem of how to train an AI to be aligned with arbitrary reward functions, then a solution would presumably have no dependence on any particular reward function. We could imagine an alien civilization solving the same problem, despite using very different reward functions to train their AIs. Unfortunately, the above argument fails because aligning an AI with our values requires giving the AI extra information that is not encoded directly in the reward function (under reasonable assumptions). The argument for my thesis is subtle, and so I will break it into pieces. First, I will more fully elaborate what I mean by inner alignment. Then I will argue that the definition implies that we can't come up with a full solution without some dependence on human values. Finally, I will provide an example, in order to make this discussion less abstract. CHARACTERIZING INNER ALIGNMENT In the last few posts I wrote (1, 2), I attempted to frame the problem of inner alignment in a way that wasn't too theory-laden. My concern was that the  previous characterization was dependent on a solving particular outcome where you have an AI that is using an explicit outer loop to evaluate strategies based on an explicit internal search. In the absence of an explicit internal objective function, it is difficult to formally define whether an agent is ""aligned"" with the reward function that is used to train it. We might therefore define alignment as the ability of our agent to perform well on the test distribution. However, if the test set is sampled from the same distribution as the training data, this definition is equivalent to the performance of a model in standard machine learning, and we haven't actual","2020-01-20","2022-01-30 04:59:58","2022-01-30 04:59:58","2020-09-07 18:20:35","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/CCM6VTKA/inner-alignment-requires-making-assumptions-about-human.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9V7CUBVJ","blogPost","2020","Wentworth, John S","Infinite Data/Compute Arguments in Alignment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/7CJBiHYxebTmMfGs3/infinite-data-compute-arguments-in-alignment","","2020-08-04","2022-01-30 04:59:58","2022-01-30 04:59:58","2020-08-24 20:19:13","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/BQ2VHVKH/infinite-data-compute-arguments-in-alignment.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNT8IZVP","conferencePaper","2019","Eckersley, Peter","Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)","SafeAI 2019: Proceedings of the AAAI Workshop on Artificial Intelligence Safety 2019","","","","http://arxiv.org/abs/1901.00064","Utility functions or their equivalents (value functions, objective functions, loss functions, reward functions, preference orderings) are a central tool in most current machine learning systems. These mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each other. Ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions (in such cases, the objective function is a social welfare function). We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense. We explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total orders. We show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility results. We close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from AI systems.","2019-03-04","2022-01-30 04:59:58","2022-01-30 04:59:58","2020-11-14 00:58:06","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: NoCitationData[s1]  ACC: 25  arXiv: 1901.00064","","/Users/jacquesthibodeau/Zotero/storage/F97PFGRR/Eckersley - 2019 - Impossibility and Uncertainty Theorems in AI Value.pdf; /Users/jacquesthibodeau/Zotero/storage/BWXENTM8/1901.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"394DPAGJ","conferencePaper","2011","Halpern, Joseph Y.; Pass, Rafael","I Don't Want to Think About it Now: Decision Theory With Costly Computation","Proceedings of the Twelfth International Conference on Principles of Knowledge Representation and Reasoning","","","","http://arxiv.org/abs/1106.2657","Computation plays a major role in decision making. Even if an agent is willing to ascribe a probability to all states and a utility to all outcomes, and maximize expected utility, doing so might present serious computational problems. Moreover, computing the outcome of a given act might be difficult. In a companion paper we develop a framework for game theory with costly computation, where the objects of choice are Turing machines. Here we apply that framework to decision theory. We show how well-known phenomena like first-impression-matters biases (i.e., people tend to put more weight on evidence they hear early on), belief polarization (two people with different prior beliefs, hearing the same evidence, can end up with diametrically opposed conclusions), and the status quo bias (people are much more likely to stick with what they already have) can be easily captured in that framework. Finally, we use the framework to define some new notions: value of computational information (a computational variant of value of information) and and computational value of conversation.","2011-06-14","2022-01-30 04:59:58","2022-01-30 04:59:58","2020-11-22 02:29:10","","","","","","","I Don't Want to Think About it Now","","","","","","","","","","","","arXiv.org","","ZSCC: 0000019[s0]  arXiv: 1106.2657","","/Users/jacquesthibodeau/Zotero/storage/CWHKI6EW/Halpern and Pass - 2011 - I Don't Want to Think About it NowDecision Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/H22KIAM5/1106.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PE2F3NVR","manuscript","2020","Schneider, Johannes","Humans learn too: Better Human-AI Interaction using Optimized Human Inputs","","","","","http://arxiv.org/abs/2009.09266","Humans rely more and more on systems with AI components. The AI community typically treats human inputs as a given and optimizes AI models only. This thinking is one-sided and it neglects the fact that humans can learn, too. In this work, human inputs are optimized for better interaction with an AI model while keeping the model fixed. The optimized inputs are accompanied by instructions on how to create them. They allow humans to save time and cut on errors, while keeping required changes to original inputs limited. We propose continuous and discrete optimization methods modifying samples in an iterative fashion. Our quantitative and qualitative evaluation including a human study on different hand-generated inputs shows that the generated proposals lead to lower error rates, require less effort to create and differ only modestly from the original samples.","2020-09-19","2022-01-30 04:59:57","2022-01-30 04:59:57","2020-11-14 00:52:26","","","","","","","Humans learn too","","","","","","","","","","","","arXiv.org","","ZSCC: 0000001  arXiv: 2009.09266","","/Users/jacquesthibodeau/Zotero/storage/XP5PX8VP/Schneider - 2020 - Humans learn too Better Human-AI Interaction usin.pdf; /Users/jacquesthibodeau/Zotero/storage/JXASWCQG/2009.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Human-Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7EKAANH7","manuscript","2018","Baek, Jongmin Jerome","How To Solve Moral Conundrums with Computability Theory","","","","","http://arxiv.org/abs/1805.08347","Various moral conundrums plague population ethics: The Non-Identity Problem, The Procreation Asymmetry, The Repugnant Conclusion, and more. I argue that the aforementioned moral conundrums have a structure neatly accounted for, and solved by, some ideas in computability theory. I introduce a mathematical model based on computability theory and show how previous arguments pertaining to these conundrums fit into the model. This paper proceeds as follows. First, I do a very brief survey of the history of computability theory in moral philosophy. Second, I follow various papers, and show how their arguments fit into, or don't fit into, our model. Third, I discuss the implications of our model to the question why the human race should or should not continue to exist. Finally, I show that our model ineluctably leads us to a Confucian moral principle.","2018-05-21","2022-01-30 04:59:57","2022-01-30 04:59:57","2020-11-14 01:09:58","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000000  arXiv: 1805.08347","","/Users/jacquesthibodeau/Zotero/storage/CCQVF6AC/Baek - 2018 - How To Solve Moral Conundrums with Computability T.pdf; /Users/jacquesthibodeau/Zotero/storage/ZH342CDG/1805.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Logic in Computer Science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMXIEJIP","blogPost","2020","Carlsmith, Joseph","How Much Computational Power Does It Take to Match the Human Brain?","Open Philanthropy","","","","https://www.openphilanthropy.org/brain-computation-report","Open Philanthropy is interested in when AI systems will be able to perform various tasks that humans can perform (“AI timelines”). To inform our thinking, I investigated what evidence the human brain provides about the computational power","2020-09-11","2022-01-30 04:59:57","2022-01-30 04:59:57","2020-12-12 02:06:58","","","","","","","","","","","","","","en","","","","","","","ZSCC: NoCitationData[s2]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/Q6XHN4Q6/brain-computation-report.html","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NPZCK7NE","blogPost","2020","Rice, Issa","How does iterated amplification exceed human abilities?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/ajQzejMYizfX4dMWK/how-does-iterated-amplification-exceed-human-abilities","When I first started learning about IDA, I thought that agents trained using IDA would be human-level after the first stage, i.e. that Distill(H) would be human-level. As I've written about before, Paul later clarified this, so my new understanding is that after the first stage, the distilled agent will be super-human in some respects and infra-human in others, but wouldn't be ""basically human"" in any sense. But IDA is aiming to eventually be super-human in almost every way (because it's aiming to be competitive with unaligned AGI), so that raises some new questions:  1. If IDA isn't going to be human-level after the first stage, then at what     stage does IDA become at-least-human-level in almost every way?  2. What exactly is the limitation that prevents the first stage of IDA from     being human-level in almost every way?  3. When IDA eventually does become at-least-human-level in almost every way,     how is the limitation from (2) avoided? That brings me to Evans et al., which contains a description of IDA in section 0. The way IDA is set up in this paper leads me to believe that the answer to (2) above is that the human overseer cannot provide a sufficient number of demonstrations for the most difficult tasks. For example, maybe the human can provide enough demonstrations for the agent to learn to answer very simple questions (tasks in T0 in the paper) but it's too time-consuming for the human to answer enough complicated questions (say, in T100). My understanding is that IDA gets around this by having an amplified system that is itself automated (i.e. does not involve humans in a major way, so cannot be bottlenecked on the slowness of humans); this allows the amplified system to provide a sufficient number of demonstrations for the distillation step to work. So in the above view, the answer to (2) is that the limitation is the number of demonstrations the human can provide, and the answer to (3) is that the human can seed the IDA process with sufficient","2020-05-02","2022-01-30 04:59:57","2022-01-30 04:59:57","2020-09-01 20:44:26","","","","","","","How does iterated amplification exceed human abilities?","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/GNVB3F6I/how-does-iterated-amplification-exceed-human-abilities.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NV87FGU4","journalArticle","2019","Riedl, Mark O.","Human-Centered Artificial Intelligence and Machine Learning","Human Behavior and Emerging Technologies","","","","http://arxiv.org/abs/1901.11184","Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.","2019-01-30","2022-01-30 04:59:57","2022-01-30 04:59:57","2020-11-14 00:32:19","","","1","1","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000092  arXiv: 1901.11184","","/Users/jacquesthibodeau/Zotero/storage/NMJIIGJ5/Riedl - 2019 - Human-Centered Artificial Intelligence and Machine.pdf; /Users/jacquesthibodeau/Zotero/storage/7IGC3HKT/1901.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QP56TU5K","journalArticle","2018","Vamplew, Peter; Dazeley, Richard; Foale, Cameron; Firmin, Sally; Mummery, Jane","Human-aligned artificial intelligence is a multiobjective problem","Ethics and Information Technology","","1572-8439","10.1007/s10676-017-9440-6","https://doi.org/10.1007/s10676-017-9440-6","As the capabilities of artificial intelligence (AI) systems improve, it becomes important to constrain their actions to ensure their behaviour remains beneficial to humanity. A variety of ethical, legal and safety-based frameworks have been proposed as a basis for designing these constraints. Despite their variations, these frameworks share the common characteristic that decision-making must consider multiple potentially conflicting factors. We demonstrate that these alignment frameworks can be represented as utility functions, but that the widely used Maximum Expected Utility (MEU) paradigm provides insufficient support for such multiobjective decision-making. We show that a Multiobjective Maximum Expected Utility paradigm based on the combination of vector utilities and non-linear action–selection can overcome many of the issues which limit MEU’s effectiveness in implementing aligned AI. We examine existing approaches to multiobjective AI, and identify how these can contribute to the development of human-aligned intelligent agents.","2018-03-01","2022-01-30 04:59:57","2022-01-30 04:59:57","2020-12-13 21:48:09","27-40","","1","20","","Ethics Inf Technol","","","","","","","","en","","","","","Springer Link","","ZSCC: 0000067","","/Users/jacquesthibodeau/Zotero/storage/3M4DM2R7/Vamplew et al. - 2018 - Human-aligned artificial intelligence is a multiob.pdf","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"87DAV94P","journalArticle","2011","Baum, Seth D.; Goertzel, Ben; Goertzel, Ted G.","How long until human-level AI? Results from an expert assessment","Technological Forecasting and Social Change","","00401625","10.1016/j.techfore.2010.09.006","https://linkinghub.elsevier.com/retrieve/pii/S0040162510002106","","2011-01","2022-01-30 04:59:57","2022-01-30 04:59:57","2020-11-22 04:16:21","185-195","","1","78","","Technological Forecasting and Social Change","How long until human-level AI?","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000132","","","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5SU7Q5U","blogPost","2021","Costa, Guilhermo","How does bee learning compare with machine learning?","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/yW3Tct2iyBMzYhTw7/how-does-bee-learning-compare-with-machine-learning","This is a write-up of work I did as an Open Philanthropy intern. However, the conclusions don't necessarily reflect Open Phil's institutional view. ABSTRACT This post investigates the biological anchor framework for thinking about AI timelines, as espoused by Ajeya Cotra in her draft report. The basic claim of this framework is that we should base our estimates of the compute required to run a transformative model on our estimates of the compute used by the human brain (although, of course, defining what this means is complicated). This line of argument also implies that current machine learning models, some of which use amounts of compute comparable to that of bee brains, should have similar task performance as bees. In this post, I compare the performance and compute usage of both bees and machine learning models at few-shot image classification tasks. I conclude that the evidence broadly supports the biological anchor framework, and I update slightly towards the hypothesis that the compute usage of a transformative model is lower than that of the human brain. The full post is viewable in a Google Drive folder here. INTRODUCTION Ajeya Cotra wrote a draft report on AI timelines (Cotra, 2020) in which she estimates when transformative artificial intelligence might be developed. To do so, she compares the size of a transformative model (defined as the number of  FLOP/s required to run it) with the computational power of the human brain, as estimated in this Open Phil report (Carlsmith, 2020)[1]. She argues that a transformative model would use roughly similar amounts of compute as the human brain. As evidence for this, she claims that computer vision models are about as capable as bees in visual tasks, while using a similar amount of compute.[2] In this post, I (Guilhermo Costa) investigate this claim. To do so, I focus on the performance of bees at few-shot image classification, one of the most difficult tasks that bees are able to perform. I find that both the","2021-03-03","2022-01-30 04:59:57","2022-01-30 04:59:57","2021-11-14 16:13:18","","","","","","","How does bee learning compare with machine learning?","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/Q3R4XXQA/how-does-bee-learning-compare-with-machine-learning.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EBXVI57Q","bookSection","2017","Zheng, Ping; Akhmad, Mohammed-Asif","How Change Agencies Can Affect Our Path Towards a Singularity","The Technological Singularity: Managing the Journey","978-3-662-54033-6","","","https://doi.org/10.1007/978-3-662-54033-6_4","SummaryThis chapter uses the perspective of change agencies to analyse how agents (such as governments, international companies, entrepreneurs and individuals) innovate, interact, assimilate, consume and ultimately determine the direction of future technologies. These are the key components to the formation of technological singularity, i.e. an artificial intelligence becoming self-aware and self-evolving leading to an unprecedented rapid technological change in human civilization. General behaviours of change agents towards relevant technological research and development are discussed with a view to the economic and social implications. The interactions of key change agents can assist in the determination of future paths towards a singularity event or possibly even an ‘anti-singularity event’. Understanding the fundamental behaviours and motivations of change agents in technology development will increase our understanding of potential mechanisms to monitor and control developments such as Artificial Intelligence research to ensure that if and when singularity occurs it can be controlled and positively utilised for social and economic benefits.","2017","2022-01-30 04:59:57","2022-01-30 04:59:57","2020-11-24 02:59:42","87-101","","","","","","","The Frontiers Collection","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","ZSCC: NoCitationData[s1]  ACC: 0  DOI: 10.1007/978-3-662-54033-6_4","","","","MetaSafety; Other-org","Change Agency; Human Brain Function; Human Race; Singularity Event; Technological Change Process","Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUJIQIGS","blogPost","2020","Campolo, Michele","Goals and short descriptions","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/d4NgfKY3cq9yiBLSM/goals-and-short-descriptions","OUTLINE I develop some contents—previously introduced in the Value Learning sequence by Rohin Shah—more formally, to clarify the distinction between agents with and without a goal. Then I present related work and make some considerations on the relation between safety and goal-directedness. The appendix contains some details on the used formalism and can be skipped without losing much information.  A BRIEF PRELIMINARY In the first post of the Value Learning sequence, Shah compares two agents that exhibit the same behaviour (a winning strategy) when playing Tic-Tac-Toe, but are different in their design: one applies the minimax algorithm to the setting and rules of the game, while the other one follows a lookup table—you can think of its code as a long sequence of if-else statements. Shah highlights the difference in terms of generalisation: the first one would still win if the winning conditions were changed, while the lookup table would not. Generalisation is one of the components of goal-directedness, and lookup tables are among the least goal-directed agent designs. Here I want to point at another difference that exists between agents with and without a goal, based on the concept of algorithmic complexity. SETUP Most problems in AI consist in finding a function π∈AO, called policy in some contexts, where A={a1,…,am} and O={o1,…,on} indicate the sets of possible actions and observations. A deterministic policy can be written as a string π=ai 1ai2…ain with aik indicating the action taken when ok is observed. Here I consider a problem setting as a triplet (A,O,D) where D stands for some kind of environmental data—could be about, for example, the transition function in a MDP, or the structure of the elements in the search space O. Since I want to analyse behaviour across different environments, instead of considering one single policy I’ll sometimes refer to a more general function g (probably closer to the concept of “agent design”, rather than just “agent”) ma","2020-07-02","2022-01-30 04:59:56","2022-01-30 04:59:56","2020-08-31 17:44:34","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/KCK9EF55/goals-and-short-descriptions.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7APCD5ZA","blogPost","2019","Grotto, Andrew","Genetically Modified Organisms: A Precautionary Tale For AI Governance | AI Pulse","AI Pulse","","","","https://aipulse.org/genetically-modified-organisms-a-precautionary-tale-for-ai-governance-2/","The fruits of a long anticipated technology finally hit the market, with promise to extend human life, revolutionize production, improve consumer welfare, reduce poverty, and inspire countless yet-imagined innovations.","2019","2022-01-30 04:59:56","2022-01-30 04:59:56","2020-12-14 22:41:19","","","","","","","Genetically Modified Organisms","","","","","","","en","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/82Q3R6JG/Grotto - Genetically Modified Organisms A Precautionary Ta.pdf","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WPQKTKEI","blogPost","2017","Alexander, Scott","G.K. Chesterton On AI Risk","Slate Star Codex","","","","https://slatestarcodex.com/2017/04/01/g-k-chesterton-on-ai-risk/","[An SSC reader working at an Oxford library stumbled across a previously undiscovered manuscript of G.K. Chesterton’s, expressing his thoughts on AI, x-risk, and superintelligence. She was ki…","2017-04-01","2022-01-30 04:59:56","2022-01-30 04:59:56","2020-12-13 21:49:10","","","","","","","","","","","","","","en-US","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DKNWFMQS","conferencePaper","2020","Tsipras, Dimitris; Santurkar, Shibani; Engstrom, Logan; Ilyas, Andrew; Madry, Aleksander","From ImageNet to Image Classification: Contextualizing Progress on Benchmarks","Proceedings of the 37th International Conference on Machine Learning","","","","http://arxiv.org/abs/2005.11295","Building rich machine learning datasets in a scalable manner often necessitates a crowd-sourced data collection pipeline. In this work, we use human studies to investigate the consequences of employing such a pipeline, focusing on the popular ImageNet dataset. We study how specific design choices in the ImageNet creation process impact the fidelity of the resulting dataset---including the introduction of biases that state-of-the-art models exploit. Our analysis pinpoints how a noisy data collection pipeline can lead to a systematic misalignment between the resulting benchmark and the real-world task it serves as a proxy for. Finally, our findings emphasize the need to augment our current model training and evaluation toolkit to take such misalignments into account. To facilitate further research, we release our refined ImageNet annotations at https://github.com/MadryLab/ImageNetMultiLabel.","2020-05-22","2022-01-30 04:59:56","2022-01-30 04:59:56","2020-08-31 18:21:48","","","","","","","From ImageNet to Image Classification","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000041  arXiv: 2005.11295","","/Users/jacquesthibodeau/Zotero/storage/HTPNAJFE/Tsipras et al. - 2020 - From ImageNet to Image Classification Contextuali.pdf","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML 2020","","","","","","","","","","","","","","",""
"PT333BJ8","blogPost","2021","Xu, Mark; Shulman, Carl","Fractional progress estimates for AI timelines and implied resource requirements","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied","This post was written by Mark Xu based on interviews with Carl Shulman. It was paid for by Open Philanthropy but is not representative of their views. A draft was sent to Robin Hanson for review but received no response. SUMMARY  * Robin Hanson estimates the time until human-level AI by surveying experts    about the percentage progress to human-level that has happened in their    particular subfield in the last 20 years, and dividing the number of years by    the percentage progress.  * Such surveys look back on a period of extremely rapid growth of compute from    both hardware improvements and more recently skyrocketing spending.  * Hanson favors using estimates from subsets of researchers with lower progress    estimates to infer AI timelines requiring centuries worth of recent growth,    implying truly extraordinary sustained compute growth is necessary to surpass    human performance.  * Extrapolated compute levels are very large to astronomically large compared    to the neural computation that took place in evolution on Earth, and thus    likely far overestimate AI requirements and timelines. INTRODUCTION Suppose that you start with $1 that grows at 10% per year. At this rate, it will take ~241 years to get $10 billion ($1010). When will you think that you’re ten percent of the way there? You might say that you’re ten percent of the way to $10 billion when you have $1  billion. However, since your money is growing exponentially, it takes 217 years to go from $1 to $1 billion and only 24 more to go from $1 billion to $10  billion, even though the latter gap is larger in absolute terms. If you tried to guess when you would have $10 billion by taking 10x the amount of time to $1  billion, you would guess 2174 years, off by a factor of nine. Instead, you might say you’re ten percent of the way to $1010 when you have $101 , equally spacing the percentile markers along the exponent and measuring progress in terms of log(wealth). Since your money is growing per","2021-07-15","2022-01-30 04:59:56","2022-01-30 04:59:56","2021-11-14 19:05:27","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/6XMMDZND/fractional-progress-estimates-for-ai-timelines-and-implied.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GGN2RAC5","manuscript","2017","Babcock, James; Kramar, Janos; Yampolskiy, Roman V.","Guidelines for Artificial Intelligence Containment","","","","","http://arxiv.org/abs/1707.08476","With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.","2017-07-24","2022-01-30 04:59:56","2022-01-30 04:59:56","2020-11-21 17:49:51","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000029[s0]  arXiv: 1707.08476","","/Users/jacquesthibodeau/Zotero/storage/I9CB87G3/Babcock et al. - 2017 - Guidelines for Artificial Intelligence Containment.pdf; /Users/jacquesthibodeau/Zotero/storage/59PWEW8F/1707.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BR95HICU","bookSection","2016","Steunebrink, Bas R.; Thórisson, Kristinn R.; Schmidhuber, Jürgen","Growing Recursive Self-Improvers","Artificial General Intelligence","978-3-319-41648-9 978-3-319-41649-6","","","http://link.springer.com/10.1007/978-3-319-41649-6_13","Research into the capability of recursive self-improvement typically only considers pairs of agent, self-modiﬁcation candidate , and asks whether the agent can determine/prove if the self-modiﬁcation is beneﬁcial and safe. But this leaves out the much more important question of how to come up with a potential self-modiﬁcation in the ﬁrst place, as well as how to build an AI system capable of evaluating one. Here we introduce a novel class of AI systems, called experience-based AI (EXPAI), which trivializes the search for beneﬁcial and safe self-modiﬁcations. Instead of distracting us with proof-theoretical issues, EXPAI systems force us to consider their education in order to control a system’s growth towards a robust and trustworthy, benevolent and well-behaved agent. We discuss what a practical instance of EXPAI looks like and build towards a “test theory” that allows us to gauge an agent’s level of understanding of educational material.","2016","2022-01-30 04:59:56","2022-01-30 04:59:56","2020-12-13 19:59:05","129-139","","","9782","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","ZSCC: NoCitationData[s2]  ACC: 20  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-41649-6_13","","/Users/jacquesthibodeau/Zotero/storage/XGG7PJ49/Steunebrink et al. - 2016 - Growing Recursive Self-Improvers.pdf","","TechSafety; Other-org","","Steunebrink, Bas; Wang, Pei; Goertzel, Ben","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MR6B7MAX","blogPost","2021","Koch, Jack","Grokking the Intentional Stance","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/jHSi6BwDKTLt5dmsG/grokking-the-intentional-stance","Considering how much I’ve been using “the intentional stance"" in my thinking about the nature of agency and goals and discussions of the matter recently, I figured it would be a good idea to, y’know, actually read what Dan Dennett originally wrote about it. While doing so, I realized that he was already considering some nuances in the subject that the Wikipedia summary of the intentional stance leaves out but that are nonetheless relevant to the issues we face when attempting to e.g. formalize the approach, or think more clearly about the nature of agency in the context of alignment. I don’t expect many LessWrongers will read the original book in full, but I do expect that some additional clarity on what exactly Dennett was claiming about the nature of agency and goals will be helpful in having less confused intuitions and discussions about the subject. In what follows, I provide an in-depth summary of Dennett’s exposition of the intentional stance, from Chapter 2 of The Intentional Stance (“True Believers: The Intentional Strategy and Why It Works”), which Dennett considers “the flagship expression” of his position. Then, I discuss a few takeaways for thinking about agency in the context of AI safety. In brief, I think 1) we should stop talking about whether the systems we build will or won’t “be agents,” and instead debate how much it will make sense to consider a given system as “an agent,” from the information available to us, and 2) we should recognize that even our internally-experienced beliefs and desires are the result of parts of our minds “applying the intentional stance” to other parts of the mind or the mind as a whole. This work was completed as a Summer Research Fellow at the Center on Long-Term Risk under the mentorship of Richard Ngo. Thanks to Richard, Adam Shimi, Kaj Sotala, Alex Fabbri, and Jack Auen for feedback on drafts of this post. SUMMARIZING DENNETT'S POSITION TLDR: There is no observer-independent “fact of the matter” of whether a syst","2021-08-31","2022-01-30 04:59:56","2022-01-30 04:59:56","2021-11-18 23:33:03","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/4FFTGKKK/grokking-the-intentional-stance.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3XXE6R8G","journalArticle","2018","Liu, Hin-Yan; Lauta, Kristian Cedervall; Maas, Matthijs Michiel","Governing Boring Apocalypses: A new typology of existential vulnerabilities and exposures for existential risk research","Futures","","0016-3287","10.1016/j.futures.2018.04.009","http://www.sciencedirect.com/science/article/pii/S0016328717301623","In recent years, the study of existential risks has explored a range of natural and man-made catastrophes, from supervolcano eruption to nuclear war, and from global pandemics to potential risks from misaligned AI. These risks share the prospect of causing outright human extinction were they to occur. In this approach, such identified existential risks are frequently characterised by relatively singular origin events and concrete pathways of harm which directly jeopardise the survival of humanity, or undercut its potential for long-term technological progress. While this approach aptly identifies the most cataclysmic fates which may befall humanity, we argue that catastrophic ‘existential outcomes’ may likely arise from a broader range of sources and societal vulnerabilities, and through the complex interactions of disparate social, cultural, and natural processes—many of which, taken in isolation, might not be seen to merit attention as a global catastrophic, let alone existential, risk. This article argues that an emphasis on mitigating the hazards (discrete causes) of existential risks is an unnecessarily narrow framing of the challenge facing humanity, one which risks prematurely curtailing the spectrum of policy responses considered. Instead, it argues existential risks constitute but a subset in a broader set of challenges which could directly or indirectly contribute to existential consequences for humanity. To illustrate, we introduce and examine a set of existential risks that often fall outside the scope of, or remain understudied within, the field. By focusing on vulnerability and exposure rather than existential hazards, we develop a new taxonomy which captures factors contributing to these existential risks. Latent structural vulnerabilities in our technological systems and in our societal arrangements may increase our susceptibility to existential hazards. Finally, different types of exposure of our society or its natural base determine if or how a given hazard can interface with pre-existing vulnerabilities, to trigger emergent existential risks. We argue that far from being peripheral footnotes to their more direct and immediately terminal counterparts, these “Boring Apocalypses” may well prove to be the more endemic and problematic, dragging down and undercutting short-term successes in mitigating more spectacular risks. If the cardinal concern is humanity’s continued survival and prosperity, then focussing academic and public advocacy efforts on reducing direct existential hazards may have the paradoxical potential of exacerbating humanity’s indirect susceptibility to such outcomes. Adopting law and policy perspectives allow us to foreground societal dimensions that complement and reinforce the discourse on existential risks.","2018-09-01","2022-01-30 04:59:56","2022-01-30 04:59:56","2020-12-13 23:17:16","6-19","","","102","","Futures","Governing Boring Apocalypses","Futures of research in catastrophic and existential risk","","","","","","en","","","","","ScienceDirect","","ZSCC: 0000030","","","","MetaSafety; AmbiguosSafety; Other-org","Boring Apocalypse; Civilisational collapse; Existential Risks; Exposure; Vulnerability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FS9XWNGD","conferencePaper","2014","Tegmark, Max","Friendly Artificial Intelligence: the Physics Challenge","Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop","","","","http://arxiv.org/abs/1409.0813","Relentless progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans on the job market, and perhaps altogether. Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent ""Friendly AI"", designed to safeguard humanity and its values. I argue that, from a physics perspective where everything is simply an arrangement of elementary particles, this might be even harder than it appears. Indeed, it may require thinking rigorously about the meaning of life: What is ""meaning"" in a particle arrangement? What is ""life""? What is the ultimate ethical imperative, i.e., how should we strive to rearrange the particles of our Universe and shape its future? If we fail to answer the last question rigorously, this future is unlikely to contain humans.","2014-09-03","2022-01-30 04:59:56","2022-01-30 04:59:56","2020-11-22 02:29:59","","","","","","","Friendly Artificial Intelligence","","","","","","","","","","","","arXiv.org","","ZSCC: 0000006[s0]  arXiv: 1409.0813","","/Users/jacquesthibodeau/Zotero/storage/D2KARUXE/Tegmark - 2014 - Friendly Artificial Intelligence the Physics Chal.pdf; /Users/jacquesthibodeau/Zotero/storage/8TK9V4IJ/1409.html","","TechSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FBI9D6CJ","conferencePaper","2011","Alur, Rajeev","Formal verification of hybrid systems","Proceedings of the ninth ACM international conference on Embedded software - EMSOFT '11","978-1-4503-0714-7","","10.1145/2038642.2038685","http://dl.acm.org/citation.cfm?doid=2038642.2038685","","2011","2022-01-30 04:59:55","2022-01-30 04:59:55","2020-11-22 01:47:17","273","","","","","","","","","","","ACM Press","Taipei, Taiwan","en","","","","","DOI.org (Crossref)","","ZSCC: 0000262","","/Users/jacquesthibodeau/Zotero/storage/ED2K2TEU/Alur - 2011 - Formal verification of hybrid systems.pdf","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","the ninth ACM international conference","","","","","","","","","","","","","","",""
"BSHKC8QE","manuscript","2019","Rupprecht, Christian; Ibrahim, Cyril; Pal, Christopher J.","Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents","","","","","http://arxiv.org/abs/1904.01318","As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved are often interesting to understand the situational awareness of the system as they can correspond to risky states. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insights for a variety of environments and reinforcement learning methods. We explore results in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify behavioural weaknesses with this technique, we believe this general approach could serve as an important tool for AI safety applications.","2019-04-02","2022-01-30 04:59:48","2022-01-30 04:59:48","2020-09-05 17:02:57","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000009  arXiv: 1904.01318","","/Users/jacquesthibodeau/Zotero/storage/KZ3E7C6S/Rupprecht et al. - 2019 - Finding and Visualizing Weaknesses of Deep Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/PJSBJRET/1904.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DQSC7JWK","report","2019","Kumar, Ram Shankar Siva; Brien, David O; Albert, Kendra; Viljöen, Salomé; Snover, Jeffrey","Failure Modes in Machine Learning - Security documentation","","","","","https://docs.microsoft.com/en-us/security/failure-modes-in-machine-learning","In the last two years, more than 200 papers have been written on how machine learning (ML) systems can fail because of adversarial attacks on the algorithms and data; this number balloons if we were to incorporate papers covering non-adversarial failure modes. The spate of papers has made it difficult for ML practitioners, let alone engineers, lawyers, and policymakers, to keep up with the attacks against and defenses of ML systems. However, as these systems become more pervasive, the need to understand how they fail, whether by the hand of an adversary or due to the inherent design of a system, will only become more pressing. In order to equip software developers, security incident responders, lawyers, and policy makers with a common vernacular to talk about this problem, we developed a framework to classify failures into ""Intentional failures"" where the failure is caused by an active adversary attempting to subvert the system to attain her goals; and ""Unintentional failures"" where the failure is because an ML system produces an inherently unsafe outcome. After developing the initial version of the taxonomy last year, we worked with security and ML teams across Microsoft, 23 external partners, standards organization, and governments to understand how stakeholders would use our framework. Throughout the paper, we attempt to highlight how machine learning failure modes are meaningfully different from traditional software failures from a technology and policy perspective.","2019","2022-01-30 04:59:48","2022-01-30 04:59:48","2019-12-16 22:40:25","","","","","","","","","","","","Microsoft Corporation, Berkman Klein Center for Internet and Society at Harvard University","","en-us","","","","","","","ZSCC: NoCitationData[s9]  ACC: 16","","/Users/jacquesthibodeau/Zotero/storage/93TJR5IH/failure-modes-in-machine-learning.html","","TechSafety; AmbiguosSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BJ8QESWH","conferencePaper","2020","Atrey, Akanksha; Clary, Kaleigh; Jensen, David","Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning","","","","","http://arxiv.org/abs/1912.05743","Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsiﬁable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.","2020-02-20","2022-01-30 04:59:48","2022-01-30 04:59:48","2020-08-31 18:36:43","","","","","","","Exploratory Not Explanatory","","","","","","","en","","","","","arXiv.org","","ZSCC: NoCitationData[s2]  ACC: 35  arXiv: 1912.05743","","/Users/jacquesthibodeau/Zotero/storage/MIXFX52G/Atrey et al. - 2020 - Exploratory Not Explanatory Counterfactual Analys.pdf","","TechSafety; Other-org","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICLR 2020","","","","","","","","","","","","","","",""
"QV2MTIZJ","journalArticle","2019","Torres, Phil","Existential risks: a philosophical analysis","Inquiry","","0020-174X","10.1080/0020174X.2019.1658626","https://doi.org/10.1080/0020174X.2019.1658626","This paper examines and analyzes five definitions of ‘existential risk.’ It tentatively adopts a pluralistic approach according to which the definition that scholars employ should depend upon the particular context of use. More specifically, the notion that existential risks are ‘risks of human extinction or civilizational collapse’ is best when communicating with the public, whereas equating existential risks with a ‘significant loss of expected value’ may be the most effective definition for establishing existential risk studies as a legitimate field of scientific and philosophical inquiry. In making these arguments, the present paper hopes to provide a modicum of clarity to foundational issues relating to the central concept of arguably the most important discussion of our times.","2019-08-23","2022-01-30 04:59:48","2022-01-30 04:59:48","2020-11-14 01:16:56","1-26","","0","0","","","Existential risks","","","","","","","","","","","","Taylor and Francis+NEJM","","ZSCC: 0000004  Publisher: Routledge _eprint: https://doi.org/10.1080/0020174X.2019.1658626","","/Users/jacquesthibodeau/Zotero/storage/F7KA53VC/0020174X.2019.html","","MetaSafety; AmbiguosSafety; Other-org","analysis; existential risk studies; Existential risks; global catastrophic risks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XWS9JVUI","conferencePaper","2020","Hase, Peter; Bansal, Mohit","Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?","Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics","","","","http://arxiv.org/abs/2005.01831","Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods. All our supporting code, data, and models are publicly available at: https://github.com/peterbhase/InterpretableNLP-ACL2020","2020-05-04","2022-01-30 04:59:48","2022-01-30 04:59:48","2020-08-31 18:40:23","","","","","","","Evaluating Explainable AI","","","","","","","en","","","","","arXiv.org","","ZSCC: 0000066  arXiv: 2005.01831","","/Users/jacquesthibodeau/Zotero/storage/ATRH8PB2/Hase and Bansal - 2020 - Evaluating Explainable AI Which Algorithmic Expla.pdf","","MetaSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2020","","","","","","","","","","","","","","",""
"CHVTVF6Z","journalArticle","2018","Green, Brian Patrick","Ethical Reflections on Artificial Intelligence","Scientia et Fides","","2353-5636, 2300-7648","10.12775/SetF.2018.015","http://apcz.umk.pl/czasopisma/index.php/SetF/article/view/SetF.2018.015","","2018-10-09","2022-01-30 04:59:48","2022-01-30 04:59:48","2020-12-13 23:07:52","9","","2","6","","SetF","","","","","","","","","","","","","DOI.org (Crossref)","","ZSCC: 0000026","","/Users/jacquesthibodeau/Zotero/storage/ADZH5EW7/Green - 2018 - Ethical Reflections on Artificial Intelligence.pdf","","TechSafety; AmbiguosSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4TCFB2D9","manuscript","2019","Gruetzemacher, Ross; Paradice, David; Lee, Kang Bok","Forecasting Transformative AI: An Expert Survey","","","","","http://arxiv.org/abs/1901.08579","Transformative AI technologies have the potential to reshape critical aspects of society in the near future. However, in order to properly prepare policy initiatives for the arrival of such technologies accurate forecasts and timelines are necessary. A survey was administered to attendees of three AI conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference). The survey included questions for estimating AI capabilities over the next decade, questions for forecasting five scenarios of transformative AI and questions concerning the impact of computational resources in AI research. Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that humans are currently paid to do) can be feasibly automated now, and that this figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts indicated a 50% probability of AI systems being capable of automating 90% of current human tasks in 25 years and 99% of current human tasks in 50 years. The conference of attendance was found to have a statistically significant impact on all forecasts, with attendees of HLAI providing more optimistic timelines with less uncertainty. These findings suggest that AI experts expect major advances in AI technology to continue over the next decade to a degree that will likely have profound transformative impacts on society.","2019-07-16","2022-01-30 04:59:48","2022-01-30 04:59:48","2020-11-14 00:34:52","","","","","","","Forecasting Transformative AI","","","","","","","","","","","","arXiv.org","","ZSCC: 0000008  arXiv: 1901.08579","","/Users/jacquesthibodeau/Zotero/storage/94FP5N9E/Gruetzemacher et al. - 2019 - Forecasting Transformative AI An Expert Survey.pdf; /Users/jacquesthibodeau/Zotero/storage/N9IVWXEE/1901.html","","MetaSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Computers and Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q8V6UFFR","blogPost","2020","Shimi, Adam","Focus: you are allowed to be bad at accomplishing your goals","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/X5WTgfX5Ly4ZNHWZD/focus-you-are-allowed-to-be-bad-at-accomplishing-your-goals","When asked about what it means for a system to be goal-directed, one common answer draws on some version of Dennett’s intentional stance: a goal-directed system is a system such that modeling it as having a goal provides accurate and efficient predictions about its behavior. I agree up to that point. But then, some people follow up by saying that the prediction is that the system will accomplish its goal. For example, it makes sense to model AlphaGo as goal-directed towards winning at Go, because it will eventually win. And taking the intentional stance allows me to predict that. But what if I make AlphaGo play against AlphaZero, which is strictly better at Go? Then AlphaGo will consistently lose. Does it mean that it’s no longer goal-directed towards winning? What feels wrong to me is the implicit link drawn between goal-directedness and competence. A bad Go player will usually lose, but it doesn’t seem any less goal-directed to me than a stronger one that consistently wins. Competence is thus not the whole story. It might be useful to compute goal-directedness; reaching some lower-bound of competency might even be a necessary condition for goal-directedness (play badly enough and it becomes debatable whether you're even trying to win). But when forcing together the two, I feel like something important is lost. To solve this problem, I propose a new metric of goal-directedness, focus: how much is the system trying to accomplish a certain goal. Focus is not the whole story about being goal-directed, but I think computing the focus of a system for some goal (details in the next paragraph) gives useful information about its goal-directedness. Given a system S (as a function from states or histories to actions) and a goal  G (as a set of states), here are the steps to compute the focus of S towards G.  * I define a reward function over states R valued 1 at states in and 0 at all    other states.  * Then I define Pol be the set of all policies that can be generate","2020-06-03","2022-01-30 04:59:48","2022-01-30 04:59:48","2020-08-31 18:22:18","","","","","","","Focus","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/TSKZS9E8/focus-you-are-allowed-to-be-bad-at-accomplishing-your-goals.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DJNA82X8","conferencePaper","2020","Guan, Lin; Verma, Mudit; Kambhampati, Subbarao","Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning","","","","","http://arxiv.org/abs/2006.14804","Human-in-the-loop Reinforcement Learning (HRL) aims to integrate human guidance with Reinforcement Learning (RL) algorithms to improve sample efficiency and performance. The usual human guidance in HRL is binary evaluative ""good"" or ""bad"" signal for queried states and actions. However, this suffers from the problems of weak supervision and poor efficiency in leveraging human feedback. To address this, we present EXPAND (Explanation Augmented Feedback) which allows for explanatory information to be given as saliency maps from the human in addition to the binary feedback. EXPAND employs a state perturbation approach based on the state salient information to augment the feedback, reducing the number of human feedback signals required. We choose two domains to evaluate this approach, Taxi and Atari-Pong. We demonstrate the effectiveness of our method on three metrics, environment sample efficiency, human feedback sample efficiency, and agent gaze. We show that our method outperforms our baselines. Finally, we present an ablation study to confirm our hypothesis that augmenting binary feedback with state salient information gives a boost in performance.","2020-07-16","2022-01-30 04:59:48","2022-01-30 04:59:48","2020-08-28 17:26:49","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000008  arXiv: 2006.14804","","/Users/jacquesthibodeau/Zotero/storage/FSDFWXIT/Guan et al. - 2020 - Explanation Augmented Feedback in Human-in-the-Loo.pdf; /Users/jacquesthibodeau/Zotero/storage/3STKCI5F/2006.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2020","","","","","","","","","","","","","","",""
"TIMZ3X84","journalArticle","2015","Davis, Ernest","Ethical guidelines for a superintelligence","Artificial Intelligence","","00043702","10.1016/j.artint.2014.12.003","https://linkinghub.elsevier.com/retrieve/pii/S0004370214001453","","2015-03","2022-01-30 04:59:47","2022-01-30 04:59:47","2020-11-22 04:16:24","121-124","","","220","","Artificial Intelligence","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000030","","","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HEBSUXVE","conferencePaper","2020","Pruthi, Garima; Liu, Frederick; Sundararajan, Mukund; Kale, Satyen","Estimating Training Data Influence by Tracking Gradient Descent","34th Conference on Neural Information Processing Systems (NeurIPS 2020)","","","","http://arxiv.org/abs/2002.08484","We introduce a method called TrackIn that computes the influence of a training example on a prediction made by the model, by tracking how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TrackIn via a combination of a few key ideas: (a) a first-order approximation to the exact computation, (b) using random projections to speed up the computation of the first-order approximation for large models, (c) using saved checkpoints of standard training procedures, and (d) cherry-picking layers of a deep neural network. An experimental evaluation shows that TrackIn is more effective in identifying mislabelled training examples than other related methods such as influence functions and representer points. We also discuss insights from applying the method on vision, regression and natural language tasks.","2020-07-13","2022-01-30 04:59:47","2022-01-30 04:59:47","2020-09-05 17:05:20","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000002[s0]  arXiv: 2002.08484","","/Users/jacquesthibodeau/Zotero/storage/Q78TZ27M/Pruthi et al. - 2020 - Estimating Training Data Influence by Tracking Gra.pdf; /Users/jacquesthibodeau/Zotero/storage/6AZ6XQXS/2002.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","34th Conference on Neural Information Processing Systems (NeurIPS 2020)","","","","","","","","","","","","","","",""
"A5CWJDJU","blogPost","2021","Shimi, Adam","Epistemological Framing for AI Alignment Research","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/Y4YHTBziAscS5WPN7/epistemological-framing-for-ai-alignment-research","INTRODUCTION You open the Alignment Forum one day, and a new post stares at you. By sheer luck you have some time, so you actually read it. And then you ask yourself the eternal question: how does this fit with the rest of the field? If you’re like me, your best guess comes from looking at the author and some keywords: this usually links the post with one of the various “schools” of AI Alignment. These tend to be affiliated with a specific researcher or lab -- there’s Paul Christiano’s kind of research, MIRI’s embedded agency, and various other approaches and agendas. Yet this is a pretty weak understanding of the place of new research. In other fields, for example Complexity Theory, you don’t really need to know who wrote the paper. It usually shows a result from one of a few types (lower bound, completeness for a class, algorithm,...), and your basic training in the field armed you with mental tools to interpret results of this type. You know the big picture of the field (defining and separating complexity classes), and how types of results are linked with it. Chances are that the authors themselves called on these mental tools to justify the value of their research. In the words of Thomas S. Kuhn, Complexity Theory is paradigmatic and AI Alignment isn’t. Paradigms, popularized in Kuhn’s The Structure of Scientific Revolutions, capture shared assumptions on theories, interesting problems, and evaluation of solutions. They are tremendously useful to foster normal science, the puzzle-solving activity of scientists; the paradigm carves out the puzzles. Being paradigmatic also makes it easier to distinguish what’s considered valuable for the field and what isn’t, as well as how it all fits together. This list of benefit logically pushed multiple people to argue that we should make AI Alignment paradigmatic. I disagree. Or to be more accurate, I agree that we should have paradigms in the field, but I think that they should be part of a bigger epistemological struct","2021-03-08","2022-01-30 04:59:47","2022-01-30 04:59:47","2021-11-14 16:14:19","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/NB5NBP7R/epistemological-framing-for-ai-alignment-research.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"87J7MWC6","bookSection","2017","Peacock, Kent A.","Energy, Complexity, and the Singularity","The Technological Singularity: Managing the Journey","978-3-662-54033-6","","","https://doi.org/10.1007/978-3-662-54033-6_8","SummaryThis paper explores the relevance of ecological limitations such as climate change and resource exhaustion to the possibility of a technologically-mediated “intelligence explosion” in the near future. The imminent risks of global carbonization and loss of biodiversity, as well as the dependency of technological development on a healthy biosphere, are greatly underestimated by singularity theorists such as Ray Kurzweil. While development of information technology should continue, we cannot rely on hypothetical advances in AI to get us out of our present ecological bottleneck. Rather, we should do everything we can to foster human ingenuity, the one factor that has a record of generating the game-changing innovations that our species has relied upon to overcome survival challenges in our past.","2017","2022-01-30 04:59:47","2022-01-30 04:59:47","2020-11-24 02:59:52","153-165","","","","","","","The Frontiers Collection","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","ZSCC: NoCitationData[s1]  ACC: 1  DOI: 10.1007/978-3-662-54033-6_8","","","","MetaSafety; Other-org","Ecological Challenge; Exponential Expansion; Global Carbonization; Human Ingenuity; Singularity Hypothesis","Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7GNR4H4","conferencePaper","2018","Behzadan, Vahid; Yampolskiy, Roman V.; Munir, Arslan","Emergence of Addictive Behaviors in Reinforcement Learning Agents","Proceedings of the AAAI Workshop on Artificial Intelligence Safety 2019","","","","http://arxiv.org/abs/1811.05590","This paper presents a novel approach to the technical analysis of wireheading in intelligent agents. Inspired by the natural analogues of wireheading and their prevalent manifestations, we propose the modeling of such phenomenon in Reinforcement Learning (RL) agents as psychological disorders. In a preliminary step towards evaluating this proposal, we study the feasibility and dynamics of emergent addictive policies in Q-learning agents in the tractable environment of the game of Snake. We consider a slightly modified settings for this game, in which the environment provides a ""drug"" seed alongside the original ""healthy"" seed for the consumption of the snake. We adopt and extend an RL-based model of natural addiction to Q-learning agents in this settings, and derive sufficient parametric conditions for the emergence of addictive behaviors in such agents. Furthermore, we evaluate our theoretical analysis with three sets of simulation-based experiments. The results demonstrate the feasibility of addictive wireheading in RL agents, and provide promising venues of further research on the psychopathological modeling of complex AI safety problems.","2018-11-13","2022-01-30 04:59:47","2022-01-30 04:59:47","2020-11-14 01:10:06","","","","","","","","","","","","","Honolulu HI USA","","","","","","arXiv.org","","ZSCC: 0000009  arXiv: 1811.05590","","/Users/jacquesthibodeau/Zotero/storage/THPJ9EUR/Behzadan et al. - 2018 - Emergence of Addictive Behaviors in Reinforcement .pdf; /Users/jacquesthibodeau/Zotero/storage/36B2C4IA/1811.html","","TechSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AAAI Workshop on Artificial Intelligence Safety 2019","","","","","","","","","","","","","","",""
"BVVDEU27","conferencePaper","2016","Greene, Joshua; Rossi, Francesca; Tasioulas, John; Venable, Kristen Brent; Williams, Brian","Embedding Ethical Principles in Collective Decision Support Systems","","","","","","The future will see autonomous machines acting in the same environment as humans, in areas as diverse as driving, assistive technology, and health care. Think of self-driving cars, companion robots, and medical diagnosis support systems. We also believe that humans and machines will often need to work together and agree on common decisions. Thus hybrid collective decision making systems will be in great need.","2016","2022-01-30 04:59:47","2022-01-30 04:59:47","","5","","","","","","","","","","","","","en","","","","","Zotero","","ZSCC: 0000050","","/Users/jacquesthibodeau/Zotero/storage/KQNS3RIQ/Greene et al. - Embedding Ethical Principles in Collective Decisio.pdf","","TechSafety; AmbiguosSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)","","","","","","","","","","","","","","",""
"4S5HQK4S","journalArticle","2008","Hanson, Robin","Economics of the singularity","IEEE Spectrum","","0018-9235","10.1109/MSPEC.2008.4531461","http://ieeexplore.ieee.org/document/4531461/","","2008-06","2022-01-30 04:59:47","2022-01-30 04:59:47","2020-11-22 02:22:43","45-50","","6","45","","IEEE Spectr.","","","","","","","","","","","","","DOI.org (Crossref)","","ZSCC: 0000085","","","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ICE4CS7","blogPost","2021","Shimi, Adam","Epistemology of HCH","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/CDSXoC54CjbXQNLGr/epistemology-of-hch","INTRODUCTION HCH is a recursive acronym meaning “Humans consulting HCH”. Coincidentally, It’s also a concept coined by Paul Christiano, central in much of the reasoning around Prosaic AI Alignment. Yet for many, me included, the various ways in which it is used are sometimes confusing. I believe that the tools of Epistemology and Philosophy of Science can help understand it better, and push further the research around it. So this post doesn’t give yet another explanation of HCH; instead, it asks about the different perspectives we can take on it. These perspectives capture the form of knowledge that HCH is, what it tells us about AI Alignment, and how to expand, judge and interpret this knowledge. I then apply these perspectives to examples of research on HCH, to show the usefulness of the different frames. Thanks to Joe Collman, Jérémy Perret, Richard Ngo, Evan Hubinger and Paul Christiano for feedback on this post. IS IT A SCIENTIFIC EXPLANATION? IS IT A MODEL OF COMPUTATION? NO, IT’S HCH! HCH was originally defined in Humans Consulting HCH: Consider a human Hugh who has access to a question-answering machine. Suppose the machine answers question Q by perfectly imitating how Hugh would answer question Q, if Hugh had access to the question-answering machine. That is, Hugh is able to consult a copy of Hugh, who is able to consult a copy of Hugh, who is able to consult a copy of Hugh… Let’s call this process HCH, for “Humans Consulting HCH.” Nowadays, this is actually called weak HCH, after the Strong HCH post which extended the definition. That being said, I’m only interested in perspective about HCH, which includes the questions asked about it and how to answer them. Although the difference between Weak and Strong HCH matters for the answers, the questions and perspective stay the same. I’ll thus use HCH to mean one or the other interchangeably. The main use of HCH is as an ideal for what a question-answerer aligned with a given human should be like. This i","2021-02-09","2022-01-30 04:59:47","2022-01-30 04:59:47","2021-11-13 22:49:01","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/XV9F5ENC/epistemology-of-hch.html","","TechSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ITAG9EEW","conferencePaper","2019","Menda, Kunal; Driggs-Campbell, Katherine; Kochenderfer, Mykel J.","EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning","2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","","","http://arxiv.org/abs/1807.08364","While imitation learning is often used in robotics, the approach frequently suffers from data mismatch and compounding errors. DAgger is an iterative algorithm that addresses these issues by aggregating training data from both the expert and novice policies, but does not consider the impact of safety. We present a probabilistic extension to DAgger, which attempts to quantify the confidence of the novice policy as a proxy for safety. Our method, EnsembleDAgger, approximates a Gaussian Process using an ensemble of neural networks. Using the variance as a measure of confidence, we compute a decision rule that captures how much we doubt the novice, thus determining when it is safe to allow the novice to act. With this approach, we aim to maximize the novice's share of actions, while constraining the probability of failure. We demonstrate improved safety and learning performance compared to other DAgger variants and classic imitation learning on an inverted pendulum and in the MuJoCo HalfCheetah environment.","2019-07-19","2022-01-30 04:59:47","2022-01-30 04:59:47","2020-12-13 23:23:33","","","","","","","EnsembleDAgger","","","","","","","","","","","","arXiv.org","","ZSCC: 0000023   arXiv: 1807.08364","","/Users/jacquesthibodeau/Zotero/storage/W3ZZIFIQ/Menda et al. - 2019 - EnsembleDAgger A Bayesian Approach to Safe Imitat.pdf; /Users/jacquesthibodeau/Zotero/storage/AXVEPNWH/1807.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","","","","","","","","","","","","","",""
"UG62BDKC","blogPost","2021","Ngo, Richard","Eight claims about multi-agent AGI safety","","","","","https://www.alignmentforum.org/posts/dSAJdi99XmqftqXXq/eight-claims-about-multi-agent-agi-safety","There are quite a few arguments about how interactions between multiple AGIs affect risks from AGI development. I’ve identified at least eight distinct but closely-related claims which it seems worthwhile to disambiguate. I’ve split them up into four claims about the process of training AGIs, and four claims about the process of deploying AGIs; after listing them, I go on to explain each in more detail. Note that while I believe that all of these ideas are interesting enough to warrant further investigation, I don’t currently believe that all of them are true as stated. In particular, I think that so far there's been little compelling explanation of why interactions between many aligned AIs might have castastrophic effects on the world (as is discussed in point 7). CLAIMS ABOUT TRAINING 1. Multi-agent training is one of the most likely ways we might build AGI. 2. Multi-agent training is one of the most dangerous ways we might build AGI. 3. Multi-agent training is a regime in which standard safety techniques won’t work. 4. Multi-agent training allows us to implement important new safety techniques. CLAIMS ABOUT DEPLOYMENT 5. We should expect the first AGIs to be deployed in a world which already contains many nearly-as-good AIs. 6. We should expect AGIs to be deployed as multi-agent collectives. 7. Lack of coordination between multiple deployed AGIs is a major source of existential risk. 8. Conflict between multiple deployed AGIs risks causing large-scale suffering. DETAILS AND ARGUMENTS 1. Multi-agent training is one of the most likely ways we might build AGI. The core argument for this thesis is that multi-agent interaction was a key feature of the evolution of human intelligence, by promoting both competition and cooperation. Competition between humans provides a series of challenges which are always at roughly the right level of difficulty; Liebo et al. (2019)  call this an autocurriculum. Autocurricula were crucial for training sophisticated reinforcem","2021","2022-01-30 04:59:47","2022-01-30 04:59:47","2021-11-13 19:29:53","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/MEED299E/eight-claims-about-multi-agent-agi-safety.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3JZGMVVU","conferencePaper","2017","Mhamdi, El Mahdi El; Guerraoui, Rachid; Hendrikx, Hadrien; Maurer, Alexandre","Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning","arXiv:1704.02882 [cs, stat]","","","","http://arxiv.org/abs/1704.02882","In reinforcement learning, agents learn by performing actions and observing their outcomes. Sometimes, it is desirable for a human operator to \textit{interrupt} an agent in order to prevent dangerous situations from happening. Yet, as part of their learning process, agents may link these interruptions, that impact their reward, to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions, but also from those of other agents. Orseau and Armstrong defined \emph{safe interruptibility} for one learner, but their work does not naturally extend to multi-agent systems. This paper introduces \textit{dynamic safe interruptibility}, an alternative definition more suited to decentralized learning problems, and studies this notion in two learning frameworks: \textit{joint action learners} and \textit{independent learners}. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners, yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions, it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.","2017-05-22","2022-01-30 04:59:46","2022-01-30 04:59:46","2020-11-21 17:31:17","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000022  arXiv: 1704.02882","","/Users/jacquesthibodeau/Zotero/storage/AH67HTNI/Mhamdi et al. - 2017 - Dynamic Safe Interruptibility for Decentralized Mu.pdf; /Users/jacquesthibodeau/Zotero/storage/A66XSDKN/1704.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS 2017","","","","","","","","","","","","","","",""
"RW8QKTQX","blogPost","2020","Barnett, Matthew","Distinguishing definitions of takeoff","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff","I find discussions about AI takeoff to be very confusing. Often, people will argue for ""slow takeoff"" or ""fast takeoff"" and then when I ask them to operationalize what those terms mean, they end up saying something quite different than what I thought those terms meant.  To help alleviate this problem, I aim to compile the definitions of AI takeoff that I'm currently aware of, with an emphasis on definitions that have clear specifications. I will continue updating the post as long as I think it serves as a useful reference for others. In this post, an AI takeoff can be roughly construed as ""the dynamics of the world associated with the development of powerful artificial intelligence."" These definitions characterize different ways that the world can evolve as  transformative AI is developed.  FOOM/HARD TAKEOFF The traditional hard takeoff position, or ""Foom"" position (these appear to be equivalent terms) was characterized in this post from Eliezer Yudkowsky. It contrasts Hanson's takeoff scenario by emphasizing local dynamics: rather than a population of artificial intelligences coming into existence, there would be a single intelligence that quickly reaches a level of competence that outstrips the world's capabilities to control it. The proposed mechanism that causes such a dynamic is recursive self improvement, though Yudkowsky later suggested that this wasn't necessary. The ability for recursive self improvement to induce a hard takeoff was defended in Intelligence Explosion Microeconomics. He argues against Robin Hanson in the  AI Foom debates. Watch this video to see the live debate. Given the word ""hard"" in this notion of takeoff, a ""soft"" takeoff could simply be defined as the negation of a hard takeoff. HANSONIAN ""SLOW"" TAKEOFF Robin Hanson objected to hard takeoff by predicting that growth in AI capabilities will not be extremely uneven between projects. In other words, there is unlikely to be one AI project, or even a small set of AI projects, that pro","2020-02-13","2022-01-30 04:59:46","2022-01-30 04:59:46","2020-09-05 18:49:44","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/X6ZTTITS/distinguishing-definitions-of-takeoff.html","","MetaSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5RHRW6VI","blogPost","2021","Ngo, Richard","Distinguishing claims about training vs deployment","AI Alignment Forum","","","","https://www.alignmentforum.org/posts/L9HcyaiWBLYe7vXid/distinguishing-claims-about-training-vs-deployment","Given the rapid progress in machine learning over the last decade in particular, I think that the core arguments about why AGI might be dangerous should be formulated primarily in terms of concepts from machine learning. One important way to do this is to distinguish between claims about training processes which produce AGIs, versus claims about AGIs themselves, which I’ll call deployment  claims. I think many foundational concepts in AI safety are clarified by this distinction. In this post I outline some of them, and state new versions of the orthogonality and instrumental convergence theses which take this distinction into account. GOAL SPECIFICATION The most important effect of thinking in terms of machine learning concepts is clarity about what it might mean to specify a goal. Early characterisations of how we might specify the goals of AGIs focused on agents which choose between actions on the basis of an objective function hand-coded by humans. Deep Blue is probably the most well-known example of this; AIXI can also be interpreted as doing so. But this isn't how modern machine learning systems work. So my current default picture of how we will specify goals for AGIs is:  * At training time, we identify a method for calculating the feedback to give    to the agent, which will consist of a mix of human evaluations and automated    evaluations. I’ll call this the objective function. I expect that we will use    an objective function which rewards the agent for following commands given to    it by humans in natural language.  * At deployment time, we give the trained agent commands in natural language.    The objective function is no longer used; hopefully the agent instead has    internalised a motivation/goal to act in ways which humans would approve of,    which leads it to follow our commands sensibly and safely. This breakdown makes the inner alignment problem a very natural concept - it’s simply the case where the agent’s learned motivations don’t corres","2021-02-03","2022-01-30 04:59:46","2022-01-30 04:59:46","2021-11-13 22:42:04","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s0]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/M5B7NFQM/distinguishing-claims-about-training-vs-deployment.html","","MetaSafety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHA2PG7A","conferencePaper","2018","Plisnier, Hélène; Steckelmacher, Denis; Brys, Tim; Roijers, Diederik M.; Nowé, Ann","Directed Policy Gradient for Safe Reinforcement Learning with Human Advice","","","","","http://arxiv.org/abs/1808.04096","Many currently deployed Reinforcement Learning agents work in an environment shared with humans, be them co-workers, users or clients. It is desirable that these agents adjust to people's preferences, learn faster thanks to their help, and act safely around them. We argue that most current approaches that learn from human feedback are unsafe: rewarding or punishing the agent a-posteriori cannot immediately prevent it from wrong-doing. In this paper, we extend Policy Gradient to make it robust to external directives, that would otherwise break the fundamentally on-policy nature of Policy Gradient. Our technique, Directed Policy Gradient (DPG), allows a teacher or backup policy to override the agent before it acts undesirably, while allowing the agent to leverage human advice or directives to learn faster. Our experiments demonstrate that DPG makes the agent learn much faster than reward-based approaches, while requiring an order of magnitude less advice.","2018-10","2022-01-30 04:59:46","2022-01-30 04:59:46","2020-11-14 01:06:08","","","","","","","","","","","","","","","","","","","arXiv.org","","ZSCC: 0000002  arXiv: 1808.04096","","/Users/jacquesthibodeau/Zotero/storage/ZJPCJCQH/Plisnier et al. - 2018 - Directed Policy Gradient for Safe Reinforcement Le.pdf; /Users/jacquesthibodeau/Zotero/storage/PBWSCFC4/1808.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","European Workshop on Reinforcement Learning 14","","","","","","","","","","","","","","",""
"6SSB5PK6","manuscript","2017","Menda, Kunal; Driggs-Campbell, Katherine; Kochenderfer, Mykel J.","DropoutDAgger: A Bayesian Approach to Safe Imitation Learning","","","","","http://arxiv.org/abs/1709.06166","While imitation learning is becoming common practice in robotics, this approach often suffers from data mismatch and compounding errors. DAgger is an iterative algorithm that addresses these issues by continually aggregating training data from both the expert and novice policies, but does not consider the impact of safety. We present a probabilistic extension to DAgger, which uses the distribution over actions provided by the novice policy, for a given observation. Our method, which we call DropoutDAgger, uses dropout to train the novice as a Bayesian neural network that provides insight to its confidence. Using the distribution over the novice's actions, we estimate a probabilistic measure of safety with respect to the expert action, tuned to balance exploration and exploitation. The utility of this approach is evaluated on the MuJoCo HalfCheetah and in a simple driving experiment, demonstrating improved performance and safety compared to other DAgger variants and classic imitation learning.","2017-09-18","2022-01-30 04:59:46","2022-01-30 04:59:46","2020-12-13 20:55:29","","","","","","","DropoutDAgger","","","","","","","","","","","","arXiv.org","","ZSCC: 0000013  arXiv: 1709.06166","","/Users/jacquesthibodeau/Zotero/storage/IGX8RKK6/Menda et al. - 2017 - DropoutDAgger A Bayesian Approach to Safe Imitati.pdf; /Users/jacquesthibodeau/Zotero/storage/KWHIGHR6/1709.html","","TechSafety; AmbiguosSafety; Other-org","Computer Science - Artificial Intelligence; Computer Science - Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PV7TJE2E","journalArticle","1999","Sobel, D.","Do the desires of rational agents converge?","Analysis","","0003-2638, 1467-8284","10.1093/analys/59.3.137","https://academic.oup.com/analysis/article-lookup/doi/10.1093/analys/59.3.137","","1999-07-01","2022-01-30 04:59:46","2022-01-30 04:59:46","2020-11-22 02:23:46","137-147","","3","59","","Analysis","","","","","","","","en","","","","","DOI.org (Crossref)","","ZSCC: 0000040","","","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XGEAUCIT","blogPost","2020","Casper, Stephen","Dissolving Confusion around Functional Decision Theory","LessWrong","","","","https://www.lesswrong.com/posts/xoQRz8tBvsznMXTkt/dissolving-confusion-around-functional-decision-theory","SUMMARY Functional Decision Theory (FDT), (see also causal, evidential, timeless,  updateless, and anthropic decision theories) recommends taking cooperative, non-greedy actions in twin prisoners dilemmas, Newcombian problems, Parfit’s hitchhiker-like games, and counterfactual muggings but not smoking lesion situations. It’s a controversial concept with important implications for designing agents that have optimal behavior when embedded in environments in which they may potentially interact with models of themselves. Unfortunately, I think that FDT is sometimes explained confusingly and misunderstood by its proponents and opponents alike. To help dissolve confusion about FDT and address key concerns of its opponents, I refute the criticism that FDT assumes that causation can happen backward in time and offer two key principles that provide a framework for clearly understanding it:  1. Questions in decision theory are not questions about what choices you should     make with some sort of unpredictable free will. They are questions about     what type of source code you should be running.   2. I should consider predictor P to “subjunctively depend” on agent A to the     extent that P makes predictions of A’s actions based on correlations that     cannot be confounded by my choice of what source code A runs.  GETTING UP TO SPEED I think that functional decision theory (FDT) is a beautifully counterintuitive and insightful framework for instrumental rationally. I will not make it my focus here to talk about what it is and what types of situations it is useful in. To gain a solid background, I recommend this post of mine or the original paper on it by Eliezer Yudkowsky and Nate Soares.  Additionally, here are four different ways that FDT can be explained. I find them all complimentary for understanding and intuiting it well.  1. The decision theory that tells you to act as if you were setting the output     to an optimal decision-making process for the task at hand.","2020-01-05","2022-01-30 04:59:46","2022-01-30 04:59:46","2020-09-07 18:26:06","","","","","","","","","","","","","","","","","","","","","ZSCC: NoCitationData[s1]  ACC: N/A","","/Users/jacquesthibodeau/Zotero/storage/7KKSDS2J/dissolving-confusion-around-functional-decision-theory.html","","TechSafety; Other-org","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U6T93H7J","bookSection","2017","Majot, Andrew; Yampolskiy, Roman","Diminishing Returns and Recursive Self Improving Artificial Intelligence","The Technological Singularity: Managing the Journey","978-3-662-54033-6","","","https://doi.org/10.1007/978-3-662-54033-6_7","SummaryIn this chapter we will examine in more detail the concept of an artificial intelligence that can improve upon itself, and show how that might not be as problematic as some researchers think. The ability for an AI to better itself over time through a process called recursive self-improvement has been considered as a promising path to creating the technological singularity. In this type of system an AI has access to its own source code and possibly even hardware, with the ability to edit both at will. This gives the AI the option to constantly improve upon itself and become increasingly intelligent. Eventually this would produce versions of the AI that are more intelligent than humans and cause us to reach the technological singularity. Researchers have speculated that this process could create an extremely dangerous situation for humanity as we get left behind in a growing intelligence gap. This chapter proposes that this gap would not be as drastic as initially thought, and that there may be natural limits on the ability for an AI to improve upon itself. Along the way we will propose that the law of diminishing returns will take effect to limit runaway intelligence. We also theorize that developing and manufacturing new hardware will introduce a latency in AI improvement that could easily be exploited to halt any dangerous situation.","2017","2022-01-30 04:59:46","2022-01-30 04:59:46","2020-11-24 02:59:49","141-152","","","","","","","The Frontiers Collection","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","ZSCC: NoCitationData[s1]  ACC: 3  DOI: 10.1007/978-3-662-54033-6_7","","","","MetaSafety; AmbiguosSafety; Other-org","Technological Singularity; Cognitive Algorithm; Hardware Improvement; Inductive Logic Programming; Logistical Chain","Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""