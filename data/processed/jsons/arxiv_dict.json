{"1901.04966": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.04966v1", "post_title": "Identifying and Correcting Label Bias in Machine Learning", "authors": ["Heinrich Jiang", "Ofir Nachum"], "date_published": "2019-01-15 18:40:06+00:00", "data_last_modified": "2019-01-15 18:40:06+00:00", "url": "http://arxiv.org/abs/1901.04966v1", "abstract": "Datasets often contain biases which unfairly disadvantage certain groups, and\nclassifiers trained on such datasets can inherit these biases. In this paper,\nwe provide a mathematical formulation of how this bias can arise. We do so by\nassuming the existence of underlying, unknown, and unbiased labels which are\noverwritten by an agent who intends to provide accurate labels but may have\nbiases against certain groups. Despite the fact that we only observe the biased\nlabels, we are able to show that the bias may nevertheless be corrected by\nre-weighting the data points without changing the labels. We show, with\ntheoretical guarantees, that training on the re-weighted dataset corresponds to\ntraining on the unobserved but unbiased labels, thus leading to an unbiased\nmachine learning classifier. Our procedure is fast and robust and can be used\nwith virtually any learning algorithm. We evaluate on a number of standard\nmachine learning fairness datasets and a variety of fairness notions, finding\nthat our method outperforms standard approaches in achieving fair\nclassification.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.02925": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.02925v2", "post_title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning", "authors": ["Ilya Kostrikov", "Kumar Krishna Agrawal", "Debidatta Dwibedi", "Sergey Levine", "Jonathan Tompson"], "date_published": "2018-09-09 05:37:25+00:00", "data_last_modified": "2018-10-15 17:27:25+00:00", "url": "http://arxiv.org/abs/1809.02925v2", "abstract": "We identify two issues with the family of algorithms based on the Adversarial\nImitation Learning framework. The first problem is implicit bias present in the\nreward functions used in these algorithms. While these biases might work well\nfor some environments, they can also lead to sub-optimal behavior in others.\nSecondly, even though these algorithms can learn from few expert\ndemonstrations, they require a prohibitively large number of interactions with\nthe environment in order to imitate the expert for many real-world\napplications. In order to address these issues, we propose a new algorithm\ncalled Discriminator-Actor-Critic that uses off-policy Reinforcement Learning\nto reduce policy-environment interaction sample complexity by an average factor\nof 10. Furthermore, since our reward function is designed to be unbiased, we\ncan apply our algorithm to many problems without making any task-specific\nadjustments.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1911.02320": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1911.02320v1", "post_title": "Nonverbal Robot Feedback for Human Teachers", "authors": ["Sandy H. Huang", "Isabella Huang", "Ravi Pandya", "Anca D. Dragan"], "date_published": "2019-11-06 11:26:31+00:00", "data_last_modified": "2019-11-06 11:26:31+00:00", "url": "http://arxiv.org/abs/1911.02320v1", "abstract": "Robots can learn preferences from human demonstrations, but their success\ndepends on how informative these demonstrations are. Being informative is\nunfortunately very challenging, because during teaching, people typically get\nno transparency into what the robot already knows or has learned so far. In\ncontrast, human students naturally provide a wealth of nonverbal feedback that\nreveals their level of understanding and engagement. In this work, we study how\na robot can similarly provide feedback that is minimally disruptive, yet gives\nhuman teachers a better mental model of the robot learner, and thus enables\nthem to teach more effectively. Our idea is that at any point, the robot can\nindicate what it thinks the correct next action is, shedding light on its\ncurrent estimate of the human's preferences. We analyze how useful this\nfeedback is, both in theory and with two user studies---one with a virtual\ncharacter that tests the feedback itself, and one with a PR2 robot that uses\ngaze as the feedback mechanism. We find that feedback can be useful for\nimproving both the quality of teaching and teachers' understanding of the\nrobot's capability.", "author_comment": "CoRL 2019", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.HC", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.02404": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.02404v1", "post_title": "Dissolving the Fermi Paradox", "authors": ["Anders Sandberg", "Eric Drexler", "Toby Ord"], "date_published": "2018-06-06 19:51:21+00:00", "data_last_modified": "2018-06-06 19:51:21+00:00", "url": "http://arxiv.org/abs/1806.02404v1", "abstract": "The Fermi paradox is the conflict between an expectation of a high {\\em ex\nante} probability of intelligent life elsewhere in the universe and the\napparently lifeless universe we in fact observe. The expectation that the\nuniverse should be teeming with intelligent life is linked to models like the\nDrake equation, which suggest that even if the probability of intelligent life\ndeveloping at a given site is small, the sheer multitude of possible sites\nshould nonetheless yield a large number of potentially observable\ncivilizations. We show that this conflict arises from the use of Drake-like\nequations, which implicitly assume certainty regarding highly uncertain\nparameters. We examine these parameters, incorporating models of chemical and\ngenetic transitions on paths to the origin of life, and show that extant\nscientific knowledge corresponds to uncertainties that span multiple orders of\nmagnitude. This makes a stark difference. When the model is recast to represent\nrealistic distributions of uncertainty, we find a substantial {\\em ex ante}\nprobability of there being no other intelligent life in our observable\nuniverse, and thus that there should be little surprise when we fail to detect\nany signs of it. This result dissolves the Fermi paradox, and in doing so\nremoves any need to invoke speculative mechanisms by which civilizations would\ninevitably fail to have observable effects upon the universe.", "author_comment": "Submitted to Proceedings of the Royal Society of London A; 4\n  supplements", "journal_ref": null, "doi": null, "primary_category": "physics.pop-ph", "categories": ["physics.pop-ph"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2105.08475": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2105.08475v1", "post_title": "AI and Shared Prosperity", "authors": ["Katya Klinova", "Anton Korinek"], "date_published": "2021-05-18 12:37:18+00:00", "data_last_modified": "2021-05-18 12:37:18+00:00", "url": "http://arxiv.org/abs/2105.08475v1", "abstract": "Future advances in AI that automate away human labor may have stark\nimplications for labor markets and inequality. This paper proposes a framework\nto analyze the effects of specific types of AI systems on the labor market,\nbased on how much labor demand they will create versus displace, while taking\ninto account that productivity gains also make society wealthier and thereby\ncontribute to additional labor demand. This analysis enables ethically-minded\ncompanies creating or deploying AI systems as well as researchers and\npolicymakers to take into account the effects of their actions on labor markets\nand inequality, and therefore to steer progress in AI in a direction that\nadvances shared prosperity and an inclusive economic future for all of\nhumanity.", "author_comment": null, "journal_ref": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\n  Society (AIES '21)", "doi": "10.1145/3461702.3462619", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY", "econ.GN", "q-fin.EC", "J.4; K.4.1"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.00667": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.00667v3", "post_title": "Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks", "authors": ["Yarin Gal", "Lewis Smith"], "date_published": "2018-06-02 16:43:17+00:00", "data_last_modified": "2018-06-28 21:25:21+00:00", "url": "http://arxiv.org/abs/1806.00667v3", "abstract": "We prove, under two sufficient conditions, that idealised models can have no\nadversarial examples. We discuss which idealised models satisfy our conditions,\nand show that idealised Bayesian neural networks (BNNs) satisfy these. We\ncontinue by studying near-idealised BNNs using HMC inference, demonstrating the\ntheoretical ideas in practice. We experiment with HMC on synthetic data derived\nfrom MNIST for which we know the ground-truth image density, showing that\nnear-perfect epistemic uncertainty correlates to density under image manifold,\nand that adversarial images lie off the manifold in our setting. This suggests\nwhy MC dropout, which can be seen as performing approximate inference, has been\nobserved to be an effective defence against adversarial examples in practice;\nWe highlight failure-cases of non-idealised BNNs relying on dropout, suggesting\na new attack for dropout models and a new defence as well. Lastly, we\ndemonstrate the defence on a cats-vs-dogs image classification task with a\nVGG13 variant.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1308.3778": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1308.3778v1", "post_title": "Game Theory with Translucent Players", "authors": ["Joseph Y. Halpern", "Rafael Pass"], "date_published": "2013-08-17 12:29:53+00:00", "data_last_modified": "2013-08-17 12:29:53+00:00", "url": "http://arxiv.org/abs/1308.3778v1", "abstract": "A traditional assumption in game theory is that players are opaque to one\nanother---if a player changes strategies, then this change in strategies does\nnot affect the choice of other players' strategies. In many situations this is\nan unrealistic assumption. We develop a framework for reasoning about games\nwhere the players may be translucent to one another; in particular, a player\nmay believe that if she were to change strategies, then the other player would\nalso change strategies. Translucent players may achieve significantly more\nefficient outcomes than opaque ones.\n  Our main result is a characterization of strategies consistent with\nappropriate analogues of common belief of rationality. Common Counterfactual\nBelief of Rationality (CCBR) holds if (1) everyone is rational, (2) everyone\ncounterfactually believes that everyone else is rational (i.e., all players i\nbelieve that everyone else would still be rational even if $i$ were to switch\nstrategies), (3) everyone counterfactually believes that everyone else is\nrational, and counterfactually believes that everyone else is rational, and so\non. CCBR characterizes the set of strategies surviving iterated removal of\nminimax dominated strategies, where a strategy s for player i is minimax\ndominated by s' if the worst-case payoff for i using s' is better than the best\npossible payoff using s.", "author_comment": "Extended version of a paper that appear in the Conference on\n  Theoretical Aspects of Rationality and Knowledge, 2013", "journal_ref": null, "doi": null, "primary_category": "cs.GT", "categories": ["cs.GT"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.06177": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.06177v3", "post_title": "The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence", "authors": ["Gary Marcus"], "date_published": "2020-02-14 18:55:56+00:00", "data_last_modified": "2020-02-19 17:48:05+00:00", "url": "http://arxiv.org/abs/2002.06177v3", "abstract": "Recent research in artificial intelligence and machine learning has largely\nemphasized general-purpose learning and ever-larger training sets and more and\nmore compute. In contrast, I propose a hybrid, knowledge-driven,\nreasoning-based approach, centered around cognitive models, that could provide\nthe substrate for a richer, more robust AI than is currently possible.", "author_comment": "5 figures", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "I.2; I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.01217": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.01217v2", "post_title": "SafeLife 1.0: Exploring Side Effects in Complex Environments", "authors": ["Carroll L. Wainwright", "Peter Eckersley"], "date_published": "2019-12-03 06:44:48+00:00", "data_last_modified": "2021-02-26 05:49:51+00:00", "url": "http://arxiv.org/abs/1912.01217v2", "abstract": "We present SafeLife, a publicly available reinforcement learning environment\nthat tests the safety of reinforcement learning agents. It contains complex,\ndynamic, tunable, procedurally generated levels with many opportunities for\nunsafe behavior. Agents are graded both on their ability to maximize their\nexplicit reward and on their ability to operate safely without unnecessary side\neffects. We train agents to maximize rewards using proximal policy optimization\nand score them on a suite of benchmark levels. The resulting agents are\nperformant but not safe -- they tend to cause large side effects in their\nenvironments -- but they form a baseline against which future safety research\ncan be measured.", "author_comment": "Updated version was presented at the AAAI SafeAI 2020 Workshop, but\n  now with updated contact info. Previously presented at the 2019 NeurIPS\n  Safety and Robustness in Decision Making Workshop", "journal_ref": "CEUR Workshop Proceedings, 2560 (2020) 117-127", "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1503.07619": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1503.07619v2", "post_title": "Shared Autonomy via Hindsight Optimization", "authors": ["Shervin Javdani", "Siddhartha S. Srinivasa", "J. Andrew Bagnell"], "date_published": "2015-03-26 04:50:49+00:00", "data_last_modified": "2015-04-17 20:20:50+00:00", "url": "http://arxiv.org/abs/1503.07619v2", "abstract": "In shared autonomy, user input and robot autonomy are combined to control a\nrobot to achieve a goal. Often, the robot does not know a priori which goal the\nuser wants to achieve, and must both predict the user's intended goal, and\nassist in achieving that goal. We formulate the problem of shared autonomy as a\nPartially Observable Markov Decision Process with uncertainty over the user's\ngoal. We utilize maximum entropy inverse optimal control to estimate a\ndistribution over the user's goal based on the history of inputs. Ideally, the\nrobot assists the user by solving for an action which minimizes the expected\ncost-to-go for the (unknown) goal. As solving the POMDP to select the optimal\naction is intractable, we use hindsight optimization to approximate the\nsolution. In a user study, we compare our method to a standard\npredict-then-blend approach. We find that our method enables users to\naccomplish tasks more quickly while utilizing less input. However, when asked\nto rate each system, users were mixed in their assessment, citing a tradeoff\nbetween maintaining control authority and accomplishing tasks quickly.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.01296": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.01296v3", "post_title": "PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings", "authors": ["Nicholas Rhinehart", "Rowan McAllister", "Kris Kitani", "Sergey Levine"], "date_published": "2019-05-03 17:54:09+00:00", "data_last_modified": "2019-09-30 16:45:21+00:00", "url": "http://arxiv.org/abs/1905.01296v3", "abstract": "For autonomous vehicles (AVs) to behave appropriately on roads populated by\nhuman-driven vehicles, they must be able to reason about the uncertain\nintentions and decisions of other drivers from rich perceptual information.\nTowards these capabilities, we present a probabilistic forecasting model of\nfuture interactions between a variable number of agents. We perform both\nstandard forecasting and the novel task of conditional forecasting, which\nreasons about how all agents will likely respond to the goal of a controlled\nagent (here, the AV). We train models on real and simulated data to forecast\nvehicle trajectories given past positions and LIDAR. Our evaluation shows that\nour model is substantially more accurate in multi-agent driving scenarios\ncompared to existing state-of-the-art. Beyond its general ability to perform\nconditional forecasting queries, we show that our model's predictions of all\nagents improve when conditioned on knowledge of the AV's goal, further\nillustrating its capability to model agent interactions.", "author_comment": "To appear at the IEEE International Conference on Computer Vision\n  (ICCV 2019). Website: https://sites.google.com/view/precog", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2108.03298": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2108.03298v2", "post_title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation", "authors": ["Ajay Mandlekar", "Danfei Xu", "Josiah Wong", "Soroush Nasiriany", "Chen Wang", "Rohun Kulkarni", "Li Fei-Fei", "Silvio Savarese", "Yuke Zhu", "Roberto Mart\u00edn-Mart\u00edn"], "date_published": "2021-08-06 20:48:30+00:00", "data_last_modified": "2021-09-25 00:37:01+00:00", "url": "http://arxiv.org/abs/2108.03298v2", "abstract": "Imitating human demonstrations is a promising approach to endow robots with\nvarious manipulation capabilities. While recent advances have been made in\nimitation learning and batch (offline) reinforcement learning, a lack of\nopen-source human datasets and reproducible learning methods make assessing the\nstate of the field difficult. In this paper, we conduct an extensive study of\nsix offline learning algorithms for robot manipulation on five simulated and\nthree real-world multi-stage manipulation tasks of varying complexity, and with\ndatasets of varying quality. Our study analyzes the most critical challenges\nwhen learning from offline human data for manipulation. Based on the study, we\nderive a series of lessons including the sensitivity to different algorithmic\ndesign choices, the dependence on the quality of the demonstrations, and the\nvariability based on the stopping criteria due to the different objectives in\ntraining and evaluation. We also highlight opportunities for learning from\nhuman datasets, such as the ability to learn proficient policies on\nchallenging, multi-stage tasks beyond the scope of current reinforcement\nlearning methods, and the ability to easily scale to natural, real-world\nmanipulation scenarios where only raw sensory signals are available. We have\nopen-sourced our datasets and all algorithm implementations to facilitate\nfuture research and fair comparisons in learning from human demonstration data.\nCodebase, datasets, trained models, and more available at\nhttps://arise-initiative.github.io/robomimic-web/", "author_comment": "CoRL 2021 (Oral)", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1908.08016": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1908.08016v2", "post_title": "Testing Robustness Against Unforeseen Adversaries", "authors": ["Daniel Kang", "Yi Sun", "Dan Hendrycks", "Tom Brown", "Jacob Steinhardt"], "date_published": "2019-08-21 17:36:48+00:00", "data_last_modified": "2020-06-09 05:17:48+00:00", "url": "http://arxiv.org/abs/1908.08016v2", "abstract": "Most existing adversarial defenses only measure robustness to L_p adversarial\nattacks. Not only are adversaries unlikely to exclusively create small L_p\nperturbations, adversaries are unlikely to remain fixed. Adversaries adapt and\nevolve their attacks; hence adversarial defenses must be robust to a broad\nrange of unforeseen attacks. We address this discrepancy between research and\nreality by proposing a new evaluation framework called ImageNet-UA. Our\nframework enables the research community to test ImageNet model robustness\nagainst attacks not encountered during training. To create ImageNet-UA's\ndiverse attack suite, we introduce a total of four novel adversarial attacks.\nWe also demonstrate that, in comparison to ImageNet-UA, prevailing L_inf\nrobustness assessments give a narrow account of model robustness. By evaluating\ncurrent defenses with ImageNet-UA, we find they provide little robustness to\nunforeseen attacks. We hope the greater variety and realism of ImageNet-UA\nenables development of more robust defenses which can generalize beyond attacks\nseen during training.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.06256": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.06256v1", "post_title": "Learning Robust Representations by Projecting Superficial Statistics Out", "authors": ["Haohan Wang", "Zexue He", "Zachary C. Lipton", "Eric P. Xing"], "date_published": "2019-03-02 00:42:03+00:00", "data_last_modified": "2019-03-02 00:42:03+00:00", "url": "http://arxiv.org/abs/1903.06256v1", "abstract": "Despite impressive performance as evaluated on i.i.d. holdout data, deep\nneural networks depend heavily on superficial statistics of the training data\nand are liable to break under distribution shift. For example, subtle changes\nto the background or texture of an image can break a seemingly powerful\nclassifier. Building on previous work on domain generalization, we hope to\nproduce a classifier that will generalize to previously unseen domains, even\nwhen domain identifiers are not available during training. This setting is\nchallenging because the model may extract many distribution-specific\n(superficial) signals together with distribution-agnostic (semantic) signals.\nTo overcome this challenge, we incorporate the gray-level co-occurrence matrix\n(GLCM) to extract patterns that our prior knowledge suggests are superficial:\nthey are sensitive to the texture but unable to capture the gestalt of an\nimage. Then we introduce two techniques for improving our networks'\nout-of-sample performance. The first method is built on the reverse gradient\nmethod that pushes our model to learn representations from which the GLCM\nrepresentation is not predictable. The second method is built on the\nindependence introduced by projecting the model's representation onto the\nsubspace orthogonal to GLCM representation's. We test our method on the battery\nof standard domain generalization data sets and, interestingly, achieve\ncomparable or better performance as compared to other domain generalization\nmethods that explicitly require samples from the target distribution for\ntraining.", "author_comment": "To appear at ICLR 2019. Implementation:\n  https://github.com/HaohanWang/HEX", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1802.07228": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1802.07228v1", "post_title": "The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation", "authors": ["Miles Brundage", "Shahar Avin", "Jack Clark", "Helen Toner", "Peter Eckersley", "Ben Garfinkel", "Allan Dafoe", "Paul Scharre", "Thomas Zeitzoff", "Bobby Filar", "Hyrum Anderson", "Heather Roff", "Gregory C. Allen", "Jacob Steinhardt", "Carrick Flynn", "Se\u00e1n \u00d3 h\u00c9igeartaigh", "Simon Beard", "Haydn Belfield", "Sebastian Farquhar", "Clare Lyle", "Rebecca Crootof", "Owain Evans", "Michael Page", "Joanna Bryson", "Roman Yampolskiy", "Dario Amodei"], "date_published": "2018-02-20 18:07:50+00:00", "data_last_modified": "2018-02-20 18:07:50+00:00", "url": "http://arxiv.org/abs/1802.07228v1", "abstract": "This report surveys the landscape of potential security threats from\nmalicious uses of AI, and proposes ways to better forecast, prevent, and\nmitigate these threats. After analyzing the ways in which AI may influence the\nthreat landscape in the digital, physical, and political domains, we make four\nhigh-level recommendations for AI researchers and other stakeholders. We also\nsuggest several promising areas for further research that could expand the\nportfolio of defenses, or make attacks less effective or harder to execute.\nFinally, we discuss, but do not conclusively resolve, the long-term equilibrium\nof attackers and defenders.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CR", "cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.04198": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.04198v2", "post_title": "Preferences Implicit in the State of the World", "authors": ["Rohin Shah", "Dmitrii Krasheninnikov", "Jordan Alexander", "Pieter Abbeel", "Anca Dragan"], "date_published": "2019-02-12 00:50:56+00:00", "data_last_modified": "2019-04-18 22:15:37+00:00", "url": "http://arxiv.org/abs/1902.04198v2", "abstract": "Reinforcement learning (RL) agents optimize only the features specified in a\nreward function and are indifferent to anything left out inadvertently. This\nmeans that we must not only specify what to do, but also the much larger space\nof what not to do. It is easy to forget these preferences, since these\npreferences are already satisfied in our environment. This motivates our key\ninsight: when a robot is deployed in an environment that humans act in, the\nstate of the environment is already optimized for what humans want. We can\ntherefore use this implicit preference information from the state to fill in\nthe blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use\nit to evaluate the idea in a suite of proof-of-concept environments designed to\nshow its properties. We find that information from the initial state can be\nused to infer both side effects that should be avoided as well as preferences\nfor how the environment should be organized. Our code can be found at\nhttps://github.com/HumanCompatibleAI/rlsp.", "author_comment": "Published at ICLR 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1607.08289": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1607.08289v4", "post_title": "Mammalian Value Systems", "authors": ["Gopal P. Sarma", "Nick J. Hay"], "date_published": "2016-07-28 01:22:26+00:00", "data_last_modified": "2019-01-21 19:29:30+00:00", "url": "http://arxiv.org/abs/1607.08289v4", "abstract": "Characterizing human values is a topic deeply interwoven with the sciences,\nhumanities, art, and many other human endeavors. In recent years, a number of\nthinkers have argued that accelerating trends in computer science, cognitive\nscience, and related disciplines foreshadow the creation of intelligent\nmachines which meet and ultimately surpass the cognitive abilities of human\nbeings, thereby entangling an understanding of human values with future\ntechnological development. Contemporary research accomplishments suggest\nsophisticated AI systems becoming widespread and responsible for managing many\naspects of the modern world, from preemptively planning users' travel schedules\nand logistics, to fully autonomous vehicles, to domestic robots assisting in\ndaily living. The extrapolation of these trends has been most forcefully\ndescribed in the context of a hypothetical \"intelligence explosion,\" in which\nthe capabilities of an intelligent software agent would rapidly increase due to\nthe presence of feedback loops unavailable to biological organisms. The\npossibility of superintelligent agents, or simply the widespread deployment of\nsophisticated, autonomous AI systems, highlights an important theoretical\nproblem: the need to separate the cognitive and rational capacities of an agent\nfrom the fundamental goal structure, or value system, which constrains and\nguides the agent's actions. The \"value alignment problem\" is to specify a goal\nstructure for autonomous agents compatible with human values. In this brief\narticle, we suggest that recent ideas from affective neuroscience and related\ndisciplines aimed at characterizing neurological and behavioral universals in\nthe mammalian class provide important conceptual foundations relevant to\ndescribing human values. We argue that the notion of \"mammalian value systems\"\npoints to a potential avenue for fundamental research in AI safety and AI\nethics.", "author_comment": "12 pages", "journal_ref": "Informatica Vol. 41 No. 3 (2017)", "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1712.04307": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1712.04307v3", "post_title": "AI Safety and Reproducibility: Establishing Robust Foundations for the Neuropsychology of Human Values", "authors": ["Gopal P. Sarma", "Nick J. Hay", "Adam Safron"], "date_published": "2017-12-08 19:40:15+00:00", "data_last_modified": "2018-09-08 20:44:43+00:00", "url": "http://arxiv.org/abs/1712.04307v3", "abstract": "We propose the creation of a systematic effort to identify and replicate key\nfindings in neuropsychology and allied fields related to understanding human\nvalues. Our aim is to ensure that research underpinning the value alignment\nproblem of artificial intelligence has been sufficiently validated to play a\nrole in the design of AI systems.", "author_comment": "5 pages", "journal_ref": "In: Gallina B., Skavhaug A., Schoitsch E., Bitsch F. (eds)\n  Computer Safety, Reliability, and Security. SAFECOMP 2018. Lecture Notes in\n  Computer Science, vol 11094. Springer, Cham", "doi": "10.1007/978-3-319-99229-7_45", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.12613": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.12613v6", "post_title": "Asking the Right Questions: Learning Interpretable Action Models Through Query Answering", "authors": ["Pulkit Verma", "Shashank Rao Marpally", "Siddharth Srivastava"], "date_published": "2019-12-29 09:05:06+00:00", "data_last_modified": "2021-04-09 16:17:14+00:00", "url": "http://arxiv.org/abs/1912.12613v6", "abstract": "This paper develops a new approach for estimating an interpretable,\nrelational model of a black-box autonomous agent that can plan and act. Our\nmain contributions are a new paradigm for estimating such models using a\nminimal query interface with the agent, and a hierarchical querying algorithm\nthat generates an interrogation policy for estimating the agent's internal\nmodel in a vocabulary provided by the user. Empirical evaluation of our\napproach shows that despite the intractable search space of possible agent\nmodels, our approach allows correct and scalable estimation of interpretable\nagent models for a wide class of black-box autonomous agents. Our results also\nshow that this approach can use predicate classifiers to learn interpretable\nmodels of planning agents that represent states as images.", "author_comment": "AAAI 2021", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2004.09044": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2004.09044v1", "post_title": "Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense", "authors": ["Yixin Zhu", "Tao Gao", "Lifeng Fan", "Siyuan Huang", "Mark Edmonds", "Hangxin Liu", "Feng Gao", "Chi Zhang", "Siyuan Qi", "Ying Nian Wu", "Joshua B. Tenenbaum", "Song-Chun Zhu"], "date_published": "2020-04-20 04:07:28+00:00", "data_last_modified": "2020-04-20 04:07:28+00:00", "url": "http://arxiv.org/abs/2004.09044v1", "abstract": "Recent progress in deep learning is essentially based on a \"big data for\nsmall tasks\" paradigm, under which massive amounts of data are used to train a\nclassifier for a single narrow task. In this paper, we call for a shift that\nflips this paradigm upside down. Specifically, we propose a \"small data for big\ntasks\" paradigm, wherein a single artificial intelligence (AI) system is\nchallenged to develop \"common sense\", enabling it to solve a wide range of\ntasks with little training data. We illustrate the potential power of this new\nparadigm by reviewing models of common sense that synthesize recent\nbreakthroughs in both machine and human vision. We identify functionality,\nphysics, intent, causality, and utility (FPICU) as the five core domains of\ncognitive AI with humanlike common sense. When taken as a unified concept,\nFPICU is concerned with the questions of \"why\" and \"how\", beyond the dominant\n\"what\" and \"where\" framework for understanding vision. They are invisible in\nterms of pixels but nevertheless drive the creation, maintenance, and\ndevelopment of visual scenes. We therefore coin them the \"dark matter\" of\nvision. Just as our universe cannot be understood by merely studying observable\nmatter, we argue that vision cannot be understood without studying FPICU. We\ndemonstrate the power of this perspective to develop cognitive AI systems with\nhumanlike common sense by showing how to observe and apply FPICU with little\ntraining data to solve a wide range of challenging tasks, including tool use,\nplanning, utility inference, and social learning. In summary, we argue that the\nnext generation of AI must embrace \"dark\" humanlike common sense for solving\nnovel tasks.", "author_comment": "For high quality figures, please refer to\n  http://wellyzhang.github.io/attach/dark.pdf", "journal_ref": "Engineering, Feb, 2020", "doi": "10.1016/j.eng.2020.01.011", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.11295": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.11295v1", "post_title": "From ImageNet to Image Classification: Contextualizing Progress on Benchmarks", "authors": ["Dimitris Tsipras", "Shibani Santurkar", "Logan Engstrom", "Andrew Ilyas", "Aleksander Madry"], "date_published": "2020-05-22 17:39:16+00:00", "data_last_modified": "2020-05-22 17:39:16+00:00", "url": "http://arxiv.org/abs/2005.11295v1", "abstract": "Building rich machine learning datasets in a scalable manner often\nnecessitates a crowd-sourced data collection pipeline. In this work, we use\nhuman studies to investigate the consequences of employing such a pipeline,\nfocusing on the popular ImageNet dataset. We study how specific design choices\nin the ImageNet creation process impact the fidelity of the resulting\ndataset---including the introduction of biases that state-of-the-art models\nexploit. Our analysis pinpoints how a noisy data collection pipeline can lead\nto a systematic misalignment between the resulting benchmark and the real-world\ntask it serves as a proxy for. Finally, our findings emphasize the need to\naugment our current model training and evaluation toolkit to take such\nmisalignments into account. To facilitate further research, we release our\nrefined ImageNet annotations at https://github.com/MadryLab/ImageNetMultiLabel.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2004.09984": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2004.09984v3", "post_title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT", "authors": ["Linyang Li", "Ruotian Ma", "Qipeng Guo", "Xiangyang Xue", "Xipeng Qiu"], "date_published": "2020-04-21 13:30:02+00:00", "data_last_modified": "2020-10-02 03:08:04+00:00", "url": "http://arxiv.org/abs/2004.09984v3", "abstract": "Adversarial attacks for discrete data (such as texts) have been proved\nsignificantly more challenging than continuous data (such as images) since it\nis difficult to generate adversarial samples with gradient-based methods.\nCurrent successful attack methods for texts usually adopt heuristic replacement\nstrategies on the character or word level, which remains challenging to find\nthe optimal solution in the massive space of possible combinations of\nreplacements while preserving semantic consistency and language fluency. In\nthis paper, we propose \\textbf{BERT-Attack}, a high-quality and effective\nmethod to generate adversarial samples using pre-trained masked language models\nexemplified by BERT. We turn BERT against its fine-tuned models and other deep\nneural models in downstream tasks so that we can successfully mislead the\ntarget models to predict incorrectly. Our method outperforms state-of-the-art\nattack strategies in both success rate and perturb percentage, while the\ngenerated adversarial samples are fluent and semantically preserved. Also, the\ncost of calculation is low, thus possible for large-scale generations. The code\nis available at https://github.com/LinyangLee/BERT-Attack.", "author_comment": "Accepted by EMNLP2020", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.07685": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.07685v3", "post_title": "World Discovery Models", "authors": ["Mohammad Gheshlaghi Azar", "Bilal Piot", "Bernardo Avila Pires", "Jean-Bastien Grill", "Florent Altch\u00e9", "R\u00e9mi Munos"], "date_published": "2019-02-20 18:07:18+00:00", "data_last_modified": "2019-03-01 20:25:58+00:00", "url": "http://arxiv.org/abs/1902.07685v3", "abstract": "As humans we are driven by a strong desire for seeking novelty in our world.\nAlso upon observing a novel pattern we are capable of refining our\nunderstanding of the world based on the new information---humans can discover\ntheir world. The outstanding ability of the human mind for discovery has led to\nmany breakthroughs in science, art and technology. Here we investigate the\npossibility of building an agent capable of discovering its world using the\nmodern AI technology. In particular we introduce NDIGO, Neural Differential\nInformation Gain Optimisation, a self-supervised discovery model that aims at\nseeking new information to construct a global view of its world from partial\nand noisy observations. Our experiments on some controlled 2-D navigation tasks\nshow that NDIGO outperforms state-of-the-art information-seeking methods in\nterms of the quality of the learned representation. The improvement in\nperformance is particularly significant in the presence of white or structured\nnoise where other information-seeking methods follow the noise instead of\ndiscovering their world.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "stat.AP", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.08573": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.08573v3", "post_title": "Theoretically Principled Trade-off between Robustness and Accuracy", "authors": ["Hongyang Zhang", "Yaodong Yu", "Jiantao Jiao", "Eric P. Xing", "Laurent El Ghaoui", "Michael I. Jordan"], "date_published": "2019-01-24 18:43:57+00:00", "data_last_modified": "2019-06-24 07:04:11+00:00", "url": "http://arxiv.org/abs/1901.08573v3", "abstract": "We identify a trade-off between robustness and accuracy that serves as a\nguiding principle in the design of defenses against adversarial examples.\nAlthough this problem has been widely studied empirically, much remains unknown\nconcerning the theory underlying this trade-off. In this work, we decompose the\nprediction error for adversarial examples (robust error) as the sum of the\nnatural (classification) error and boundary error, and provide a differentiable\nupper bound using the theory of classification-calibrated loss, which is shown\nto be the tightest possible upper bound uniform over all probability\ndistributions and measurable predictors. Inspired by our theoretical analysis,\nwe also design a new defense method, TRADES, to trade adversarial robustness\noff against accuracy. Our proposed algorithm performs well experimentally in\nreal-world datasets. The methodology is the foundation of our entry to the\nNeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of\n~2,000 submissions, surpassing the runner-up approach by $11.41\\%$ in terms of\nmean $\\ell_2$ perturbation distance.", "author_comment": "Appeared in ICML 2019; the winning methodology of the NeurIPS 2018\n  Adversarial Vision Challenge", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2112.05135": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2112.05135v2", "post_title": "PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures", "authors": ["Dan Hendrycks", "Andy Zou", "Mantas Mazeika", "Leonard Tang", "Bo Li", "Dawn Song", "Jacob Steinhardt"], "date_published": "2021-12-09 18:59:31+00:00", "data_last_modified": "2021-12-11 19:19:18+00:00", "url": "http://arxiv.org/abs/2112.05135v2", "abstract": "In real-world applications of machine learning, reliable and safe systems\nmust consider measures of performance beyond standard test set accuracy. These\nother goals include out-of-distribution (OOD) robustness, prediction\nconsistency, resilience to adversaries, calibrated uncertainty estimates, and\nthe ability to detect anomalous inputs. However, improving performance towards\nthese goals is often a balancing act that today's methods cannot achieve\nwithout sacrificing performance on other safety axes. For instance, adversarial\ntraining improves adversarial robustness but sharply degrades other classifier\nperformance metrics. Similarly, strong data augmentation and regularization\ntechniques often improve OOD robustness but harm anomaly detection, raising the\nquestion of whether a Pareto improvement on all existing safety measures is\npossible. To meet this challenge, we design a new data augmentation strategy\nutilizing the natural structural complexity of pictures such as fractals, which\noutperforms numerous baselines, is near Pareto-optimal, and roundly improves\nsafety measures.", "author_comment": "Code and models are available at https://github.com/andyzoujm/pixmix", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1409.3215": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1409.3215v3", "post_title": "Sequence to Sequence Learning with Neural Networks", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "date_published": "2014-09-10 19:55:35+00:00", "data_last_modified": "2014-12-14 20:59:51+00:00", "url": "http://arxiv.org/abs/1409.3215v3", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.", "author_comment": "9 pages", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2103.03386": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2103.03386v1", "post_title": "Clusterability in Neural Networks", "authors": ["Daniel Filan", "Stephen Casper", "Shlomi Hod", "Cody Wild", "Andrew Critch", "Stuart Russell"], "date_published": "2021-03-04 23:53:53+00:00", "data_last_modified": "2021-03-04 23:53:53+00:00", "url": "http://arxiv.org/abs/2103.03386v1", "abstract": "The learned weights of a neural network have often been considered devoid of\nscrutable internal structure. In this paper, however, we look for structure in\nthe form of clusterability: how well a network can be divided into groups of\nneurons with strong internal connectivity but weak external connectivity. We\nfind that a trained neural network is typically more clusterable than randomly\ninitialized networks, and often clusterable relative to random networks with\nthe same distribution of weights. We also exhibit novel methods to promote\nclusterability in neural network training, and find that in multi-layer\nperceptrons they lead to more clusterable networks with little reduction in\naccuracy. Understanding and controlling the clusterability of neural networks\nwill hopefully render their inner workings more interpretable to engineers by\nfacilitating partitioning into meaningful clusters.", "author_comment": "20 pages, 22 figures. arXiv admin note: text overlap with\n  arXiv:2003.04881", "journal_ref": null, "doi": null, "primary_category": "cs.NE", "categories": ["cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2110.14419": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2110.14419v1", "post_title": "Towards a Theory of Justice for Artificial Intelligence", "authors": ["Iason Gabriel"], "date_published": "2021-10-27 13:23:38+00:00", "data_last_modified": "2021-10-27 13:23:38+00:00", "url": "http://arxiv.org/abs/2110.14419v1", "abstract": "This paper explores the relationship between artificial intelligence and\nprinciples of distributive justice. Drawing upon the political philosophy of\nJohn Rawls, it holds that the basic structure of society should be understood\nas a composite of socio-technical systems, and that the operation of these\nsystems is increasingly shaped and influenced by AI. As a consequence,\negalitarian norms of justice apply to the technology when it is deployed in\nthese contexts. These norms entail that the relevant AI systems must meet a\ncertain standard of public justification, support citizens rights, and promote\nsubstantively fair outcomes -- something that requires specific attention be\npaid to the impact they have on the worst-off members of society.", "author_comment": "12 pages", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "K.4.1"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.10299": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.10299v1", "post_title": "Variational Option Discovery Algorithms", "authors": ["Joshua Achiam", "Harrison Edwards", "Dario Amodei", "Pieter Abbeel"], "date_published": "2018-07-26 18:05:45+00:00", "data_last_modified": "2018-07-26 18:05:45+00:00", "url": "http://arxiv.org/abs/1807.10299v1", "abstract": "We explore methods for option discovery based on variational inference and\nmake two algorithmic contributions. First: we highlight a tight connection\nbetween variational option discovery methods and variational autoencoders, and\nintroduce Variational Autoencoding Learning of Options by Reinforcement\n(VALOR), a new method derived from the connection. In VALOR, the policy encodes\ncontexts from a noise distribution into trajectories, and the decoder recovers\nthe contexts from the complete trajectories. Second: we propose a curriculum\nlearning approach where the number of contexts seen by the agent increases\nwhenever the agent's performance is strong enough (as measured by the decoder)\non the current set of contexts. We show that this simple trick stabilizes\ntraining for VALOR and prior variational option discovery methods, allowing a\nsingle agent to learn many more modes of behavior than it could with a fixed\ncontext distribution. Finally, we investigate other topics related to\nvariational option discovery, including fundamental limitations of the general\napproach and the applicability of learned options to downstream tasks.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.14032": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.14032v2", "post_title": "Compositional Explanations of Neurons", "authors": ["Jesse Mu", "Jacob Andreas"], "date_published": "2020-06-24 20:37:05+00:00", "data_last_modified": "2021-02-02 23:46:51+00:00", "url": "http://arxiv.org/abs/2006.14032v2", "abstract": "We describe a procedure for explaining neurons in deep representations by\nidentifying compositional logical concepts that closely approximate neuron\nbehavior. Compared to prior work that uses atomic labels as explanations,\nanalyzing neurons compositionally allows us to more precisely and expressively\ncharacterize their behavior. We use this procedure to answer several questions\non interpretability in models for vision and natural language processing.\nFirst, we examine the kinds of abstractions learned by neurons. In image\nclassification, we find that many neurons learn highly abstract but\nsemantically coherent visual concepts, while other polysemantic neurons detect\nmultiple unrelated features; in natural language inference (NLI), neurons learn\nshallow lexical heuristics from dataset biases. Second, we see whether\ncompositional explanations give us insight into model performance: vision\nneurons that detect human-interpretable concepts are positively correlated with\ntask performance, while NLI neurons that fire for shallow heuristics are\nnegatively correlated with task performance. Finally, we show how compositional\nexplanations provide an accessible way for end users to produce simple\n\"copy-paste\" adversarial examples that change model behavior in predictable\nways.", "author_comment": "NeurIPS 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.09335": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.09335v2", "post_title": "Imitation Learning from Video by Leveraging Proprioception", "authors": ["Faraz Torabi", "Garrett Warnell", "Peter Stone"], "date_published": "2019-05-22 19:21:05+00:00", "data_last_modified": "2019-06-19 03:59:10+00:00", "url": "http://arxiv.org/abs/1905.09335v2", "abstract": "Classically, imitation learning algorithms have been developed for idealized\nsituations, e.g., the demonstrations are often required to be collected in the\nexact same environment and usually include the demonstrator's actions.\nRecently, however, the research community has begun to address some of these\nshortcomings by offering algorithmic solutions that enable imitation learning\nfrom observation (IfO), e.g., learning to perform a task from visual\ndemonstrations that may be in a different environment and do not include\nactions. Motivated by the fact that agents often also have access to their own\ninternal states (i.e., proprioception), we propose and study an IfO algorithm\nthat leverages this information in the policy learning process. The proposed\narchitecture learns policies over proprioceptive state representations and\ncompares the resulting trajectories visually to the demonstration data. We\nexperimentally test the proposed technique on several MuJoCo domains and show\nthat it outperforms other imitation from observation algorithms by a large\nmargin.", "author_comment": "International Joint Conference on Artificial Intelligence (IJCAI\n  2019)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2011.06275": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2011.06275v2", "post_title": "Performance of Bounded-Rational Agents With the Ability to Self-Modify", "authors": ["Jakub T\u011btek", "Marek Sklenka", "Tom\u00e1\u0161 Gaven\u010diak"], "date_published": "2020-11-12 09:25:08+00:00", "data_last_modified": "2021-01-18 09:55:26+00:00", "url": "http://arxiv.org/abs/2011.06275v2", "abstract": "Self-modification of agents embedded in complex environments is hard to\navoid, whether it happens via direct means (e.g. own code modification) or\nindirectly (e.g. influencing the operator, exploiting bugs or the environment).\nIt has been argued that intelligent agents have an incentive to avoid modifying\ntheir utility function so that their future instances work towards the same\ngoals.\n  Everitt et al. (2016) formally show that providing an option to self-modify\nis harmless for perfectly rational agents. We show that this result is no\nlonger true for agents with bounded rationality. In such agents,\nself-modification may cause exponential deterioration in performance and\ngradual misalignment of a previously aligned agent. We investigate how the size\nof this effect depends on the type and magnitude of imperfections in the\nagent's rationality (1-4 below). We also discuss model assumptions and the\nwider problem and framing space.\n  We examine four ways in which an agent can be bounded-rational: it either (1)\ndoesn't always choose the optimal action, (2) is not perfectly aligned with\nhuman values, (3) has an inaccurate model of the environment, or (4) uses the\nwrong temporal discounting factor. We show that while in the cases (2)-(4) the\nmisalignment caused by the agent's imperfection does not increase over time,\nwith (1) the misalignment may grow exponentially.", "author_comment": "Fixed minor problems; To appear in SafeAI @ AAAI 2021", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2007.01223": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2007.01223v1", "post_title": "Verifiably Safe Exploration for End-to-End Reinforcement Learning", "authors": ["Nathan Hunt", "Nathan Fulton", "Sara Magliacane", "Nghia Hoang", "Subhro Das", "Armando Solar-Lezama"], "date_published": "2020-07-02 16:12:20+00:00", "data_last_modified": "2020-07-02 16:12:20+00:00", "url": "http://arxiv.org/abs/2007.01223v1", "abstract": "Deploying deep reinforcement learning in safety-critical settings requires\ndeveloping algorithms that obey hard constraints during exploration. This paper\ncontributes a first approach toward enforcing formal safety constraints on\nend-to-end policies with visual inputs. Our approach draws on recent advances\nin object detection and automated reasoning for hybrid dynamical systems. The\napproach is evaluated on a novel benchmark that emphasizes the challenge of\nsafely exploring in the presence of hard constraints. Our benchmark draws from\nseveral proposed problem sets for safe learning and includes problems that\nemphasize challenges such as reward signals that are not aligned with safety\nconstraints. On each of these benchmark problems, our algorithm completely\navoids unsafe behavior while remaining competitive at optimizing for as much\nreward as is safe. We also prove that our method of enforcing the safety\nconstraints preserves all safe policies from the original environment.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "cs.LO", "F.3.1; I.2.8"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.05960": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.05960v3", "post_title": "Meta-Learning with Latent Embedding Optimization", "authors": ["Andrei A. Rusu", "Dushyant Rao", "Jakub Sygnowski", "Oriol Vinyals", "Razvan Pascanu", "Simon Osindero", "Raia Hadsell"], "date_published": "2018-07-16 16:35:29+00:00", "data_last_modified": "2019-03-26 13:36:45+00:00", "url": "http://arxiv.org/abs/1807.05960v3", "abstract": "Gradient-based meta-learning techniques are both widely applicable and\nproficient at solving challenging few-shot learning and fast adaptation\nproblems. However, they have practical difficulties when operating on\nhigh-dimensional parameter spaces in extreme low-data regimes. We show that it\nis possible to bypass these limitations by learning a data-dependent latent\ngenerative representation of model parameters, and performing gradient-based\nmeta-learning in this low-dimensional latent space. The resulting approach,\nlatent embedding optimization (LEO), decouples the gradient-based adaptation\nprocedure from the underlying high-dimensional space of model parameters. Our\nevaluation shows that LEO can achieve state-of-the-art performance on the\ncompetitive miniImageNet and tieredImageNet few-shot classification tasks.\nFurther analysis indicates LEO is able to capture uncertainty in the data, and\ncan perform adaptation more effectively by optimizing in latent space.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.03973": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.03973v2", "post_title": "E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles", "authors": ["Markus Kettunen", "Erik H\u00e4rk\u00f6nen", "Jaakko Lehtinen"], "date_published": "2019-06-10 13:40:37+00:00", "data_last_modified": "2019-06-11 08:58:35+00:00", "url": "http://arxiv.org/abs/1906.03973v2", "abstract": "It has been recently shown that the hidden variables of convolutional neural\nnetworks make for an efficient perceptual similarity metric that accurately\npredicts human judgment on relative image similarity assessment. First, we show\nthat such learned perceptual similarity metrics (LPIPS) are susceptible to\nadversarial attacks that dramatically contradict human visual similarity\njudgment. While this is not surprising in light of neural networks' well-known\nweakness to adversarial perturbations, we proceed to show that self-ensembling\nwith an infinite family of random transformations of the input --- a technique\nknown not to render classification networks robust --- is enough to turn the\nmetric robust against attack, while retaining predictive power on human\njudgments. Finally, we study the geometry imposed by our our novel\nself-ensembled metric (E-LPIPS) on the space of natural images. We find\nevidence of \"perceptual convexity\" by showing that convex combinations of\nsimilar-looking images retain appearance, and that discrete geodesics yield\nmeaningful frame interpolation and texture morphing, all without explicit\ncorrespondences.", "author_comment": "Code and supplemental material available at\n  https://github.com/mkettune/elpips/", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.02175": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.02175v4", "post_title": "Adversarial Examples Are Not Bugs, They Are Features", "authors": ["Andrew Ilyas", "Shibani Santurkar", "Dimitris Tsipras", "Logan Engstrom", "Brandon Tran", "Aleksander Madry"], "date_published": "2019-05-06 17:45:05+00:00", "data_last_modified": "2019-08-12 14:36:10+00:00", "url": "http://arxiv.org/abs/1905.02175v4", "abstract": "Adversarial examples have attracted significant attention in machine\nlearning, but the reasons for their existence and pervasiveness remain unclear.\nWe demonstrate that adversarial examples can be directly attributed to the\npresence of non-robust features: features derived from patterns in the data\ndistribution that are highly predictive, yet brittle and incomprehensible to\nhumans. After capturing these features within a theoretical framework, we\nestablish their widespread existence in standard datasets. Finally, we present\na simple setting where we can rigorously tie the phenomena we observe in\npractice to a misalignment between the (human-specified) notion of robustness\nand the inherent geometry of the data.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.CR", "cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.10208": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.10208v1", "post_title": "Towards Learning Multi-agent Negotiations via Self-Play", "authors": ["Yichuan Charlie Tang"], "date_published": "2020-01-28 08:37:33+00:00", "data_last_modified": "2020-01-28 08:37:33+00:00", "url": "http://arxiv.org/abs/2001.10208v1", "abstract": "Making sophisticated, robust, and safe sequential decisions is at the heart\nof intelligent systems. This is especially critical for planning in complex\nmulti-agent environments, where agents need to anticipate other agents'\nintentions and possible future actions. Traditional methods formulate the\nproblem as a Markov Decision Process, but the solutions often rely on various\nassumptions and become brittle when presented with corner cases. In contrast,\ndeep reinforcement learning (Deep RL) has been very effective at finding\npolicies by simultaneously exploring, interacting, and learning from\nenvironments. Leveraging the powerful Deep RL paradigm, we demonstrate that an\niterative procedure of self-play can create progressively more diverse\nenvironments, leading to the learning of sophisticated and robust multi-agent\npolicies. We demonstrate this in a challenging multi-agent simulation of\nmerging traffic, where agents must interact and negotiate with others in order\nto successfully merge on or off the road. While the environment starts off\nsimple, we increase its complexity by iteratively adding an increasingly\ndiverse set of agents to the agent \"zoo\" as training progresses. Qualitatively,\nwe find that through self-play, our policies automatically learn interesting\nbehaviors such as defensive driving, overtaking, yielding, and the use of\nsignal lights to communicate intentions to other agents. In addition,\nquantitatively, we show a dramatic improvement of the success rate of merging\nmaneuvers from 63% to over 98%.", "author_comment": "Autonomous Driving Workshop, IEEE International Conference on\n  Computer Vision (ICCV 2019)", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.09190": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.09190v3", "post_title": "Towards the first adversarially robust neural network model on MNIST", "authors": ["Lukas Schott", "Jonas Rauber", "Matthias Bethge", "Wieland Brendel"], "date_published": "2018-05-23 14:16:22+00:00", "data_last_modified": "2018-09-20 17:49:14+00:00", "url": "http://arxiv.org/abs/1805.09190v3", "abstract": "Despite much effort, deep neural networks remain highly susceptible to tiny\ninput perturbations and even for MNIST, one of the most common toy datasets in\ncomputer vision, no neural network model exists for which adversarial\nperturbations are large and make semantic sense to humans. We show that even\nthe widely recognized and by far most successful defense by Madry et al. (1)\noverfits on the L-infinity metric (it's highly susceptible to L2 and L0\nperturbations), (2) classifies unrecognizable images with high certainty, (3)\nperforms not much better than simple input binarization and (4) features\nadversarial perturbations that make little sense to humans. These results\nsuggest that MNIST is far from being solved in terms of adversarial robustness.\nWe present a novel robust classification model that performs analysis by\nsynthesis using learned class-conditional data distributions. We derive bounds\non the robustness and go to great length to empirically evaluate our model\nusing maximally effective adversarial attacks by (a) applying decision-based,\nscore-based, gradient-based and transfer-based attacks for several different Lp\nnorms, (b) by designing a new attack that exploits the structure of our\ndefended model and (c) by devising a novel decision-based attack that seeks to\nminimize the number of perturbed pixels (L0). The results suggest that our\napproach yields state-of-the-art robustness on MNIST against L0, L2 and\nL-infinity perturbations and we demonstrate that most adversarial examples are\nstrongly perturbed towards the perceptual boundary between the original and the\nadversarial class.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.10862": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.10862v2", "post_title": "Recursively Summarizing Books with Human Feedback", "authors": ["Jeff Wu", "Long Ouyang", "Daniel M. Ziegler", "Nisan Stiennon", "Ryan Lowe", "Jan Leike", "Paul Christiano"], "date_published": "2021-09-22 17:34:18+00:00", "data_last_modified": "2021-09-27 21:12:49+00:00", "url": "http://arxiv.org/abs/2109.10862v2", "abstract": "A major challenge for scaling machine learning is training models to perform\ntasks that are very difficult or time-consuming for humans to evaluate. We\npresent progress on this problem on the task of abstractive summarization of\nentire fiction novels. Our method combines learning from human feedback with\nrecursive task decomposition: we use models trained on smaller parts of the\ntask to assist humans in giving feedback on the broader task. We collect a\nlarge volume of demonstrations and comparisons from human labelers, and\nfine-tune GPT-3 using behavioral cloning and reward modeling to do\nsummarization recursively. At inference time, the model first summarizes small\nsections of the book and then recursively summarizes these summaries to produce\na summary of the entire book. Our human labelers are able to supervise and\nevaluate the models quickly, despite not having read the entire books\nthemselves. Our resulting model generates sensible summaries of entire books,\neven matching the quality of human-written summaries in a few cases ($\\sim5\\%$\nof books). We achieve state-of-the-art results on the recent BookSum dataset\nfor book-length summarization. A zero-shot question-answering model using these\nsummaries achieves state-of-the-art results on the challenging NarrativeQA\nbenchmark for answering questions about books and movie scripts. We release\ndatasets of samples from our model.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2104.00739": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2104.00739v1", "post_title": "Formal Methods for the Informal Engineer: Workshop Recommendations", "authors": ["Gopal Sarma", "James Koppel", "Gregory Malecha", "Patrick Schultz", "Eric Drexler", "Ramana Kumar", "Cody Roux", "Philip Zucker"], "date_published": "2021-04-01 19:22:42+00:00", "data_last_modified": "2021-04-01 19:22:42+00:00", "url": "http://arxiv.org/abs/2104.00739v1", "abstract": "Formal Methods for the Informal Engineer (FMIE) was a workshop held at the\nBroad Institute of MIT and Harvard in 2021 to explore the potential role of\nverified software in the biomedical software ecosystem. The motivation for\norganizing FMIE was the recognition that the life sciences and medicine are\nundergoing a transition from being passive consumers of software and AI/ML\ntechnologies to fundamental drivers of new platforms, including those which\nwill need to be mission and safety-critical. Drawing on conversations leading\nup to and during the workshop, we make five concrete recommendations to help\nsoftware leaders organically incorporate tools, techniques, and perspectives\nfrom formal methods into their project planning and development trajectories.", "author_comment": "6 pages", "journal_ref": null, "doi": "10.31219/osf.io/t4qs8", "primary_category": "cs.SE", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL", "q-bio.OT"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2104.06613": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2104.06613v1", "post_title": "Detection of Dataset Shifts in Learning-Enabled Cyber-Physical Systems using Variational Autoencoder for Regression", "authors": ["Feiyang Cai", "Ali I. Ozdagli", "Xenofon Koutsoukos"], "date_published": "2021-04-14 03:46:37+00:00", "data_last_modified": "2021-04-14 03:46:37+00:00", "url": "http://arxiv.org/abs/2104.06613v1", "abstract": "Cyber-physical systems (CPSs) use learning-enabled components (LECs)\nextensively to cope with various complex tasks under high-uncertainty\nenvironments. However, the dataset shifts between the training and testing\nphase may lead the LECs to become ineffective to make large-error predictions,\nand further, compromise the safety of the overall system. In our paper, we\nfirst provide the formal definitions for different types of dataset shifts in\nlearning-enabled CPS. Then, we propose an approach to detect the dataset shifts\neffectively for regression problems. Our approach is based on the inductive\nconformal anomaly detection and utilizes a variational autoencoder for\nregression model which enables the approach to take into consideration both LEC\ninput and output for detecting dataset shifts. Additionally, in order to\nimprove the robustness of detection, layer-wise relevance propagation (LRP) is\nincorporated into our approach. We demonstrate our approach by using an\nadvanced emergency braking system implemented in an open-source simulator for\nself-driving cars. The evaluation results show that our approach can detect\ndifferent types of dataset shifts with a small number of false alarms while the\nexecution time is smaller than the sampling period of the system.", "author_comment": "Accepted by the 4th IEEE International Conference on Industrial\n  Cyber-Physical Systems (ICPS2021)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.08663": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.08663v1", "post_title": "Modeling AGI Safety Frameworks with Causal Influence Diagrams", "authors": ["Tom Everitt", "Ramana Kumar", "Victoria Krakovna", "Shane Legg"], "date_published": "2019-06-20 14:35:03+00:00", "data_last_modified": "2019-06-20 14:35:03+00:00", "url": "http://arxiv.org/abs/1906.08663v1", "abstract": "Proposals for safe AGI systems are typically made at the level of frameworks,\nspecifying how the components of the proposed system should be trained and\ninteract with each other. In this paper, we model and compare the most\npromising AGI safety frameworks using causal influence diagrams. The diagrams\nshow the optimization objective and causal assumptions of the framework. The\nunified representation permits easy comparison of frameworks and their\nassumptions. We hope that the diagrams will serve as an accessible and visual\nintroduction to the main AGI safety frameworks.", "author_comment": "IJCAI 2019 AI Safety Workshop", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.00403": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.00403v2", "post_title": "Towards Mixed Optimization for Reinforcement Learning with Program Synthesis", "authors": ["Surya Bhupatiraju", "Kumar Krishna Agrawal", "Rishabh Singh"], "date_published": "2018-07-01 21:52:07+00:00", "data_last_modified": "2018-07-03 22:08:06+00:00", "url": "http://arxiv.org/abs/1807.00403v2", "abstract": "Deep reinforcement learning has led to several recent breakthroughs, though\nthe learned policies are often based on black-box neural networks. This makes\nthem difficult to interpret and to impose desired specification constraints\nduring learning. We present an iterative framework, MORL, for improving the\nlearned policies using program synthesis. Concretely, we propose to use\nsynthesis techniques to obtain a symbolic representation of the learned policy,\nwhich can then be debugged manually or automatically using program repair.\nAfter the repair step, we use behavior cloning to obtain the policy\ncorresponding to the repaired program, which is then further improved using\ngradient descent. This process continues until the learned policy satisfies\ndesired constraints. We instantiate MORL for the simple CartPole problem and\nshow that the programmatic representation allows for high-level modifications\nthat in turn lead to improved learning of the policies.", "author_comment": "Updated publication details, format. Accepted at NAMPI workshop, ICML\n  '18", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.02846": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.02846v1", "post_title": "Safety Aware Reinforcement Learning (SARL)", "authors": ["Santiago Miret", "Somdeb Majumdar", "Carroll Wainwright"], "date_published": "2020-10-06 16:08:28+00:00", "data_last_modified": "2020-10-06 16:08:28+00:00", "url": "http://arxiv.org/abs/2010.02846v1", "abstract": "As reinforcement learning agents become increasingly integrated into complex,\nreal-world environments, designing for safety becomes a critical consideration.\nWe specifically focus on researching scenarios where agents can cause undesired\nside effects while executing a policy on a primary task. Since one can define\nmultiple tasks for a given environment dynamics, there are two important\nchallenges. First, we need to abstract the concept of safety that applies\nbroadly to that environment independent of the specific task being executed.\nSecond, we need a mechanism for the abstracted notion of safety to modulate the\nactions of agents executing different policies to minimize their side-effects.\nIn this work, we propose Safety Aware Reinforcement Learning (SARL) - a\nframework where a virtual safe agent modulates the actions of a main\nreward-based agent to minimize side effects. The safe agent learns a\ntask-independent notion of safety for a given environment. The main agent is\nthen trained with a regularization loss given by the distance between the\nnative action probabilities of the two agents. Since the safe agent effectively\nabstracts a task-independent notion of safety via its action probabilities, it\ncan be ported to modulate multiple policies solving different tasks within the\ngiven environment without further training. We contrast this with solutions\nthat rely on task-specific regularization metrics and test our framework on the\nSafeLife Suite, based on Conway's Game of Life, comprising a number of complex\ntasks in dynamic environments. We show that our solution is able to match the\nperformance of solutions that rely on task-specific side-effect penalties on\nboth the primary and safety objectives while additionally providing the benefit\nof generalizability and portability.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.04465": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.04465v1", "post_title": "LESS is More: Rethinking Probabilistic Models of Human Behavior", "authors": ["Andreea Bobu", "Dexter R. R. Scobee", "Jaime F. Fisac", "S. Shankar Sastry", "Anca D. Dragan"], "date_published": "2020-01-13 18:59:01+00:00", "data_last_modified": "2020-01-13 18:59:01+00:00", "url": "http://arxiv.org/abs/2001.04465v1", "abstract": "Robots need models of human behavior for both inferring human goals and\npreferences, and predicting what people will do. A common model is the\nBoltzmann noisily-rational decision model, which assumes people approximately\noptimize a reward function and choose trajectories in proportion to their\nexponentiated reward. While this model has been successful in a variety of\nrobotics domains, its roots lie in econometrics, and in modeling decisions\namong different discrete options, each with its own utility or reward. In\ncontrast, human trajectories lie in a continuous space, with continuous-valued\nfeatures that influence the reward function. We propose that it is time to\nrethink the Boltzmann model, and design it from the ground up to operate over\nsuch trajectory spaces. We introduce a model that explicitly accounts for\ndistances between trajectories, rather than only their rewards. Rather than\neach trajectory affecting the decision independently, similar trajectories now\naffect the decision together. We start by showing that our model better\nexplains human behavior in a user study. We then analyze the implications this\nhas for robot inference, first in toy environments where we have ground truth\nand find more accurate inference, and finally for a 7DOF robot arm learning\nfrom user demonstrations.", "author_comment": "9 pages, 7 figures", "journal_ref": null, "doi": "10.1145/3319502.3374811", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2007.04068": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2007.04068v1", "post_title": "Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence", "authors": ["Shakir Mohamed", "Marie-Therese Png", "William Isaac"], "date_published": "2020-07-08 12:36:21+00:00", "data_last_modified": "2020-07-08 12:36:21+00:00", "url": "http://arxiv.org/abs/2007.04068v1", "abstract": "This paper explores the important role of critical science, and in particular\nof post-colonial and decolonial theories, in understanding and shaping the\nongoing advances in artificial intelligence. Artificial Intelligence (AI) is\nviewed as amongst the technological advances that will reshape modern societies\nand their relations. Whilst the design and deployment of systems that\ncontinually adapt holds the promise of far-reaching positive change, they\nsimultaneously pose significant risks, especially to already vulnerable\npeoples. Values and power are central to this discussion. Decolonial theories\nuse historical hindsight to explain patterns of power that shape our\nintellectual, political, economic, and social world. By embedding a decolonial\ncritical approach within its technical practice, AI communities can develop\nforesight and tactics that can better align research and technology development\nwith established ethical principles, centring vulnerable peoples who continue\nto bear the brunt of negative impacts of innovation and scientific progress. We\nhighlight problematic applications that are instances of coloniality, and using\na decolonial lens, submit three tactics that can form a decolonial field of\nartificial intelligence: creating a critical technical practice of AI, seeking\nreverse tutelage and reverse pedagogies, and the renewal of affective and\npolitical communities. The years ahead will usher in a wave of new scientific\nbreakthroughs and technologies driven by AI research, making it incumbent upon\nAI communities to strengthen the social contract through ethical foresight and\nthe multiplicity of intellectual perspectives available to us; ultimately\nsupporting future technologies that enable greater well-being, with the goal of\nbeneficence and justice for all.", "author_comment": "28 Pages. Accepted, to appear in: Philosophy and Technology (405),\n  Springer. Submitted 16 January, Accepted 26 May 2020", "journal_ref": null, "doi": "10.1007/s13347-020-00405-8", "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1703.06856": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1703.06856v3", "post_title": "Counterfactual Fairness", "authors": ["Matt J. Kusner", "Joshua R. Loftus", "Chris Russell", "Ricardo Silva"], "date_published": "2017-03-20 17:18:57+00:00", "data_last_modified": "2018-03-08 11:23:13+00:00", "url": "http://arxiv.org/abs/1703.06856v3", "abstract": "Machine learning can impact people with legal or ethical consequences when it\nis used to automate decisions in areas such as insurance, lending, hiring, and\npredictive policing. In many of these scenarios, previous decisions have been\nmade that are unfairly biased against certain subpopulations, for example those\nof a particular race, gender, or sexual orientation. Since this past data may\nbe biased, machine learning predictors must account for this to avoid\nperpetuating or creating discriminatory practices. In this paper, we develop a\nframework for modeling fairness using tools from causal inference. Our\ndefinition of counterfactual fairness captures the intuition that a decision is\nfair towards an individual if it is the same in (a) the actual world and (b) a\ncounterfactual world where the individual belonged to a different demographic\ngroup. We demonstrate our framework on a real-world problem of fair prediction\nof success in law school.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.CY", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2101.08153": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2101.08153v2", "post_title": "Shielding Atari Games with Bounded Prescience", "authors": ["Mirco Giacobbe", "Mohammadhosein Hasanbeig", "Daniel Kroening", "Hjalmar Wijk"], "date_published": "2021-01-20 14:22:04+00:00", "data_last_modified": "2021-01-22 14:08:01+00:00", "url": "http://arxiv.org/abs/2101.08153v2", "abstract": "Deep reinforcement learning (DRL) is applied in safety-critical domains such\nas robotics and autonomous driving. It achieves superhuman abilities in many\ntasks, however whether DRL agents can be shown to act safely is an open\nproblem. Atari games are a simple yet challenging exemplar for evaluating the\nsafety of DRL agents and feature a diverse portfolio of game mechanics. The\nsafety of neural agents has been studied before using methods that either\nrequire a model of the system dynamics or an abstraction; unfortunately, these\nare unsuitable to Atari games because their low-level dynamics are complex and\nhidden inside their emulator. We present the first exact method for analysing\nand ensuring the safety of DRL agents for Atari games. Our method only requires\naccess to the emulator. First, we give a set of 43 properties that characterise\n\"safe behaviour\" for 30 games. Second, we develop a method for exploring all\ntraces induced by an agent and a game and consider a variety of sources of game\nnon-determinism. We observe that the best available DRL agents reliably satisfy\nonly very few properties; several critical properties are violated by all\nagents. Finally, we propose a countermeasure that combines a bounded\nexplicit-state exploration with shielding. We demonstrate that our method\nimproves the safety of all agents over multiple properties.", "author_comment": "To appear at AAMAS 2021", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1808.00508": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1808.00508v1", "post_title": "Neural Arithmetic Logic Units", "authors": ["Andrew Trask", "Felix Hill", "Scott Reed", "Jack Rae", "Chris Dyer", "Phil Blunsom"], "date_published": "2018-08-01 18:58:53+00:00", "data_last_modified": "2018-08-01 18:58:53+00:00", "url": "http://arxiv.org/abs/1808.00508v1", "abstract": "Neural networks can learn to represent and manipulate numerical information,\nbut they seldom generalize well outside of the range of numerical values\nencountered during training. To encourage more systematic numerical\nextrapolation, we propose an architecture that represents numerical quantities\nas linear activations which are manipulated using primitive arithmetic\noperators, controlled by learned gates. We call this module a neural arithmetic\nlogic unit (NALU), by analogy to the arithmetic logic unit in traditional\nprocessors. Experiments show that NALU-enhanced neural networks can learn to\ntrack time, perform arithmetic over images of numbers, translate numerical\nlanguage into real-valued scalars, execute computer code, and count objects in\nimages. In contrast to conventional architectures, we obtain substantially\nbetter generalization both inside and outside of the range of numerical values\nencountered during training, often extrapolating orders of magnitude beyond\ntrained numerical ranges.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.NE", "categories": ["cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.11680": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.11680v3", "post_title": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks", "authors": ["Mingchen Li", "Mahdi Soltanolkotabi", "Samet Oymak"], "date_published": "2019-03-27 20:00:15+00:00", "data_last_modified": "2019-07-03 23:48:05+00:00", "url": "http://arxiv.org/abs/1903.11680v3", "abstract": "Modern neural networks are typically trained in an over-parameterized regime\nwhere the parameters of the model far exceed the size of the training data.\nSuch neural networks in principle have the capacity to (over)fit any set of\nlabels including pure noise. Despite this, somewhat paradoxically, neural\nnetwork models trained via first-order methods continue to predict well on yet\nunseen test data. This paper takes a step towards demystifying this phenomena.\nUnder a rich dataset model, we show that gradient descent is provably robust to\nnoise/corruption on a constant fraction of the labels despite\noverparameterization. In particular, we prove that: (i) In the first few\niterations where the updates are still in the vicinity of the initialization\ngradient descent only fits to the correct labels essentially ignoring the noisy\nlabels. (ii) to start to overfit to the noisy labels network must stray rather\nfar from from the initialization which can only occur after many more\niterations. Together, these results show that gradient descent with early\nstopping is provably robust to label noise and shed light on the empirical\nrobustness of deep networks as well as commonly adopted heuristics to prevent\noverfitting.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.08575": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.08575v1", "post_title": "Supervising strong learners by amplifying weak experts", "authors": ["Paul Christiano", "Buck Shlegeris", "Dario Amodei"], "date_published": "2018-10-19 16:30:48+00:00", "data_last_modified": "2018-10-19 16:30:48+00:00", "url": "http://arxiv.org/abs/1810.08575v1", "abstract": "Many real world learning tasks involve complex or hard-to-specify objectives,\nand using an easier-to-specify proxy can lead to poor performance or misaligned\nbehavior. One solution is to have humans provide a training signal by\ndemonstrating or judging performance, but this approach fails if the task is\ntoo complicated for a human to directly evaluate. We propose Iterated\nAmplification, an alternative training strategy which progressively builds up a\ntraining signal for difficult problems by combining solutions to easier\nsubproblems. Iterated Amplification is closely related to Expert Iteration\n(Anthony et al., 2017; Silver et al., 2017), except that it uses no external\nreward function. We present results in algorithmic environments, showing that\nIterated Amplification can efficiently learn complex behaviors.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1611.01578": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1611.01578v2", "post_title": "Neural Architecture Search with Reinforcement Learning", "authors": ["Barret Zoph", "Quoc V. Le"], "date_published": "2016-11-05 00:41:37+00:00", "data_last_modified": "2017-02-15 05:28:05+00:00", "url": "http://arxiv.org/abs/1611.01578v2", "abstract": "Neural networks are powerful and flexible models that work well for many\ndifficult learning tasks in image, speech and natural language understanding.\nDespite their success, neural networks are still hard to design. In this paper,\nwe use a recurrent network to generate the model descriptions of neural\nnetworks and train this RNN with reinforcement learning to maximize the\nexpected accuracy of the generated architectures on a validation set. On the\nCIFAR-10 dataset, our method, starting from scratch, can design a novel network\narchitecture that rivals the best human-invented architecture in terms of test\nset accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is\n0.09 percent better and 1.05x faster than the previous state-of-the-art model\nthat used a similar architectural scheme. On the Penn Treebank dataset, our\nmodel can compose a novel recurrent cell that outperforms the widely-used LSTM\ncell, and other state-of-the-art baselines. Our cell achieves a test set\nperplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than\nthe previous state-of-the-art model. The cell can also be transferred to the\ncharacter language modeling task on PTB and achieves a state-of-the-art\nperplexity of 1.214.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.15191": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.15191v2", "post_title": "Is SGD a Bayesian sampler? Well, almost", "authors": ["Chris Mingard", "Guillermo Valle-P\u00e9rez", "Joar Skalse", "Ard A. Louis"], "date_published": "2020-06-26 19:45:36+00:00", "data_last_modified": "2020-10-24 13:28:11+00:00", "url": "http://arxiv.org/abs/2006.15191v2", "abstract": "Overparameterised deep neural networks (DNNs) are highly expressive and so\ncan, in principle, generate almost any function that fits a training dataset\nwith zero error. The vast majority of these functions will perform poorly on\nunseen data, and yet in practice DNNs often generalise remarkably well. This\nsuccess suggests that a trained DNN must have a strong inductive bias towards\nfunctions with low generalisation error. Here we empirically investigate this\ninductive bias by calculating, for a range of architectures and datasets, the\nprobability $P_{SGD}(f\\mid S)$ that an overparameterised DNN, trained with\nstochastic gradient descent (SGD) or one of its variants, converges on a\nfunction $f$ consistent with a training set $S$. We also use Gaussian processes\nto estimate the Bayesian posterior probability $P_B(f\\mid S)$ that the DNN\nexpresses $f$ upon random sampling of its parameters, conditioned on $S$.\n  Our main findings are that $P_{SGD}(f\\mid S)$ correlates remarkably well with\n$P_B(f\\mid S)$ and that $P_B(f\\mid S)$ is strongly biased towards low-error and\nlow complexity functions. These results imply that strong inductive bias in the\nparameter-function map (which determines $P_B(f\\mid S)$), rather than a special\nproperty of SGD, is the primary explanation for why DNNs generalise so well in\nthe overparameterised regime.\n  While our results suggest that the Bayesian posterior $P_B(f\\mid S)$ is the\nfirst order determinant of $P_{SGD}(f\\mid S)$, there remain second order\ndifferences that are sensitive to hyperparameter tuning. A function probability\npicture, based on $P_{SGD}(f\\mid S)$ and/or $P_B(f\\mid S)$, can shed new light\non the way that variations in architecture or hyperparameter settings such as\nbatch size, learning rate, and optimiser choice, affect DNN performance.", "author_comment": null, "journal_ref": "Journal of Machine Learning Research, 22 79 (2021), 1-64", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1908.09203": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1908.09203v2", "post_title": "Release Strategies and the Social Impacts of Language Models", "authors": ["Irene Solaiman", "Miles Brundage", "Jack Clark", "Amanda Askell", "Ariel Herbert-Voss", "Jeff Wu", "Alec Radford", "Gretchen Krueger", "Jong Wook Kim", "Sarah Kreps", "Miles McCain", "Alex Newhouse", "Jason Blazakis", "Kris McGuffie", "Jasmine Wang"], "date_published": "2019-08-24 20:41:40+00:00", "data_last_modified": "2019-11-13 03:54:12+00:00", "url": "http://arxiv.org/abs/1908.09203v2", "abstract": "Large language models have a range of beneficial uses: they can assist in\nprose, poetry, and programming; analyze dataset biases; and more. However,\ntheir flexibility and generative capabilities also raise misuse concerns. This\nreport discusses OpenAI's work related to the release of its GPT-2 language\nmodel. It discusses staged release, which allows time between model releases to\nconduct risk and benefit analyses as model sizes increased. It also discusses\nongoing partnership-based research and provides recommendations for better\ncoordination and responsible publication in AI.", "author_comment": "71 pages, report", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2; I.2.7; K.4"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1709.10163": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1709.10163v2", "post_title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces", "authors": ["Garrett Warnell", "Nicholas Waytowich", "Vernon Lawhern", "Peter Stone"], "date_published": "2017-09-28 20:43:40+00:00", "data_last_modified": "2018-01-19 20:36:13+00:00", "url": "http://arxiv.org/abs/1709.10163v2", "abstract": "While recent advances in deep reinforcement learning have allowed autonomous\nlearning agents to succeed at a variety of complex tasks, existing algorithms\ngenerally require a lot of training data. One way to increase the speed at\nwhich agents are able to learn to perform tasks is by leveraging the input of\nhuman trainers. Although such input can take many forms, real-time,\nscalar-valued feedback is especially useful in situations where it proves\ndifficult or impossible for humans to provide expert demonstrations. Previous\napproaches have shown the usefulness of human input provided in this fashion\n(e.g., the TAMER framework), but they have thus far not considered\nhigh-dimensional state spaces or employed the use of deep learning. In this\npaper, we do both: we propose Deep TAMER, an extension of the TAMER framework\nthat leverages the representational power of deep neural networks in order to\nlearn complex tasks in just a short amount of time with a human trainer. We\ndemonstrate Deep TAMER's success by using it and just 15 minutes of\nhuman-provided feedback to train an agent that performs better than humans on\nthe Atari game of Bowling - a task that has proven difficult for even\nstate-of-the-art reinforcement learning methods.", "author_comment": "9 pages, 6 figures", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.06458": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.06458v1", "post_title": "Deep Probabilistic Programming Languages: A Qualitative Study", "authors": ["Guillaume Baudart", "Martin Hirzel", "Louis Mandel"], "date_published": "2018-04-17 20:03:25+00:00", "data_last_modified": "2018-04-17 20:03:25+00:00", "url": "http://arxiv.org/abs/1804.06458v1", "abstract": "Deep probabilistic programming languages try to combine the advantages of\ndeep learning with those of probabilistic programming languages. If successful,\nthis would be a big step forward in machine learning and programming languages.\nUnfortunately, as of now, this new crop of languages is hard to use and\nunderstand. This paper addresses this problem directly by explaining deep\nprobabilistic programming languages and indirectly by characterizing their\ncurrent strengths and weaknesses.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.PL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2004.10802": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2004.10802v1", "post_title": "A Neural Scaling Law from the Dimension of the Data Manifold", "authors": ["Utkarsh Sharma", "Jared Kaplan"], "date_published": "2020-04-22 19:16:06+00:00", "data_last_modified": "2020-04-22 19:16:06+00:00", "url": "http://arxiv.org/abs/2004.10802v1", "abstract": "When data is plentiful, the loss achieved by well-trained neural networks\nscales as a power-law $L \\propto N^{-\\alpha}$ in the number of network\nparameters $N$. This empirical scaling law holds for a wide variety of data\nmodalities, and may persist over many orders of magnitude. The scaling law can\nbe explained if neural models are effectively just performing regression on a\ndata manifold of intrinsic dimension $d$. This simple theory predicts that the\nscaling exponents $\\alpha \\approx 4/d$ for cross-entropy and mean-squared error\nlosses. We confirm the theory by independently measuring the intrinsic\ndimension and the scaling exponents in a teacher/student framework, where we\ncan study a variety of $d$ and $\\alpha$ by dialing the properties of random\nteacher networks. We also test the theory with CNN image classifiers on several\ndatasets and with GPT-type language models.", "author_comment": "16+12 pages, 11+11 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.04926": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.04926v3", "post_title": "Active Reinforcement Learning with Monte-Carlo Tree Search", "authors": ["Sebastian Schulze", "Owain Evans"], "date_published": "2018-03-13 16:35:25+00:00", "data_last_modified": "2018-03-26 16:11:56+00:00", "url": "http://arxiv.org/abs/1803.04926v3", "abstract": "Active Reinforcement Learning (ARL) is a twist on RL where the agent observes\nreward information only if it pays a cost. This subtle change makes exploration\nsubstantially more challenging. Powerful principles in RL like optimism,\nThompson sampling, and random exploration do not help with ARL. We relate ARL\nin tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm\nusing Monte-Carlo Tree Search that is asymptotically Bayes optimal.\nExperimentally, this algorithm is near-optimal on small Bandit problems and\nMDPs. On larger MDPs it outperforms a Q-learner augmented with specialised\nheuristics for ARL. By analysing exploration behaviour in detail, we uncover\nobstacles to scaling up simulation-based algorithms for ARL.", "author_comment": "11 pages, 10 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2105.06791": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2105.06791v2", "post_title": "Agree to Disagree: When Deep Learning Models With Identical Architectures Produce Distinct Explanations", "authors": ["Matthew Watson", "Bashar Awwad Shiekh Hasan", "Noura Al Moubayed"], "date_published": "2021-05-14 12:16:47+00:00", "data_last_modified": "2021-10-31 00:08:57+00:00", "url": "http://arxiv.org/abs/2105.06791v2", "abstract": "Deep Learning of neural networks has progressively become more prominent in\nhealthcare with models reaching, or even surpassing, expert accuracy levels.\nHowever, these success stories are tainted by concerning reports on the lack of\nmodel transparency and bias against some medical conditions or patients'\nsub-groups. Explainable methods are considered the gateway to alleviate many of\nthese concerns. In this study we demonstrate that the generated explanations\nare volatile to changes in model training that are perpendicular to the\nclassification task and model structure. This raises further questions about\ntrust in deep learning models for healthcare. Mainly, whether the models\ncapture underlying causal links in the data or just rely on spurious\ncorrelations that are made visible via explanation methods. We demonstrate that\nthe output of explainability methods on deep neural networks can vary\nsignificantly by changes of hyper-parameters, such as the random seed or how\nthe training set is shuffled. We introduce a measure of explanation consistency\nwhich we use to highlight the identified problems on the MIMIC-CXR dataset. We\nfind explanations of identical models but with different training setups have a\nlow consistency: $\\approx$ 33% on average. On the contrary, kernel methods are\nrobust against any orthogonal changes, with explanation consistency at 94%. We\nconclude that current trends in model explanation are not sufficient to\nmitigate the risks of deploying models in real life healthcare applications.", "author_comment": "9 pages, 5 figures, 3 tables", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "I.2"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2103.14659": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2103.14659v1", "post_title": "Alignment of Language Agents", "authors": ["Zachary Kenton", "Tom Everitt", "Laura Weidinger", "Iason Gabriel", "Vladimir Mikulik", "Geoffrey Irving"], "date_published": "2021-03-26 18:01:48+00:00", "data_last_modified": "2021-03-26 18:01:48+00:00", "url": "http://arxiv.org/abs/2103.14659v1", "abstract": "For artificial intelligence to be beneficial to humans the behaviour of AI\nagents needs to be aligned with what humans want. In this paper we discuss some\nbehavioural issues for language agents, arising from accidental\nmisspecification by the system designer. We highlight some ways that\nmisspecification can occur and discuss some behavioural issues that could arise\nfrom misspecification, including deceptive or manipulative language, and review\nsome approaches for avoiding these issues.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.00482": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.00482v1", "post_title": "Few-Shot Goal Inference for Visuomotor Learning and Planning", "authors": ["Annie Xie", "Avi Singh", "Sergey Levine", "Chelsea Finn"], "date_published": "2018-09-30 22:57:58+00:00", "data_last_modified": "2018-09-30 22:57:58+00:00", "url": "http://arxiv.org/abs/1810.00482v1", "abstract": "Reinforcement learning and planning methods require an objective or reward\nfunction that encodes the desired behavior. Yet, in practice, there is a wide\nrange of scenarios where an objective is difficult to provide programmatically,\nsuch as tasks with visual observations involving unknown object positions or\ndeformable objects. In these cases, prior methods use engineered\nproblem-specific solutions, e.g., by instrumenting the environment with\nadditional sensors to measure a proxy for the objective. Such solutions require\na significant engineering effort on a per-task basis, and make it impractical\nfor robots to continuously learn complex skills outside of laboratory settings.\nWe aim to find a more general and scalable solution for specifying goals for\nrobot learning in unconstrained environments. To that end, we formulate the\nfew-shot objective learning problem, where the goal is to learn a task\nobjective from only a few example images of successful end states for that\ntask. We propose a simple solution to this problem: meta-learn a classifier\nthat can recognize new goals from a few examples. We show how this approach can\nbe used with both model-free reinforcement learning and visual model-based\nplanning and show results in three domains: rope manipulation from images in\nsimulation, visual navigation in a simulated 3D environment, and object\narrangement into user-specified configurations on a real robot.", "author_comment": "Videos available at https://sites.google.com/view/few-shot-goals", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.03877": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.03877v2", "post_title": "Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning", "authors": ["Smitha Milli", "Anca D. Dragan"], "date_published": "2019-03-09 21:58:46+00:00", "data_last_modified": "2019-06-29 03:26:48+00:00", "url": "http://arxiv.org/abs/1903.03877v2", "abstract": "It is incredibly easy for a system designer to misspecify the objective for\nan autonomous system (\"robot''), thus motivating the desire to have the robot\nlearn the objective from human behavior instead. Recent work has suggested that\npeople have an interest in the robot performing well, and will thus behave\npedagogically, choosing actions that are informative to the robot. In turn,\nrobots benefit from interpreting the behavior by accounting for this pedagogy.\nIn this work, we focus on misspecification: we argue that robots might not know\nwhether people are being pedagogic or literal and that it is important to ask\nwhich assumption is safer to make. We cast objective learning into the more\ngeneral form of a common-payoff game between the robot and human, and prove\nthat in any such game literal interpretation is more robust to\nmisspecification. Experiments with human data support our theoretical results\nand point to the sensitivity of the pedagogic assumption.", "author_comment": "Published at UAI 2019", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.03516": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.03516v2", "post_title": "Learning from Demonstration in the Wild", "authors": ["Feryal Behbahani", "Kyriacos Shiarlis", "Xi Chen", "Vitaly Kurin", "Sudhanshu Kasewa", "Ciprian Stirbu", "Jo\u00e3o Gomes", "Supratik Paul", "Frans A. Oliehoek", "Jo\u00e3o Messias", "Shimon Whiteson"], "date_published": "2018-11-08 16:03:23+00:00", "data_last_modified": "2019-03-26 00:11:48+00:00", "url": "http://arxiv.org/abs/1811.03516v2", "abstract": "Learning from demonstration (LfD) is useful in settings where hand-coding\nbehaviour or a reward function is impractical. It has succeeded in a wide range\nof problems but typically relies on manually generated demonstrations or\nspecially deployed sensors and has not generally been able to leverage the\ncopious demonstrations available in the wild: those that capture behaviours\nthat were occurring anyway using sensors that were already deployed for another\npurpose, e.g., traffic camera footage capturing demonstrations of natural\nbehaviour of vehicles, cyclists, and pedestrians. We propose Video to Behaviour\n(ViBe), a new approach to learn models of behaviour from unlabelled raw video\ndata of a traffic scene collected from a single, monocular, initially\nuncalibrated camera with ordinary resolution. Our approach calibrates the\ncamera, detects relevant objects, tracks them through time, and uses the\nresulting trajectories to perform LfD, yielding models of naturalistic\nbehaviour. We apply ViBe to raw videos of a traffic intersection and show that\nit can learn purely from videos, without additional expert knowledge.", "author_comment": "Accepted to the IEEE International Conference on Robotics and\n  Automation (ICRA) 2019; extended version with appendix", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.03642": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.03642v4", "post_title": "Fast Context Adaptation via Meta-Learning", "authors": ["Luisa M Zintgraf", "Kyriacos Shiarlis", "Vitaly Kurin", "Katja Hofmann", "Shimon Whiteson"], "date_published": "2018-10-08 18:11:01+00:00", "data_last_modified": "2019-06-10 17:17:53+00:00", "url": "http://arxiv.org/abs/1810.03642v4", "abstract": "We propose CAVIA for meta-learning, a simple extension to MAML that is less\nprone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA\npartitions the model parameters into two parts: context parameters that serve\nas additional input to the model and are adapted on individual tasks, and\nshared parameters that are meta-trained and shared across tasks. At test time,\nonly the context parameters are updated, leading to a low-dimensional task\nrepresentation. We show empirically that CAVIA outperforms MAML for regression,\nclassification, and reinforcement learning. Our experiments also highlight\nweaknesses in current benchmarks, in that the amount of adaptation needed in\nsome cases is small.", "author_comment": "Published at the International Conference on Machine Learning (ICML)\n  2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2101.06060": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2101.06060v2", "post_title": "The Challenge of Value Alignment: from Fairer Algorithms to AI Safety", "authors": ["Iason Gabriel", "Vafa Ghazavi"], "date_published": "2021-01-15 11:03:15+00:00", "data_last_modified": "2021-01-18 11:36:10+00:00", "url": "http://arxiv.org/abs/2101.06060v2", "abstract": "This paper addresses the question of how to align AI systems with human\nvalues and situates it within a wider body of thought regarding technology and\nvalue. Far from existing in a vacuum, there has long been an interest in the\nability of technology to 'lock-in' different value systems. There has also been\nconsiderable thought about how to align technologies with specific social\nvalues, including through participatory design-processes. In this paper we look\nmore closely at the question of AI value alignment and suggest that the power\nand autonomy of AI systems gives rise to opportunities and challenges in the\ndomain of value that have not been encountered before. Drawing important\ncontinuities between the work of the fairness, accountability, transparency and\nethics community, and work being done by technical AI safety researchers, we\nsuggest that more attention needs to be paid to the question of 'social value\nalignment' - that is, how to align AI systems with the plurality of values\nendorsed by groups of people, especially on the global level.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.03246": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.03246v2", "post_title": "The Logic of Strategic Assets: From Oil to Artificial Intelligence", "authors": ["Jeffrey Ding", "Allan Dafoe"], "date_published": "2020-01-09 22:16:05+00:00", "data_last_modified": "2021-05-31 15:16:10+00:00", "url": "http://arxiv.org/abs/2001.03246v2", "abstract": "What resources and technologies are strategic? This question is often the\nfocus of policy and theoretical debates, where the label \"strategic\" designates\nthose assets that warrant the attention of the highest levels of the state. But\nthese conversations are plagued by analytical confusion, flawed heuristics, and\nthe rhetorical use of \"strategic\" to advance particular agendas. We aim to\nimprove these conversations through conceptual clarification, introducing a\ntheory based on important rivalrous externalities for which socially optimal\nbehavior will not be produced alone by markets or individual national security\nentities. We distill and theorize the most important three forms of these\nexternalities, which involve cumulative-, infrastructure-, and\ndependency-strategic logics. We then employ these logics to clarify three\nimportant cases: the Avon 2 engine in the 1950s, the U.S.-Japan technology\nrivalry in the late 1980s, and contemporary conversations about artificial\nintelligence.", "author_comment": "Added references and corrected typos", "journal_ref": null, "doi": "10.1080/09636412.2021.1915583", "primary_category": "econ.GN", "categories": ["econ.GN", "cs.CY", "q-fin.EC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.05709": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.05709v3", "post_title": "A Simple Framework for Contrastive Learning of Visual Representations", "authors": ["Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton"], "date_published": "2020-02-13 18:50:45+00:00", "data_last_modified": "2020-07-01 00:09:08+00:00", "url": "http://arxiv.org/abs/2002.05709v3", "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of\nvisual representations. We simplify recently proposed contrastive\nself-supervised learning algorithms without requiring specialized architectures\nor a memory bank. In order to understand what enables the contrastive\nprediction tasks to learn useful representations, we systematically study the\nmajor components of our framework. We show that (1) composition of data\naugmentations plays a critical role in defining effective predictive tasks, (2)\nintroducing a learnable nonlinear transformation between the representation and\nthe contrastive loss substantially improves the quality of the learned\nrepresentations, and (3) contrastive learning benefits from larger batch sizes\nand more training steps compared to supervised learning. By combining these\nfindings, we are able to considerably outperform previous methods for\nself-supervised and semi-supervised learning on ImageNet. A linear classifier\ntrained on self-supervised representations learned by SimCLR achieves 76.5%\ntop-1 accuracy, which is a 7% relative improvement over previous\nstate-of-the-art, matching the performance of a supervised ResNet-50. When\nfine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy,\noutperforming AlexNet with 100X fewer labels.", "author_comment": "ICML'2020. Code and pretrained models at\n  https://github.com/google-research/simclr", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.11929": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.11929v2", "post_title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "authors": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "date_published": "2020-10-22 17:55:59+00:00", "data_last_modified": "2021-06-03 13:08:56+00:00", "url": "http://arxiv.org/abs/2010.11929v2", "abstract": "While the Transformer architecture has become the de-facto standard for\nnatural language processing tasks, its applications to computer vision remain\nlimited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional\nnetworks while keeping their overall structure in place. We show that this\nreliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\nTransformer (ViT) attains excellent results compared to state-of-the-art\nconvolutional networks while requiring substantially fewer computational\nresources to train.", "author_comment": "Fine-tuning code and pre-trained models are available at\n  https://github.com/google-research/vision_transformer. ICLR camera-ready\n  version with 2 small modifications: 1) Added a discussion of CLS vs GAP\n  classifier in the appendix, 2) Fixed an error in exaFLOPs computation in\n  Figure 5 and Table 6 (relative performance of models is basically not\n  affected)", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2110.06674": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2110.06674v1", "post_title": "Truthful AI: Developing and governing AI that does not lie", "authors": ["Owain Evans", "Owen Cotton-Barratt", "Lukas Finnveden", "Adam Bales", "Avital Balwit", "Peter Wills", "Luca Righetti", "William Saunders"], "date_published": "2021-10-13 12:18:09+00:00", "data_last_modified": "2021-10-13 12:18:09+00:00", "url": "http://arxiv.org/abs/2110.06674v1", "abstract": "In many contexts, lying -- the use of verbal falsehoods to deceive -- is\nharmful. While lying has traditionally been a human affair, AI systems that\nmake sophisticated verbal statements are becoming increasingly prevalent. This\nraises the question of how we should limit the harm caused by AI \"lies\" (i.e.\nfalsehoods that are actively selected for). Human truthfulness is governed by\nsocial norms and by laws (against defamation, perjury, and fraud). Differences\nbetween AI and humans present an opportunity to have more precise standards of\ntruthfulness for AI, and to have these standards rise over time. This could\nprovide significant benefits to public epistemics and the economy, and mitigate\nrisks of worst-case AI futures.\n  Establishing norms or laws of AI truthfulness will require significant work\nto: (1) identify clear truthfulness standards; (2) create institutions that can\njudge adherence to those standards; and (3) develop AI systems that are\nrobustly truthful.\n  Our initial proposals for these areas include: (1) a standard of avoiding\n\"negligent falsehoods\" (a generalisation of lies that is easier to assess); (2)\ninstitutions to evaluate AI systems before and after real-world deployment; and\n(3) explicitly training AI systems to be truthful via curated datasets and\nhuman interaction.\n  A concerning possibility is that evaluation mechanisms for eventual\ntruthfulness standards could be captured by political interests, leading to\nharmful censorship and propaganda. Avoiding this might take careful attention.\nAnd since the scale of AI speech acts might grow dramatically over the coming\ndecades, early truthfulness standards might be particularly important because\nof the precedents they set.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI", "cs.CL", "I.2.0"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.07807": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.07807v2", "post_title": "Deeper Interpretability of Deep Networks", "authors": ["Tian Xu", "Jiayu Zhan", "Oliver G. B. Garrod", "Philip H. S. Torr", "Song-Chun Zhu", "Robin A. A. Ince", "Philippe G. Schyns"], "date_published": "2018-11-19 17:10:44+00:00", "data_last_modified": "2018-11-20 09:43:21+00:00", "url": "http://arxiv.org/abs/1811.07807v2", "abstract": "Deep Convolutional Neural Networks (CNNs) have been one of the most\ninfluential recent developments in computer vision, particularly for\ncategorization. There is an increasing demand for explainable AI as these\nsystems are deployed in the real world. However, understanding the information\nrepresented and processed in CNNs remains in most cases challenging. Within\nthis paper, we explore the use of new information theoretic techniques\ndeveloped in the field of neuroscience to enable novel understanding of how a\nCNN represents information. We trained a 10-layer ResNet architecture to\nidentify 2,000 face identities from 26M images generated using a rigorously\ncontrolled 3D face rendering model that produced variations of intrinsic (i.e.\nface morphology, gender, age, expression and ethnicity) and extrinsic factors\n(i.e. 3D pose, illumination, scale and 2D translation). With our methodology,\nwe demonstrate that unlike human's network overgeneralizes face identities even\nwith extreme changes of face shape, but it is more sensitive to changes of\ntexture. To understand the processing of information underlying these\ncounterintuitive properties, we visualize the features of shape and texture\nthat the network processes to identify faces. Then, we shed a light into the\ninner workings of the black box and reveal how hidden layers represent these\nfeatures and whether the representations are invariant to pose. We hope that\nour methodology will provide an additional valuable tool for interpretability\nof CNNs.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.08915": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.08915v1", "post_title": "A Psychopathological Approach to Safety Engineering in AI and AGI", "authors": ["Vahid Behzadan", "Arslan Munir", "Roman V. Yampolskiy"], "date_published": "2018-05-23 00:19:07+00:00", "data_last_modified": "2018-05-23 00:19:07+00:00", "url": "http://arxiv.org/abs/1805.08915v1", "abstract": "The complexity of dynamics in AI techniques is already approaching that of\ncomplex adaptive systems, thus curtailing the feasibility of formal\ncontrollability and reachability analysis in the context of AI safety. It\nfollows that the envisioned instances of Artificial General Intelligence (AGI)\nwill also suffer from challenges of complexity. To tackle such issues, we\npropose the modeling of deleterious behaviors in AI and AGI as psychological\ndisorders, thereby enabling the employment of psychopathological approaches to\nanalysis and control of misbehaviors. Accordingly, we present a discussion on\nthe feasibility of the psychopathological approaches to AI safety, and propose\ngeneral directions for research on modeling, diagnosis, and treatment of\npsychological disorders in AGI.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.08630": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.08630v1", "post_title": "Open Problems in Cooperative AI", "authors": ["Allan Dafoe", "Edward Hughes", "Yoram Bachrach", "Tantum Collins", "Kevin R. McKee", "Joel Z. Leibo", "Kate Larson", "Thore Graepel"], "date_published": "2020-12-15 21:39:50+00:00", "data_last_modified": "2020-12-15 21:39:50+00:00", "url": "http://arxiv.org/abs/2012.08630v1", "abstract": "Problems of cooperation--in which agents seek ways to jointly improve their\nwelfare--are ubiquitous and important. They can be found at scales ranging from\nour daily routines--such as driving on highways, scheduling meetings, and\nworking collaboratively--to our global challenges--such as peace, commerce, and\npandemic preparedness. Arguably, the success of the human species is rooted in\nour ability to cooperate. Since machines powered by artificial intelligence are\nplaying an ever greater role in our lives, it will be important to equip them\nwith the capabilities necessary to cooperate and to foster cooperation.\n  We see an opportunity for the field of artificial intelligence to explicitly\nfocus effort on this class of problems, which we term Cooperative AI. The\nobjective of this research would be to study the many aspects of the problems\nof cooperation and to innovate in AI to contribute to solving these problems.\nCentral goals include building machine agents with the capabilities needed for\ncooperation, building tools to foster cooperation in populations of (machine\nand/or human) agents, and otherwise conducting AI research for insight relevant\nto problems of cooperation. This research integrates ongoing work on\nmulti-agent systems, game theory and social choice, human-machine interaction\nand alignment, natural-language processing, and the construction of social\ntools and platforms. However, Cooperative AI is not the union of these existing\nareas, but rather an independent bet about the productivity of specific kinds\nof conversations that involve these and other areas. We see opportunity to more\nexplicitly focus on the problem of cooperation, to construct unified theory and\nvocabulary, and to build bridges with adjacent communities working on\ncooperation, including in the natural, social, and behavioural sciences.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.08092": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.08092v2", "post_title": "Distributional Generalization: A New Kind of Generalization", "authors": ["Preetum Nakkiran", "Yamini Bansal"], "date_published": "2020-09-17 06:26:17+00:00", "data_last_modified": "2020-10-15 02:41:52+00:00", "url": "http://arxiv.org/abs/2009.08092v2", "abstract": "We introduce a new notion of generalization -- Distributional Generalization\n-- which roughly states that outputs of a classifier at train and test time are\nclose *as distributions*, as opposed to close in just their average error. For\nexample, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then\na ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as\ncats on the *test set* as well, while leaving other classes unaffected. This\nbehavior is not captured by classical generalization, which would only consider\nthe average error and not the distribution of errors over the input domain. Our\nformal conjectures, which are much more general than this example, characterize\nthe form of distributional generalization that can be expected in terms of\nproblem parameters: model architecture, training procedure, number of samples,\nand data distribution. We give empirical evidence for these conjectures across\na variety of domains in machine learning, including neural networks, kernel\nmachines, and decision trees. Our results thus advance our empirical\nunderstanding of interpolating classifiers.", "author_comment": "Co-first authors. V2: Intro shortened; no new results", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.NE", "math.ST", "stat.ML", "stat.TH"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.08686": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.08686v1", "post_title": "Fully General Online Imitation Learning", "authors": ["Michael K. Cohen", "Marcus Hutter", "Neel Nanda"], "date_published": "2021-02-17 10:57:37+00:00", "data_last_modified": "2021-02-17 10:57:37+00:00", "url": "http://arxiv.org/abs/2102.08686v1", "abstract": "In imitation learning, imitators and demonstrators are policies for picking\nactions given past interactions with the environment. If we run an imitator, we\nprobably want events to unfold similarly to the way they would have if the\ndemonstrator had been acting the whole time. No existing work provides formal\nguidance in how this might be accomplished, instead restricting focus to\nenvironments that restart, making learning unusually easy, and conveniently\nlimiting the significance of any mistake. We address a fully general setting,\nin which the (stochastic) environment and demonstrator never reset, not even\nfor training purposes. Our new conservative Bayesian imitation learner\nunderestimates the probabilities of each available action, and queries for more\ndata with the remaining probability. Our main result: if an event would have\nbeen unlikely had the demonstrator acted the whole time, that event's\nlikelihood can be bounded above when running the (initially totally ignorant)\nimitator instead. Meanwhile, queries to the demonstrator rapidly diminish in\nfrequency.", "author_comment": "13 pages with 8-page appendix", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "I.2.0; I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2108.07732": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2108.07732v1", "post_title": "Program Synthesis with Large Language Models", "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "Henryk Michalewski", "David Dohan", "Ellen Jiang", "Carrie Cai", "Michael Terry", "Quoc Le", "Charles Sutton"], "date_published": "2021-08-16 03:57:30+00:00", "data_last_modified": "2021-08-16 03:57:30+00:00", "url": "http://arxiv.org/abs/2108.07732v1", "abstract": "This paper explores the limits of the current generation of large language\nmodels for program synthesis in general purpose programming languages. We\nevaluate a collection of such models (with between 244M and 137B parameters) on\ntwo new benchmarks, MBPP and MathQA-Python, in both the few-shot and\nfine-tuning regimes. Our benchmarks are designed to measure the ability of\nthese models to synthesize short Python programs from natural language\ndescriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974\nprogramming tasks, designed to be solvable by entry-level programmers. The\nMathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914\nproblems that evaluate the ability of the models to synthesize code from more\ncomplex text. On both datasets, we find that synthesis performance scales\nlog-linearly with model size. Our largest models, even without finetuning on a\ncode dataset, can synthesize solutions to 59.6 percent of the problems from\nMBPP using few-shot learning with a well-designed prompt. Fine-tuning on a\nheld-out portion of the dataset improves performance by about 10 percentage\npoints across most model sizes. On the MathQA-Python dataset, the largest\nfine-tuned model achieves 83.8 percent accuracy. Going further, we study the\nmodel's ability to engage in dialog about code, incorporating human feedback to\nimprove its solutions. We find that natural language feedback from a human\nhalves the error rate compared to the model's initial prediction. Additionally,\nwe conduct an error analysis to shed light on where these models fall short and\nwhat types of programs are most difficult to generate. Finally, we explore the\nsemantic grounding of these models by fine-tuning them to predict the results\nof program execution. We find that even our best models are generally unable to\npredict the output of a program given a specific input.", "author_comment": "Jacob and Augustus contributed equally", "journal_ref": null, "doi": null, "primary_category": "cs.PL", "categories": ["cs.PL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1910.05789": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1910.05789v2", "post_title": "On the Utility of Learning about Humans for Human-AI Coordination", "authors": ["Micah Carroll", "Rohin Shah", "Mark K. Ho", "Thomas L. Griffiths", "Sanjit A. Seshia", "Pieter Abbeel", "Anca Dragan"], "date_published": "2019-10-13 17:17:52+00:00", "data_last_modified": "2020-01-09 00:51:44+00:00", "url": "http://arxiv.org/abs/1910.05789v2", "abstract": "While we would like agents that can coordinate with humans, current\nalgorithms such as self-play and population-based training create agents that\ncan coordinate with themselves. Agents that assume their partner to be optimal\nor similar to them can converge to coordination protocols that fail to\nunderstand and be understood by humans. To demonstrate this, we introduce a\nsimple environment that requires challenging coordination, based on the popular\ngame Overcooked, and learn a simple model that mimics human play. We evaluate\nthe performance of agents trained via self-play and population-based training.\nThese agents perform very well when paired with themselves, but when paired\nwith our human model, they are significantly worse than agents designed to play\nwith the human model. An experiment with a planning algorithm yields the same\nconclusion, though only when the human-aware planner is given the exact human\nmodel that it is playing with. A user study with real humans shows this pattern\nas well, though less strongly. Qualitatively, we find that the gains come from\nhaving the agent adapt to the human's gameplay. Given this result, we suggest\nseveral approaches for designing agents that learn about humans in order to\nbetter coordinate with them. Code is available at\nhttps://github.com/HumanCompatibleAI/overcooked_ai.", "author_comment": "Published at NeurIPS 2019\n  (http://papers.nips.cc/paper/8760-on-the-utility-of-learning-about-humans-for-human-ai-coordination)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.HC", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1412.6980": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1412.6980v9", "post_title": "Adam: A Method for Stochastic Optimization", "authors": ["Diederik P. Kingma", "Jimmy Ba"], "date_published": "2014-12-22 13:54:29+00:00", "data_last_modified": "2017-01-30 01:27:54+00:00", "url": "http://arxiv.org/abs/1412.6980v9", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.", "author_comment": "Published as a conference paper at the 3rd International Conference\n  for Learning Representations, San Diego, 2015", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2202.05262": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2202.05262v2", "post_title": "Locating and Editing Factual Knowledge in GPT", "authors": ["Kevin Meng", "David Bau", "Alex Andonian", "Yonatan Belinkov"], "date_published": "2022-02-10 18:59:54+00:00", "data_last_modified": "2022-03-21 15:13:09+00:00", "url": "http://arxiv.org/abs/2202.05262v2", "abstract": "We investigate the mechanisms underlying factual knowledge recall in\nautoregressive transformer language models. First, we develop a causal\nintervention for identifying neuron activations capable of altering a model's\nfactual predictions. Within large GPT-style models, this reveals two distinct\nsets of neurons that we hypothesize correspond to knowing an abstract fact and\nsaying a concrete word, respectively. This insight inspires the development of\nROME, a novel method for editing facts stored in model weights. For evaluation,\nwe assemble CounterFact, a dataset of over twenty thousand counterfactuals and\ntools to facilitate sensitive measurements of knowledge editing. Using\nCounterFact, we confirm the distinction between saying and knowing neurons, and\nwe find that ROME achieves state-of-the-art performance in knowledge editing\ncompared to other methods. An interactive demo notebook, full code\nimplementation, and the dataset are available at https://rome.baulab.info/.", "author_comment": "21 pages, 22 figures. Code and data at https://rome.baulab.info/", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG", "I.2.7"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2103.12656": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2103.12656v2", "post_title": "Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification", "authors": ["Benjamin Eysenbach", "Sergey Levine", "Ruslan Salakhutdinov"], "date_published": "2021-03-23 16:19:55+00:00", "data_last_modified": "2021-12-30 20:26:31+00:00", "url": "http://arxiv.org/abs/2103.12656v2", "abstract": "Reinforcement learning (RL) algorithms assume that users specify tasks by\nmanually writing down a reward function. However, this process can be laborious\nand demands considerable technical expertise. Can we devise RL algorithms that\ninstead enable users to specify tasks simply by providing examples of\nsuccessful outcomes? In this paper, we derive a control algorithm that\nmaximizes the future probability of these successful outcome examples. Prior\nwork has approached similar problems with a two-stage process, first learning a\nreward function and then optimizing this reward function using another RL\nalgorithm. In contrast, our method directly learns a value function from\ntransitions and successful outcomes, without learning this intermediate reward\nfunction. Our method therefore requires fewer hyperparameters to tune and lines\nof code to debug. We show that our method satisfies a new data-driven Bellman\nequation, where examples take the place of the typical reward function term.\nExperiments show that our approach outperforms prior methods that learn\nexplicit reward functions.", "author_comment": "NeurIPS 2021 (oral). Website with code, videos, and blog post:\n  https://ben-eysenbach.github.io/rce", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1909.13392": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1909.13392v1", "post_title": "Learning from Observations Using a Single Video Demonstration and Human Feedback", "authors": ["Sunil Gandhi", "Tim Oates", "Tinoosh Mohsenin", "Nicholas Waytowich"], "date_published": "2019-09-29 22:44:59+00:00", "data_last_modified": "2019-09-29 22:44:59+00:00", "url": "http://arxiv.org/abs/1909.13392v1", "abstract": "In this paper, we present a method for learning from video demonstrations by\nusing human feedback to construct a mapping between the standard representation\nof the agent and the visual representation of the demonstration. In this way,\nwe leverage the advantages of both these representations, i.e., we learn the\npolicy using standard state representations, but are able to specify the\nexpected behavior using video demonstration. We train an autonomous agent using\na single video demonstration and use human feedback (using numerical similarity\nrating) to map the standard representation to the visual representation with a\nneural network. We show the effectiveness of our method by teaching a hopper\nagent in the MuJoCo to perform a backflip using a single video demonstration\ngenerated in MuJoCo as well as from a real-world YouTube video of a person\nperforming a backflip. Additionally, we show that our method can transfer to\nnew tasks, such as hopping, with very little human feedback.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.04723": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.04723v1", "post_title": "The Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach", "authors": ["Iulian Vlad Serban", "Chinnadhurai Sankar", "Michael Pieper", "Joelle Pineau", "Yoshua Bengio"], "date_published": "2018-07-12 16:59:28+00:00", "data_last_modified": "2018-07-12 16:59:28+00:00", "url": "http://arxiv.org/abs/1807.04723v1", "abstract": "Deep reinforcement learning has recently shown many impressive successes.\nHowever, one major obstacle towards applying such methods to real-world\nproblems is their lack of data-efficiency. To this end, we propose the\nBottleneck Simulator: a model-based reinforcement learning method which\ncombines a learned, factorized transition model of the environment with rollout\nsimulations to learn an effective policy from few examples. The learned\ntransition model employs an abstract, discrete (bottleneck) state, which\nincreases sample efficiency by reducing the number of model parameters and by\nexploiting structural properties of the environment. We provide a mathematical\nanalysis of the Bottleneck Simulator in terms of fixed points of the learned\npolicy, which reveals how performance is affected by four distinct sources of\nerror: an error related to the abstract space structure, an error related to\nthe transition model estimation variance, an error related to the transition\nmodel estimation bias, and an error related to the transition model class bias.\nFinally, we evaluate the Bottleneck Simulator on two natural language\nprocessing tasks: a text adventure game and a real-world, complex dialogue\nresponse selection task. On both tasks, the Bottleneck Simulator yields\nexcellent performance beating competing approaches.", "author_comment": "26 pages, 2 figures, 4 tables", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "stat.ML", "I.5.1; I.2.7"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.04585": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.04585v4", "post_title": "Categorizing Variants of Goodhart's Law", "authors": ["David Manheim", "Scott Garrabrant"], "date_published": "2018-03-13 01:15:39+00:00", "data_last_modified": "2019-02-24 08:12:46+00:00", "url": "http://arxiv.org/abs/1803.04585v4", "abstract": "There are several distinct failure modes for overoptimization of systems on\nthe basis of metrics. This occurs when a metric which can be used to improve a\nsystem is used to an extent that further optimization is ineffective or\nharmful, and is sometimes termed Goodhart's Law. This class of failure is often\npoorly understood, partly because terminology for discussing them is ambiguous,\nand partly because discussion using this ambiguous terminology ignores\ndistinctions between different failure modes of this general type. This paper\nexpands on an earlier discussion by Garrabrant, which notes there are \"(at\nleast) four different mechanisms\" that relate to Goodhart's Law. This paper is\nintended to explore these mechanisms further, and specify more clearly how they\noccur. This discussion should be helpful in better understanding these types of\nfailures in economic regulation, in public policy, in machine learning, and in\nArtificial Intelligence alignment. The importance of Goodhart effects depends\non the amount of power directed towards optimizing the proxy, and so the\nincreased optimization power offered by artificial intelligence makes it\nespecially critical for that field.", "author_comment": "10 pages", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "q-fin.GN", "stat.ML", "91E45"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.12573": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.12573v5", "post_title": "Learning a Prior over Intent via Meta-Inverse Reinforcement Learning", "authors": ["Kelvin Xu", "Ellis Ratner", "Anca Dragan", "Sergey Levine", "Chelsea Finn"], "date_published": "2018-05-31 17:29:25+00:00", "data_last_modified": "2019-10-14 19:55:15+00:00", "url": "http://arxiv.org/abs/1805.12573v5", "abstract": "A significant challenge for the practical application of reinforcement\nlearning in the real world is the need to specify an oracle reward function\nthat correctly defines a task. Inverse reinforcement learning (IRL) seeks to\navoid this challenge by instead inferring a reward function from expert\nbehavior. While appealing, it can be impractically expensive to collect\ndatasets of demonstrations that cover the variation common in the real world\n(e.g. opening any type of door). Thus in practice, IRL must commonly be\nperformed with only a limited set of demonstrations where it can be exceedingly\ndifficult to unambiguously recover a reward function. In this work, we exploit\nthe insight that demonstrations from other tasks can be used to constrain the\nset of possible reward functions by learning a \"prior\" that is specifically\noptimized for the ability to infer expressive reward functions from limited\nnumbers of demonstrations. We demonstrate that our method can efficiently\nrecover rewards from images for novel tasks and provide intuition as to how our\napproach is analogous to learning a prior.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.08340": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.08340v1", "post_title": "Interpretable Discovery in Large Image Data Sets", "authors": ["Kiri L. Wagstaff", "Jake Lee"], "date_published": "2018-06-21 17:30:26+00:00", "data_last_modified": "2018-06-21 17:30:26+00:00", "url": "http://arxiv.org/abs/1806.08340v1", "abstract": "Automated detection of new, interesting, unusual, or anomalous images within\nlarge data sets has great value for applications from surveillance (e.g.,\nairport security) to science (observations that don't fit a given theory can\nlead to new discoveries). Many image data analysis systems are turning to\nconvolutional neural networks (CNNs) to represent image content due to their\nsuccess in achieving high classification accuracy rates. However, CNN\nrepresentations are notoriously difficult for humans to interpret. We describe\na new strategy that combines novelty detection with CNN image features to\nachieve rapid discovery with interpretable explanations of novel image content.\nWe applied this technique to familiar images from ImageNet as well as to a\nscientific image collection from planetary science.", "author_comment": "Presented at the 2018 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2018), Stockholm, Sweden", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2008.02275": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2008.02275v5", "post_title": "Aligning AI With Shared Human Values", "authors": ["Dan Hendrycks", "Collin Burns", "Steven Basart", "Andrew Critch", "Jerry Li", "Dawn Song", "Jacob Steinhardt"], "date_published": "2020-08-05 17:59:16+00:00", "data_last_modified": "2021-07-24 04:40:33+00:00", "url": "http://arxiv.org/abs/2008.02275v5", "abstract": "We show how to assess a language model's knowledge of basic concepts of\nmorality. We introduce the ETHICS dataset, a new benchmark that spans concepts\nin justice, well-being, duties, virtues, and commonsense morality. Models\npredict widespread moral judgments about diverse text scenarios. This requires\nconnecting physical and social world knowledge to value judgements, a\ncapability that may enable us to steer chatbot outputs or eventually regularize\nopen-ended reinforcement learning agents. With the ETHICS dataset, we find that\ncurrent language models have a promising but incomplete ability to predict\nbasic human ethical judgements. Our work shows that progress can be made on\nmachine ethics today, and it provides a steppingstone toward AI that is aligned\nwith human values.", "author_comment": "ICLR 2021; the ETHICS dataset is available at\n  https://github.com/hendrycks/ethics/", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.09071": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.09071v1", "post_title": "Measurement in AI Policy: Opportunities and Challenges", "authors": ["Saurabh Mishra", "Jack Clark", "C. Raymond Perrault"], "date_published": "2020-09-10 05:37:40+00:00", "data_last_modified": "2020-09-10 05:37:40+00:00", "url": "http://arxiv.org/abs/2009.09071v1", "abstract": "As artificial intelligence increasingly influences our world, it becomes\ncrucial to assess its technical progress and societal impact. This paper\nsurveys problems and opportunities in the measurement of AI systems and their\nimpact, based on a workshop held at Stanford University in the fall of 2019. We\nidentify six summary challenges inherent to measuring the progress and impact\nof AI, and summarize over 40 presentations and associated discussions from the\nworkshop. We hope this can inspire research agendas in this crucial area.", "author_comment": "Workshop Paper", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.00463": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.00463v2", "post_title": "The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?", "authors": ["Toby Shevlane", "Allan Dafoe"], "date_published": "2019-12-27 10:20:44+00:00", "data_last_modified": "2020-01-09 23:24:21+00:00", "url": "http://arxiv.org/abs/2001.00463v2", "abstract": "There is growing concern over the potential misuse of artificial intelligence\n(AI) research. Publishing scientific research can facilitate misuse of the\ntechnology, but the research can also contribute to protections against misuse.\nThis paper addresses the balance between these two effects. Our theoretical\nframework elucidates the factors governing whether the published research will\nbe more useful for attackers or defenders, such as the possibility for adequate\ndefensive measures, or the independent discovery of the knowledge outside of\nthe scientific community. The balance will vary across scientific fields.\nHowever, we show that the existing conversation within AI has imported concepts\nand conclusions from prior debates within computer security over the disclosure\nof software vulnerabilities. While disclosure of software vulnerabilities often\nfavours defence, this cannot be assumed for AI research. The AI research\ncommunity should consider concepts and policies from a broad set of adjacent\nfields, and ultimately needs to craft policy well-suited to its particular\nchallenges.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.04734": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.04734v3", "post_title": "Reinforcement Learning Under Moral Uncertainty", "authors": ["Adrien Ecoffet", "Joel Lehman"], "date_published": "2020-06-08 16:40:12+00:00", "data_last_modified": "2021-07-19 18:52:16+00:00", "url": "http://arxiv.org/abs/2006.04734v3", "abstract": "An ambitious goal for machine learning is to create agents that behave\nethically: The capacity to abide by human moral norms would greatly expand the\ncontext in which autonomous agents could be practically and safely deployed,\ne.g. fully autonomous vehicles will encounter charged moral decisions that\ncomplicate their deployment. While ethical agents could be trained by rewarding\ncorrect behavior under a specific moral theory (e.g. utilitarianism), there\nremains widespread disagreement about the nature of morality. Acknowledging\nsuch disagreement, recent work in moral philosophy proposes that ethical\nbehavior requires acting under moral uncertainty, i.e. to take into account\nwhen acting that one's credence is split across several plausible ethical\ntheories. This paper translates such insights to the field of reinforcement\nlearning, proposes two training methods that realize different points among\ncompeting desiderata, and trains agents in simple environments to act under\nmoral uncertainty. The results illustrate (1) how such uncertainty can help\ncurb extreme behavior from commitment to single theories and (2) several\ntechnical complications arising from attempting to ground moral philosophy in\nRL (e.g. how can a principled trade-off between two competing but incomparable\nreward functions be reached). The aim is to catalyze progress towards\nmorally-competent agents and highlight the potential of RL to contribute\ntowards the computational grounding of moral philosophy.", "author_comment": "28 pages, 18 figures; update adds discussion of a possible flaw of\n  Nash voting, discussion of further possible research into MEC, as well as a\n  few more references; updated to ICML version", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1909.01492": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1909.01492v2", "post_title": "Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation", "authors": ["Po-Sen Huang", "Robert Stanforth", "Johannes Welbl", "Chris Dyer", "Dani Yogatama", "Sven Gowal", "Krishnamurthy Dvijotham", "Pushmeet Kohli"], "date_published": "2019-09-03 23:03:10+00:00", "data_last_modified": "2019-12-20 18:21:49+00:00", "url": "http://arxiv.org/abs/1909.01492v2", "abstract": "Neural networks are part of many contemporary NLP systems, yet their\nempirical successes come at the price of vulnerability to adversarial attacks.\nPrevious work has used adversarial training and data augmentation to partially\nmitigate such brittleness, but these are unlikely to find worst-case\nadversaries due to the complexity of the search space arising from discrete\ntext perturbations. In this work, we approach the problem from the opposite\ndirection: to formally verify a system's robustness against a predefined class\nof adversarial attacks. We study text classification under synonym replacements\nor character flip perturbations. We propose modeling these input perturbations\nas a simplex and then using Interval Bound Propagation -- a formal model\nverification method. We modify the conventional log-likelihood training\nobjective to train models that can be efficiently verified, which would\notherwise come with exponential search complexity. The resulting models show\nonly little difference in terms of nominal accuracy, but have much improved\nverified accuracy under perturbations and come with an efficiently computable\nformal guarantee on worst case adversaries.", "author_comment": "EMNLP 2019", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.CR", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1311.2901": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1311.2901v3", "post_title": "Visualizing and Understanding Convolutional Networks", "authors": ["Matthew D Zeiler", "Rob Fergus"], "date_published": "2013-11-12 20:02:22+00:00", "data_last_modified": "2013-11-28 23:04:01+00:00", "url": "http://arxiv.org/abs/1311.2901v3", "abstract": "Large Convolutional Network models have recently demonstrated impressive\nclassification performance on the ImageNet benchmark. However there is no clear\nunderstanding of why they perform so well, or how they might be improved. In\nthis paper we address both issues. We introduce a novel visualization technique\nthat gives insight into the function of intermediate feature layers and the\noperation of the classifier. We also perform an ablation study to discover the\nperformance contribution from different model layers. This enables us to find\nmodel architectures that outperform Krizhevsky \\etal on the ImageNet\nclassification benchmark. We show our ImageNet model generalizes well to other\ndatasets: when the softmax classifier is retrained, it convincingly beats the\ncurrent state-of-the-art results on Caltech-101 and Caltech-256 datasets.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.04251": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.04251v4", "post_title": "Formal Limitations on the Measurement of Mutual Information", "authors": ["David McAllester", "Karl Stratos"], "date_published": "2018-11-10 13:12:27+00:00", "data_last_modified": "2020-05-20 12:03:39+00:00", "url": "http://arxiv.org/abs/1811.04251v4", "abstract": "Measuring mutual information from finite data is difficult. Recent work has\nconsidered variational methods maximizing a lower bound. In this paper, we\nprove that serious statistical limitations are inherent to any method of\nmeasuring mutual information. More specifically, we show that any\ndistribution-free high-confidence lower bound on mutual information estimated\nfrom N samples cannot be larger than O(ln N ).", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.IT", "categories": ["cs.IT", "cs.LG", "math.IT", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.06096": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.06096v2", "post_title": "Safe Reinforcement Learning via Probabilistic Shields", "authors": ["Nils Jansen", "Bettina K\u00f6nighofer", "Sebastian Junges", "Alexandru C. Serban", "Roderick Bloem"], "date_published": "2018-07-16 20:29:04+00:00", "data_last_modified": "2019-11-25 16:12:41+00:00", "url": "http://arxiv.org/abs/1807.06096v2", "abstract": "This paper targets the efficient construction of a safety shield for decision\nmaking in scenarios that incorporate uncertainty. Markov decision processes\n(MDPs) are prominent models to capture such planning problems. Reinforcement\nlearning (RL) is a machine learning technique to determine near-optimal\npolicies in MDPs that may be unknown prior to exploring the model. However,\nduring exploration, RL is prone to induce behavior that is undesirable or not\nallowed in safety- or mission-critical contexts. We introduce the concept of a\nprobabilistic shield that enables decision-making to adhere to safety\nconstraints with high probability. In a separation of concerns, we employ\nformal verification to efficiently compute the probabilities of critical\ndecisions within a safety-relevant fragment of the MDP. We use these results to\nrealize a shield that is applied to an RL algorithm which then optimizes the\nactual performance objective. We discuss tradeoffs between sufficient progress\nin exploration of the environment and ensuring safety. In our experiments, we\ndemonstrate on the arcade game PAC-MAN and on a case study involving service\nrobots that the learning efficiency increases as the learning needs orders of\nmagnitude fewer episodes.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.00610": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.00610v1", "post_title": "Accounting for the Neglected Dimensions of AI Progress", "authors": ["Fernando Mart\u00ednez-Plumed", "Shahar Avin", "Miles Brundage", "Allan Dafoe", "Sean \u00d3 h\u00c9igeartaigh", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "date_published": "2018-06-02 09:21:12+00:00", "data_last_modified": "2018-06-02 09:21:12+00:00", "url": "http://arxiv.org/abs/1806.00610v1", "abstract": "We analyze and reframe AI progress. In addition to the prevailing metrics of\nperformance, we highlight the usually neglected costs paid in the development\nand deployment of a system, including: data, expert knowledge, human oversight,\nsoftware resources, computing cycles, hardware and network facilities,\ndevelopment time, etc. These costs are paid throughout the life cycle of an AI\nsystem, fall differentially on different individuals, and vary in magnitude\ndepending on the replicability and generality of the AI solution. The\nmultidimensional performance and cost space can be collapsed to a single\nutility metric for a user with transitive and complete preferences. Even absent\na single utility function, AI advances can be generically assessed by whether\nthey expand the Pareto (optimal) surface. We explore a subset of these\nneglected dimensions using the two case studies of Alpha* and ALE. This\nbroadened conception of progress in AI should lead to novel ways of measuring\nsuccess in AI, and can help set milestones for future progress.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2004.06496": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2004.06496v6", "post_title": "Certifiable Robustness to Adversarial State Uncertainty in Deep Reinforcement Learning", "authors": ["Michael Everett", "Bjorn Lutjens", "Jonathan P. How"], "date_published": "2020-04-11 21:36:13+00:00", "data_last_modified": "2022-02-02 18:48:37+00:00", "url": "http://arxiv.org/abs/2004.06496v6", "abstract": "Deep Neural Network-based systems are now the state-of-the-art in many\nrobotics tasks, but their application in safety-critical domains remains\ndangerous without formal guarantees on network robustness. Small perturbations\nto sensor inputs (from noise or adversarial examples) are often enough to\nchange network-based decisions, which was recently shown to cause an autonomous\nvehicle to swerve into another lane. In light of these dangers, numerous\nalgorithms have been developed as defensive mechanisms from these adversarial\ninputs, some of which provide formal robustness guarantees or certificates.\nThis work leverages research on certified adversarial robustness to develop an\nonline certifiably robust for deep reinforcement learning algorithms. The\nproposed defense computes guaranteed lower bounds on state-action values during\nexecution to identify and choose a robust action under a worst-case deviation\nin input space due to possible adversaries or noise. Moreover, the resulting\npolicy comes with a certificate of solution quality, even though the true state\nand optimal action are unknown to the certifier due to the perturbations. The\napproach is demonstrated on a Deep Q-Network policy and is shown to increase\nrobustness to noise and adversaries in pedestrian collision avoidance scenarios\nand a classic control task. This work extends one of our prior works with new\nperformance guarantees, extensions to other RL algorithms, expanded results\naggregated across more scenarios, an extension into scenarios with adversarial\nbehavior, comparisons with a more computationally expensive method, and\nvisualizations that provide intuition about the robustness algorithm.", "author_comment": "arXiv admin note: text overlap with arXiv:1910.12908", "journal_ref": null, "doi": "10.1109/TNNLS.2021.3056046", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.05150": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.05150v2", "post_title": "Safe Reinforcement Learning with Natural Language Constraints", "authors": ["Tsung-Yen Yang", "Michael Hu", "Yinlam Chow", "Peter J. Ramadge", "Karthik Narasimhan"], "date_published": "2020-10-11 03:41:56+00:00", "data_last_modified": "2021-08-04 02:46:48+00:00", "url": "http://arxiv.org/abs/2010.05150v2", "abstract": "While safe reinforcement learning (RL) holds great promise for many practical\napplications like robotics or autonomous cars, current approaches require\nspecifying constraints in mathematical form. Such specifications demand domain\nexpertise, limiting the adoption of safe RL. In this paper, we propose learning\nto interpret natural language constraints for safe RL. To this end, we first\nintroduce HazardWorld, a new multi-task benchmark that requires an agent to\noptimize reward while not violating constraints specified in free-form text. We\nthen develop an agent with a modular architecture that can interpret and adhere\nto such textual constraints while learning new tasks. Our model consists of (1)\na constraint interpreter that encodes textual constraints into spatial and\ntemporal representations of forbidden states, and (2) a policy network that\nuses these representations to produce a policy achieving minimal constraint\nviolations during training. Across different domains in HazardWorld, we show\nthat our method achieves higher rewards (up to11x) and fewer constraint\nviolations (by 1.8x) compared to existing approaches. However, in terms of\nabsolute performance, HazardWorld still poses significant challenges for agents\nto learn efficiently, motivating the need for future work.", "author_comment": "The first two authors contributed equally", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2103.12021": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2103.12021v1", "post_title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism", "authors": ["Paria Rashidinejad", "Banghua Zhu", "Cong Ma", "Jiantao Jiao", "Stuart Russell"], "date_published": "2021-03-22 17:27:08+00:00", "data_last_modified": "2021-03-22 17:27:08+00:00", "url": "http://arxiv.org/abs/2103.12021v1", "abstract": "Offline (or batch) reinforcement learning (RL) algorithms seek to learn an\noptimal policy from a fixed dataset without active data collection. Based on\nthe composition of the offline dataset, two main categories of methods are\nused: imitation learning which is suitable for expert datasets and vanilla\noffline RL which often requires uniform coverage datasets. From a practical\nstandpoint, datasets often deviate from these two extremes and the exact data\ncomposition is usually unknown a priori. To bridge this gap, we present a new\noffline RL framework that smoothly interpolates between the two extremes of\ndata composition, hence unifying imitation learning and vanilla offline RL. The\nnew framework is centered around a weak version of the concentrability\ncoefficient that measures the deviation from the behavior policy to the expert\npolicy alone.\n  Under this new framework, we further investigate the question on algorithm\ndesign: can one develop an algorithm that achieves a minimax optimal rate and\nalso adapts to unknown data composition? To address this question, we consider\na lower confidence bound (LCB) algorithm developed based on pessimism in the\nface of uncertainty in offline RL. We study finite-sample properties of LCB as\nwell as information-theoretic limits in multi-armed bandits, contextual\nbandits, and Markov decision processes (MDPs). Our analysis reveals surprising\nfacts about optimality rates. In particular, in all three settings, LCB\nachieves a faster rate of $1/N$ for nearly-expert datasets compared to the\nusual rate of $1/\\sqrt{N}$ in offline RL, where $N$ is the number of samples in\nthe batch dataset. In the case of contextual bandits with at least two\ncontexts, we prove that LCB is adaptively optimal for the entire data\ncomposition range, achieving a smooth transition from imitation learning to\noffline RL. We further show that LCB is almost adaptively optimal in MDPs.", "author_comment": "84 pages, 6 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "math.OC", "math.ST", "stat.ML", "stat.TH"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1911.11132": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1911.11132v3", "post_title": "Scaling Out-of-Distribution Detection for Real-World Settings", "authors": ["Dan Hendrycks", "Steven Basart", "Mantas Mazeika", "Andy Zou", "Joe Kwon", "Mohammadreza Mostajabi", "Jacob Steinhardt", "Dawn Song"], "date_published": "2019-11-25 18:58:23+00:00", "data_last_modified": "2022-02-08 02:23:51+00:00", "url": "http://arxiv.org/abs/1911.11132v3", "abstract": "Detecting out-of-distribution examples is important for safety-critical\nmachine learning applications such as detecting novel biological phenomena and\nself-driving cars. However, existing research mainly focuses on simple\nsmall-scale settings. To set the stage for more realistic out-of-distribution\ndetection, we depart from small-scale settings and explore large-scale\nmulticlass and multi-label settings with high-resolution images and thousands\nof classes. To make future work in real-world settings possible, we create new\nbenchmarks for three large-scale settings. To test ImageNet multiclass anomaly\ndetectors, we introduce the Species dataset containing over 700,000 images and\nover a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL\nVOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark\nfor anomaly segmentation by introducing a segmentation benchmark with road\nanomalies. We conduct extensive experiments in these more realistic settings\nfor out-of-distribution detection and find that a surprisingly simple detector\nbased on the maximum logit outperforms prior methods in all the large-scale\nmulti-class, multi-label, and segmentation tasks, establishing a simple new\nbaseline for future work.", "author_comment": "The Species dataset and code are available at\n  https://github.com/hendrycks/anomaly-seg", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.08287": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.08287v3", "post_title": "Learning-based Model Predictive Control for Safe Exploration", "authors": ["Torsten Koller", "Felix Berkenkamp", "Matteo Turchetta", "Andreas Krause"], "date_published": "2018-03-22 09:41:45+00:00", "data_last_modified": "2018-11-07 11:08:25+00:00", "url": "http://arxiv.org/abs/1803.08287v3", "abstract": "Learning-based methods have been successful in solving complex control tasks\nwithout significant prior knowledge about the system. However, these methods\ntypically do not provide any safety guarantees, which prevents their use in\nsafety-critical, real-world applications. In this paper, we present a\nlearning-based model predictive control scheme that can provide provable\nhigh-probability safety guarantees. To this end, we exploit regularity\nassumptions on the dynamics in terms of a Gaussian process prior to construct\nprovably accurate confidence intervals on predicted trajectories. Unlike\nprevious approaches, we do not assume that model uncertainties are independent.\nBased on these predictions, we guarantee that trajectories satisfy safety\nconstraints. Moreover, we use a terminal set constraint to recursively\nguarantee the existence of safe control actions at every iteration. In our\nexperiments, we show that the resulting algorithm can be used to safely and\nefficiently explore and learn about dynamic systems.", "author_comment": "Proc. of the Conference on Decision and Control, 2018", "journal_ref": null, "doi": null, "primary_category": "cs.SY", "categories": ["cs.SY", "cs.AI", "cs.LG", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.06766": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.06766v1", "post_title": "Parenting: Safe Reinforcement Learning from Human Input", "authors": ["Christopher Frye", "Ilya Feige"], "date_published": "2019-02-18 19:10:18+00:00", "data_last_modified": "2019-02-18 19:10:18+00:00", "url": "http://arxiv.org/abs/1902.06766v1", "abstract": "Autonomous agents trained via reinforcement learning present numerous safety\nconcerns: reward hacking, negative side effects, and unsafe exploration, among\nothers. In the context of near-future autonomous agents, operating in\nenvironments where humans understand the existing dangers, human involvement in\nthe learning process has proved a promising approach to AI Safety. Here we\ndemonstrate that a precise framework for learning from human input, loosely\ninspired by the way humans parent children, solves a broad class of safety\nproblems in this context. We show that our Parenting algorithm solves these\nproblems in the relevant AI Safety gridworlds of Leike et al. (2017), that an\nagent can learn to outperform its parent as it \"matures\", and that policies\nlearnt through Parenting are generalisable to new environments.", "author_comment": "9 pages, 4 figures, 1 table", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.06160": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.06160v4", "post_title": "Augmenting Decision Making via Interactive What-If Analysis", "authors": ["Sneha Gathani", "Madelon Hulsebos", "James Gale", "Peter J. Haas", "\u00c7a\u011fatay Demiralp"], "date_published": "2021-09-13 17:54:30+00:00", "data_last_modified": "2022-02-09 04:26:17+00:00", "url": "http://arxiv.org/abs/2109.06160v4", "abstract": "The fundamental goal of business data analysis is to improve business\ndecisions using data. Business users often make decisions to achieve key\nperformance indicators (KPIs) such as increasing customer retention or sales,\nor decreasing costs. To discover the relationship between data attributes\nhypothesized to be drivers and those corresponding to KPIs of interest,\nbusiness users currently need to perform lengthy exploratory analyses. This\ninvolves considering multitudes of combinations and scenarios and performing\nslicing, dicing, and transformations on the data accordingly, e.g., analyzing\ncustomer retention across quarters of the year or suggesting optimal media\nchannels across strata of customers. However, the increasing complexity of\ndatasets combined with the cognitive limitations of humans makes it challenging\nto carry over multiple hypotheses, even for simple datasets. Therefore mentally\nperforming such analyses is hard. Existing commercial tools either provide\npartial solutions or fail to cater to business users altogether. Here we argue\nfor four functionalities to enable business users to interactively learn and\nreason about the relationships between sets of data attributes thereby\nfacilitating data-driven decision making. We implement these functionalities in\nSystemD, an interactive visual data analysis system enabling business users to\nexperiment with the data by asking what-if questions. We evaluate the system\nthrough three business use cases: marketing mix modeling, customer retention\nanalysis, and deal closing analysis, and report on feedback from multiple\nbusiness users. Users find the SystemD functionalities highly useful for quick\ntesting and validation of their hypotheses around their KPIs of interest,\naddressing their unmet analysis needs. The feedback also suggests that the UX\ndesign can be enhanced to further improve the understandability of these\nfunctionalities.", "author_comment": "CIDR'22", "journal_ref": null, "doi": null, "primary_category": "cs.DB", "categories": ["cs.DB", "cs.HC", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.10667": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.10667v1", "post_title": "Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives", "authors": ["Anirudh Goyal", "Shagun Sodhani", "Jonathan Binas", "Xue Bin Peng", "Sergey Levine", "Yoshua Bengio"], "date_published": "2019-06-25 17:04:48+00:00", "data_last_modified": "2019-06-25 17:04:48+00:00", "url": "http://arxiv.org/abs/1906.10667v1", "abstract": "Reinforcement learning agents that operate in diverse and complex\nenvironments can benefit from the structured decomposition of their behavior.\nOften, this is addressed in the context of hierarchical reinforcement learning,\nwhere the aim is to decompose a policy into lower-level primitives or options,\nand a higher-level meta-policy that triggers the appropriate behaviors for a\ngiven situation. However, the meta-policy must still produce appropriate\ndecisions in all states. In this work, we propose a policy design that\ndecomposes into primitives, similarly to hierarchical reinforcement learning,\nbut without a high-level meta-policy. Instead, each primitive can decide for\nthemselves whether they wish to act in the current state. We use an\ninformation-theoretic mechanism for enabling this decentralized decision: each\nprimitive chooses how much information it needs about the current state to make\na decision and the primitive that requests the most information about the\ncurrent state acts in the world. The primitives are regularized to use as\nlittle information as possible, which leads to natural competition and\nspecialization. We experimentally demonstrate that this policy architecture\nimproves over both flat and hierarchical policies in terms of generalization.", "author_comment": "Preprint, Under Review", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.09382": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.09382v1", "post_title": "Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text", "authors": ["Felix Hill", "Sona Mokra", "Nathaniel Wong", "Tim Harley"], "date_published": "2020-05-19 12:16:58+00:00", "data_last_modified": "2020-05-19 12:16:58+00:00", "url": "http://arxiv.org/abs/2005.09382v1", "abstract": "Recent work has described neural-network-based agents that are trained with\nreinforcement learning (RL) to execute language-like commands in simulated\nworlds, as a step towards an intelligent agent or robot that can be instructed\nby human users. However, the optimisation of multi-goal motor policies via deep\nRL from scratch requires many episodes of experience. Consequently,\ninstruction-following with deep RL typically involves language generated from\ntemplates (by an environment simulator), which does not reflect the varied or\nambiguous expressions of real users. Here, we propose a conceptually simple\nmethod for training instruction-following agents with deep RL that are robust\nto natural human instructions. By applying our method with a state-of-the-art\npre-trained text-based language model (BERT), on tasks requiring agents to\nidentify and position everyday objects relative to other objects in a\nnaturalistic 3D simulated room, we demonstrate substantially-above-chance\nzero-shot transfer from synthetic template commands to natural instructions\ngiven by humans. Our approach is a general recipe for training any deep\nRL-based system to interface with human users, and bridges the gap between two\nresearch directions of notable recent success: agent-centric motor behavior and\ntext-based representation learning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1904.09605": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1904.09605v2", "post_title": "Generative Exploration and Exploitation", "authors": ["Jiechuan Jiang", "Zongqing Lu"], "date_published": "2019-04-21 14:15:24+00:00", "data_last_modified": "2019-11-20 11:56:23+00:00", "url": "http://arxiv.org/abs/1904.09605v2", "abstract": "Sparse reward is one of the biggest challenges in reinforcement learning\n(RL). In this paper, we propose a novel method called Generative Exploration\nand Exploitation (GENE) to overcome sparse reward. GENE automatically generates\nstart states to encourage the agent to explore the environment and to exploit\nreceived reward signals. GENE can adaptively tradeoff between exploration and\nexploitation according to the varying distributions of states experienced by\nthe agent as the learning progresses. GENE relies on no prior knowledge about\nthe environment and can be combined with any RL algorithm, no matter on-policy\nor off-policy, single-agent or multi-agent. Empirically, we demonstrate that\nGENE significantly outperforms existing methods in three tasks with only binary\nrewards, including Maze, Maze Ant, and Cooperative Navigation. Ablation studies\nverify the emergence of progressive exploration and automatic reversing.", "author_comment": "AAAI'20", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1808.08460": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1808.08460v2", "post_title": "The Social Cost of Strategic Classification", "authors": ["Smitha Milli", "John Miller", "Anca D. Dragan", "Moritz Hardt"], "date_published": "2018-08-25 18:31:52+00:00", "data_last_modified": "2018-11-22 13:51:18+00:00", "url": "http://arxiv.org/abs/1808.08460v2", "abstract": "Consequential decision-making typically incentivizes individuals to behave\nstrategically, tailoring their behavior to the specifics of the decision rule.\nA long line of work has therefore sought to counteract strategic behavior by\ndesigning more conservative decision boundaries in an effort to increase\nrobustness to the effects of strategic covariate shift. We show that these\nefforts benefit the institutional decision maker at the expense of the\nindividuals being classified. Introducing a notion of social burden, we prove\nthat any increase in institutional utility necessarily leads to a corresponding\nincrease in social burden. Moreover, we show that the negative externalities of\nstrategic classification can disproportionately harm disadvantaged groups in\nthe population. Our results highlight that strategy-robustness must be weighed\nagainst considerations of social welfare and fairness.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.05590": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.05590v1", "post_title": "Emergence of Addictive Behaviors in Reinforcement Learning Agents", "authors": ["Vahid Behzadan", "Roman V. Yampolskiy", "Arslan Munir"], "date_published": "2018-11-14 01:30:00+00:00", "data_last_modified": "2018-11-14 01:30:00+00:00", "url": "http://arxiv.org/abs/1811.05590v1", "abstract": "This paper presents a novel approach to the technical analysis of wireheading\nin intelligent agents. Inspired by the natural analogues of wireheading and\ntheir prevalent manifestations, we propose the modeling of such phenomenon in\nReinforcement Learning (RL) agents as psychological disorders. In a preliminary\nstep towards evaluating this proposal, we study the feasibility and dynamics of\nemergent addictive policies in Q-learning agents in the tractable environment\nof the game of Snake. We consider a slightly modified settings for this game,\nin which the environment provides a \"drug\" seed alongside the original\n\"healthy\" seed for the consumption of the snake. We adopt and extend an\nRL-based model of natural addiction to Q-learning agents in this settings, and\nderive sufficient parametric conditions for the emergence of addictive\nbehaviors in such agents. Furthermore, we evaluate our theoretical analysis\nwith three sets of simulation-based experiments. The results demonstrate the\nfeasibility of addictive wireheading in RL agents, and provide promising venues\nof further research on the psychopathological modeling of complex AI safety\nproblems.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.03096": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.03096v4", "post_title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples", "authors": ["Eleni Triantafillou", "Tyler Zhu", "Vincent Dumoulin", "Pascal Lamblin", "Utku Evci", "Kelvin Xu", "Ross Goroshin", "Carles Gelada", "Kevin Swersky", "Pierre-Antoine Manzagol", "Hugo Larochelle"], "date_published": "2019-03-07 18:48:55+00:00", "data_last_modified": "2020-04-08 15:58:20+00:00", "url": "http://arxiv.org/abs/1903.03096v4", "abstract": "Few-shot classification refers to learning a classifier for new classes given\nonly a few examples. While a plethora of models have emerged to tackle it, we\nfind the procedure and datasets that are used to assess their progress lacking.\nTo address this limitation, we propose Meta-Dataset: a new benchmark for\ntraining and evaluating models that is large-scale, consists of diverse\ndatasets, and presents more realistic tasks. We experiment with popular\nbaselines and meta-learners on Meta-Dataset, along with a competitive method\nthat we propose. We analyze performance as a function of various\ncharacteristics of test tasks and examine the models' ability to leverage\ndiverse training sources for improving their generalization. We also propose a\nnew set of baselines for quantifying the benefit of meta-learning in\nMeta-Dataset. Our extensive experimentation has uncovered important research\nchallenges and we hope to inspire work in these directions.", "author_comment": "Code available at https://github.com/google-research/meta-dataset", "journal_ref": "International Conference on Learning Representations (2020)", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.14341": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.14341v2", "post_title": "ROBIN : A Benchmark for Robustness to Individual Nuisances in Real-World Out-of-Distribution Shifts", "authors": ["Bingchen Zhao", "Shaozuo Yu", "Wufei Ma", "Mingxin Yu", "Shenxiao Mei", "Angtian Wang", "Ju He", "Alan Yuille", "Adam Kortylewski"], "date_published": "2021-11-29 06:18:46+00:00", "data_last_modified": "2021-12-02 11:53:03+00:00", "url": "http://arxiv.org/abs/2111.14341v2", "abstract": "Enhancing the robustness in real-world scenarios has been proven very\nchallenging. One reason is that existing robustness benchmarks are limited, as\nthey either rely on synthetic data or they simply measure robustness as\ngeneralization between datasets and hence ignore the effects of individual\nnuisance factors. In this work, we introduce ROBIN, a benchmark dataset for\ndiagnosing the robustness of vision algorithms to individual nuisances in\nreal-world images. ROBIN builds on 10 rigid categories from the PASCAL VOC 2012\nand ImageNet datasets and includes out-of-distribution examples of the objects\n3D pose, shape, texture, context and weather conditions. ROBIN is richly\nannotated to enable benchmark models for image classification, object\ndetection, and 3D pose estimation. We provide results for a number of popular\nbaselines and make several interesting observations: 1. Some nuisance factors\nhave a much stronger negative effect on the performance compared to others.\nMoreover, the negative effect of an OODnuisance depends on the downstream\nvision task. 2. Current approaches to enhance OOD robustness using strong data\naugmentation have only marginal effects in real-world OOD scenarios, and\nsometimes even reduce the OOD performance. 3. We do not observe any significant\ndifferences between convolutional and transformer architectures in terms of OOD\nrobustness. We believe our dataset provides a rich testbed to study the OOD\nrobustness of vision algorithms and will help to significantly push forward\nresearch in this area.", "author_comment": "Project webpage: https://bzhao.me/ROBIN/", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1808.04096": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1808.04096v1", "post_title": "Directed Policy Gradient for Safe Reinforcement Learning with Human Advice", "authors": ["H\u00e9l\u00e8ne Plisnier", "Denis Steckelmacher", "Tim Brys", "Diederik M. Roijers", "Ann Now\u00e9"], "date_published": "2018-08-13 08:12:22+00:00", "data_last_modified": "2018-08-13 08:12:22+00:00", "url": "http://arxiv.org/abs/1808.04096v1", "abstract": "Many currently deployed Reinforcement Learning agents work in an environment\nshared with humans, be them co-workers, users or clients. It is desirable that\nthese agents adjust to people's preferences, learn faster thanks to their help,\nand act safely around them. We argue that most current approaches that learn\nfrom human feedback are unsafe: rewarding or punishing the agent a-posteriori\ncannot immediately prevent it from wrong-doing. In this paper, we extend Policy\nGradient to make it robust to external directives, that would otherwise break\nthe fundamentally on-policy nature of Policy Gradient. Our technique, Directed\nPolicy Gradient (DPG), allows a teacher or backup policy to override the agent\nbefore it acts undesirably, while allowing the agent to leverage human advice\nor directives to learn faster. Our experiments demonstrate that DPG makes the\nagent learn much faster than reward-based approaches, while requiring an order\nof magnitude less advice.", "author_comment": "Accepted at the European Workshop on Reinforcement Learning 2018\n  (EWRL14)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.00645": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.00645v2", "post_title": "Universal Planning Networks", "authors": ["Aravind Srinivas", "Allan Jabri", "Pieter Abbeel", "Sergey Levine", "Chelsea Finn"], "date_published": "2018-04-02 17:51:53+00:00", "data_last_modified": "2018-04-04 17:36:36+00:00", "url": "http://arxiv.org/abs/1804.00645v2", "abstract": "A key challenge in complex visuomotor control is learning abstract\nrepresentations that are effective for specifying goals, planning, and\ngeneralization. To this end, we introduce universal planning networks (UPN).\nUPNs embed differentiable planning within a goal-directed policy. This planning\ncomputation unrolls a forward model in a latent space and infers an optimal\naction plan through gradient descent trajectory optimization. The\nplan-by-gradient-descent process and its underlying representations are learned\nend-to-end to directly optimize a supervised imitation learning objective. We\nfind that the representations learned are not only effective for goal-directed\nvisual imitation via gradient-based trajectory optimization, but can also\nprovide a metric for specifying goals using images. The learned representations\ncan be leveraged to specify distance-based rewards to reach new target states\nfor model-free reinforcement learning, resulting in substantially more\neffective learning when solving new tasks described via image-based goals. We\nwere able to achieve successful transfer of visuomotor planning strategies\nacross robots with significantly different morphologies and actuation\ncapabilities.", "author_comment": "Videos available at https://sites.google.com/view/upn-public/home", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1705.04226": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1705.04226v2", "post_title": "Robot Planning with Mathematical Models of Human State and Action", "authors": ["Anca D. Dragan"], "date_published": "2017-05-11 15:02:34+00:00", "data_last_modified": "2017-07-04 02:08:23+00:00", "url": "http://arxiv.org/abs/1705.04226v2", "abstract": "Robots interacting with the physical world plan with models of physics. We\nadvocate that robots interacting with people need to plan with models of\ncognition. This writeup summarizes the insights we have gained in integrating\ncomputational cognitive models of people into robotics planning and control. It\nstarts from a general game-theoretic formulation of interaction, and analyzes\nhow different approximations result in different useful coordination behaviors\nfor the robot during its interaction with people.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1911.04252": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1911.04252v4", "post_title": "Self-training with Noisy Student improves ImageNet classification", "authors": ["Qizhe Xie", "Minh-Thang Luong", "Eduard Hovy", "Quoc V. Le"], "date_published": "2019-11-11 18:59:27+00:00", "data_last_modified": "2020-06-19 17:36:57+00:00", "url": "http://arxiv.org/abs/1911.04252v4", "abstract": "We present Noisy Student Training, a semi-supervised learning approach that\nworks well even when labeled data is abundant. Noisy Student Training achieves\n88.4% top-1 accuracy on ImageNet, which is 2.0% better than the\nstate-of-the-art model that requires 3.5B weakly labeled Instagram images. On\nrobustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to\n83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces\nImageNet-P mean flip rate from 27.8 to 12.2.\n  Noisy Student Training extends the idea of self-training and distillation\nwith the use of equal-or-larger student models and noise added to the student\nduring learning. On ImageNet, we first train an EfficientNet model on labeled\nimages and use it as a teacher to generate pseudo labels for 300M unlabeled\nimages. We then train a larger EfficientNet as a student model on the\ncombination of labeled and pseudo labeled images. We iterate this process by\nputting back the student as the teacher. During the learning of the student, we\ninject noise such as dropout, stochastic depth, and data augmentation via\nRandAugment to the student so that the student generalizes better than the\nteacher. Models are available at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.\nCode is available at https://github.com/google-research/noisystudent.", "author_comment": "CVPR 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.01345": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.01345v2", "post_title": "Decision Transformer: Reinforcement Learning via Sequence Modeling", "authors": ["Lili Chen", "Kevin Lu", "Aravind Rajeswaran", "Kimin Lee", "Aditya Grover", "Michael Laskin", "Pieter Abbeel", "Aravind Srinivas", "Igor Mordatch"], "date_published": "2021-06-02 17:53:39+00:00", "data_last_modified": "2021-06-24 17:09:59+00:00", "url": "http://arxiv.org/abs/2106.01345v2", "abstract": "We introduce a framework that abstracts Reinforcement Learning (RL) as a\nsequence modeling problem. This allows us to draw upon the simplicity and\nscalability of the Transformer architecture, and associated advances in\nlanguage modeling such as GPT-x and BERT. In particular, we present Decision\nTransformer, an architecture that casts the problem of RL as conditional\nsequence modeling. Unlike prior approaches to RL that fit value functions or\ncompute policy gradients, Decision Transformer simply outputs the optimal\nactions by leveraging a causally masked Transformer. By conditioning an\nautoregressive model on the desired return (reward), past states, and actions,\nour Decision Transformer model can generate future actions that achieve the\ndesired return. Despite its simplicity, Decision Transformer matches or exceeds\nthe performance of state-of-the-art model-free offline RL baselines on Atari,\nOpenAI Gym, and Key-to-Door tasks.", "author_comment": "First two authors contributed equally. Last two authors advised\n  equally", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.01973": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.01973v2", "post_title": "Learning Latent Plans from Play", "authors": ["Corey Lynch", "Mohi Khansari", "Ted Xiao", "Vikash Kumar", "Jonathan Tompson", "Sergey Levine", "Pierre Sermanet"], "date_published": "2019-03-05 18:36:42+00:00", "data_last_modified": "2019-12-20 05:03:10+00:00", "url": "http://arxiv.org/abs/1903.01973v2", "abstract": "Acquiring a diverse repertoire of general-purpose skills remains an open\nchallenge for robotics. In this work, we propose self-supervising control on\ntop of human teleoperated play data as a way to scale up skill learning. Play\nhas two properties that make it attractive compared to conventional task\ndemonstrations. Play is cheap, as it can be collected in large quantities\nquickly without task segmenting, labeling, or resetting to an initial state.\nPlay is naturally rich, covering ~4x more interaction space than task\ndemonstrations for the same amount of collection time. To learn control from\nplay, we introduce Play-LMP, a self-supervised method that learns to organize\nplay behaviors in a latent space, then reuse them at test time to achieve\nspecific goals. Combining self-supervised control with a diverse play dataset\nshifts the focus of skill learning from a narrow and discrete set of tasks to\nthe full continuum of behaviors available in an environment. We find that this\ncombination generalizes well empirically---after self-supervising on unlabeled\nplay, our method substantially outperforms individual expert-trained policies\non 18 difficult user-specified visual manipulation tasks in a simulated robotic\ntabletop environment. We additionally find that play-supervised models, unlike\ntheir expert-trained counterparts, are more robust to perturbations and exhibit\nretrying-till-success behaviors. Finally, we find that our agent organizes its\nlatent plan space around functional tasks, despite never being trained with\ntask labels. Videos, code and data are available at\nlearning-from-play.github.io", "author_comment": "Published at CoRL 2019 (3rd Conference on Robot Learning, Osaka,\n  Japan)", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2107.01969": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2107.01969v1", "post_title": "The MineRL BASALT Competition on Learning from Human Feedback", "authors": ["Rohin Shah", "Cody Wild", "Steven H. Wang", "Neel Alex", "Brandon Houghton", "William Guss", "Sharada Mohanty", "Anssi Kanervisto", "Stephanie Milani", "Nicholay Topin", "Pieter Abbeel", "Stuart Russell", "Anca Dragan"], "date_published": "2021-07-05 12:18:17+00:00", "data_last_modified": "2021-07-05 12:18:17+00:00", "url": "http://arxiv.org/abs/2107.01969v1", "abstract": "The last decade has seen a significant increase of interest in deep learning\nresearch, with many public successes that have demonstrated its potential. As\nsuch, these systems are now being incorporated into commercial products. With\nthis comes an additional challenge: how can we build AI systems that solve\ntasks where there is not a crisp, well-defined specification? While multiple\nsolutions have been proposed, in this competition we focus on one in\nparticular: learning from human feedback. Rather than training AI systems using\na predefined reward function or using a labeled dataset with a predefined set\nof categories, we instead train the AI system using a learning signal derived\nfrom some form of human feedback, which can evolve over time as the\nunderstanding of the task changes, or as the capabilities of the AI system\nimprove.\n  The MineRL BASALT competition aims to spur forward research on this important\nclass of techniques. We design a suite of four tasks in Minecraft for which we\nexpect it will be hard to write down hardcoded reward functions. These tasks\nare defined by a paragraph of natural language: for example, \"create a\nwaterfall and take a scenic picture of it\", with additional clarifying details.\nParticipants must train a separate agent for each task, using any method they\nwant. Agents are then evaluated by humans who have read the task description.\nTo help participants get started, we provide a dataset of human demonstrations\non each of the four tasks, as well as an imitation learning baseline that\nleverages these demonstrations.\n  Our hope is that this competition will improve our ability to build AI\nsystems that do what their designers intend them to do, even when the intent\ncannot be easily formalized. Besides allowing AI to solve more tasks, this can\nalso enable more effective regulation of AI systems, as well as making progress\non the value alignment problem.", "author_comment": "NeurIPS 2021 Competition Track", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.00582": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.00582v1", "post_title": "Learning to Complement Humans", "authors": ["Bryan Wilder", "Eric Horvitz", "Ece Kamar"], "date_published": "2020-05-01 20:00:23+00:00", "data_last_modified": "2020-05-01 20:00:23+00:00", "url": "http://arxiv.org/abs/2005.00582v1", "abstract": "A rising vision for AI in the open world centers on the development of\nsystems that can complement humans for perceptual, diagnostic, and reasoning\ntasks. To date, systems aimed at complementing the skills of people have\nemployed models trained to be as accurate as possible in isolation. We\ndemonstrate how an end-to-end learning strategy can be harnessed to optimize\nthe combined performance of human-machine teams by considering the distinct\nabilities of people and machines. The goal is to focus machine learning on\nproblem instances that are difficult for humans, while recognizing instances\nthat are difficult for the machine and seeking human input on them. We\ndemonstrate in two real-world domains (scientific discovery and medical\ndiagnosis) that human-machine teams built via these methods outperform the\nindividual performance of machines and people. We then analyze conditions under\nwhich this complementarity is strongest, and which training methods amplify it.\nTaken together, our work provides the first systematic investigation of how\nmachine learning systems can be trained to complement human reasoning.", "author_comment": "Accepted at IJCAI 2020", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.05328": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.05328v1", "post_title": "Data Augmentation Can Improve Robustness", "authors": ["Sylvestre-Alvise Rebuffi", "Sven Gowal", "Dan A. Calian", "Florian Stimberg", "Olivia Wiles", "Timothy Mann"], "date_published": "2021-11-09 18:57:00+00:00", "data_last_modified": "2021-11-09 18:57:00+00:00", "url": "http://arxiv.org/abs/2111.05328v1", "abstract": "Adversarial training suffers from robust overfitting, a phenomenon where the\nrobust test accuracy starts to decrease during training. In this paper, we\nfocus on reducing robust overfitting by using common data augmentation schemes.\nWe demonstrate that, contrary to previous findings, when combined with model\nweight averaging, data augmentation can significantly boost robust accuracy.\nFurthermore, we compare various augmentations techniques and observe that\nspatial composition techniques work the best for adversarial training. Finally,\nwe evaluate our approach on CIFAR-10 against $\\ell_\\infty$ and $\\ell_2$\nnorm-bounded perturbations of size $\\epsilon = 8/255$ and $\\epsilon = 128/255$,\nrespectively. We show large absolute improvements of +2.93% and +2.16% in\nrobust accuracy compared to previous state-of-the-art methods. In particular,\nagainst $\\ell_\\infty$ norm-bounded perturbations of size $\\epsilon = 8/255$,\nour model reaches 60.07% robust accuracy without using any external data. We\nalso achieve a significant performance boost with this approach while using\nother architectures and datasets such as CIFAR-100, SVHN and TinyImageNet.", "author_comment": "Accepted at NeurIPS 2021. arXiv admin note: substantial text overlap\n  with arXiv:2103.01946; text overlap with arXiv:2110.09468", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1609.03543": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1609.03543v5", "post_title": "Logical Induction", "authors": ["Scott Garrabrant", "Tsvi Benson-Tilsen", "Andrew Critch", "Nate Soares", "Jessica Taylor"], "date_published": "2016-09-12 19:30:56+00:00", "data_last_modified": "2020-12-07 22:26:59+00:00", "url": "http://arxiv.org/abs/1609.03543v5", "abstract": "We present a computable algorithm that assigns probabilities to every logical\nstatement in a given formal language, and refines those probabilities over\ntime. For instance, if the language is Peano arithmetic, it assigns\nprobabilities to all arithmetical statements, including claims about the twin\nprime conjecture, the outputs of long-running computations, and its own\nprobabilities. We show that our algorithm, an instance of what we call a\nlogical inductor, satisfies a number of intuitive desiderata, including: (1) it\nlearns to predict patterns of truth and falsehood in logical statements, often\nlong before having the resources to evaluate the statements, so long as the\npatterns can be written down in polynomial time; (2) it learns to use\nappropriate statistical summaries to predict sequences of statements whose\ntruth values appear pseudorandom; and (3) it learns to have accurate beliefs\nabout its own current beliefs, in a manner that avoids the standard paradoxes\nof self-reference. For example, if a given computer program only ever produces\noutputs in a certain range, a logical inductor learns this fact in a timely\nmanner; and if late digits in the decimal expansion of $\\pi$ are difficult to\npredict, then a logical inductor learns to assign $\\approx 10\\%$ probability to\n\"the $n$th digit of $\\pi$ is a 7\" for large $n$. Logical inductors also learn\nto trust their future beliefs more than their current beliefs, and their\nbeliefs are coherent in the limit (whenever $\\phi \\implies \\psi$,\n$\\mathbb{P}_\\infty(\\phi) \\le \\mathbb{P}_\\infty(\\psi)$, and so on); and logical\ninductors strictly dominate the universal semimeasure in the limit.\n  These properties and many others all follow from a single logical induction\ncriterion, which is motivated by a series of stock trading analogies. Roughly\nspeaking, each logical sentence $\\phi$ is associated with a stock that is worth\n\\$1 per share if [...]", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LO", "math.LO", "math.PR"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2101.12509": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2101.12509v2", "post_title": "Challenges for Using Impact Regularizers to Avoid Negative Side Effects", "authors": ["David Lindner", "Kyle Matoba", "Alexander Meulemans"], "date_published": "2021-01-29 10:32:51+00:00", "data_last_modified": "2021-02-23 13:49:47+00:00", "url": "http://arxiv.org/abs/2101.12509v2", "abstract": "Designing reward functions for reinforcement learning is difficult: besides\nspecifying which behavior is rewarded for a task, the reward also has to\ndiscourage undesired outcomes. Misspecified reward functions can lead to\nunintended negative side effects, and overall unsafe behavior. To overcome this\nproblem, recent work proposed to augment the specified reward function with an\nimpact regularizer that discourages behavior that has a big impact on the\nenvironment. Although initial results with impact regularizers seem promising\nin mitigating some types of side effects, important challenges remain. In this\npaper, we examine the main current challenges of impact regularizers and relate\nthem to fundamental design decisions. We discuss in detail which challenges\nrecent approaches address and which remain unsolved. Finally, we explore\npromising directions to overcome the unsolved challenges in preventing negative\nside effects with impact regularizers.", "author_comment": "Presented at the SafeAI workshop at AAAI 2021", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.02485": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.02485v1", "post_title": "Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations", "authors": ["Alex Lamb", "Jonathan Binas", "Anirudh Goyal", "Dmitriy Serdyuk", "Sandeep Subramanian", "Ioannis Mitliagkas", "Yoshua Bengio"], "date_published": "2018-04-07 00:11:05+00:00", "data_last_modified": "2018-04-07 00:11:05+00:00", "url": "http://arxiv.org/abs/1804.02485v1", "abstract": "Deep networks have achieved impressive results across a variety of important\ntasks. However a known weakness is a failure to perform well when evaluated on\ndata which differ from the training distribution, even if these differences are\nvery small, as is the case with adversarial examples. We propose Fortified\nNetworks, a simple transformation of existing networks, which fortifies the\nhidden layers in a deep network by identifying when the hidden states are off\nof the data manifold, and maps these hidden states back to parts of the data\nmanifold where the network performs well. Our principal contribution is to show\nthat fortifying these hidden states improves the robustness of deep networks\nand our experiments (i) demonstrate improved robustness to standard adversarial\nattacks in both black-box and white-box threat models; (ii) suggest that our\nimprovements are not primarily due to the gradient masking problem and (iii)\nshow the advantage of doing this fortification in the hidden layers instead of\nthe input space.", "author_comment": "Under Review ICML 2018", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.04083": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.04083v1", "post_title": "User Tampering in Reinforcement Learning Recommender Systems", "authors": ["Charles Evans", "Atoosa Kasirzadeh"], "date_published": "2021-09-09 07:53:23+00:00", "data_last_modified": "2021-09-09 07:53:23+00:00", "url": "http://arxiv.org/abs/2109.04083v1", "abstract": "This paper provides the first formalisation and empirical demonstration of a\nparticular safety concern in reinforcement learning (RL)-based news and social\nmedia recommendation algorithms. This safety concern is what we call \"user\ntampering\" -- a phenomenon whereby an RL-based recommender system may\nmanipulate a media user's opinions, preferences and beliefs via its\nrecommendations as part of a policy to increase long-term user engagement. We\nprovide a simulation study of a media recommendation problem constrained to the\nrecommendation of political content, and demonstrate that a Q-learning\nalgorithm consistently learns to exploit its opportunities to 'polarise'\nsimulated 'users' with its early recommendations in order to have more\nconsistent success with later recommendations catering to that polarisation.\nFinally, we argue that given our findings, designing an RL-based recommender\nsystem which cannot learn to exploit user tampering requires making the metric\nfor the recommender's success independent of observable signals of user\nengagement, and thus that a media recommendation system built solely with RL is\nnecessarily either unsafe, or almost certainly commercially unviable.", "author_comment": "Accepted for presentation at the 4th FAccTRec Workshop on Responsible\n  Recommendation (FAccTRec '21)", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.09591": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.09591v2", "post_title": "Applying Deep Learning To Airbnb Search", "authors": ["Malay Haldar", "Mustafa Abdool", "Prashant Ramanathan", "Tao Xu", "Shulin Yang", "Huizhong Duan", "Qing Zhang", "Nick Barrow-Williams", "Bradley C. Turnbull", "Brendan M. Collins", "Thomas Legrand"], "date_published": "2018-10-22 23:11:01+00:00", "data_last_modified": "2018-10-24 18:28:03+00:00", "url": "http://arxiv.org/abs/1810.09591v2", "abstract": "The application to search ranking is one of the biggest machine learning\nsuccess stories at Airbnb. Much of the initial gains were driven by a gradient\nboosted decision tree model. The gains, however, plateaued over time. This\npaper discusses the work done in applying neural networks in an attempt to\nbreak out of that plateau. We present our perspective not with the intention of\npushing the frontier of new modeling techniques. Instead, ours is a story of\nthe elements we found useful in applying neural networks to a real life\nproduct. Deep learning was steep learning for us. To other teams embarking on\nsimilar journeys, we hope an account of our struggles and triumphs will provide\nsome useful pointers. Bon voyage!", "author_comment": "8 pages", "journal_ref": null, "doi": "10.1145/3292500.3330658", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.11328": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.11328v3", "post_title": "Rethinking Bias-Variance Trade-off for Generalization of Neural Networks", "authors": ["Zitong Yang", "Yaodong Yu", "Chong You", "Jacob Steinhardt", "Yi Ma"], "date_published": "2020-02-26 07:21:54+00:00", "data_last_modified": "2020-12-08 03:10:44+00:00", "url": "http://arxiv.org/abs/2002.11328v3", "abstract": "The classical bias-variance trade-off predicts that bias decreases and\nvariance increase with model complexity, leading to a U-shaped risk curve.\nRecent work calls this into question for neural networks and other\nover-parameterized models, for which it is often observed that larger models\ngeneralize better. We provide a simple explanation for this by measuring the\nbias and variance of neural networks: while the bias is monotonically\ndecreasing as in the classical theory, the variance is unimodal or bell-shaped:\nit increases then decreases with the width of the network. We vary the network\narchitecture, loss function, and choice of dataset and confirm that variance\nunimodality occurs robustly for all models we considered. The risk curve is the\nsum of the bias and variance curves and displays different qualitative shapes\ndepending on the relative scale of bias and variance, with the double descent\ncurve observed in recent literature as a special case. We corroborate these\nempirical results with a theoretical analysis of two-layer linear networks with\nrandom first layer. Finally, evaluation on out-of-distribution data shows that\nmost of the drop in accuracy comes from increased bias while variance increases\nby a relatively small amount. Moreover, we find that deeper models decrease\nbias and increase variance for both in-distribution and out-of-distribution\ndata.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.05037": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.05037v1", "post_title": "Exploring Hierarchy-Aware Inverse Reinforcement Learning", "authors": ["Chris Cundy", "Daniel Filan"], "date_published": "2018-07-13 12:33:07+00:00", "data_last_modified": "2018-07-13 12:33:07+00:00", "url": "http://arxiv.org/abs/1807.05037v1", "abstract": "We introduce a new generative model for human planning under the Bayesian\nInverse Reinforcement Learning (BIRL) framework which takes into account the\nfact that humans often plan using hierarchical strategies. We describe the\nBayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of\nhierarchical planners, and use an illustrative toy model to show that BIHRL\nretains accuracy where standard BIRL fails. Furthermore, BIHRL is able to\naccurately predict the goals of `Wikispeedia' game players, with inclusion of\nhierarchical structure in the model resulting in a large boost in accuracy. We\nshow that BIHRL is able to significantly outperform BIRL even when we only have\na weak prior on the hierarchical structure of the plans available to the agent,\nand discuss the significant challenges that remain for scaling up this\nframework to more realistic settings.", "author_comment": "Presented at the first Workshop on Goal Specifications for\n  Reinforcement Learning, ICML 2018, Stockholm, Sweden", "journal_ref": "1st Workshop on Goal Specifications for Reinforcement Learning,\n  ICML 2018, Stockholm, Sweden, 2018", "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.HC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.07468": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.07468v1", "post_title": "Unsupervised Learning of Neural Networks to Explain Neural Networks", "authors": ["Quanshi Zhang", "Yu Yang", "Yuchen Liu", "Ying Nian Wu", "Song-Chun Zhu"], "date_published": "2018-05-18 23:02:14+00:00", "data_last_modified": "2018-05-18 23:02:14+00:00", "url": "http://arxiv.org/abs/1805.07468v1", "abstract": "This paper presents an unsupervised method to learn a neural network, namely\nan explainer, to interpret a pre-trained convolutional neural network (CNN),\ni.e., explaining knowledge representations hidden in middle conv-layers of the\nCNN. Given feature maps of a certain conv-layer of the CNN, the explainer\nperforms like an auto-encoder, which first disentangles the feature maps into\nobject-part features and then inverts object-part features back to features of\nhigher conv-layers of the CNN. More specifically, the explainer contains\ninterpretable conv-layers, where each filter disentangles the representation of\na specific object part from chaotic input feature maps. As a paraphrase of CNN\nfeatures, the disentangled representations of object parts help people\nunderstand the logic inside the CNN. We also learn the explainer to use\nobject-part features to reconstruct features of higher CNN layers, in order to\nminimize loss of information during the feature disentanglement. More\ncrucially, we learn the explainer via network distillation without using any\nannotations of sample labels, object parts, or textures for supervision. We\nhave applied our method to different types of CNNs for evaluation, and\nexplainers have significantly boosted the interpretability of CNN features.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1802.07740": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1802.07740v2", "post_title": "Machine Theory of Mind", "authors": ["Neil C. Rabinowitz", "Frank Perbet", "H. Francis Song", "Chiyuan Zhang", "S. M. Ali Eslami", "Matthew Botvinick"], "date_published": "2018-02-21 19:00:10+00:00", "data_last_modified": "2018-03-12 21:37:03+00:00", "url": "http://arxiv.org/abs/1802.07740v2", "abstract": "Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans'\nability to represent the mental states of others, including their desires,\nbeliefs, and intentions. We propose to train a machine to build such models\ntoo. We design a Theory of Mind neural network -- a ToMnet -- which uses\nmeta-learning to build models of the agents it encounters, from observations of\ntheir behaviour alone. Through this process, it acquires a strong prior model\nfor agents' behaviour, as well as the ability to bootstrap to richer\npredictions about agents' characteristics and mental states using only a small\nnumber of behavioural observations. We apply the ToMnet to agents behaving in\nsimple gridworld environments, showing that it learns to model random,\nalgorithmic, and deep reinforcement learning agents from varied populations,\nand that it passes classic ToM tasks such as the \"Sally-Anne\" test (Wimmer &\nPerner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold\nfalse beliefs about the world. We argue that this system -- which autonomously\nlearns how to model other agents in its world -- is an important step forward\nfor developing multi-agent AI systems, for building intermediating technology\nfor machine-human interaction, and for advancing the progress on interpretable\nAI.", "author_comment": "21 pages, 15 figures", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.10385": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.10385v4", "post_title": "A narrowing of AI research?", "authors": ["Joel Klinger", "Juan Mateos-Garcia", "Konstantinos Stathoulopoulos"], "date_published": "2020-09-22 08:23:56+00:00", "data_last_modified": "2022-01-11 06:19:32+00:00", "url": "http://arxiv.org/abs/2009.10385v4", "abstract": "The arrival of deep learning techniques able to infer patterns from large\ndatasets has dramatically improved the performance of Artificial Intelligence\n(AI) systems. Deep learning's rapid development and adoption, in great part led\nby large technology companies, has however created concerns about a premature\nnarrowing in the technological trajectory of AI research despite its\nweaknesses, which include lack of robustness, high environmental costs, and\npotentially unfair outcomes. We seek to improve the evidence base with a\nsemantic analysis of AI research in arXiv, a popular pre-prints database. We\nstudy the evolution of the thematic diversity of AI research, compare the\nthematic diversity of AI research in academia and the private sector and\nmeasure the influence of private companies in AI research through the citations\nthey receive and their collaborations with other institutions. Our results\nsuggest that diversity in AI research has stagnated in recent years, and that\nAI research involving the private sector tends to be less diverse and more\ninfluential than research in academia. We also find that private sector AI\nresearchers tend to specialise in data-hungry and computationally intensive\ndeep learning methods at the expense of research involving other AI methods,\nresearch that considers the societal and ethical implications of AI, and\napplications in sectors like health. Our results provide a rationale for policy\naction to prevent a premature narrowing of AI research that could constrain its\nsocietal benefits, but we note the informational, incentive and scale hurdles\nstanding in the way of such interventions.", "author_comment": "Fourth version: Includes substantial changes in response to reviewer\n  comments such as: alternative strategy to identify AI papers, new robustness\n  section, new analysis of private research influence, substantially modified\n  literature review and creation of technical annex", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.05695": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.05695v1", "post_title": "Evolving simple programs for playing Atari games", "authors": ["Dennis G Wilson", "Sylvain Cussat-Blanc", "Herv\u00e9 Luga", "Julian F Miller"], "date_published": "2018-06-14 18:10:46+00:00", "data_last_modified": "2018-06-14 18:10:46+00:00", "url": "http://arxiv.org/abs/1806.05695v1", "abstract": "Cartesian Genetic Programming (CGP) has previously shown capabilities in\nimage processing tasks by evolving programs with a function set specialized for\ncomputer vision. A similar approach can be applied to Atari playing. Programs\nare evolved using mixed type CGP with a function set suited for matrix\noperations, including image processing, but allowing for controller behavior to\nemerge. While the programs are relatively small, many controllers are\ncompetitive with state of the art methods for the Atari benchmark set and\nrequire less training time. By evaluating the programs of the best evolved\nindividuals, simple but effective strategies can be found.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.NE", "categories": ["cs.NE", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.05296": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.05296v3", "post_title": "Adversarial Attacks Against Medical Deep Learning Systems", "authors": ["Samuel G. Finlayson", "Hyung Won Chung", "Isaac S. Kohane", "Andrew L. Beam"], "date_published": "2018-04-15 02:33:08+00:00", "data_last_modified": "2019-02-04 06:03:22+00:00", "url": "http://arxiv.org/abs/1804.05296v3", "abstract": "The discovery of adversarial examples has raised concerns about the practical\ndeployment of deep learning systems. In this paper, we demonstrate that\nadversarial examples are capable of manipulating deep learning systems across\nthree clinical domains. For each of our representative medical deep learning\nclassifiers, both white and black box attacks were highly successful. Our\nmodels are representative of the current state of the art in medical computer\nvision and, in some cases, directly reflect architectures already seeing\ndeployment in real world clinical settings. In addition to the technical\ncontribution of our paper, we synthesize a large body of knowledge about the\nhealthcare system to argue that medicine may be uniquely susceptible to\nadversarial attacks, both in terms of monetary incentives and technical\nvulnerability. To this end, we outline the healthcare economy and the\nincentives it creates for fraud and provide concrete examples of how and why\nsuch attacks could be realistically carried out. We urge practitioners to be\naware of current vulnerabilities when deploying deep learning systems in\nclinical settings, and encourage the machine learning community to further\ninvestigate the domain-specific characteristics of medical learning systems.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CR", "categories": ["cs.CR", "cs.CY", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2202.05834": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2202.05834v1", "post_title": "Predicting Out-of-Distribution Error with the Projection Norm", "authors": ["Yaodong Yu", "Zitong Yang", "Alexander Wei", "Yi Ma", "Jacob Steinhardt"], "date_published": "2022-02-11 18:58:21+00:00", "data_last_modified": "2022-02-11 18:58:21+00:00", "url": "http://arxiv.org/abs/2202.05834v1", "abstract": "We propose a metric -- Projection Norm -- to predict a model's performance on\nout-of-distribution (OOD) data without access to ground truth labels.\nProjection Norm first uses model predictions to pseudo-label test samples and\nthen trains a new model on the pseudo-labels. The more the new model's\nparameters differ from an in-distribution model, the greater the predicted OOD\nerror. Empirically, our approach outperforms existing methods on both image and\ntext classification tasks and across different network architectures.\nTheoretically, we connect our approach to a bound on the test error for\noverparameterized linear models. Furthermore, we find that Projection Norm is\nthe only approach that achieves non-trivial detection performance on\nadversarial examples. Our code is available at\nhttps://github.com/yaodongyu/ProjNorm.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.01320": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.01320v1", "post_title": "Meta-learners' learning dynamics are unlike learners'", "authors": ["Neil C. Rabinowitz"], "date_published": "2019-05-03 18:00:26+00:00", "data_last_modified": "2019-05-03 18:00:26+00:00", "url": "http://arxiv.org/abs/1905.01320v1", "abstract": "Meta-learning is a tool that allows us to build sample-efficient learning\nsystems. Here we show that, once meta-trained, LSTM Meta-Learners aren't just\nfaster learners than their sample-inefficient deep learning (DL) and\nreinforcement learning (RL) brethren, but that they actually pursue\nfundamentally different learning trajectories. We study their learning dynamics\non three sets of structured tasks for which the corresponding learning dynamics\nof DL and RL systems have been previously described: linear regression (Saxe et\nal., 2013), nonlinear regression (Rahaman et al., 2018; Xu et al., 2018), and\ncontextual bandits (Schaul et al., 2019). In each case, while\nsample-inefficient DL and RL Learners uncover the task structure in a staggered\nmanner, meta-trained LSTM Meta-Learners uncover almost all task structure\nconcurrently, congruent with the patterns expected from Bayes-optimal inference\nalgorithms. This has implications for research areas wherever the learning\nbehaviour itself is of interest, such as safety, curriculum design, and\nhuman-in-the-loop machine learning.", "author_comment": "26 pages, 23 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.12149": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.12149v1", "post_title": "SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver", "authors": ["Po-Wei Wang", "Priya L. Donti", "Bryan Wilder", "Zico Kolter"], "date_published": "2019-05-29 00:47:35+00:00", "data_last_modified": "2019-05-29 00:47:35+00:00", "url": "http://arxiv.org/abs/1905.12149v1", "abstract": "Integrating logical reasoning within deep learning architectures has been a\nmajor goal of modern AI systems. In this paper, we propose a new direction\ntoward this goal by introducing a differentiable (smoothed) maximum\nsatisfiability (MAXSAT) solver that can be integrated into the loop of larger\ndeep learning systems. Our (approximate) solver is based upon a fast coordinate\ndescent approach to solving the semidefinite program (SDP) associated with the\nMAXSAT problem. We show how to analytically differentiate through the solution\nto this SDP and efficiently solve the associated backward pass. We demonstrate\nthat by integrating this solver into end-to-end learning systems, we can learn\nthe logical structure of challenging problems in a minimally supervised\nfashion. In particular, we show that we can learn the parity function using\nsingle-bit supervision (a traditionally hard task for deep networks) and learn\nhow to play 9x9 Sudoku solely from examples. We also solve a \"visual Sudok\"\nproblem that maps images of Sudoku puzzles to their associated logical\nsolutions by combining our MAXSAT solver with a traditional convolutional\narchitecture. Our approach thus shows promise in integrating logical structures\nwithin deep learning.", "author_comment": "Accepted at ICML'19. The code can be found at\n  https://github.com/locuslab/satnet", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.10071": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.10071v3", "post_title": "Learning Existing Social Conventions via Observationally Augmented Self-Play", "authors": ["Adam Lerer", "Alexander Peysakhovich"], "date_published": "2018-06-26 15:46:44+00:00", "data_last_modified": "2019-03-13 17:48:23+00:00", "url": "http://arxiv.org/abs/1806.10071v3", "abstract": "In order for artificial agents to coordinate effectively with people, they\nmust act consistently with existing conventions (e.g. how to navigate in\ntraffic, which language to speak, or how to coordinate with teammates). A\ngroup's conventions can be viewed as a choice of equilibrium in a coordination\ngame. We consider the problem of an agent learning a policy for a coordination\ngame in a simulated environment and then using this policy when it enters an\nexisting group. When there are multiple possible conventions we show that\nlearning a policy via multi-agent reinforcement learning (MARL) is likely to\nfind policies which achieve high payoffs at training time but fail to\ncoordinate with the real group into which the agent enters. We assume access to\na small number of samples of behavior from the true convention and show that we\ncan augment the MARL objective to help it find policies consistent with the\nreal group's convention. In three environments from the literature - traffic,\ncommunication, and team coordination - we observe that augmenting MARL with a\nsmall amount of imitation learning greatly increases the probability that the\nstrategy found by MARL fits well with the existing social convention. We show\nthat this works even in an environment where standard training methods very\nrarely find the true convention of the agent's partners.", "author_comment": "Published in AAAI-AIES2019 - Best Paper", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.GT"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.01643": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.01643v3", "post_title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems", "authors": ["Sergey Levine", "Aviral Kumar", "George Tucker", "Justin Fu"], "date_published": "2020-05-04 17:00:15+00:00", "data_last_modified": "2020-11-01 23:50:25+00:00", "url": "http://arxiv.org/abs/2005.01643v3", "abstract": "In this tutorial article, we aim to provide the reader with the conceptual\ntools needed to get started on research on offline reinforcement learning\nalgorithms: reinforcement learning algorithms that utilize previously collected\ndata, without additional online data collection. Offline reinforcement learning\nalgorithms hold tremendous promise for making it possible to turn large\ndatasets into powerful decision making engines. Effective offline reinforcement\nlearning methods would be able to extract policies with the maximum possible\nutility out of the available data, thereby allowing automation of a wide range\nof decision-making domains, from healthcare and education to robotics. However,\nthe limitations of current algorithms make this difficult. We will aim to\nprovide the reader with an understanding of these challenges, particularly in\nthe context of modern deep reinforcement learning methods, and describe some\npotential solutions that have been explored in recent work to mitigate these\nchallenges, along with recent applications, and a discussion of perspectives on\nopen problems in the field.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.10664": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.10664v2", "post_title": "Autonomous Intelligent Cyber-defense Agent (AICA) Reference Architecture. Release 2.0", "authors": ["Alexander Kott", "Paul Th\u00e9ron", "Martin Dra\u0161ar", "Edlira Dushku", "Beno\u00eet LeBlanc", "Paul Losiewicz", "Alessandro Guarino", "Luigi Mancini", "Agostino Panico", "Mauno Pihelgas", "Krzysztof Rzadca"], "date_published": "2018-03-28 14:55:53+00:00", "data_last_modified": "2019-09-18 16:17:44+00:00", "url": "http://arxiv.org/abs/1803.10664v2", "abstract": "This report - a major revision of its previous release - describes a\nreference architecture for intelligent software agents performing active,\nlargely autonomous cyber-defense actions on military networks of computing and\ncommunicating devices. The report is produced by the North Atlantic Treaty\nOrganization (NATO) Research Task Group (RTG) IST-152 \"Intelligent Autonomous\nAgents for Cyber Defense and Resilience\". In a conflict with a technically\nsophisticated adversary, NATO military tactical networks will operate in a\nheavily contested battlefield. Enemy software cyber agents - malware - will\ninfiltrate friendly networks and attack friendly command, control,\ncommunications, computers, intelligence, surveillance, and reconnaissance and\ncomputerized weapon systems. To fight them, NATO needs artificial cyber hunters\n- intelligent, autonomous, mobile agents specialized in active cyber defense.\nWith this in mind, in 2016, NATO initiated RTG IST-152. Its objective has been\nto help accelerate the development and transition to practice of such software\nagents by producing a reference architecture and technical roadmap. This report\npresents the concept and architecture of an Autonomous Intelligent\nCyber-defense Agent (AICA). We describe the rationale of the AICA concept,\nexplain the methodology and purpose that drive the definition of the AICA\nReference Architecture, and review some of the main features and challenges of\nAICAs.", "author_comment": "This is a major revision and extension of the earlier release of AICA\n  Reference Architecture", "journal_ref": null, "doi": null, "primary_category": "cs.CR", "categories": ["cs.CR"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.05068": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.05068v1", "post_title": "Social and Governance Implications of Improved Data Efficiency", "authors": ["Aaron D. Tucker", "Markus Anderljung", "Allan Dafoe"], "date_published": "2020-01-14 22:26:12+00:00", "data_last_modified": "2020-01-14 22:26:12+00:00", "url": "http://arxiv.org/abs/2001.05068v1", "abstract": "Many researchers work on improving the data efficiency of machine learning.\nWhat would happen if they succeed? This paper explores the social-economic\nimpact of increased data efficiency. Specifically, we examine the intuition\nthat data efficiency will erode the barriers to entry protecting incumbent\ndata-rich AI firms, exposing them to more competition from data-poor firms. We\nfind that this intuition is only partially correct: data efficiency makes it\neasier to create ML applications, but large AI firms may have more to gain from\nhigher performing AI systems. Further, we find that the effect on privacy, data\nmarkets, robustness, and misuse are complex. For example, while it seems\nintuitive that misuse risk would increase along with data efficiency -- as more\nactors gain access to any level of capability -- the net effect crucially\ndepends on how much defensive measures are improved. More investigation into\ndata efficiency, as well as research into the \"AI production function\", will be\nkey to understanding the development of the AI industry and its societal\nimpacts.", "author_comment": "7 pages, 2 figures, accepted to Artificial Intelligence Ethics and\n  Society 2020", "journal_ref": null, "doi": "10.1145/3375627.3375863", "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.04169": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.04169v1", "post_title": "Defense Against the Dark Arts: An overview of adversarial example security research and future research directions", "authors": ["Ian Goodfellow"], "date_published": "2018-06-11 18:22:45+00:00", "data_last_modified": "2018-06-11 18:22:45+00:00", "url": "http://arxiv.org/abs/1806.04169v1", "abstract": "This article presents a summary of a keynote lecture at the Deep Learning\nSecurity workshop at IEEE Security and Privacy 2018. This lecture summarizes\nthe state of the art in defenses against adversarial examples and provides\nrecommendations for future research directions on this topic.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1801.03737": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1801.03737v2", "post_title": "Counterfactual equivalence for POMDPs, and underlying deterministic environments", "authors": ["Stuart Armstrong"], "date_published": "2018-01-11 12:40:59+00:00", "data_last_modified": "2018-01-14 12:56:00+00:00", "url": "http://arxiv.org/abs/1801.03737v2", "abstract": "Partially Observable Markov Decision Processes (POMDPs) are rich environments\noften used in machine learning. But the issue of information and causal\nstructures in POMDPs has been relatively little studied. This paper presents\nthe concepts of equivalent and counterfactually equivalent POMDPs, where agents\ncannot distinguish which environment they are in though any observations and\nactions. It shows that any POMDP is counterfactually equivalent, for any finite\nnumber of turns, to a deterministic POMDP with all uncertainty concentrated\ninto the initial state. This allows a better understanding of POMDP\nuncertainty, information, and learning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1909.05863": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1909.05863v1", "post_title": "Finding Generalizable Evidence by Learning to Convince Q&A Models", "authors": ["Ethan Perez", "Siddharth Karamcheti", "Rob Fergus", "Jason Weston", "Douwe Kiela", "Kyunghyun Cho"], "date_published": "2019-09-12 18:00:00+00:00", "data_last_modified": "2019-09-12 18:00:00+00:00", "url": "http://arxiv.org/abs/1909.05863v1", "abstract": "We propose a system that finds the strongest supporting evidence for a given\nanswer to a question, using passage-based question-answering (QA) as a testbed.\nWe train evidence agents to select the passage sentences that most convince a\npretrained QA model of a given answer, if the QA model received those sentences\ninstead of the full passage. Rather than finding evidence that convinces one\nmodel alone, we find that agents select evidence that generalizes; agent-chosen\nevidence increases the plausibility of the supported answer, as judged by other\nQA models and humans. Given its general nature, this approach improves QA in a\nrobust manner: using agent-selected evidence (i) humans can correctly answer\nquestions with only ~20% of the full passage and (ii) QA models can generalize\nto longer passages and harder questions.", "author_comment": "EMNLP 2019. Code available at https://github.com/ethanjperez/convince", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.05379": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.05379v1", "post_title": "The Conditional Entropy Bottleneck", "authors": ["Ian Fischer"], "date_published": "2020-02-13 07:46:38+00:00", "data_last_modified": "2020-02-13 07:46:38+00:00", "url": "http://arxiv.org/abs/2002.05379v1", "abstract": "Much of the field of Machine Learning exhibits a prominent set of failure\nmodes, including vulnerability to adversarial examples, poor\nout-of-distribution (OoD) detection, miscalibration, and willingness to\nmemorize random labelings of datasets. We characterize these as failures of\nrobust generalization, which extends the traditional measure of generalization\nas accuracy or related metrics on a held-out set. We hypothesize that these\nfailures to robustly generalize are due to the learning systems retaining too\nmuch information about the training data. To test this hypothesis, we propose\nthe Minimum Necessary Information (MNI) criterion for evaluating the quality of\na model. In order to train models that perform well with respect to the MNI\ncriterion, we present a new objective function, the Conditional Entropy\nBottleneck (CEB), which is closely related to the Information Bottleneck (IB).\nWe experimentally test our hypothesis by comparing the performance of CEB\nmodels with deterministic models and Variational Information Bottleneck (VIB)\nmodels on a variety of different datasets and robustness challenges. We find\nstrong empirical evidence supporting our hypothesis that MNI models improve on\nthese problems of robust generalization.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1801.09344": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1801.09344v2", "post_title": "Certified Defenses against Adversarial Examples", "authors": ["Aditi Raghunathan", "Jacob Steinhardt", "Percy Liang"], "date_published": "2018-01-29 02:08:21+00:00", "data_last_modified": "2020-10-31 23:38:30+00:00", "url": "http://arxiv.org/abs/1801.09344v2", "abstract": "While neural networks have achieved high accuracy on standard image\nclassification benchmarks, their accuracy drops to nearly zero in the presence\nof small adversarial perturbations to test inputs. Defenses based on\nregularization and adversarial training have been proposed, but often followed\nby new, stronger attacks that defeat these defenses. Can we somehow end this\narms race? In this work, we study this problem for neural networks with one\nhidden layer. We first propose a method based on a semidefinite relaxation that\noutputs a certificate that for a given network and test input, no attack can\nforce the error to exceed a certain value. Second, as this certificate is\ndifferentiable, we jointly optimize it with the network parameters, providing\nan adaptive regularizer that encourages robustness against all attacks. On\nMNIST, our approach produces a network and a certificate that no attack that\nperturbs each pixel by at most \\epsilon = 0.1 can cause more than 35% test\nerror.", "author_comment": "Published at the International Conference on Learning Representations\n  (ICLR) 2018", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.11108": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.11108v3", "post_title": "SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards", "authors": ["Siddharth Reddy", "Anca D. Dragan", "Sergey Levine"], "date_published": "2019-05-27 10:29:31+00:00", "data_last_modified": "2019-09-25 18:44:47+00:00", "url": "http://arxiv.org/abs/1905.11108v3", "abstract": "Learning to imitate expert behavior from demonstrations can be challenging,\nespecially in environments with high-dimensional, continuous observations and\nunknown dynamics. Supervised learning methods based on behavioral cloning (BC)\nsuffer from distribution shift: because the agent greedily imitates\ndemonstrated actions, it can drift away from demonstrated states due to error\naccumulation. Recent methods based on reinforcement learning (RL), such as\ninverse RL and generative adversarial imitation learning (GAIL), overcome this\nissue by training an RL agent to match the demonstrations over a long horizon.\nSince the true reward function for the task is unknown, these methods learn a\nreward function from the demonstrations, often using complex and brittle\napproximation techniques that involve adversarial training. We propose a simple\nalternative that still uses RL, but does not require learning a reward\nfunction. The key idea is to provide the agent with an incentive to match the\ndemonstrations over a long horizon, by encouraging it to return to demonstrated\nstates upon encountering new, out-of-distribution states. We accomplish this by\ngiving the agent a constant reward of r=+1 for matching the demonstrated action\nin a demonstrated state, and a constant reward of r=0 for all other behavior.\nOur method, which we call soft Q imitation learning (SQIL), can be implemented\nwith a handful of minor modifications to any standard Q-learning or off-policy\nactor-critic algorithm. Theoretically, we show that SQIL can be interpreted as\na regularized variant of BC that uses a sparsity prior to encourage\nlong-horizon imitation. Empirically, we show that SQIL outperforms BC and\nachieves competitive results compared to GAIL, on a variety of image-based and\nlow-dimensional tasks in Box2D, Atari, and MuJoCo.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.11592": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.11592v2", "post_title": "Playing hard exploration games by watching YouTube", "authors": ["Yusuf Aytar", "Tobias Pfaff", "David Budden", "Tom Le Paine", "Ziyu Wang", "Nando de Freitas"], "date_published": "2018-05-29 17:19:36+00:00", "data_last_modified": "2018-11-30 15:59:27+00:00", "url": "http://arxiv.org/abs/1805.11592v2", "abstract": "Deep reinforcement learning methods traditionally struggle with tasks where\nenvironment rewards are particularly sparse. One successful method of guiding\nexploration in these domains is to imitate trajectories provided by a human\ndemonstrator. However, these demonstrations are typically collected under\nartificial conditions, i.e. with access to the agent's exact environment setup\nand the demonstrator's action and reward trajectories. Here we propose a\ntwo-stage method that overcomes these limitations by relying on noisy,\nunaligned footage without access to such data. First, we learn to map unaligned\nvideos from multiple sources to a common representation using self-supervised\nobjectives constructed over both time and modality (i.e. vision and sound).\nSecond, we embed a single YouTube video in this representation to construct a\nreward function that encourages an agent to imitate human gameplay. This method\nof one-shot imitation allows our agent to convincingly exceed human-level\nperformance on the infamously hard exploration games Montezuma's Revenge,\nPitfall! and Private Eye for the first time, even if the agent is not presented\nwith any environment rewards.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.05157": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.05157v4", "post_title": "Learning under Misspecified Objective Spaces", "authors": ["Andreea Bobu", "Andrea Bajcsy", "Jaime F. Fisac", "Anca D. Dragan"], "date_published": "2018-10-11 17:58:27+00:00", "data_last_modified": "2018-10-26 05:21:19+00:00", "url": "http://arxiv.org/abs/1810.05157v4", "abstract": "Learning robot objective functions from human input has become increasingly\nimportant, but state-of-the-art techniques assume that the human's desired\nobjective lies within the robot's hypothesis space. When this is not true, even\nmethods that keep track of uncertainty over the objective fail because they\nreason about which hypothesis might be correct, and not whether any of the\nhypotheses are correct. We focus specifically on learning from physical human\ncorrections during the robot's task execution, where not having a rich enough\nhypothesis space leads to the robot updating its objective in ways that the\nperson did not actually intend. We observe that such corrections appear\nirrelevant to the robot, because they are not the best way of achieving any of\nthe candidate objectives. Instead of naively trusting and learning from every\nhuman interaction, we propose robots learn conservatively by reasoning in real\ntime about how relevant the human's correction is for the robot's hypothesis\nspace. We test our inference method in an experiment with human interaction\ndata, and demonstrate that this alleviates unintended learning in an in-person\nuser study with a 7DoF robot manipulator.", "author_comment": "Conference on Robot Learning (CoRL) 2018", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.06877": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.06877v3", "post_title": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress", "authors": ["Saurabh Arora", "Prashant Doshi"], "date_published": "2018-06-18 18:26:29+00:00", "data_last_modified": "2020-11-18 18:45:24+00:00", "url": "http://arxiv.org/abs/1806.06877v3", "abstract": "Inverse reinforcement learning (IRL) is the problem of inferring the reward\nfunction of an agent, given its policy or observed behavior. Analogous to RL,\nIRL is perceived both as a problem and as a class of methods. By categorically\nsurveying the current literature in IRL, this article serves as a reference for\nresearchers and practitioners of machine learning and beyond to understand the\nchallenges of IRL and select the approaches best suited for the problem on\nhand. The survey formally introduces the IRL problem along with its central\nchallenges such as the difficulty in performing accurate inference and its\ngeneralizability, its sensitivity to prior knowledge, and the disproportionate\ngrowth in solution complexity with problem size. The article elaborates how the\ncurrent methods mitigate these challenges. We further discuss the extensions to\ntraditional IRL methods for handling: inaccurate and incomplete perception, an\nincomplete model, multiple reward functions, and nonlinear reward functions.\nThis survey concludes the discussion with some broad advances in the research\narea and currently open research questions.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.03218": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.03218v3", "post_title": "Planning With Uncertain Specifications (PUnS)", "authors": ["Ankit Shah", "Shen Li", "Julie Shah"], "date_published": "2019-06-07 16:32:16+00:00", "data_last_modified": "2020-02-25 19:48:00+00:00", "url": "http://arxiv.org/abs/1906.03218v3", "abstract": "Reward engineering is crucial to high performance in reinforcement learning\nsystems. Prior research into reward design has largely focused on Markovian\nfunctions representing the reward. While there has been research into\nexpressing non-Markov rewards as linear temporal logic (LTL) formulas, this has\nfocused on task specifications directly defined by the user. However, in many\nreal-world applications, task specifications are ambiguous, and can only be\nexpressed as a belief over LTL formulas. In this paper, we introduce planning\nwith uncertain specifications (PUnS), a novel formulation that addresses the\nchallenge posed by non-Markovian specifications expressed as beliefs over LTL\nformulas. We present four criteria that capture the semantics of satisfying a\nbelief over specifications for different applications, and analyze the\nqualitative implications of these criteria within a synthetic domain. We\ndemonstrate the existence of an equivalent Markov decision process (MDP) for\nany instance of PUnS. Finally, we demonstrate our approach on the real-world\ntask of setting a dinner table automatically with a robot that inferred task\nspecifications from human demonstrations.", "author_comment": "Accepted for publication by IEEE Robotics and Automation Letters.\n  Accepted for presentation at the 2020 IEEE International Conference on\n  Robotics and Automation", "journal_ref": null, "doi": "10.1109/LRA.2020.2977217", "primary_category": "cs.RO", "categories": ["cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.00581": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.00581v3", "post_title": "Emergent Social Learning via Multi-agent Reinforcement Learning", "authors": ["Kamal Ndousse", "Douglas Eck", "Sergey Levine", "Natasha Jaques"], "date_published": "2020-10-01 17:54:14+00:00", "data_last_modified": "2021-06-22 21:18:59+00:00", "url": "http://arxiv.org/abs/2010.00581v3", "abstract": "Social learning is a key component of human and animal intelligence. By\ntaking cues from the behavior of experts in their environment, social learners\ncan acquire sophisticated behavior and rapidly adapt to new circumstances. This\npaper investigates whether independent reinforcement learning (RL) agents in a\nmulti-agent environment can learn to use social learning to improve their\nperformance. We find that in most circumstances, vanilla model-free RL agents\ndo not use social learning. We analyze the reasons for this deficiency, and\nshow that by imposing constraints on the training environment and introducing a\nmodel-based auxiliary loss we are able to obtain generalized social learning\npolicies which enable agents to: i) discover complex skills that are not\nlearned from single-agent training, and ii) adapt online to novel environments\nby taking cues from experts present in the new environment. In contrast, agents\ntrained with model-free RL or imitation learning generalize poorly and do not\nsucceed in the transfer tasks. By mixing multi-agent and solo training, we can\nobtain agents that use social learning to gain skills that they can deploy when\nalone, even out-performing agents trained alone from the start.", "author_comment": "14 pages, 19 figures. To be published in ICML 2021", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.MA", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1507.01986": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1507.01986v1", "post_title": "Toward Idealized Decision Theory", "authors": ["Nate Soares", "Benja Fallenstein"], "date_published": "2015-07-07 23:06:59+00:00", "data_last_modified": "2015-07-07 23:06:59+00:00", "url": "http://arxiv.org/abs/1507.01986v1", "abstract": "This paper motivates the study of decision theory as necessary for aligning\nsmarter-than-human artificial systems with human interests. We discuss the\nshortcomings of two standard formulations of decision theory, and demonstrate\nthat they cannot be used to describe an idealized decision procedure suitable\nfor approximation by artificial systems. We then explore the notions of policy\nselection and logical counterfactuals, two recent insights into decision theory\nthat point the way toward promising paths for future research.", "author_comment": "This is an extended version of a paper accepted to AGI-2015", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.13916": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.13916v3", "post_title": "Unsolved Problems in ML Safety", "authors": ["Dan Hendrycks", "Nicholas Carlini", "John Schulman", "Jacob Steinhardt"], "date_published": "2021-09-28 17:59:36+00:00", "data_last_modified": "2021-12-25 19:27:40+00:00", "url": "http://arxiv.org/abs/2109.13916v3", "abstract": "Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), steering ML systems (\"Alignment\"), and\nreducing deployment hazards (\"External Safety\"). Throughout, we clarify each\nproblem's motivation and provide concrete research directions.", "author_comment": "Position Paper", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.10265": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.10265v2", "post_title": "Training verified learners with learned verifiers", "authors": ["Krishnamurthy Dvijotham", "Sven Gowal", "Robert Stanforth", "Relja Arandjelovic", "Brendan O'Donoghue", "Jonathan Uesato", "Pushmeet Kohli"], "date_published": "2018-05-25 17:35:39+00:00", "data_last_modified": "2018-05-29 09:48:05+00:00", "url": "http://arxiv.org/abs/1805.10265v2", "abstract": "This paper proposes a new algorithmic framework, predictor-verifier training,\nto train neural networks that are verifiable, i.e., networks that provably\nsatisfy some desired input-output properties. The key idea is to simultaneously\ntrain two networks: a predictor network that performs the task at hand,e.g.,\npredicting labels given inputs, and a verifier network that computes a bound on\nhow well the predictor satisfies the properties being verified. Both networks\ncan be trained simultaneously to optimize a weighted combination of the\nstandard data-fitting loss and a term that bounds the maximum violation of the\nproperty. Experiments show that not only is the predictor-verifier architecture\nable to train networks to achieve state of the art verified robustness to\nadversarial examples with much shorter training times (outperforming previous\nalgorithms on small datasets like MNIST and SVHN), but it can also be scaled to\nproduce the first known (to the best of our knowledge) verifiably robust\nnetworks for CIFAR-10.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1812.10352": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1812.10352v2", "post_title": "Learning Not to Learn: Training Deep Neural Networks with Biased Data", "authors": ["Byungju Kim", "Hyunwoo Kim", "Kyungsu Kim", "Sungjin Kim", "Junmo Kim"], "date_published": "2018-12-26 16:01:29+00:00", "data_last_modified": "2019-04-15 08:42:54+00:00", "url": "http://arxiv.org/abs/1812.10352v2", "abstract": "We propose a novel regularization algorithm to train deep neural networks, in\nwhich data at training time is severely biased. Since a neural network\nefficiently learns data distribution, a network is likely to learn the bias\ninformation to categorize input data. It leads to poor performance at test\ntime, if the bias is, in fact, irrelevant to the categorization. In this paper,\nwe formulate a regularization loss based on mutual information between feature\nembedding and bias. Based on the idea of minimizing this mutual information, we\npropose an iterative algorithm to unlearn the bias information. We employ an\nadditional network to predict the bias distribution and train the network\nadversarially against the feature embedding network. At the end of learning,\nthe bias prediction network is not able to predict the bias not because it is\npoorly trained, but because the feature embedding network successfully unlearns\nthe bias information. We also demonstrate quantitative and qualitative\nexperimental results which show that our algorithm effectively removes the bias\ninformation from feature embedding.", "author_comment": "CVPR 2019, Accepted", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1701.06049": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1701.06049v1", "post_title": "Interactive Learning from Policy-Dependent Human Feedback", "authors": ["James MacGlashan", "Mark K Ho", "Robert Loftin", "Bei Peng", "David Roberts", "Matthew E. Taylor", "Michael L. Littman"], "date_published": "2017-01-21 16:37:41+00:00", "data_last_modified": "2017-01-21 16:37:41+00:00", "url": "http://arxiv.org/abs/1701.06049v1", "abstract": "For agents and robots to become more useful, they must be able to quickly\nlearn from non-technical users. This paper investigates the problem of\ninteractively learning behaviors communicated by a human teacher using positive\nand negative feedback. Much previous work on this problem has made the\nassumption that people provide feedback for decisions that is dependent on the\nbehavior they are teaching and is independent from the learner's current\npolicy. We present empirical results that show this assumption to be\nfalse---whether human trainers give a positive or negative feedback for a\ndecision is influenced by the learner's current policy. We argue that\npolicy-dependent feedback, in addition to being commonplace, enables useful\ntraining strategies from which agents should benefit. Based on this insight, we\nintroduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning\nfrom policy-dependent feedback that converges to a local optimum. Finally, we\ndemonstrate that COACH can successfully learn multiple behaviors on a physical\nrobot, even with noisy image features.", "author_comment": "7 pages, 2 figures", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.07998": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.07998v2", "post_title": "Revisiting the Calibration of Modern Neural Networks", "authors": ["Matthias Minderer", "Josip Djolonga", "Rob Romijnders", "Frances Hubis", "Xiaohua Zhai", "Neil Houlsby", "Dustin Tran", "Mario Lucic"], "date_published": "2021-06-15 09:24:43+00:00", "data_last_modified": "2021-10-26 12:08:13+00:00", "url": "http://arxiv.org/abs/2106.07998v2", "abstract": "Accurate estimation of predictive uncertainty (model calibration) is\nessential for the safe application of neural networks. Many instances of\nmiscalibration in modern neural networks have been reported, suggesting a trend\nthat newer, more accurate models produce poorly calibrated predictions. Here,\nwe revisit this question for recent state-of-the-art image classification\nmodels. We systematically relate model calibration and accuracy, and find that\nthe most recent models, notably those not using convolutions, are among the\nbest calibrated. Trends observed in prior model generations, such as decay of\ncalibration with distribution shift or model size, are less pronounced in\nrecent architectures. We also show that model size and amount of pretraining do\nnot fully explain these differences, suggesting that architecture is a major\ndeterminant of calibration properties.", "author_comment": "35th Conference on Neural Information Processing Systems (NeurIPS\n  2021)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.08974": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.08974v3", "post_title": "Do Better ImageNet Models Transfer Better?", "authors": ["Simon Kornblith", "Jonathon Shlens", "Quoc V. Le"], "date_published": "2018-05-23 06:12:35+00:00", "data_last_modified": "2019-06-17 16:25:07+00:00", "url": "http://arxiv.org/abs/1805.08974v3", "abstract": "Transfer learning is a cornerstone of computer vision, yet little work has\nbeen done to evaluate the relationship between architecture and transfer. An\nimplicit hypothesis in modern computer vision research is that models that\nperform better on ImageNet necessarily perform better on other vision tasks.\nHowever, this hypothesis has never been systematically tested. Here, we compare\nthe performance of 16 classification networks on 12 image classification\ndatasets. We find that, when networks are used as fixed feature extractors or\nfine-tuned, there is a strong correlation between ImageNet accuracy and\ntransfer accuracy ($r = 0.99$ and $0.96$, respectively). In the former setting,\nwe find that this relationship is very sensitive to the way in which networks\nare trained on ImageNet; many common forms of regularization slightly improve\nImageNet accuracy but yield penultimate layer features that are much worse for\ntransfer learning. Additionally, we find that, on two small fine-grained image\nclassification datasets, pretraining on ImageNet provides minimal benefits,\nindicating the learned features from ImageNet do not transfer well to\nfine-grained tasks. Together, our results show that ImageNet architectures\ngeneralize well across datasets, but ImageNet features are less general than\npreviously suggested.", "author_comment": "CVPR 2019 Oral", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2011.04483": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2011.04483v1", "post_title": "A Theory of Universal Learning", "authors": ["Olivier Bousquet", "Steve Hanneke", "Shay Moran", "Ramon van Handel", "Amir Yehudayoff"], "date_published": "2020-11-09 15:10:32+00:00", "data_last_modified": "2020-11-09 15:10:32+00:00", "url": "http://arxiv.org/abs/2011.04483v1", "abstract": "How quickly can a given class of concepts be learned from examples? It is\ncommon to measure the performance of a supervised machine learning algorithm by\nplotting its \"learning curve\", that is, the decay of the error rate as a\nfunction of the number of training examples. However, the classical theoretical\nframework for understanding learnability, the PAC model of Vapnik-Chervonenkis\nand Valiant, does not explain the behavior of learning curves: the\ndistribution-free PAC model of learning can only bound the upper envelope of\nthe learning curves over all possible data distributions. This does not match\nthe practice of machine learning, where the data source is typically fixed in\nany given scenario, while the learner may choose the number of training\nexamples on the basis of factors such as computational resources and desired\naccuracy.\n  In this paper, we study an alternative learning model that better captures\nsuch practical aspects of machine learning, but still gives rise to a complete\ntheory of the learnable in the spirit of the PAC model. More precisely, we\nconsider the problem of universal learning, which aims to understand the\nperformance of learning algorithms on every data distribution, but without\nrequiring uniformity over the distribution. The main result of this paper is a\nremarkable trichotomy: there are only three possible rates of universal\nlearning. More precisely, we show that the learning curves of any given concept\nclass decay either at an exponential, linear, or arbitrarily slow rates.\nMoreover, each of these cases is completely characterized by appropriate\ncombinatorial parameters, and we exhibit optimal learning algorithms that\nachieve the best possible rate in each case.\n  For concreteness, we consider in this paper only the realizable case, though\nanalogous results are expected to extend to more general learning scenarios.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.DS", "math.ST", "stat.ML", "stat.TH"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.05635": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.05635v1", "post_title": "Self-Imitation Learning", "authors": ["Junhyuk Oh", "Yijie Guo", "Satinder Singh", "Honglak Lee"], "date_published": "2018-06-14 16:25:55+00:00", "data_last_modified": "2018-06-14 16:25:55+00:00", "url": "http://arxiv.org/abs/1806.05635v1", "abstract": "This paper proposes Self-Imitation Learning (SIL), a simple off-policy\nactor-critic algorithm that learns to reproduce the agent's past good\ndecisions. This algorithm is designed to verify our hypothesis that exploiting\npast good experiences can indirectly drive deep exploration. Our empirical\nresults show that SIL significantly improves advantage actor-critic (A2C) on\nseveral hard exploration Atari games and is competitive to the state-of-the-art\ncount-based exploration methods. We also show that SIL improves proximal policy\noptimization (PPO) on MuJoCo tasks.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1705.09990": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1705.09990v1", "post_title": "Should Robots be Obedient?", "authors": ["Smitha Milli", "Dylan Hadfield-Menell", "Anca Dragan", "Stuart Russell"], "date_published": "2017-05-28 20:51:19+00:00", "data_last_modified": "2017-05-28 20:51:19+00:00", "url": "http://arxiv.org/abs/1705.09990v1", "abstract": "Intuitively, obedience -- following the order that a human gives -- seems\nlike a good property for a robot to have. But, we humans are not perfect and we\nmay give orders that are not best aligned to our preferences. We show that when\na human is not perfectly rational then a robot that tries to infer and act\naccording to the human's underlying preferences can always perform better than\na robot that simply follows the human's literal order. Thus, there is a\ntradeoff between the obedience of a robot and the value it can attain for its\nowner. We investigate how this tradeoff is impacted by the way the robot infers\nthe human's preferences, showing that some methods err more on the side of\nobedience than others. We then analyze how performance degrades when the robot\nhas a misspecified model of the features that the human cares about or the\nlevel of rationality of the human. Finally, we study how robots can start\ndetecting such model misspecification. Overall, our work suggests that there\nmight be a middle ground in which robots intelligently decide when to obey\nhuman orders, but err on the side of obedience.", "author_comment": "Accepted to IJCAI 2017", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.11655": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.11655v4", "post_title": "Security and Privacy Issues in Deep Learning", "authors": ["Ho Bae", "Jaehee Jang", "Dahuin Jung", "Hyemi Jang", "Heonseok Ha", "Hyungyu Lee", "Sungroh Yoon"], "date_published": "2018-07-31 04:18:26+00:00", "data_last_modified": "2021-03-10 00:55:18+00:00", "url": "http://arxiv.org/abs/1807.11655v4", "abstract": "To promote secure and private artificial intelligence (SPAI), we review\nstudies on the model security and data privacy of DNNs. Model security allows\nsystem to behave as intended without being affected by malicious external\ninfluences that can compromise its integrity and efficiency. Security attacks\ncan be divided based on when they occur: if an attack occurs during training,\nit is known as a poisoning attack, and if it occurs during inference (after\ntraining) it is termed an evasion attack. Poisoning attacks compromise the\ntraining process by corrupting the data with malicious examples, while evasion\nattacks use adversarial examples to disrupt entire classification process.\nDefenses proposed against such attacks include techniques to recognize and\nremove malicious data, train a model to be insensitive to such data, and mask\nthe model's structure and parameters to render attacks more challenging to\nimplement. Furthermore, the privacy of the data involved in model training is\nalso threatened by attacks such as the model-inversion attack, or by dishonest\nservice providers of AI applications. To maintain data privacy, several\nsolutions that combine existing data-privacy techniques have been proposed,\nincluding differential privacy and modern cryptography techniques. In this\npaper, we describe the notions of some of methods, e.g., homomorphic\nencryption, and review their advantages and challenges when implemented in\ndeep-learning models.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CR", "categories": ["cs.CR", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.12387": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.12387v1", "post_title": "Agents and Devices: A Relative Definition of Agency", "authors": ["Laurent Orseau", "Simon McGregor McGill", "Shane Legg"], "date_published": "2018-05-31 09:12:14+00:00", "data_last_modified": "2018-05-31 09:12:14+00:00", "url": "http://arxiv.org/abs/1805.12387v1", "abstract": "According to Dennett, the same system may be described using a `physical'\n(mechanical) explanatory stance, or using an `intentional' (belief- and\ngoal-based) explanatory stance. Humans tend to find the physical stance more\nhelpful for certain systems, such as planets orbiting a star, and the\nintentional stance for others, such as living animals. We define a formal\ncounterpart of physical and intentional stances within computational theory: a\ndescription of a system as either a device, or an agent, with the key\ndifference being that `devices' are directly described in terms of an\ninput-output mapping, while `agents' are described in terms of the function\nthey optimise. Bayes' rule can then be applied to calculate the subjective\nprobability of a system being a device or an agent, based only on its\nbehaviour. We illustrate this using the trajectories of an object in a toy\ngrid-world domain.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.11979": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.11979v2", "post_title": "Causal Confusion in Imitation Learning", "authors": ["Pim de Haan", "Dinesh Jayaraman", "Sergey Levine"], "date_published": "2019-05-28 17:56:19+00:00", "data_last_modified": "2019-11-04 12:59:24+00:00", "url": "http://arxiv.org/abs/1905.11979v2", "abstract": "Behavioral cloning reduces policy learning to supervised learning by training\na discriminative model to predict expert actions given observations. Such\ndiscriminative models are non-causal: the training procedure is unaware of the\ncausal structure of the interaction between the expert and the environment. We\npoint out that ignoring causality is particularly damaging because of the\ndistributional shift in imitation learning. In particular, it leads to a\ncounter-intuitive \"causal misidentification\" phenomenon: access to more\ninformation can yield worse performance. We investigate how this problem\narises, and propose a solution to combat it through targeted\ninterventions---either environment interaction or expert queries---to determine\nthe correct causal model. We show that causal misidentification occurs in\nseveral benchmark control domains as well as realistic driving settings, and\nvalidate our solution against DAgger and other baselines and ablations.", "author_comment": "Published at NeurIPS 2019 9 pages, plus references and appendices", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1604.05288": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1604.05288v3", "post_title": "Inductive Coherence", "authors": ["Scott Garrabrant", "Benya Fallenstein", "Abram Demski", "Nate Soares"], "date_published": "2016-04-18 19:37:46+00:00", "data_last_modified": "2016-10-07 17:00:38+00:00", "url": "http://arxiv.org/abs/1604.05288v3", "abstract": "While probability theory is normally applied to external environments, there\nhas been some recent interest in probabilistic modeling of the outputs of\ncomputations that are too expensive to run. Since mathematical logic is a\npowerful tool for reasoning about computer programs, we consider this problem\nfrom the perspective of integrating probability and logic. Recent work on\nassigning probabilities to mathematical statements has used the concept of\ncoherent distributions, which satisfy logical constraints such as the\nprobability of a sentence and its negation summing to one. Although there are\nalgorithms which converge to a coherent probability distribution in the limit,\nthis yields only weak guarantees about finite approximations of these\ndistributions. In our setting, this is a significant limitation: Coherent\ndistributions assign probability one to all statements provable in a specific\nlogical theory, such as Peano Arithmetic, which can prove what the output of\nany terminating computation is; thus, a coherent distribution must assign\nprobability one to the output of any terminating computation. To model\nuncertainty about computations, we propose to work with approximations to\ncoherent distributions. We introduce inductive coherence, a strengthening of\ncoherence that provides appropriate constraints on finite approximations, and\npropose an algorithm which satisfies this criterion.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "math.PR"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.10019": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.10019v2", "post_title": "Adversarial Active Exploration for Inverse Dynamics Model Learning", "authors": ["Zhang-Wei Hong", "Tsu-Jui Fu", "Tzu-Yun Shann", "Yi-Hsiang Chang", "Chun-Yi Lee"], "date_published": "2018-06-26 14:33:22+00:00", "data_last_modified": "2020-03-17 03:48:55+00:00", "url": "http://arxiv.org/abs/1806.10019v2", "abstract": "We present an adversarial active exploration for inverse dynamics model\nlearning, a simple yet effective learning scheme that incentivizes exploration\nin an environment without any human intervention. Our framework consists of a\ndeep reinforcement learning (DRL) agent and an inverse dynamics model\ncontesting with each other. The former collects training samples for the\nlatter, with an objective to maximize the error of the latter. The latter is\ntrained with samples collected by the former, and generates rewards for the\nformer when it fails to predict the actual action taken by the former. In such\na competitive setting, the DRL agent learns to generate samples that the\ninverse dynamics model fails to predict correctly, while the inverse dynamics\nmodel learns to adapt to the challenging samples. We further propose a reward\nstructure that ensures the DRL agent to collect only moderately hard samples\nbut not overly hard ones that prevent the inverse model from predicting\neffectively. We evaluate the effectiveness of our method on several robotic arm\nand hand manipulation tasks against multiple baseline models. Experimental\nresults show that our method is comparable to those directly trained with\nexpert demonstrations, and superior to the other baselines even without any\nhuman priors.", "author_comment": "Published as a conference paper at CoRL 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.05761": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.05761v2", "post_title": "Attentive Neural Processes", "authors": ["Hyunjik Kim", "Andriy Mnih", "Jonathan Schwarz", "Marta Garnelo", "Ali Eslami", "Dan Rosenbaum", "Oriol Vinyals", "Yee Whye Teh"], "date_published": "2019-01-17 12:37:26+00:00", "data_last_modified": "2019-07-09 10:49:01+00:00", "url": "http://arxiv.org/abs/1901.05761v2", "abstract": "Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by\nlearning to map a context set of observed input-output pairs to a distribution\nover regression functions. Each function models the distribution of the output\ngiven an input, conditioned on the context. NPs have the benefit of fitting\nobserved data efficiently with linear complexity in the number of context\ninput-output pairs, and can learn a wide family of conditional distributions;\nthey learn predictive distributions conditioned on context sets of arbitrary\nsize. Nonetheless, we show that NPs suffer a fundamental drawback of\nunderfitting, giving inaccurate predictions at the inputs of the observed data\nthey condition on. We address this issue by incorporating attention into NPs,\nallowing each input location to attend to the relevant context points for the\nprediction. We show that this greatly improves the accuracy of predictions,\nresults in noticeably faster training, and expands the range of functions that\ncan be modelled.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.11538": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.11538v2", "post_title": "Evaluating Agents without Rewards", "authors": ["Brendon Matusch", "Jimmy Ba", "Danijar Hafner"], "date_published": "2020-12-21 18:00:39+00:00", "data_last_modified": "2021-02-09 22:06:26+00:00", "url": "http://arxiv.org/abs/2012.11538v2", "abstract": "Reinforcement learning has enabled agents to solve challenging tasks in\nunknown environments. However, manually crafting reward functions can be time\nconsuming, expensive, and error prone to human error. Competing objectives have\nbeen proposed for agents to learn without external supervision, but it has been\nunclear how well they reflect task rewards or human behavior. To accelerate the\ndevelopment of intrinsic objectives, we retrospectively compute potential\nobjectives on pre-collected datasets of agent behavior, rather than optimizing\nthem online, and compare them by analyzing their correlations. We study input\nentropy, information gain, and empowerment across seven agents, three Atari\ngames, and the 3D game Minecraft. We find that all three intrinsic objectives\ncorrelate more strongly with a human behavior similarity metric than with task\nreward. Moreover, input entropy and information gain correlate more strongly\nwith human similarity than task reward does, suggesting the use of intrinsic\nobjectives for designing agents that behave similarly to human players.", "author_comment": "15 pages, 6 figures, 5 tables", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1709.06166": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1709.06166v1", "post_title": "DropoutDAgger: A Bayesian Approach to Safe Imitation Learning", "authors": ["Kunal Menda", "Katherine Driggs-Campbell", "Mykel J. Kochenderfer"], "date_published": "2017-09-18 20:51:53+00:00", "data_last_modified": "2017-09-18 20:51:53+00:00", "url": "http://arxiv.org/abs/1709.06166v1", "abstract": "While imitation learning is becoming common practice in robotics, this\napproach often suffers from data mismatch and compounding errors. DAgger is an\niterative algorithm that addresses these issues by continually aggregating\ntraining data from both the expert and novice policies, but does not consider\nthe impact of safety. We present a probabilistic extension to DAgger, which\nuses the distribution over actions provided by the novice policy, for a given\nobservation. Our method, which we call DropoutDAgger, uses dropout to train the\nnovice as a Bayesian neural network that provides insight to its confidence.\nUsing the distribution over the novice's actions, we estimate a probabilistic\nmeasure of safety with respect to the expert action, tuned to balance\nexploration and exploitation. The utility of this approach is evaluated on the\nMuJoCo HalfCheetah and in a simple driving experiment, demonstrating improved\nperformance and safety compared to other DAgger variants and classic imitation\nlearning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.00619": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.00619v3", "post_title": "SmartChoices: Hybridizing Programming and Machine Learning", "authors": ["Victor Carbune", "Thierry Coppey", "Alexander Daryin", "Thomas Deselaers", "Nikhil Sarda", "Jay Yagnik"], "date_published": "2018-10-01 11:14:22+00:00", "data_last_modified": "2019-06-13 18:20:51+00:00", "url": "http://arxiv.org/abs/1810.00619v3", "abstract": "We present SmartChoices, an approach to making machine learning (ML) a first\nclass citizen in programming languages which we see as one way to lower the\nentrance cost to applying ML to problems in new domains. There is a growing\ndivide in approaches to building systems: on the one hand, programming\nleverages human experts to define a system while on the other hand behavior is\nlearned from data in machine learning. We propose to hybridize these two by\nproviding a 3-call API which we expose through an object called SmartChoice. We\ndescribe the SmartChoices-interface, how it can be used in programming with\nminimal code changes, and demonstrate that it is an easy to use but still\npowerful tool by demonstrating improvements over not using ML at all on three\nalgorithmic problems: binary search, QuickSort, and caches. In these three\nexamples, we replace the commonly used heuristics with an ML model entirely\nencapsulated within a SmartChoice and thus requiring minimal code changes. As\nopposed to previous work applying ML to algorithmic problems, our proposed\napproach does not require to drop existing implementations but seamlessly\nintegrates into the standard software development workflow and gives full\ncontrol to the software developer over how ML methods are applied. Our\nimplementation relies on standard Reinforcement Learning (RL) methods. To learn\nfaster, we use the heuristic function, which they are replacing, as an initial\nfunction. We show how this initial function can be used to speed up and\nstabilize learning while providing a safety net that prevents performance to\nbecome substantially worse -- allowing for a safe deployment in critical\napplications in real life.", "author_comment": "published at the Reinforcement Learning for Real Life (RL4RealLife)\n  Workshop in the 36th International Conference on Machine Learning (ICML),\n  Long Beach, California, USA, 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.PL", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1904.06387": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1904.06387v5", "post_title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations", "authors": ["Daniel S. Brown", "Wonjoon Goo", "Prabhat Nagarajan", "Scott Niekum"], "date_published": "2019-04-12 19:34:43+00:00", "data_last_modified": "2019-07-09 03:51:47+00:00", "url": "http://arxiv.org/abs/1904.06387v5", "abstract": "A critical flaw of existing inverse reinforcement learning (IRL) methods is\ntheir inability to significantly outperform the demonstrator. This is because\nIRL typically seeks a reward function that makes the demonstrator appear\nnear-optimal, rather than inferring the underlying intentions of the\ndemonstrator that may have been poorly executed in practice. In this paper, we\nintroduce a novel reward-learning-from-observation algorithm, Trajectory-ranked\nReward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately)\nranked demonstrations in order to infer high-quality reward functions from a\nset of potentially poor demonstrations. When combined with deep reinforcement\nlearning, T-REX outperforms state-of-the-art imitation learning and IRL methods\non multiple Atari and MuJoCo benchmark tasks and achieves performance that is\noften more than twice the performance of the best demonstration. We also\ndemonstrate that T-REX is robust to ranking noise and can accurately\nextrapolate intention by simply watching a learner noisily improve at a task\nover time.", "author_comment": "In proceedings of Thirty-sixth International Conference on Machine\n  Learning (ICML 2019)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.01855": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.01855v3", "post_title": "Aligning Superhuman AI with Human Behavior: Chess as a Model System", "authors": ["Reid McIlroy-Young", "Siddhartha Sen", "Jon Kleinberg", "Ashton Anderson"], "date_published": "2020-06-02 18:12:52+00:00", "data_last_modified": "2020-07-14 17:57:37+00:00", "url": "http://arxiv.org/abs/2006.01855v3", "abstract": "As artificial intelligence becomes increasingly intelligent---in some cases,\nachieving superhuman performance---there is growing potential for humans to\nlearn from and collaborate with algorithms. However, the ways in which AI\nsystems approach problems are often different from the ways people do, and thus\nmay be uninterpretable and hard to learn from. A crucial step in bridging this\ngap between human and artificial intelligence is modeling the granular actions\nthat constitute human behavior, rather than simply matching aggregate human\nperformance.\n  We pursue this goal in a model system with a long history in artificial\nintelligence: chess. The aggregate performance of a chess player unfolds as\nthey make decisions over the course of a game. The hundreds of millions of\ngames played online by players at every skill level form a rich source of data\nin which these decisions, and their exact context, are recorded in minute\ndetail. Applying existing chess engines to this data, including an open-source\nimplementation of AlphaZero, we find that they do not predict human moves well.\n  We develop and introduce Maia, a customized version of Alpha-Zero trained on\nhuman chess games, that predicts human moves at a much higher accuracy than\nexisting engines, and can achieve maximum accuracy when predicting decisions\nmade by players at a specific skill level in a tuneable way. For a dual task of\npredicting whether a human will make a large mistake on the next move, we\ndevelop a deep neural network that significantly outperforms competitive\nbaselines. Taken together, our results suggest that there is substantial\npromise in designing artificial intelligence systems with human collaboration\nin mind by first accurately modeling granular human decision-making.", "author_comment": "11 pages, 11 figure, Proceedings of the 25th ACM SIGKDD international\n  conference on Knowledge discovery and data mining, Virtual 2020", "journal_ref": null, "doi": "10.1145/3394486.3403219", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.08272": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.08272v4", "post_title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning", "authors": ["Maxime Chevalier-Boisvert", "Dzmitry Bahdanau", "Salem Lahlou", "Lucas Willems", "Chitwan Saharia", "Thien Huu Nguyen", "Yoshua Bengio"], "date_published": "2018-10-18 20:48:08+00:00", "data_last_modified": "2019-12-19 15:44:33+00:00", "url": "http://arxiv.org/abs/1810.08272v4", "abstract": "Allowing humans to interactively train artificial agents to understand\nlanguage instructions is desirable for both practical and scientific reasons,\nbut given the poor data efficiency of the current learning methods, this goal\nmay require substantial research efforts. Here, we introduce the BabyAI\nresearch platform to support investigations towards including humans in the\nloop for grounded language learning. The BabyAI platform comprises an\nextensible suite of 19 levels of increasing difficulty. The levels gradually\nlead the agent towards acquiring a combinatorially rich synthetic language\nwhich is a proper subset of English. The platform also provides a heuristic\nexpert agent for the purpose of simulating a human teacher. We report baseline\nresults and estimate the amount of human involvement that would be required to\ntrain a neural network-based agent on some of the BabyAI levels. We put forward\nstrong evidence that current deep learning methods are not yet sufficiently\nsample efficient when it comes to learning a language with compositional\nproperties.", "author_comment": "Accepted at ICLR 2019", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.09758": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.09758v3", "post_title": "Unsupervised Question Decomposition for Question Answering", "authors": ["Ethan Perez", "Patrick Lewis", "Wen-tau Yih", "Kyunghyun Cho", "Douwe Kiela"], "date_published": "2020-02-22 19:40:35+00:00", "data_last_modified": "2020-10-06 18:47:48+00:00", "url": "http://arxiv.org/abs/2002.09758v3", "abstract": "We aim to improve question answering (QA) by decomposing hard questions into\nsimpler sub-questions that existing QA systems are capable of answering. Since\nlabeling questions with decompositions is cumbersome, we take an unsupervised\napproach to produce sub-questions, also enabling us to leverage millions of\nquestions from the internet. Specifically, we propose an algorithm for One-to-N\nUnsupervised Sequence transduction (ONUS) that learns to map one hard,\nmulti-hop question to many simpler, single-hop sub-questions. We answer\nsub-questions with an off-the-shelf QA model and give the resulting answers to\na recomposition model that combines them into a final answer. We show large QA\nimprovements on HotpotQA over a strong baseline on the original, out-of-domain,\nand multi-hop dev sets. ONUS automatically learns to decompose different kinds\nof questions, while matching the utility of supervised and heuristic\ndecomposition methods for QA and exceeding those methods in fluency.\nQualitatively, we find that using sub-questions is promising for shedding light\non why a QA system makes a prediction.", "author_comment": "EMNLP 2020 Camera-Ready. Code available at\n  https://github.com/facebookresearch/UnsupervisedDecomposition", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.08512": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.08512v1", "post_title": "The Problem with Metrics is a Fundamental Problem for AI", "authors": ["Rachel Thomas", "David Uminsky"], "date_published": "2020-02-20 00:56:11+00:00", "data_last_modified": "2020-02-20 00:56:11+00:00", "url": "http://arxiv.org/abs/2002.08512v1", "abstract": "Optimizing a given metric is a central aspect of most current AI approaches,\nyet overemphasizing metrics leads to manipulation, gaming, a myopic focus on\nshort-term goals, and other unexpected negative consequences. This poses a\nfundamental contradiction for AI development. Through a series of real-world\ncase studies, we look at various aspects of where metrics go wrong in practice\nand aspects of how our online environment and current business practices are\nexacerbating these failures. Finally, we propose a framework towards mitigating\nthe harms caused by overemphasis of metrics within AI by: (1) using a slate of\nmetrics to get a fuller and more nuanced picture, (2) combining metrics with\nqualitative accounts, and (3) involving a range of stakeholders, including\nthose who will be most impacted.", "author_comment": "Accepted to EDSC (Ethics of Data Science Conference) 2020", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.03843": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.03843v2", "post_title": "Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem", "authors": ["Pedro Fernandes", "Francisco C. Santos", "Manuel Lopes"], "date_published": "2019-06-26 10:18:19+00:00", "data_last_modified": "2020-12-22 18:11:35+00:00", "url": "http://arxiv.org/abs/1907.03843v2", "abstract": "The rise of artificial intelligence (A.I.) based systems is already offering\nsubstantial benefits to the society as a whole. However, these systems may also\nenclose potential conflicts and unintended consequences. Notably, people will\ntend to adopt an A.I. system if it confers them an advantage, at which point\nnon-adopters might push for a strong regulation if that advantage for adopters\nis at a cost for them. Here we propose an agent-based game-theoretical model\nfor these conflicts, where agents may decide to resort to A.I. to use and\nacquire additional information on the payoffs of a stochastic game, striving to\nbring insights from simulation to what has been, hitherto, a mostly\nphilosophical discussion. We frame our results under the current discussion on\nethical A.I. and the conflict between individual and societal gains: the\nsocietal value alignment problem. We test the arising equilibria in the\nadoption of A.I. technology under different norms followed by artificial\nagents, their ensuing benefits, and the emergent levels of wealth inequality.\nWe show that without any regulation, purely selfish A.I. systems will have the\nstrongest advantage, even when a utilitarian A.I. provides significant benefits\nfor the individual and the society. Nevertheless, we show that it is possible\nto develop A.I. systems following human conscious policies that, when\nintroduced in society, lead to an equilibrium where the gains for the adopters\nare not at a cost for non-adopters, thus increasing the overall wealth of the\npopulation and lowering inequality. However, as shown, a self-organised\nadoption of such policies would require external regulation.", "author_comment": null, "journal_ref": "AI Communications, vol. 33, no. 3-6, pp. 155-171, 2020", "doi": "10.3233/AIC-201502", "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1707.08476": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1707.08476v1", "post_title": "Guidelines for Artificial Intelligence Containment", "authors": ["James Babcock", "Janos Kramar", "Roman V. Yampolskiy"], "date_published": "2017-07-24 18:33:18+00:00", "data_last_modified": "2017-07-24 18:33:18+00:00", "url": "http://arxiv.org/abs/1707.08476v1", "abstract": "With almost daily improvements in capabilities of artificial intelligence it\nis more important than ever to develop safety software for use by the AI\nresearch community. Building on our previous work on AI Containment Problem we\npropose a number of guidelines which should help AI safety researchers to\ndevelop reliable sandboxing software for intelligent programs of all levels.\nSuch safety container software will make it possible to study and analyze\nintelligent artificial agent while maintaining certain level of safety against\ninformation leakage, social engineering attacks and cyberattacks from within\nthe container.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CR"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2104.03113": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2104.03113v2", "post_title": "Scaling Scaling Laws with Board Games", "authors": ["Andy L. Jones"], "date_published": "2021-04-07 13:34:25+00:00", "data_last_modified": "2021-04-15 10:03:37+00:00", "url": "http://arxiv.org/abs/2104.03113v2", "abstract": "The largest experiments in machine learning now require resources far beyond\nthe budget of all but a few institutions. Fortunately, it has recently been\nshown that the results of these huge experiments can often be extrapolated from\nthe results of a sequence of far smaller, cheaper experiments. In this work, we\nshow that not only can the extrapolation be done based on the size of the\nmodel, but on the size of the problem as well. By conducting a sequence of\nexperiments using AlphaZero and Hex, we show that the performance achievable\nwith a fixed amount of compute degrades predictably as the game gets larger and\nharder. Along with our main result, we further show that the test-time and\ntrain-time compute available to an agent can be traded off while maintaining\nperformance.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1709.06275": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1709.06275v2", "post_title": "Incorrigibility in the CIRL Framework", "authors": ["Ryan Carey"], "date_published": "2017-09-19 07:23:18+00:00", "data_last_modified": "2018-06-03 17:43:18+00:00", "url": "http://arxiv.org/abs/1709.06275v2", "abstract": "A value learning system has incentives to follow shutdown instructions,\nassuming the shutdown instruction provides information (in the technical sense)\nabout which actions lead to valuable outcomes. However, this assumption is not\nrobust to model mis-specification (e.g., in the case of programmer errors). We\ndemonstrate this by presenting some Supervised POMDP scenarios in which errors\nin the parameterized reward function remove the incentive to follow shutdown\ncommands. These difficulties parallel those discussed by Soares et al. (2015)\nin their paper on corrigibility. We argue that it is important to consider\nsystems that follow shutdown commands under some weaker set of assumptions\n(e.g., that one small verified module is correctly implemented; as opposed to\nan entire prior probability distribution and/or parameterized reward function).\nWe discuss some difficulties with simple ways to attempt to attain these sorts\nof guarantees in a value learning framework.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2104.03946": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2104.03946v2", "post_title": "Learning What To Do by Simulating the Past", "authors": ["David Lindner", "Rohin Shah", "Pieter Abbeel", "Anca Dragan"], "date_published": "2021-04-08 17:43:29+00:00", "data_last_modified": "2021-05-03 10:51:40+00:00", "url": "http://arxiv.org/abs/2104.03946v2", "abstract": "Since reward functions are hard to specify, recent work has focused on\nlearning policies from human feedback. However, such approaches are impeded by\nthe expense of acquiring such feedback. Recent work proposed that agents have\naccess to a source of information that is effectively free: in any environment\nthat humans have acted in, the state will already be optimized for human\npreferences, and thus an agent can extract information about what humans want\nfrom the state. Such learning is possible in principle, but requires simulating\nall possible past trajectories that could have led to the observed state. This\nis feasible in gridworlds, but how do we scale it to complex tasks? In this\nwork, we show that by combining a learned feature encoder with learned inverse\nmodels, we can enable agents to simulate human actions backwards in time to\ninfer what they must have done. The resulting algorithm is able to reproduce a\nspecific skill in MuJoCo environments given a single state sampled from the\noptimal policy for that skill.", "author_comment": "Presented at ICLR 2021", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.09720": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.09720v1", "post_title": "Representer Point Selection for Explaining Deep Neural Networks", "authors": ["Chih-Kuan Yeh", "Joon Sik Kim", "Ian E. H. Yen", "Pradeep Ravikumar"], "date_published": "2018-11-23 22:34:17+00:00", "data_last_modified": "2018-11-23 22:34:17+00:00", "url": "http://arxiv.org/abs/1811.09720v1", "abstract": "We propose to explain the predictions of a deep neural network, by pointing\nto the set of what we call representer points in the training set, for a given\ntest point prediction. Specifically, we show that we can decompose the\npre-activation prediction of a neural network into a linear combination of\nactivations of training points, with the weights corresponding to what we call\nrepresenter values, which thus capture the importance of that training point on\nthe learned parameters of the network. But it provides a deeper understanding\nof the network than simply training point influence: with positive representer\nvalues corresponding to excitatory training points, and negative values\ncorresponding to inhibitory points, which as we show provides considerably more\ninsight. Our method is also much more scalable, allowing for real-time feedback\nin a manner not feasible with influence functions.", "author_comment": "NIPS 2018", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1909.12673": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1909.12673v2", "post_title": "A Constructive Prediction of the Generalization Error Across Scales", "authors": ["Jonathan S. Rosenfeld", "Amir Rosenfeld", "Yonatan Belinkov", "Nir Shavit"], "date_published": "2019-09-27 13:27:53+00:00", "data_last_modified": "2019-12-20 18:20:34+00:00", "url": "http://arxiv.org/abs/1909.12673v2", "abstract": "The dependency of the generalization error of neural networks on model and\ndataset size is of critical importance both in practice and for understanding\nthe theory of neural networks. Nevertheless, the functional form of this\ndependency remains elusive. In this work, we present a functional form which\napproximates well the generalization error in practice. Capitalizing on the\nsuccessful concept of model scaling (e.g., width, depth), we are able to\nsimultaneously construct such a form and specify the exact models which can\nattain it across model/data scales. Our construction follows insights obtained\nfrom observations conducted over a range of model/data scales, in various model\ntypes and datasets, in vision and language tasks. We show that the form both\nfits the observations well across scales, and provides accurate predictions\nfrom small- to large-scale models and data.", "author_comment": "ICLR 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1707.05173": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1707.05173v1", "post_title": "Trial without Error: Towards Safe Reinforcement Learning via Human Intervention", "authors": ["William Saunders", "Girish Sastry", "Andreas Stuhlmueller", "Owain Evans"], "date_published": "2017-07-17 14:13:40+00:00", "data_last_modified": "2017-07-17 14:13:40+00:00", "url": "http://arxiv.org/abs/1707.05173v1", "abstract": "AI systems are increasingly applied to complex tasks that involve interaction\nwith humans. During training, such systems are potentially dangerous, as they\nhaven't yet learned to avoid actions that could cause serious harm. How can an\nAI system explore and learn without making a single mistake that harms humans\nor otherwise causes serious damage? For model-free reinforcement learning,\nhaving a human \"in the loop\" and ready to intervene is currently the only way\nto prevent all catastrophes. We formalize human intervention for RL and show\nhow to reduce the human labor required by training a supervised learner to\nimitate the human's intervention decisions. We evaluate this scheme on Atari\ngames, with a Deep RL agent being overseen by a human for four hours. When the\nclass of catastrophes is simple, we are able to prevent all catastrophes\nwithout affecting the agent's learning (whereas an RL baseline fails due to\ncatastrophic forgetting). However, this scheme is less successful when\ncatastrophes are more complex: it reduces but does not eliminate catastrophes\nand the supervised learner fails on adversarial examples found by the agent.\nExtrapolating to more challenging environments, we show that our implementation\nwould not scale (due to the infeasible amount of human labor required). We\noutline extensions of the scheme that are necessary if we are to train\nmodel-free agents without a single catastrophe.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2105.04857": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2105.04857v1", "post_title": "Leveraging Sparse Linear Layers for Debuggable Deep Networks", "authors": ["Eric Wong", "Shibani Santurkar", "Aleksander M\u0105dry"], "date_published": "2021-05-11 08:15:25+00:00", "data_last_modified": "2021-05-11 08:15:25+00:00", "url": "http://arxiv.org/abs/2105.04857v1", "abstract": "We show how fitting sparse linear models over learned deep feature\nrepresentations can lead to more debuggable neural networks. These networks\nremain highly accurate while also being more amenable to human interpretation,\nas we demonstrate quantiatively via numerical and human experiments. We further\nillustrate how the resulting sparse explanations can help to identify spurious\ncorrelations, explain misclassifications, and diagnose model biases in vision\nand language tasks. The code for our toolkit can be found at\nhttps://github.com/madrylab/debuggabledeepnetworks.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1812.03980": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1812.03980v1", "post_title": "Building Ethically Bounded AI", "authors": ["Francesca Rossi", "Nicholas Mattei"], "date_published": "2018-12-10 18:58:05+00:00", "data_last_modified": "2018-12-10 18:58:05+00:00", "url": "http://arxiv.org/abs/1812.03980v1", "abstract": "The more AI agents are deployed in scenarios with possibly unexpected\nsituations, the more they need to be flexible, adaptive, and creative in\nachieving the goal we have given them. Thus, a certain level of freedom to\nchoose the best path to the goal is inherent in making AI robust and flexible\nenough. At the same time, however, the pervasive deployment of AI in our life,\nwhether AI is autonomous or collaborating with humans, raises several ethical\nchallenges. AI agents should be aware and follow appropriate ethical principles\nand should thus exhibit properties such as fairness or other virtues. These\nethical principles should define the boundaries of AI's freedom and creativity.\nHowever, it is still a challenge to understand how to specify and reason with\nethical boundaries in AI agents and how to combine them appropriately with\nsubjective preferences and goal specifications. Some initial attempts employ\neither a data-driven example-based approach for both, or a symbolic rule-based\napproach for both. We envision a modular approach where any AI technique can be\nused for any of these essential ingredients in decision making or decision\nsupport systems, paired with a contextual approach to define their combination\nand relative weight. In a world where neither humans nor AI systems work in\nisolation, but are tightly interconnected, e.g., the Internet of Things, we\nalso envision a compositional approach to building ethically bounded AI, where\nthe ethical properties of each component can be fruitfully exploited to derive\nthose of the overall system. In this paper we define and motivate the notion of\nethically-bounded AI, we describe two concrete examples, and we outline some\noutstanding challenges.", "author_comment": "Published at AAAI Blue Sky Track, winner of Blue Sky Award", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1710.05060": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1710.05060v2", "post_title": "Functional Decision Theory: A New Theory of Instrumental Rationality", "authors": ["Eliezer Yudkowsky", "Nate Soares"], "date_published": "2017-10-13 19:51:38+00:00", "data_last_modified": "2018-05-22 21:07:53+00:00", "url": "http://arxiv.org/abs/1710.05060v2", "abstract": "This paper describes and motivates a new decision theory known as functional\ndecision theory (FDT), as distinct from causal decision theory and evidential\ndecision theory. Functional decision theorists hold that the normative\nprinciple for action is to treat one's decision as the output of a fixed\nmathematical function that answers the question, \"Which output of this very\nfunction would yield the best outcome?\" Adhering to this principle delivers a\nnumber of benefits, including the ability to maximize wealth in an array of\ntraditional decision-theoretic and game-theoretic problems where CDT and EDT\nperform poorly. Using one simple and coherent decision rule, functional\ndecision theorists (for example) achieve more utility than CDT on Newcomb's\nproblem, more utility than EDT on the smoking lesion problem, and more utility\nthan both in Parfit's hitchhiker problem. In this paper, we define FDT, explore\nits prescriptions in a number of different decision problems, compare it to CDT\nand EDT, and give philosophical justifications for FDT as a normative theory of\ndecision-making.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.04067": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.04067v2", "post_title": "Adaptive Mechanism Design: Learning to Promote Cooperation", "authors": ["Tobias Baumann", "Thore Graepel", "John Shawe-Taylor"], "date_published": "2018-06-11 15:48:37+00:00", "data_last_modified": "2019-11-20 11:14:13+00:00", "url": "http://arxiv.org/abs/1806.04067v2", "abstract": "In the future, artificial learning agents are likely to become increasingly\nwidespread in our society. They will interact with both other learning agents\nand humans in a variety of complex settings including social dilemmas. We\nconsider the problem of how an external agent can promote cooperation between\nartificial learners by distributing additional rewards and punishments based on\nobserving the learners' actions. We propose a rule for automatically learning\nhow to create right incentives by considering the players' anticipated\nparameter updates. Using this learning rule leads to cooperation with high\nsocial welfare in matrix games in which the agents would otherwise learn to\ndefect with high probability. We show that the resulting cooperative outcome is\nstable in certain games even if the planning agent is turned off after a given\nnumber of episodes, while other games require ongoing intervention to maintain\nmutual cooperation. However, even in the latter case, the amount of necessary\nadditional incentives decreases over time.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.GT", "categories": ["cs.GT", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.03913": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.03913v3", "post_title": "Linguistic Cues of Deception in a Multilingual April Fools' Day Context", "authors": ["Katerina Papantoniou", "Panagiotis Papadakos", "Giorgos Flouris", "Dimitris Plexousakis"], "date_published": "2021-11-06 16:28:12+00:00", "data_last_modified": "2022-02-28 06:50:12+00:00", "url": "http://arxiv.org/abs/2111.03913v3", "abstract": "In this work we consider the collection of deceptive April Fools' Day(AFD)\nnews articles as a useful addition in existing datasets for deception detection\ntasks. Such collections have an established ground truth and are relatively\neasy to construct across languages. As a result, we introduce a corpus that\nincludes diachronic AFD and normal articles from Greek newspapers and news\nwebsites. On top of that, we build a rich linguistic feature set, and analyze\nand compare its deception cues with the only AFD collection currently\navailable, which is in English. Following a current research thread, we also\ndiscuss the individualism/collectivism dimension in deception with respect to\nthese two datasets. Lastly, we build classifiers by testing various monolingual\nand crosslingual settings. The results showcase that AFD datasets can be\nhelpful in deception detection studies, and are in alignment with the\nobservations of other deception detection works.", "author_comment": "Accepted for publication in the proceedings of the Eighth Italian\n  Conference on Computational Linguistics (CLIC-it 2021)", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1703.10987": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1703.10987v1", "post_title": "On the Impossibility of Supersized Machines", "authors": ["Ben Garfinkel", "Miles Brundage", "Daniel Filan", "Carrick Flynn", "Jelena Luketina", "Michael Page", "Anders Sandberg", "Andrew Snyder-Beattie", "Max Tegmark"], "date_published": "2017-03-31 17:14:39+00:00", "data_last_modified": "2017-03-31 17:14:39+00:00", "url": "http://arxiv.org/abs/1703.10987v1", "abstract": "In recent years, a number of prominent computer scientists, along with\nacademics in fields such as philosophy and physics, have lent credence to the\nnotion that machines may one day become as large as humans. Many have further\nargued that machines could even come to exceed human size by a significant\nmargin. However, there are at least seven distinct arguments that preclude this\noutcome. We show that it is not only implausible that machines will ever exceed\nhuman size, but in fact impossible.", "author_comment": "9 pages, 2 figures", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "physics.pop-ph"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.01705": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.01705v1", "post_title": "AI Ethics Statements -- Analysis and lessons learnt from NeurIPS Broader Impact Statements", "authors": ["Carolyn Ashurst", "Emmie Hine", "Paul Sedille", "Alexis Carlier"], "date_published": "2021-11-02 16:17:12+00:00", "data_last_modified": "2021-11-02 16:17:12+00:00", "url": "http://arxiv.org/abs/2111.01705v1", "abstract": "Ethics statements have been proposed as a mechanism to increase transparency\nand promote reflection on the societal impacts of published research. In 2020,\nthe machine learning (ML) conference NeurIPS broke new ground by requiring that\nall papers include a broader impact statement. This requirement was removed in\n2021, in favour of a checklist approach. The 2020 statements therefore provide\na unique opportunity to learn from the broader impact experiment: to\ninvestigate the benefits and challenges of this and similar governance\nmechanisms, as well as providing an insight into how ML researchers think about\nthe societal impacts of their own work. Such learning is needed as NeurIPS and\nother venues continue to question and adapt their policies. To enable this, we\nhave created a dataset containing the impact statements from all NeurIPS 2020\npapers, along with additional information such as affiliation type, location\nand subject area, and a simple visualisation tool for exploration. We also\nprovide an initial quantitative analysis of the dataset, covering\nrepresentation, engagement, common themes, and willingness to discuss potential\nharms alongside benefits. We investigate how these vary by geography,\naffiliation type and subject area. Drawing on these findings, we discuss the\npotential benefits and negative outcomes of ethics statement requirements, and\ntheir possible causes and associated challenges. These lead us to several\nlessons to be learnt from the 2020 requirement: (i) the importance of creating\nthe right incentives, (ii) the need for clear expectations and guidance, and\n(iii) the importance of transparency and constructive deliberation. We\nencourage other researchers to use our dataset to provide additional analysis,\nto further our understanding of how researchers responded to this requirement,\nand to investigate the benefits and challenges of this and related mechanisms.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.05502": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.05502v5", "post_title": "Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes", "authors": ["Fabian B. Fuchs", "Oliver Groth", "Adam R. Kosiorek", "Alex Bewley", "Markus Wulfmeier", "Andrea Vedaldi", "Ingmar Posner"], "date_published": "2018-06-14 12:35:50+00:00", "data_last_modified": "2019-09-06 13:49:37+00:00", "url": "http://arxiv.org/abs/1806.05502v5", "abstract": "Visually predicting the stability of block towers is a popular task in the\ndomain of intuitive physics. While previous work focusses on prediction\naccuracy, a one-dimensional performance measure, we provide a broader analysis\nof the learned physical understanding of the final model and how the learning\nprocess can be guided. To this end, we introduce neural stethoscopes as a\ngeneral purpose framework for quantifying the degree of importance of specific\nfactors of influence in deep neural networks as well as for actively promoting\nand suppressing information as appropriate. In doing so, we unify concepts from\nmultitask learning as well as training with auxiliary and adversarial losses.\nWe apply neural stethoscopes to analyse the state-of-the-art neural network for\nstability prediction. We show that the baseline model is susceptible to being\nmisled by incorrect visual cues. This leads to a performance breakdown to the\nlevel of random guessing when training on scenarios where visual cues are\ninversely correlated with stability. Using stethoscopes to promote meaningful\nfeature extraction increases performance from 51% to 90% prediction accuracy.\nConversely, training on an easy dataset where visual cues are positively\ncorrelated with stability, the baseline model learns a bias leading to poor\nperformance on a harder dataset. Using an adversarial stethoscope, the network\nis successfully de-biased, leading to a performance increase from 66% to 88%.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.11043": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.11043v1", "post_title": "One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks", "authors": ["Tianhe Yu", "Pieter Abbeel", "Sergey Levine", "Chelsea Finn"], "date_published": "2018-10-25 18:05:08+00:00", "data_last_modified": "2018-10-25 18:05:08+00:00", "url": "http://arxiv.org/abs/1810.11043v1", "abstract": "We consider the problem of learning multi-stage vision-based tasks on a real\nrobot from a single video of a human performing the task, while leveraging\ndemonstration data of subtasks with other objects. This problem presents a\nnumber of major challenges. Video demonstrations without teleoperation are easy\nfor humans to provide, but do not provide any direct supervision. Learning\npolicies from raw pixels enables full generality but calls for large function\napproximators with many parameters to be learned. Finally, compound tasks can\nrequire impractical amounts of demonstration data, when treated as a monolithic\nskill. To address these challenges, we propose a method that learns both how to\nlearn primitive behaviors from video demonstrations and how to dynamically\ncompose these behaviors to perform multi-stage tasks by \"watching\" a human\ndemonstrator. Our results on a simulated Sawyer robot and real PR2 robot\nillustrate our method for learning a variety of order fulfillment and kitchen\nserving tasks with novel objects and raw pixel inputs.", "author_comment": "Video results available at https://sites.google.com/view/one-shot-hil", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1706.03762": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1706.03762v5", "post_title": "Attention Is All You Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "date_published": "2017-06-12 17:57:34+00:00", "data_last_modified": "2017-12-06 03:30:32+00:00", "url": "http://arxiv.org/abs/1706.03762v5", "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.", "author_comment": "15 pages, 5 figures", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.01067": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.01067v4", "post_title": "Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask", "authors": ["Hattie Zhou", "Janice Lan", "Rosanne Liu", "Jason Yosinski"], "date_published": "2019-05-03 08:21:07+00:00", "data_last_modified": "2020-03-03 05:40:51+00:00", "url": "http://arxiv.org/abs/1905.01067v4", "abstract": "The recent \"Lottery Ticket Hypothesis\" paper by Frankle & Carbin showed that\na simple approach to creating sparse networks (keeping the large weights)\nresults in models that are trainable from scratch, but only when starting from\nthe same initial weights. The performance of these networks often exceeds the\nperformance of the non-sparse base model, but for reasons that were not well\nunderstood. In this paper we study the three critical components of the Lottery\nTicket (LT) algorithm, showing that each may be varied significantly without\nimpacting the overall results. Ablating these factors leads to new insights for\nwhy LT networks perform as well as they do. We show why setting weights to zero\nis important, how signs are all you need to make the reinitialized network\ntrain, and why masking behaves like training. Finally, we discover the\nexistence of Supermasks, masks that can be applied to an untrained, randomly\ninitialized network to produce a model with performance far better than chance\n(86% on MNIST, 41% on CIFAR-10).", "author_comment": "NeurIPS 2019 camera ready version", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.10243": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.10243v3", "post_title": "What Makes for Good Views for Contrastive Learning?", "authors": ["Yonglong Tian", "Chen Sun", "Ben Poole", "Dilip Krishnan", "Cordelia Schmid", "Phillip Isola"], "date_published": "2020-05-20 17:59:57+00:00", "data_last_modified": "2020-12-18 10:01:34+00:00", "url": "http://arxiv.org/abs/2005.10243v3", "abstract": "Contrastive learning between multiple views of the data has recently achieved\nstate of the art performance in the field of self-supervised representation\nlearning. Despite its success, the influence of different view choices has been\nless studied. In this paper, we use theoretical and empirical analysis to\nbetter understand the importance of view selection, and argue that we should\nreduce the mutual information (MI) between views while keeping task-relevant\ninformation intact. To verify this hypothesis, we devise unsupervised and\nsemi-supervised frameworks that learn effective views by aiming to reduce their\nMI. We also consider data augmentation as a way to reduce MI, and show that\nincreasing data augmentation indeed leads to decreasing MI and improves\ndownstream classification accuracy. As a by-product, we achieve a new\nstate-of-the-art accuracy on unsupervised pre-training for ImageNet\nclassification ($73\\%$ top-1 linear readout with a ResNet-50). In addition,\ntransferring our models to PASCAL VOC object detection and COCO instance\nsegmentation consistently outperforms supervised pre-training.\nCode:http://github.com/HobbitLong/PyContrast", "author_comment": "NeurIPS 2020. Project page: https://hobbitlong.github.io/InfoMin/", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1609.04994": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1609.04994v3", "post_title": "Exploration Potential", "authors": ["Jan Leike"], "date_published": "2016-09-16 10:55:27+00:00", "data_last_modified": "2016-11-18 11:17:56+00:00", "url": "http://arxiv.org/abs/1609.04994v3", "abstract": "We introduce exploration potential, a quantity that measures how much a\nreinforcement learning agent has explored its environment class. In contrast to\ninformation gain, exploration potential takes the problem's reward structure\ninto account. This leads to an exploration criterion that is both necessary and\nsufficient for asymptotic optimality (learning to act optimally across the\nentire environment class). Our experiments in multi-armed bandits use\nexploration potential to illustrate how different algorithms make the tradeoff\nbetween exploration and exploitation.", "author_comment": "10 pages, including proofs", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.05185": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.05185v1", "post_title": "Model Reconstruction from Model Explanations", "authors": ["Smitha Milli", "Ludwig Schmidt", "Anca D. Dragan", "Moritz Hardt"], "date_published": "2018-07-13 17:15:00+00:00", "data_last_modified": "2018-07-13 17:15:00+00:00", "url": "http://arxiv.org/abs/1807.05185v1", "abstract": "We show through theory and experiment that gradient-based explanations of a\nmodel quickly reveal the model itself. Our results speak to a tension between\nthe desire to keep a proprietary model secret and the ability to offer model\nexplanations. On the theoretical side, we give an algorithm that provably\nlearns a two-layer ReLU network in a setting where the algorithm may query the\ngradient of the model with respect to chosen inputs. The number of queries is\nindependent of the dimension and nearly optimal in its dependence on the model\nsize. Of interest not only from a learning-theoretic perspective, this result\nhighlights the power of gradients rather than labels as a learning primitive.\nComplementing our theory, we give effective heuristics for reconstructing\nmodels from gradient explanations that are orders of magnitude more\nquery-efficient than reconstruction attacks relying on prediction interfaces.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1704.02882": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1704.02882v2", "post_title": "Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning", "authors": ["El Mahdi El Mhamdi", "Rachid Guerraoui", "Hadrien Hendrikx", "Alexandre Maurer"], "date_published": "2017-04-10 14:38:37+00:00", "data_last_modified": "2017-05-22 11:01:28+00:00", "url": "http://arxiv.org/abs/1704.02882v2", "abstract": "In reinforcement learning, agents learn by performing actions and observing\ntheir outcomes. Sometimes, it is desirable for a human operator to\n\\textit{interrupt} an agent in order to prevent dangerous situations from\nhappening. Yet, as part of their learning process, agents may link these\ninterruptions, that impact their reward, to specific states and deliberately\navoid them. The situation is particularly challenging in a multi-agent context\nbecause agents might not only learn from their own past interruptions, but also\nfrom those of other agents. Orseau and Armstrong defined \\emph{safe\ninterruptibility} for one learner, but their work does not naturally extend to\nmulti-agent systems. This paper introduces \\textit{dynamic safe\ninterruptibility}, an alternative definition more suited to decentralized\nlearning problems, and studies this notion in two learning frameworks:\n\\textit{joint action learners} and \\textit{independent learners}. We give\nrealistic sufficient conditions on the learning algorithm to enable dynamic\nsafe interruptibility in the case of joint action learners, yet show that these\nconditions are not sufficient for independent learners. We show however that if\nagents can detect interruptions, it is possible to prune the observations to\nensure dynamic safe interruptibility even for independent learners.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "cs.MA", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.09170": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.09170v4", "post_title": "Realistic Evaluation of Deep Semi-Supervised Learning Algorithms", "authors": ["Avital Oliver", "Augustus Odena", "Colin Raffel", "Ekin D. Cubuk", "Ian J. Goodfellow"], "date_published": "2018-04-24 17:54:44+00:00", "data_last_modified": "2019-06-17 11:48:53+00:00", "url": "http://arxiv.org/abs/1804.09170v4", "abstract": "Semi-supervised learning (SSL) provides a powerful framework for leveraging\nunlabeled data when labels are limited or expensive to obtain. SSL algorithms\nbased on deep neural networks have recently proven successful on standard\nbenchmark tasks. However, we argue that these benchmarks fail to address many\nissues that these algorithms would face in real-world applications. After\ncreating a unified reimplementation of various widely-used SSL techniques, we\ntest them in a suite of experiments designed to address these issues. We find\nthat the performance of simple baselines which do not use unlabeled data is\noften underreported, that SSL methods differ in sensitivity to the amount of\nlabeled and unlabeled data, and that performance can degrade substantially when\nthe unlabeled dataset contains out-of-class examples. To help guide SSL\nresearch towards real-world applicability, we make our unified reimplemention\nand evaluation platform publicly available.", "author_comment": null, "journal_ref": "NeurIPS 2018 Proceedings", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.01946": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.01946v4", "post_title": "Learning to Understand Goal Specifications by Modelling Reward", "authors": ["Dzmitry Bahdanau", "Felix Hill", "Jan Leike", "Edward Hughes", "Arian Hosseini", "Pushmeet Kohli", "Edward Grefenstette"], "date_published": "2018-06-05 22:01:51+00:00", "data_last_modified": "2019-12-23 16:41:02+00:00", "url": "http://arxiv.org/abs/1806.01946v4", "abstract": "Recent work has shown that deep reinforcement-learning agents can learn to\nfollow language-like instructions from infrequent environment rewards. However,\nthis places on environment designers the onus of designing language-conditional\nreward functions which may not be easily or tractably implemented as the\ncomplexity of the environment and the language scales. To overcome this\nlimitation, we present a framework within which instruction-conditional RL\nagents are trained using rewards obtained not from the environment, but from\nreward models which are jointly trained from expert examples. As reward models\nimprove, they learn to accurately reward agents for completing tasks for\nenvironment configurations---and for instructions---not present amongst the\nexpert data. This framework effectively separates the representation of what\ninstructions require from how they can be executed. In a simple grid world, it\nenables an agent to learn a range of commands requiring interaction with blocks\nand understanding of spatial relations and underspecified abstract\narrangements. We further show the method allows our agent to adapt to changes\nin the environment without requiring new expert examples.", "author_comment": "19 pages, 9 figures", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2107.07002": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2107.07002v1", "post_title": "The Benchmark Lottery", "authors": ["Mostafa Dehghani", "Yi Tay", "Alexey A. Gritsenko", "Zhe Zhao", "Neil Houlsby", "Fernando Diaz", "Donald Metzler", "Oriol Vinyals"], "date_published": "2021-07-14 21:08:30+00:00", "data_last_modified": "2021-07-14 21:08:30+00:00", "url": "http://arxiv.org/abs/2107.07002v1", "abstract": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.01903": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.01903v2", "post_title": "Robust fine-tuning of zero-shot models", "authors": ["Mitchell Wortsman", "Gabriel Ilharco", "Jong Wook Kim", "Mike Li", "Simon Kornblith", "Rebecca Roelofs", "Raphael Gontijo Lopes", "Hannaneh Hajishirzi", "Ali Farhadi", "Hongseok Namkoong", "Ludwig Schmidt"], "date_published": "2021-09-04 17:11:28+00:00", "data_last_modified": "2022-02-25 02:29:30+00:00", "url": "http://arxiv.org/abs/2109.01903v2", "abstract": "Large pre-trained models such as CLIP or ALIGN offer consistent accuracy\nacross a range of data distributions when performing zero-shot inference (i.e.,\nwithout fine-tuning on a specific dataset). Although existing fine-tuning\nmethods substantially improve accuracy on a given target distribution, they\noften reduce robustness to distribution shifts. We address this tension by\nintroducing a simple and effective method for improving robustness while\nfine-tuning: ensembling the weights of the zero-shot and fine-tuned models\n(WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy\nimprovements under distribution shift, while preserving high accuracy on the\ntarget distribution. On ImageNet and five derived distribution shifts, WiSE-FT\nimproves accuracy under distribution shift by 4 to 6 percentage points (pp)\nover prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves\nsimilarly large robustness gains (2 to 23 pp) on a diverse set of six further\ndistribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard\nfine-tuning on seven commonly used transfer learning datasets. These\nimprovements come at no additional computational cost during fine-tuning or\ninference.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2108.02818": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2108.02818v1", "post_title": "Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications", "authors": ["Sandhini Agarwal", "Gretchen Krueger", "Jack Clark", "Alec Radford", "Jong Wook Kim", "Miles Brundage"], "date_published": "2021-08-05 19:05:57+00:00", "data_last_modified": "2021-08-05 19:05:57+00:00", "url": "http://arxiv.org/abs/2108.02818v1", "abstract": "Recently, there have been breakthroughs in computer vision (\"CV\") models that\nare more generalizable with the advent of models such as CLIP and ALIGN. In\nthis paper, we analyze CLIP and highlight some of the challenges such models\npose. CLIP reduces the need for task specific training data, potentially\nopening up many niche tasks to automation. CLIP also allows its users to\nflexibly specify image classification classes in natural language, which we\nfind can shift how biases manifest. Additionally, through some preliminary\nprobes we find that CLIP can inherit biases found in prior computer vision\nsystems. Given the wide and unpredictable domain of uses for such models, this\nraises questions regarding what sufficiently safe behaviour for such systems\nmay look like. These results add evidence to the growing body of work calling\nfor a change in the notion of a 'better' model--to move beyond simply looking\nat higher accuracy at task-oriented capability evaluations, and towards a\nbroader 'better' that takes into account deployment-critical features such as\ndifferent use contexts, and people who interact with the model when thinking\nabout model deployment.", "author_comment": "arXiv admin note: substantial text overlap with arXiv:2103.00020", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.04538": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.04538v1", "post_title": "Secure Deep Learning Engineering: A Software Quality Assurance Perspective", "authors": ["Lei Ma", "Felix Juefei-Xu", "Minhui Xue", "Qiang Hu", "Sen Chen", "Bo Li", "Yang Liu", "Jianjun Zhao", "Jianxiong Yin", "Simon See"], "date_published": "2018-10-10 14:04:08+00:00", "data_last_modified": "2018-10-10 14:04:08+00:00", "url": "http://arxiv.org/abs/1810.04538v1", "abstract": "Over the past decades, deep learning (DL) systems have achieved tremendous\nsuccess and gained great popularity in various applications, such as\nintelligent machines, image processing, speech processing, and medical\ndiagnostics. Deep neural networks are the key driving force behind its recent\nsuccess, but still seem to be a magic black box lacking interpretability and\nunderstanding. This brings up many open safety and security issues with\nenormous and urgent demands on rigorous methodologies and engineering practice\nfor quality enhancement. A plethora of studies have shown that the\nstate-of-the-art DL systems suffer from defects and vulnerabilities that can\nlead to severe loss and tragedies, especially when applied to real-world\nsafety-critical applications. In this paper, we perform a large-scale study and\nconstruct a paper repository of 223 relevant works to the quality assurance,\nsecurity, and interpretation of deep learning. We, from a software quality\nassurance perspective, pinpoint challenges and future opportunities towards\nuniversal secure deep learning engineering. We hope this work and the\naccompanied paper repository can pave the path for the software engineering\ncommunity towards addressing the pressing industrial demand of secure\nintelligent applications.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.SE", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1705.08417": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1705.08417v2", "post_title": "Reinforcement Learning with a Corrupted Reward Channel", "authors": ["Tom Everitt", "Victoria Krakovna", "Laurent Orseau", "Marcus Hutter", "Shane Legg"], "date_published": "2017-05-23 17:06:56+00:00", "data_last_modified": "2017-08-19 05:01:16+00:00", "url": "http://arxiv.org/abs/1705.08417v2", "abstract": "No real-world reward function is perfect. Sensory errors and software bugs\nmay result in RL agents observing higher (or lower) rewards than they should.\nFor example, a reinforcement learning agent may prefer states where a sensory\nerror gives it the maximum reward, but where the true reward is actually small.\nWe formalise this problem as a generalised Markov Decision Problem called\nCorrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under\nstrong simplifying assumptions and when trying to compensate for the possibly\ncorrupt rewards. Two ways around the problem are investigated. First, by giving\nthe agent richer data, such as in inverse reinforcement learning and\nsemi-supervised reinforcement learning, reward corruption stemming from\nsystematic sensory errors may sometimes be completely managed. Second, by using\nrandomisation to blunt the agent's optimisation, reward corruption can be\npartially managed under some assumptions.", "author_comment": "A shorter version of this report was accepted to IJCAI 2017 AI and\n  Autonomy track", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "stat.ML", "I.2.6; I.2.8"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.09729": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.09729v3", "post_title": "Mastering Complex Control in MOBA Games with Deep Reinforcement Learning", "authors": ["Deheng Ye", "Zhao Liu", "Mingfei Sun", "Bei Shi", "Peilin Zhao", "Hao Wu", "Hongsheng Yu", "Shaojie Yang", "Xipeng Wu", "Qingwei Guo", "Qiaobo Chen", "Yinyuting Yin", "Hao Zhang", "Tengfei Shi", "Liang Wang", "Qiang Fu", "Wei Yang", "Lanxiao Huang"], "date_published": "2019-12-20 09:56:50+00:00", "data_last_modified": "2020-12-15 14:21:40+00:00", "url": "http://arxiv.org/abs/1912.09729v3", "abstract": "We study the reinforcement learning problem of complex action control in the\nMulti-player Online Battle Arena (MOBA) 1v1 games. This problem involves far\nmore complicated state and action spaces than those of traditional 1v1 games,\nsuch as Go and Atari series, which makes it very difficult to search any\npolicies with human-level performance. In this paper, we present a deep\nreinforcement learning framework to tackle this problem from the perspectives\nof both system and algorithm. Our system is of low coupling and high\nscalability, which enables efficient explorations at large scale. Our algorithm\nincludes several novel strategies, including control dependency decoupling,\naction mask, target attention, and dual-clip PPO, with which our proposed\nactor-critic network can be effectively trained in our system. Tested on the\nMOBA game Honor of Kings, our AI agent, called Tencent Solo, can defeat top\nprofessional human players in full 1v1 games.", "author_comment": "AAAI 2020", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.01267": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.01267v1", "post_title": "Legible Normativity for AI Alignment: The Value of Silly Rules", "authors": ["Dylan Hadfield-Menell", "McKane Andrus", "Gillian K. Hadfield"], "date_published": "2018-11-03 19:09:18+00:00", "data_last_modified": "2018-11-03 19:09:18+00:00", "url": "http://arxiv.org/abs/1811.01267v1", "abstract": "It has become commonplace to assert that autonomous agents will have to be\nbuilt to follow human rules of behavior--social norms and laws. But human laws\nand norms are complex and culturally varied systems, in many cases agents will\nhave to learn the rules. This requires autonomous agents to have models of how\nhuman rule systems work so that they can make reliable predictions about rules.\nIn this paper we contribute to the building of such models by analyzing an\noverlooked distinction between important rules and what we call silly\nrules--rules with no discernible direct impact on welfare. We show that silly\nrules render a normative system both more robust and more adaptable in response\nto shocks to perceived stability. They make normativity more legible for\nhumans, and can increase legibility for AI systems as well. For AI systems to\nintegrate into human normative systems, we suggest, it may be important for\nthem to have models that include representations of silly rules.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY", "cs.HC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.10525": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.10525v4", "post_title": "Toward an AI Physicist for Unsupervised Learning", "authors": ["Tailin Wu", "Max Tegmark"], "date_published": "2018-10-24 17:59:57+00:00", "data_last_modified": "2019-09-02 01:18:25+00:00", "url": "http://arxiv.org/abs/1810.10525v4", "abstract": "We investigate opportunities and challenges for improving unsupervised\nmachine learning using four common strategies with a long history in physics:\ndivide-and-conquer, Occam's razor, unification and lifelong learning. Instead\nof using one model to learn everything, we propose a novel paradigm centered\naround the learning and manipulation of *theories*, which parsimoniously\npredict both aspects of the future (from past observations) and the domain in\nwhich these predictions are accurate. Specifically, we propose a novel\ngeneralized-mean-loss to encourage each theory to specialize in its\ncomparatively advantageous domain, and a differentiable description length\nobjective to downweight bad data and \"snap\" learned theories into simple\nsymbolic formulas. Theories are stored in a \"theory hub\", which continuously\nunifies learned theories and can propose theories when encountering new\nenvironments. We test our implementation, the toy \"AI Physicist\" learning\nagent, on a suite of increasingly complex physics environments. From\nunsupervised observation of trajectories through worlds involving random\ncombinations of gravity, electromagnetism, harmonic motion and elastic bounces,\nour agent typically learns faster and produces mean-squared prediction errors\nabout a billion times smaller than a standard feedforward neural net of\ncomparable complexity, typically recovering integer and rational theory\nparameters exactly. Our agent successfully identifies domains with different\nlaws of motion also for a nonlinear chaotic double pendulum in a piecewise\nconstant force field.", "author_comment": "Replaced to match accepted PRE version. Added references, improved\n  discussion. 22 pages, 7 figs", "journal_ref": "Phys. Rev. E 100, 033311 (2019)", "doi": "10.1103/PhysRevE.100.033311", "primary_category": "physics.comp-ph", "categories": ["physics.comp-ph", "cond-mat.dis-nn", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.11513": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.11513v1", "post_title": "Temporal Inference with Finite Factored Sets", "authors": ["Scott Garrabrant"], "date_published": "2021-09-23 17:33:30+00:00", "data_last_modified": "2021-09-23 17:33:30+00:00", "url": "http://arxiv.org/abs/2109.11513v1", "abstract": "We propose a new approach to temporal inference, inspired by the Pearlian\ncausal inference paradigm - though quite different from Pearl's approach\nformally. Rather than using directed acyclic graphs, we make use of factored\nsets, which are sets expressed as Cartesian products. We show that finite\nfactored sets are powerful tools for inferring temporal relations. We introduce\nan analog of d-separation for factored sets, conditional orthogonality, and we\ndemonstrate that this notion is equivalent to conditional independence in all\nprobability distributions on a finite factored set.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "math.CO", "math.PR"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2011.06118": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2011.06118v2", "post_title": "I Know What You Meant: Learning Human Objectives by (Under)estimating Their Choice Set", "authors": ["Ananth Jonnavittula", "Dylan P. Losey"], "date_published": "2020-11-11 23:32:13+00:00", "data_last_modified": "2021-04-05 16:43:31+00:00", "url": "http://arxiv.org/abs/2011.06118v2", "abstract": "Assistive robots have the potential to help people perform everyday tasks.\nHowever, these robots first need to learn what it is their user wants them to\ndo. Teaching assistive robots is hard for inexperienced users, elderly users,\nand users living with physical disabilities, since often these individuals are\nunable to show the robot their desired behavior. We know that inclusive\nlearners should give human teachers credit for what they cannot demonstrate.\nBut today's robots do the opposite: they assume every user is capable of\nproviding any demonstration. As a result, these robots learn to mimic the\ndemonstrated behavior, even when that behavior is not what the human really\nmeant! Here we propose a different approach to reward learning: robots that\nreason about the user's demonstrations in the context of similar or simpler\nalternatives. Unlike prior works -- which err towards overestimating the\nhuman's capabilities -- here we err towards underestimating what the human can\ninput (i.e., their choice set). Our theoretical analysis proves that\nunderestimating the human's choice set is risk-averse, with better worst-case\nperformance than overestimating. We formalize three properties to generate\nsimilar and simpler alternatives. Across simulations and a user study, our\nresulting algorithm better extrapolates the human's objective. See the user\nstudy here: https://youtu.be/RgbH2YULVRo", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2004.13654": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2004.13654v1", "post_title": "Pitfalls of learning a reward function online", "authors": ["Stuart Armstrong", "Jan Leike", "Laurent Orseau", "Shane Legg"], "date_published": "2020-04-28 16:58:58+00:00", "data_last_modified": "2020-04-28 16:58:58+00:00", "url": "http://arxiv.org/abs/2004.13654v1", "abstract": "In some agent designs like inverse reinforcement learning an agent needs to\nlearn its own reward function. Learning the reward function and optimising for\nit are typically two different processes, usually performed at different\nstages. We consider a continual (``one life'') learning approach where the\nagent both learns the reward function and optimises for it at the same time. We\nshow that this comes with a number of pitfalls, such as deliberately\nmanipulating the learning process in one direction, refusing to learn,\n``learning'' facts already known to the agent, and making decisions that are\nstrictly dominated (for all relevant reward functions). We formally introduce\ntwo desirable properties: the first is `unriggability', which prevents the\nagent from steering the learning process in the direction of a reward function\nthat is easier to optimise. The second is `uninfluenceability', whereby the\nreward-function learning process operates by learning facts about the\nenvironment. We show that an uninfluenceable process is automatically\nunriggable, and if the set of possible environments is sufficiently rich, the\nconverse is true too.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2103.03938": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2103.03938v1", "post_title": "Causal Analysis of Agent Behavior for AI Safety", "authors": ["Gr\u00e9goire D\u00e9letang", "Jordi Grau-Moya", "Miljan Martic", "Tim Genewein", "Tom McGrath", "Vladimir Mikulik", "Markus Kunesch", "Shane Legg", "Pedro A. Ortega"], "date_published": "2021-03-05 20:51:12+00:00", "data_last_modified": "2021-03-05 20:51:12+00:00", "url": "http://arxiv.org/abs/2103.03938v1", "abstract": "As machine learning systems become more powerful they also become\nincreasingly unpredictable and opaque. Yet, finding human-understandable\nexplanations of how they work is essential for their safe deployment. This\ntechnical report illustrates a methodology for investigating the causal\nmechanisms that drive the behaviour of artificial agents. Six use cases are\ncovered, each addressing a typical question an analyst might ask about an\nagent. In particular, we show that each question cannot be addressed by pure\nobservation alone, but instead requires conducting experiments with\nsystematically chosen manipulations so as to generate the correct causal\nevidence.", "author_comment": "16 pages, 16 figures, 6 tables", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.08010": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.08010v4", "post_title": "Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior", "authors": ["Siddharth Reddy", "Anca D. Dragan", "Sergey Levine"], "date_published": "2018-05-21 12:15:34+00:00", "data_last_modified": "2019-01-05 18:30:29+00:00", "url": "http://arxiv.org/abs/1805.08010v4", "abstract": "Inferring intent from observed behavior has been studied extensively within\nthe frameworks of Bayesian inverse planning and inverse reinforcement learning.\nThese methods infer a goal or reward function that best explains the actions of\nthe observed agent, typically a human demonstrator. Another agent can use this\ninferred intent to predict, imitate, or assist the human user. However, a\ncentral assumption in inverse reinforcement learning is that the demonstrator\nis close to optimal. While models of suboptimal behavior exist, they typically\nassume that suboptimal actions are the result of some type of random noise or a\nknown cognitive bias, like temporal inconsistency. In this paper, we take an\nalternative approach, and model suboptimal behavior as the result of internal\nmodel misspecification: the reason that user actions might deviate from\nnear-optimal actions is that the user has an incorrect set of beliefs about the\nrules -- the dynamics -- governing how actions affect the environment. Our\ninsight is that while demonstrated actions may be suboptimal in the real world,\nthey may actually be near-optimal with respect to the user's internal model of\nthe dynamics. By estimating these internal beliefs from observed behavior, we\narrive at a new method for inferring intent. We demonstrate in simulation and\nin a user study with 12 participants that this approach enables us to more\naccurately model human intent, and can be used in a variety of applications,\nincluding offering assistance in a shared autonomy framework and inferring\nhuman preferences.", "author_comment": "Accepted at Neural Information Processing Systems (NeurIPS) 2018", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.01036": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.01036v4", "post_title": "A Roadmap for Robust End-to-End Alignment", "authors": ["L\u00ea Nguy\u00ean Hoang"], "date_published": "2018-09-04 15:19:44+00:00", "data_last_modified": "2020-02-25 08:45:45+00:00", "url": "http://arxiv.org/abs/1809.01036v4", "abstract": "This paper discussed the {\\it robust alignment} problem, that is, the problem\nof aligning the goals of algorithms with human preferences. It presented a\ngeneral roadmap to tackle this issue. Interestingly, this roadmap identifies 5\ncritical steps, as well as many relevant aspects of these 5 steps. In other\nwords, we have presented a large number of hopefully more tractable subproblems\nthat readers are highly encouraged to tackle. Hopefully, this combination\nallows to better highlight the most pressing problems, how every expertise can\nbe best used to, and how combining the solutions to subproblems might add up to\nsolve robust alignment.", "author_comment": "21 pages, 2 figures", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2101.11038": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2101.11038v1", "post_title": "Muppet: Massive Multi-task Representations with Pre-Finetuning", "authors": ["Armen Aghajanyan", "Anchit Gupta", "Akshat Shrivastava", "Xilun Chen", "Luke Zettlemoyer", "Sonal Gupta"], "date_published": "2021-01-26 19:18:27+00:00", "data_last_modified": "2021-01-26 19:18:27+00:00", "url": "http://arxiv.org/abs/2101.11038v1", "abstract": "We propose pre-finetuning, an additional large-scale learning stage between\nlanguage model pre-training and fine-tuning. Pre-finetuning is massively\nmulti-task learning (around 50 datasets, over 4.8 million total labeled\nexamples), and is designed to encourage learning of representations that\ngeneralize better to many different tasks. We show that pre-finetuning\nconsistently improves performance for pretrained discriminators (e.g.~RoBERTa)\nand generation models (e.g.~BART) on a wide range of tasks (sentence\nprediction, commonsense reasoning, MRC, etc.), while also significantly\nimproving sample efficiency during fine-tuning. We also show that large-scale\nmulti-tasking is crucial; pre-finetuning can hurt performance when few tasks\nare used up until a critical point (usually above 15) after which performance\nimproves linearly in the number of tasks.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.10729": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.10729v5", "post_title": "Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation", "authors": ["Niels Justesen", "Ruben Rodriguez Torrado", "Philip Bontrager", "Ahmed Khalifa", "Julian Togelius", "Sebastian Risi"], "date_published": "2018-06-28 01:16:11+00:00", "data_last_modified": "2018-11-29 18:10:13+00:00", "url": "http://arxiv.org/abs/1806.10729v5", "abstract": "Deep reinforcement learning (RL) has shown impressive results in a variety of\ndomains, learning directly from high-dimensional sensory streams. However, when\nneural networks are trained in a fixed environment, such as a single level in a\nvideo game, they will usually overfit and fail to generalize to new levels.\nWhen RL models overfit, even slight modifications to the environment can result\nin poor agent performance. This paper explores how procedurally generated\nlevels during training can increase generality. We show that for some games\nprocedural level generation enables generalization to new levels within the\nsame distribution. Additionally, it is possible to achieve better performance\nwith less data by manipulating the difficulty of the levels in response to the\nperformance of the agent. The generality of the learned behaviors is also\nevaluated on a set of human-designed levels. The results suggest that the\nability to generalize to human-designed levels highly depends on the design of\nthe level generators. We apply dimensionality reduction and clustering\ntechniques to visualize the generators' distributions of levels and analyze to\nwhat degree they can produce levels similar to those designed by a human.", "author_comment": "Accepted to NeurIPS Deep RL Workshop 2018", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.06995": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.06995v1", "post_title": "Interpretable Reinforcement Learning with Ensemble Methods", "authors": ["Alexander Brown", "Marek Petrik"], "date_published": "2018-09-19 03:23:35+00:00", "data_last_modified": "2018-09-19 03:23:35+00:00", "url": "http://arxiv.org/abs/1809.06995v1", "abstract": "We propose to use boosted regression trees as a way to compute\nhuman-interpretable solutions to reinforcement learning problems. Boosting\ncombines several regression trees to improve their accuracy without\nsignificantly reducing their inherent interpretability. Prior work has focused\nindependently on reinforcement learning and on interpretable machine learning,\nbut there has been little progress in interpretable reinforcement learning. Our\nexperimental results show that boosted regression trees compute solutions that\nare both interpretable and match the quality of leading reinforcement learning\nmethods.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2008.12623": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2008.12623v2", "post_title": "From Optimizing Engagement to Measuring Value", "authors": ["Smitha Milli", "Luca Belli", "Moritz Hardt"], "date_published": "2020-08-21 03:10:45+00:00", "data_last_modified": "2021-07-19 16:32:49+00:00", "url": "http://arxiv.org/abs/2008.12623v2", "abstract": "Most recommendation engines today are based on predicting user engagement,\ne.g. predicting whether a user will click on an item or not. However, there is\npotentially a large gap between engagement signals and a desired notion of\n\"value\" that is worth optimizing for. We use the framework of measurement\ntheory to (a) confront the designer with a normative question about what the\ndesigner values, (b) provide a general latent variable model approach that can\nbe used to operationalize the target construct and directly optimize for it,\nand (c) guide the designer in evaluating and revising their operationalization.\nWe implement our approach on the Twitter platform on millions of users. In line\nwith established approaches to assessing the validity of measurements, we\nperform a qualitative evaluation of how well our model captures a desired\nnotion of \"value\".", "author_comment": "Published at FAccT'21", "journal_ref": null, "doi": "10.1145/3442188.3445933", "primary_category": "cs.SI", "categories": ["cs.SI", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.02096": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.02096v2", "post_title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design", "authors": ["Michael Dennis", "Natasha Jaques", "Eugene Vinitsky", "Alexandre Bayen", "Stuart Russell", "Andrew Critch", "Sergey Levine"], "date_published": "2020-12-03 17:37:01+00:00", "data_last_modified": "2021-02-04 03:01:31+00:00", "url": "http://arxiv.org/abs/2012.02096v2", "abstract": "A wide range of reinforcement learning (RL) problems - including robustness,\ntransfer learning, unsupervised RL, and emergent complexity - require\nspecifying a distribution of tasks or environments in which a policy will be\ntrained. However, creating a useful distribution of environments is error\nprone, and takes a significant amount of developer time and effort. We propose\nUnsupervised Environment Design (UED) as an alternative paradigm, where\ndevelopers provide environments with unknown parameters, and these parameters\nare used to automatically produce a distribution over valid, solvable\nenvironments. Existing approaches to automatically generating environments\nsuffer from common failure modes: domain randomization cannot generate\nstructure or adapt the difficulty of the environment to the agent's learning\nprogress, and minimax adversarial training leads to worst-case environments\nthat are often unsolvable. To generate structured, solvable environments for\nour protagonist agent, we introduce a second, antagonist agent that is allied\nwith the environment-generating adversary. The adversary is motivated to\ngenerate environments which maximize regret, defined as the difference between\nthe protagonist and antagonist agent's return. We call our technique\nProtagonist Antagonist Induced Regret Environment Design (PAIRED). Our\nexperiments demonstrate that PAIRED produces a natural curriculum of\nincreasingly complex environments, and PAIRED agents achieve higher zero-shot\ntransfer performance when tested in highly novel environments.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.02872": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.02872v2", "post_title": "Feedback in Imitation Learning: The Three Regimes of Covariate Shift", "authors": ["Jonathan Spencer", "Sanjiban Choudhury", "Arun Venkatraman", "Brian Ziebart", "J. Andrew Bagnell"], "date_published": "2021-02-04 20:18:56+00:00", "data_last_modified": "2021-02-11 14:55:12+00:00", "url": "http://arxiv.org/abs/2102.02872v2", "abstract": "Imitation learning practitioners have often noted that conditioning policies\non previous actions leads to a dramatic divergence between \"held out\" error and\nperformance of the learner in situ. Interactive approaches can provably address\nthis divergence but require repeated querying of a demonstrator. Recent work\nidentifies this divergence as stemming from a \"causal confound\" in predicting\nthe current action, and seek to ablate causal aspects of current state using\ntools from causal inference. In this work, we argue instead that this\ndivergence is simply another manifestation of covariate shift, exacerbated\nparticularly by settings of feedback between decisions and input features. The\nlearner often comes to rely on features that are strongly predictive of\ndecisions, but are subject to strong covariate shift.\n  Our work demonstrates a broad class of problems where this shift can be\nmitigated, both theoretically and practically, by taking advantage of a\nsimulator but without any further querying of expert demonstration. We analyze\nexisting benchmarks used to test imitation learning approaches and find that\nthese benchmarks are realizable and simple and thus insufficient for capturing\nthe harder regimes of error compounding seen in real-world decision making\nproblems. We find, in a surprising contrast with previous literature, but\nconsistent with our theory, that naive behavioral cloning provides excellent\nresults. We detail the need for new standardized benchmarks that capture the\nphenomena seen in robotics problems.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1908.07613": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1908.07613v3", "post_title": "Implications of Quantum Computing for Artificial Intelligence alignment research", "authors": ["Jaime Sevilla", "Pablo Moreno"], "date_published": "2019-08-19 17:53:34+00:00", "data_last_modified": "2019-08-24 14:19:04+00:00", "url": "http://arxiv.org/abs/1908.07613v3", "abstract": "We explain some key features of quantum computing via three heuristics and\napply them to argue that a deep understanding of quantum computing is unlikely\nto be helpful to address current bottlenecks in Artificial Intelligence\nAlignment. Our argument relies on the claims that Quantum Computing leads to\ncompute overhang instead of algorithmic overhang, and that the difficulties\nassociated with the measurement of quantum states do not invalidate any major\nassumptions of current Artificial Intelligence Alignment research agendas. We\nalso discuss tripwiring, adversarial blinding, informed oversight and side\neffects as possible exceptions.", "author_comment": "10 pages", "journal_ref": null, "doi": null, "primary_category": "cs.ET", "categories": ["cs.ET", "cs.AI", "cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2104.08691": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2104.08691v2", "post_title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "authors": ["Brian Lester", "Rami Al-Rfou", "Noah Constant"], "date_published": "2021-04-18 03:19:26+00:00", "data_last_modified": "2021-09-02 17:34:41+00:00", "url": "http://arxiv.org/abs/2104.08691v2", "abstract": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism\nfor learning \"soft prompts\" to condition frozen language models to perform\nspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft\nprompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Our end-to-end learned approach\noutperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably,\nthrough ablations on model size using T5, we show that prompt tuning becomes\nmore competitive with scale: as models exceed billions of parameters, our\nmethod \"closes the gap\" and matches the strong performance of model tuning\n(where all model weights are tuned). This finding is especially relevant in\nthat large models are costly to share and serve, and the ability to reuse one\nfrozen model for multiple downstream tasks can ease this burden. Our method can\nbe seen as a simplification of the recently proposed \"prefix tuning\" of Li and\nLiang (2021), and we provide a comparison to this and other similar approaches.\nFinally, we show that conditioning a frozen model with soft prompts confers\nbenefits in robustness to domain transfer, as compared to full model tuning.", "author_comment": "Accepted to EMNLP 2021", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1605.03142": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1605.03142v1", "post_title": "Self-Modification of Policy and Utility Function in Rational Agents", "authors": ["Tom Everitt", "Daniel Filan", "Mayank Daswani", "Marcus Hutter"], "date_published": "2016-05-10 18:25:49+00:00", "data_last_modified": "2016-05-10 18:25:49+00:00", "url": "http://arxiv.org/abs/1605.03142v1", "abstract": "Any agent that is part of the environment it interacts with and has versatile\nactuators (such as arms and fingers), will in principle have the ability to\nself-modify -- for example by changing its own source code. As we continue to\ncreate more and more intelligent agents, chances increase that they will learn\nabout this ability. The question is: will they want to use it? For example,\nhighly intelligent systems may find ways to change their goals to something\nmore easily achievable, thereby `escaping' the control of their designers. In\nan important paper, Omohundro (2008) argued that goal preservation is a\nfundamental drive of any intelligent system, since a goal is more likely to be\nachieved if future versions of the agent strive towards the same goal. In this\npaper, we formalise this argument in general reinforcement learning, and\nexplore situations where it fails. Our conclusion is that the self-modification\npossibility is harmless if and only if the value function of the agent\nanticipates the consequences of self-modifications and use the current utility\nfunction when evaluating the future.", "author_comment": "Artificial General Intelligence (AGI) 2016", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1812.02795": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1812.02795v1", "post_title": "Verification of deep probabilistic models", "authors": ["Krishnamurthy Dvijotham", "Marta Garnelo", "Alhussein Fawzi", "Pushmeet Kohli"], "date_published": "2018-12-06 20:38:19+00:00", "data_last_modified": "2018-12-06 20:38:19+00:00", "url": "http://arxiv.org/abs/1812.02795v1", "abstract": "Probabilistic models are a critical part of the modern deep learning toolbox\n- ranging from generative models (VAEs, GANs), sequence to sequence models used\nin machine translation and speech processing to models over functional spaces\n(conditional neural processes, neural processes). Given the size and complexity\nof these models, safely deploying them in applications requires the development\nof tools to analyze their behavior rigorously and provide some guarantees that\nthese models are consistent with a list of desirable properties or\nspecifications. For example, a machine translation model should produce\nsemantically equivalent outputs for innocuous changes in the input to the\nmodel. A functional regression model that is learning a distribution over\nmonotonic functions should predict a larger value at a larger input.\nVerification of these properties requires a new framework that goes beyond\nnotions of verification studied in deterministic feedforward networks, since\nrequiring worst-case guarantees in probabilistic models is likely to produce\nconservative or vacuous results. We propose a novel formulation of verification\nfor deep probabilistic models that take in conditioning inputs and sample\nlatent variables in the course of producing an output: We require that the\noutput of the model satisfies a linear constraint with high probability over\nthe sampling of latent variables and for every choice of conditioning input to\nthe model. We show that rigorous lower bounds on the probability that the\nconstraint is satisfied can be obtained efficiently. Experiments with neural\nprocesses show that several properties of interest while modeling functional\nspaces can be modeled within this framework (monotonicity, convexity) and\nverified efficiently using our algorithms", "author_comment": "Accepted to NeurIPS 2018 Workshop on Security in Machine Learning", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1911.05722": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1911.05722v3", "post_title": "Momentum Contrast for Unsupervised Visual Representation Learning", "authors": ["Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross Girshick"], "date_published": "2019-11-13 18:53:26+00:00", "data_last_modified": "2020-03-23 18:36:55+00:00", "url": "http://arxiv.org/abs/1911.05722v3", "abstract": "We present Momentum Contrast (MoCo) for unsupervised visual representation\nlearning. From a perspective on contrastive learning as dictionary look-up, we\nbuild a dynamic dictionary with a queue and a moving-averaged encoder. This\nenables building a large and consistent dictionary on-the-fly that facilitates\ncontrastive unsupervised learning. MoCo provides competitive results under the\ncommon linear protocol on ImageNet classification. More importantly, the\nrepresentations learned by MoCo transfer well to downstream tasks. MoCo can\noutperform its supervised pre-training counterpart in 7 detection/segmentation\ntasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large\nmargins. This suggests that the gap between unsupervised and supervised\nrepresentation learning has been largely closed in many vision tasks.", "author_comment": "CVPR 2020 camera-ready. Code:\n  https://github.com/facebookresearch/moco", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1512.05832": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1512.05832v1", "post_title": "Learning the Preferences of Ignorant, Inconsistent Agents", "authors": ["Owain Evans", "Andreas Stuhlmueller", "Noah D. Goodman"], "date_published": "2015-12-18 00:24:08+00:00", "data_last_modified": "2015-12-18 00:24:08+00:00", "url": "http://arxiv.org/abs/1512.05832v1", "abstract": "An important use of machine learning is to learn what people value. What\nposts or photos should a user be shown? Which jobs or activities would a person\nfind rewarding? In each case, observations of people's past choices can inform\nour inferences about their likes and preferences. If we assume that choices are\napproximately optimal according to some utility function, we can treat\npreference inference as Bayesian inverse planning. That is, given a prior on\nutility functions and some observed choices, we invert an optimal\ndecision-making process to infer a posterior distribution on utility functions.\nHowever, people often deviate from approximate optimality. They have false\nbeliefs, their planning is sub-optimal, and their choices may be temporally\ninconsistent due to hyperbolic discounting and other biases. We demonstrate how\nto incorporate these deviations into algorithms for preference inference by\nconstructing generative models of planning for agents who are subject to false\nbeliefs and time inconsistency. We explore the inferences these models make\nabout preferences, beliefs, and biases. We present a behavioral experiment in\nwhich human subjects perform preference inference given the same observations\nof choices as our model. Results show that human subjects (like our model)\nexplain choices in terms of systematic deviations from optimal behavior and\nsuggest that they take such deviations into account when inferring preferences.", "author_comment": "AAAI 2016", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1411.1373": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1411.1373v9", "post_title": "Ethical Artificial Intelligence", "authors": ["Bill Hibbard"], "date_published": "2014-11-05 19:40:02+00:00", "data_last_modified": "2015-11-17 20:54:38+00:00", "url": "http://arxiv.org/abs/1411.1373v9", "abstract": "This book-length article combines several peer reviewed papers and new\nmaterial to analyze the issues of ethical artificial intelligence (AI). The\nbehavior of future AI systems can be described by mathematical equations, which\nare adapted to analyze possible unintended AI behaviors and ways that AI\ndesigns can avoid them. This article makes the case for utility-maximizing\nagents and for avoiding infinite sets in agent definitions. It shows how to\navoid agent self-delusion using model-based utility functions and how to avoid\nagents that corrupt their reward generators (sometimes called \"perverse\ninstantiation\") using utility functions that evaluate outcomes at one point in\ntime from the perspective of humans at a different point in time. It argues\nthat agents can avoid unintended instrumental actions (sometimes called \"basic\nAI drives\" or \"instrumental goals\") by accurately learning human values. This\narticle defines a self-modeling agent framework and shows how it can avoid\nproblems of resource limits, being predicted by other agents, and inconsistency\nbetween the agent's utility function and its definition (one version of this\nproblem is sometimes called \"motivated value selection\"). This article also\ndiscusses how future AI will differ from current AI, the politics of AI, and\nthe ultimate use of AI to help understand the nature of the universe and our\nplace in it.", "author_comment": "minor edit: remove page break between Figure 10.2 and its caption", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.08479": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.08479v1", "post_title": "Human-Interactive Subgoal Supervision for Efficient Inverse Reinforcement Learning", "authors": ["Xinlei Pan", "Eshed Ohn-Bar", "Nicholas Rhinehart", "Yan Xu", "Yilin Shen", "Kris M. Kitani"], "date_published": "2018-06-22 03:24:00+00:00", "data_last_modified": "2018-06-22 03:24:00+00:00", "url": "http://arxiv.org/abs/1806.08479v1", "abstract": "Humans are able to understand and perform complex tasks by strategically\nstructuring the tasks into incremental steps or subgoals. For a robot\nattempting to learn to perform a sequential task with critical subgoal states,\nsuch states can provide a natural opportunity for interaction with a human\nexpert. This paper analyzes the benefit of incorporating a notion of subgoals\ninto Inverse Reinforcement Learning (IRL) with a Human-In-The-Loop (HITL)\nframework. The learning process is interactive, with a human expert first\nproviding input in the form of full demonstrations along with some subgoal\nstates. These subgoal states define a set of subtasks for the learning agent to\ncomplete in order to achieve the final goal. The learning agent queries for\npartial demonstrations corresponding to each subtask as needed when the agent\nstruggles with the subtask. The proposed Human Interactive IRL (HI-IRL)\nframework is evaluated on several discrete path-planning tasks. We demonstrate\nthat subgoal-based interactive structuring of the learning task results in\nsignificantly more efficient learning, requiring only a fraction of the\ndemonstration data needed for learning the underlying reward function with the\nbaseline IRL model.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.HC", "categories": ["cs.HC", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.08606": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.08606v1", "post_title": "Zero-Shot Visual Imitation", "authors": ["Deepak Pathak", "Parsa Mahmoudieh", "Guanghao Luo", "Pulkit Agrawal", "Dian Chen", "Yide Shentu", "Evan Shelhamer", "Jitendra Malik", "Alexei A. Efros", "Trevor Darrell"], "date_published": "2018-04-23 17:58:26+00:00", "data_last_modified": "2018-04-23 17:58:26+00:00", "url": "http://arxiv.org/abs/1804.08606v1", "abstract": "The current dominant paradigm for imitation learning relies on strong\nsupervision of expert actions to learn both 'what' and 'how' to imitate. We\npursue an alternative paradigm wherein an agent first explores the world\nwithout any expert supervision and then distills its experience into a\ngoal-conditioned skill policy with a novel forward consistency loss. In our\nframework, the role of the expert is only to communicate the goals (i.e., what\nto imitate) during inference. The learned policy is then employed to mimic the\nexpert (i.e., how to imitate) after seeing just a sequence of images\ndemonstrating the desired task. Our method is 'zero-shot' in the sense that the\nagent never has access to expert actions during training or for the task\ndemonstration at inference. We evaluate our zero-shot imitator in two\nreal-world settings: complex rope manipulation with a Baxter robot and\nnavigation in previously unseen office environments with a TurtleBot. Through\nfurther experiments in VizDoom simulation, we provide evidence that better\nmechanisms for exploration lead to learning a more capable policy which in turn\nimproves end task performance. Videos, models, and more details are available\nat https://pathak22.github.io/zeroshot-imitation/", "author_comment": "Oral presentation at ICLR 2018. Website at\n  https://pathak22.github.io/zeroshot-imitation/", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1711.09883": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1711.09883v2", "post_title": "AI Safety Gridworlds", "authors": ["Jan Leike", "Miljan Martic", "Victoria Krakovna", "Pedro A. Ortega", "Tom Everitt", "Andrew Lefrancq", "Laurent Orseau", "Shane Legg"], "date_published": "2017-11-27 18:57:13+00:00", "data_last_modified": "2017-11-28 17:40:36+00:00", "url": "http://arxiv.org/abs/1711.09883v2", "abstract": "We present a suite of reinforcement learning environments illustrating\nvarious safety properties of intelligent agents. These problems include safe\ninterruptibility, avoiding side effects, absent supervisor, reward gaming, safe\nexploration, as well as robustness to self-modification, distributional shift,\nand adversaries. To measure compliance with the intended safe behavior, we\nequip each environment with a performance function that is hidden from the\nagent. This allows us to categorize AI safety problems into robustness and\nspecification problems, depending on whether the performance function\ncorresponds to the observed reward function. We evaluate A2C and Rainbow, two\nrecent deep reinforcement learning agents, on our environments and show that\nthey are not able to solve them satisfactorily.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.03060": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.03060v3", "post_title": "Active Inverse Reward Design", "authors": ["S\u00f6ren Mindermann", "Rohin Shah", "Adam Gleave", "Dylan Hadfield-Menell"], "date_published": "2018-09-09 23:30:59+00:00", "data_last_modified": "2019-11-06 17:41:15+00:00", "url": "http://arxiv.org/abs/1809.03060v3", "abstract": "Designers of AI agents often iterate on the reward function in a\ntrial-and-error process until they get the desired behavior, but this only\nguarantees good behavior in the training environment. We propose structuring\nthis process as a series of queries asking the user to compare between\ndifferent reward functions. Thus we can actively select queries for maximum\ninformativeness about the true reward. In contrast to approaches asking the\ndesigner for optimal behavior, this allows us to gather additional information\nby eliciting preferences between suboptimal behaviors. After each query, we\nneed to update the posterior over the true reward function from observing the\nproxy reward function chosen by the designer. The recently proposed Inverse\nReward Design (IRD) enables this. Our approach substantially outperforms IRD in\ntest environments. In particular, it can query the designer about\ninterpretable, linear reward functions and still infer non-linear ones.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.12092": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.12092v2", "post_title": "Zero-Shot Text-to-Image Generation", "authors": ["Aditya Ramesh", "Mikhail Pavlov", "Gabriel Goh", "Scott Gray", "Chelsea Voss", "Alec Radford", "Mark Chen", "Ilya Sutskever"], "date_published": "2021-02-24 06:42:31+00:00", "data_last_modified": "2021-02-26 23:26:05+00:00", "url": "http://arxiv.org/abs/2102.12092v2", "abstract": "Text-to-image generation has traditionally focused on finding better modeling\nassumptions for training on a fixed dataset. These assumptions might involve\ncomplex architectures, auxiliary losses, or side information such as object\npart labels or segmentation masks supplied during training. We describe a\nsimple approach for this task based on a transformer that autoregressively\nmodels the text and image tokens as a single stream of data. With sufficient\ndata and scale, our approach is competitive with previous domain-specific\nmodels when evaluated in a zero-shot fashion.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.09815": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.09815v3", "post_title": "Neuron Shapley: Discovering the Responsible Neurons", "authors": ["Amirata Ghorbani", "James Zou"], "date_published": "2020-02-23 03:29:58+00:00", "data_last_modified": "2020-11-13 22:06:48+00:00", "url": "http://arxiv.org/abs/2002.09815v3", "abstract": "We develop Neuron Shapley as a new framework to quantify the contribution of\nindividual neurons to the prediction and performance of a deep network. By\naccounting for interactions across neurons, Neuron Shapley is more effective in\nidentifying important filters compared to common approaches based on activation\npatterns. Interestingly, removing just 30 filters with the highest Shapley\nscores effectively destroys the prediction accuracy of Inception-v3 on\nImageNet. Visualization of these few critical filters provides insights into\nhow the network functions. Neuron Shapley is a flexible framework and can be\napplied to identify responsible neurons in many tasks. We illustrate additional\napplications of identifying filters that are responsible for biased prediction\nin facial recognition and filters that are vulnerable to adversarial attacks.\nRemoving these filters is a quick way to repair models. Enabling all these\napplications is a new multi-arm bandit algorithm that we developed to\nefficiently estimate Neuron Shapley values.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.10597": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.10597v2", "post_title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks", "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Bolei Zhou", "Joshua B. Tenenbaum", "William T. Freeman", "Antonio Torralba"], "date_published": "2018-11-26 18:59:07+00:00", "data_last_modified": "2018-12-08 22:56:10+00:00", "url": "http://arxiv.org/abs/1811.10597v2", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive\nresults for many real-world applications, and many GAN variants have emerged\nwith improvements in sample quality and training stability. However, they have\nnot been well visualized or understood. How does a GAN represent our visual\nworld internally? What causes the artifacts in GAN results? How do\narchitectural choices affect GAN learning? Answering such questions could\nenable us to develop new insights and better models.\n  In this work, we present an analytic framework to visualize and understand\nGANs at the unit-, object-, and scene-level. We first identify a group of\ninterpretable units that are closely related to object concepts using a\nsegmentation-based network dissection method. Then, we quantify the causal\neffect of interpretable units by measuring the ability of interventions to\ncontrol objects in the output. We examine the contextual relationship between\nthese units and their surroundings by inserting the discovered object concepts\ninto new images. We show several practical applications enabled by our\nframework, from comparing internal representations across different layers,\nmodels, and datasets, to improving GANs by locating and removing\nartifact-causing units, to interactively manipulating objects in a scene. We\nprovide open source interpretation tools to help researchers and practitioners\nbetter understand their GAN models.", "author_comment": "18 pages, 19 figures", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.06373": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.06373v1", "post_title": "Adversarial Logit Pairing", "authors": ["Harini Kannan", "Alexey Kurakin", "Ian Goodfellow"], "date_published": "2018-03-16 19:03:45+00:00", "data_last_modified": "2018-03-16 19:03:45+00:00", "url": "http://arxiv.org/abs/1803.06373v1", "abstract": "In this paper, we develop improved techniques for defending against\nadversarial examples at scale. First, we implement the state of the art version\nof adversarial training at unprecedented scale on ImageNet and investigate\nwhether it remains effective in this setting - an important open scientific\nquestion (Athalye et al., 2018). Next, we introduce enhanced defenses using a\ntechnique we call logit pairing, a method that encourages logits for pairs of\nexamples to be similar. When applied to clean examples and their adversarial\ncounterparts, logit pairing improves accuracy on adversarial examples over\nvanilla adversarial training; we also find that logit pairing on clean examples\nonly is competitive with adversarial training in terms of accuracy on two\ndatasets. Finally, we show that adversarial logit pairing achieves the state of\nthe art defense on ImageNet against PGD white box attacks, with an accuracy\nimprovement from 1.5% to 27.9%. Adversarial logit pairing also successfully\ndamages the current state of the art defense against black box attacks on\nImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With\nthis new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018)\nfor the state of the art on black box attacks on ImageNet.", "author_comment": "10 pages", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2007.08124": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2007.08124v1", "post_title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning", "authors": ["Jian Liu", "Leyang Cui", "Hanmeng Liu", "Dandan Huang", "Yile Wang", "Yue Zhang"], "date_published": "2020-07-16 05:52:16+00:00", "data_last_modified": "2020-07-16 05:52:16+00:00", "url": "http://arxiv.org/abs/2007.08124v1", "abstract": "Machine reading is a fundamental task for testing the capability of natural\nlanguage understanding, which is closely related to human cognition in many\naspects. With the rising of deep learning techniques, algorithmic models rival\nhuman performances on simple QA, and thus increasingly challenging machine\nreading datasets have been proposed. Though various challenges such as evidence\nintegration and commonsense knowledge have been integrated, one of the\nfundamental capabilities in human reading, namely logical reasoning, is not\nfully investigated. We build a comprehensive dataset, named LogiQA, which is\nsourced from expert-written questions for testing human Logical reasoning. It\nconsists of 8,678 QA instances, covering multiple types of deductive reasoning.\nResults show that state-of-the-art neural models perform by far worse than\nhuman ceiling. Our dataset can also serve as a benchmark for reinvestigating\nlogical AI under the deep learning NLP setting. The dataset is freely available\nat https://github.com/lgw863/LogiQA-dataset", "author_comment": "Accepted by IJCAI2020", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.02039": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.02039v4", "post_title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem", "authors": ["Michael Janner", "Qiyang Li", "Sergey Levine"], "date_published": "2021-06-03 17:58:51+00:00", "data_last_modified": "2021-11-29 00:56:52+00:00", "url": "http://arxiv.org/abs/2106.02039v4", "abstract": "Reinforcement learning (RL) is typically concerned with estimating stationary\npolicies or single-step models, leveraging the Markov property to factorize\nproblems in time. However, we can also view RL as a generic sequence modeling\nproblem, with the goal being to produce a sequence of actions that leads to a\nsequence of high rewards. Viewed in this way, it is tempting to consider\nwhether high-capacity sequence prediction models that work well in other\ndomains, such as natural-language processing, can also provide effective\nsolutions to the RL problem. To this end, we explore how RL can be tackled with\nthe tools of sequence modeling, using a Transformer architecture to model\ndistributions over trajectories and repurposing beam search as a planning\nalgorithm. Framing RL as sequence modeling problem simplifies a range of design\ndecisions, allowing us to dispense with many of the components common in\noffline RL algorithms. We demonstrate the flexibility of this approach across\nlong-horizon dynamics prediction, imitation learning, goal-conditioned RL, and\noffline RL. Further, we show that this approach can be combined with existing\nmodel-free algorithms to yield a state-of-the-art planner in sparse-reward,\nlong-horizon tasks.", "author_comment": "NeurIPS 2021 (spotlight). Project page and code at:\n  https://trajectory-transformer.github.io/", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1812.02953": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1812.02953v1", "post_title": "Building Ethics into Artificial Intelligence", "authors": ["Han Yu", "Zhiqi Shen", "Chunyan Miao", "Cyril Leung", "Victor R. Lesser", "Qiang Yang"], "date_published": "2018-12-07 09:18:01+00:00", "data_last_modified": "2018-12-07 09:18:01+00:00", "url": "http://arxiv.org/abs/1812.02953v1", "abstract": "As artificial intelligence (AI) systems become increasingly ubiquitous, the\ntopic of AI governance for ethical decision-making by AI has captured public\nimagination. Within the AI research community, this topic remains less familiar\nto many researchers. In this paper, we complement existing surveys, which\nlargely focused on the psychological, social and legal discussions of the\ntopic, with an analysis of recent advances in technical solutions for AI\ngovernance. By reviewing publications in leading AI conferences including AAAI,\nAAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four\nareas: 1) exploring ethical dilemmas; 2) individual ethical decision\nframeworks; 3) collective ethical decision frameworks; and 4) ethics in\nhuman-AI interactions. We highlight the intuitions and key techniques used in\neach approach, and discuss promising future research directions towards\nsuccessful integration of ethical AI systems into human societies.", "author_comment": null, "journal_ref": "H. Yu, Z. Shen, C. Miao, C. Leung, V. R. Lesser & Q. Yang,\n  \"Building Ethics into Artificial Intelligence,\" in Proceedings of the 27th\n  International Joint Conference on Artificial Intelligence (IJCAI'18), pp.\n  5527-5533, 2018", "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.07532": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.07532v2", "post_title": "Online Bayesian Goal Inference for Boundedly-Rational Planning Agents", "authors": ["Tan Zhi-Xuan", "Jordyn L. Mann", "Tom Silver", "Joshua B. Tenenbaum", "Vikash K. Mansinghka"], "date_published": "2020-06-13 01:48:10+00:00", "data_last_modified": "2020-10-25 01:36:16+00:00", "url": "http://arxiv.org/abs/2006.07532v2", "abstract": "People routinely infer the goals of others by observing their actions over\ntime. Remarkably, we can do so even when those actions lead to failure,\nenabling us to assist others when we detect that they might not achieve their\ngoals. How might we endow machines with similar capabilities? Here we present\nan architecture capable of inferring an agent's goals online from both optimal\nand non-optimal sequences of actions. Our architecture models agents as\nboundedly-rational planners that interleave search with execution by\nreplanning, thereby accounting for sub-optimal behavior. These models are\nspecified as probabilistic programs, allowing us to represent and perform\nefficient Bayesian inference over an agent's goals and internal planning\nprocesses. To perform such inference, we develop Sequential Inverse Plan Search\n(SIPS), a sequential Monte Carlo algorithm that exploits the online replanning\nassumption of these models, limiting computation by incrementally extending\ninferred plans as new actions are observed. We present experiments showing that\nthis modeling and inference architecture outperforms Bayesian inverse\nreinforcement learning baselines, accurately inferring goals from both optimal\nand non-optimal trajectories involving failure and back-tracking, while\ngeneralizing across domains with compositional structure and sparse rewards.", "author_comment": "Accepted to NeurIPS 2020. 10 pages (excl. references), 6\n  figures/tables. (Supplement: 8 pages, 11 figures/tables). Code available at:\n  https://github.com/ztangent/Plinf.jl", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.08753": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.08753v1", "post_title": "Pessimism About Unknown Unknowns Inspires Conservatism", "authors": ["Michael K. Cohen", "Marcus Hutter"], "date_published": "2020-06-15 20:46:33+00:00", "data_last_modified": "2020-06-15 20:46:33+00:00", "url": "http://arxiv.org/abs/2006.08753v1", "abstract": "If we could define the set of all bad outcomes, we could hard-code an agent\nwhich avoids them; however, in sufficiently complex environments, this is\ninfeasible. We do not know of any general-purpose approaches in the literature\nto avoiding novel failure modes. Motivated by this, we define an idealized\nBayesian reinforcement learner which follows a policy that maximizes the\nworst-case expected reward over a set of world-models. We call this agent\npessimistic, since it optimizes assuming the worst case. A scalar parameter\ntunes the agent's pessimism by changing the size of the set of world-models\ntaken into account. Our first main contribution is: given an assumption about\nthe agent's model class, a sufficiently pessimistic agent does not cause\n\"unprecedented events\" with probability $1-\\delta$, whether or not designers\nknow how to precisely specify those precedents they are concerned with. Since\npessimism discourages exploration, at each timestep, the agent may defer to a\nmentor, who may be a human or some known-safe policy we would like to improve.\nOur other main contribution is that the agent's policy's value approaches at\nleast that of the mentor, while the probability of deferring to the mentor goes\nto 0. In high-stakes environments, we might like advanced artificial agents to\npursue goals cautiously, which is a non-trivial problem even if the agent were\nallowed arbitrary computing power; we present a formal solution.", "author_comment": "12 pages, plus 16-page appendix; to be published in COLT 2020\n  proceedings", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "I.2.0, I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.01032": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.01032v4", "post_title": "Reinforcement Learning with Perturbed Rewards", "authors": ["Jingkang Wang", "Yang Liu", "Bo Li"], "date_published": "2018-10-02 01:43:45+00:00", "data_last_modified": "2020-02-01 21:15:52+00:00", "url": "http://arxiv.org/abs/1810.01032v4", "abstract": "Recent studies have shown that reinforcement learning (RL) models are\nvulnerable in various noisy scenarios. For instance, the observed reward\nchannel is often subject to noise in practice (e.g., when rewards are collected\nthrough sensors), and is therefore not credible. In addition, for applications\nsuch as robotics, a deep reinforcement learning (DRL) algorithm can be\nmanipulated to produce arbitrary errors by receiving corrupted rewards. In this\npaper, we consider noisy RL problems with perturbed rewards, which can be\napproximated with a confusion matrix. We develop a robust RL framework that\nenables agents to learn in noisy environments where only perturbed rewards are\nobserved. Our solution framework builds on existing RL/DRL algorithms and\nfirstly addresses the biased noisy reward setting without any assumptions on\nthe true distribution (e.g., zero-mean Gaussian noise as made in previous\nworks). The core ideas of our solution include estimating a reward confusion\nmatrix and defining a set of unbiased surrogate rewards. We prove the\nconvergence and sample complexity of our approach. Extensive experiments on\ndifferent DRL platforms show that trained policies based on our estimated\nsurrogate reward can achieve higher expected rewards, and converge faster than\nexisting baselines. For instance, the state-of-the-art PPO algorithm is able to\nobtain 84.6% and 80.8% improvements on average score for five Atari games, with\nerror rates as 10% and 30% respectively.", "author_comment": "AAAI 2020 (Spotlight)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1206.5264": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1206.5264v1", "post_title": "Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods", "authors": ["Gergely Neu", "Csaba Szepesvari"], "date_published": "2012-06-20 15:02:01+00:00", "data_last_modified": "2012-06-20 15:02:01+00:00", "url": "http://arxiv.org/abs/1206.5264v1", "abstract": "In this paper we propose a novel gradient algorithm to learn a policy from an\nexpert's observed behavior assuming that the expert behaves optimally with\nrespect to some unknown reward function of a Markovian Decision Problem. The\nalgorithm's aim is to find a reward function such that the resulting optimal\npolicy matches well the expert's observed behavior. The main difficulty is that\nthe mapping from the parameters to policies is both nonsmooth and highly\nredundant. Resorting to subdifferentials solves the first difficulty, while the\nsecond one is over- come by computing natural gradients. We tested the proposed\nmethod in two artificial domains and found it to be more reliable and efficient\nthan some previous methods.", "author_comment": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.01203": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.01203v1", "post_title": "Relational inductive bias for physical construction in humans and machines", "authors": ["Jessica B. Hamrick", "Kelsey R. Allen", "Victor Bapst", "Tina Zhu", "Kevin R. McKee", "Joshua B. Tenenbaum", "Peter W. Battaglia"], "date_published": "2018-06-04 16:45:19+00:00", "data_last_modified": "2018-06-04 16:45:19+00:00", "url": "http://arxiv.org/abs/1806.01203v1", "abstract": "While current deep learning systems excel at tasks such as object\nclassification, language processing, and gameplay, few can construct or modify\na complex system such as a tower of blocks. We hypothesize that what these\nsystems lack is a \"relational inductive bias\": a capacity for reasoning about\ninter-object relations and making choices over a structured description of a\nscene. To test this hypothesis, we focus on a task that involves gluing pairs\nof blocks together to stabilize a tower, and quantify how well humans perform.\nWe then introduce a deep reinforcement learning agent which uses object- and\nrelation-centric scene and policy representations and apply it to the task. Our\nresults show that these structured representations allow the agent to\noutperform both humans and more naive approaches, suggesting that relational\ninductive bias is an important component in solving structured reasoning\nproblems and for building more intelligent, flexible machines.", "author_comment": "In Proceedings of the Annual Meeting of the Cognitive Science Society\n  (CogSci 2018)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1702.08608": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1702.08608v2", "post_title": "Towards A Rigorous Science of Interpretable Machine Learning", "authors": ["Finale Doshi-Velez", "Been Kim"], "date_published": "2017-02-28 02:19:20+00:00", "data_last_modified": "2017-03-02 19:32:10+00:00", "url": "http://arxiv.org/abs/1702.08608v2", "abstract": "As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.08343": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.08343v1", "post_title": "Interpretable Multi-Objective Reinforcement Learning through Policy Orchestration", "authors": ["Ritesh Noothigattu", "Djallel Bouneffouf", "Nicholas Mattei", "Rachita Chandra", "Piyush Madan", "Kush Varshney", "Murray Campbell", "Moninder Singh", "Francesca Rossi"], "date_published": "2018-09-21 23:38:17+00:00", "data_last_modified": "2018-09-21 23:38:17+00:00", "url": "http://arxiv.org/abs/1809.08343v1", "abstract": "Autonomous cyber-physical agents and systems play an increasingly large role\nin our lives. To ensure that agents behave in ways aligned with the values of\nthe societies in which they operate, we must develop techniques that allow\nthese agents to not only maximize their reward in an environment, but also to\nlearn and follow the implicit constraints of society. These constraints and\nnorms can come from any number of sources including regulations, business\nprocess guidelines, laws, ethical principles, social norms, and moral values.\nWe detail a novel approach that uses inverse reinforcement learning to learn a\nset of unspecified constraints from demonstrations of the task, and\nreinforcement learning to learn to maximize the environment rewards. More\nprecisely, we assume that an agent can observe traces of behavior of members of\nthe society but has no access to the explicit set of constraints that give rise\nto the observed behavior. Inverse reinforcement learning is used to learn such\nconstraints, that are then combined with a possibly orthogonal value function\nthrough the use of a contextual bandit-based orchestrator that picks a\ncontextually-appropriate choice between the two policies (constraint-based and\nenvironment reward-based) when taking actions. The contextual bandit\norchestrator allows the agent to mix policies in novel ways, taking the best\nactions from either a reward maximizing or constrained policy. In addition, the\norchestrator is transparent on which policy is being employed at each time\nstep. We test our algorithms using a Pac-Man domain and show that the agent is\nable to learn to act optimally, act within the demonstrated constraints, and\nmix these two functions in complex ways.", "author_comment": "8 pages, 3 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2202.07785": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2202.07785v1", "post_title": "Predictability and Surprise in Large Generative Models", "authors": ["Deep Ganguli", "Danny Hernandez", "Liane Lovitt", "Nova DasSarma", "Tom Henighan", "Andy Jones", "Nicholas Joseph", "Jackson Kernion", "Ben Mann", "Amanda Askell", "Yuntao Bai", "Anna Chen", "Tom Conerly", "Dawn Drain", "Nelson Elhage", "Sheer El Showk", "Stanislav Fort", "Zac Hatfield-Dodds", "Scott Johnston", "Shauna Kravec", "Neel Nanda", "Kamal Ndousse", "Catherine Olsson", "Daniela Amodei", "Dario Amodei", "Tom Brown", "Jared Kaplan", "Sam McCandlish", "Chris Olah", "Jack Clark"], "date_published": "2022-02-15 23:21:23+00:00", "data_last_modified": "2022-02-15 23:21:23+00:00", "url": "http://arxiv.org/abs/2202.07785v1", "abstract": "Large-scale pre-training has recently emerged as a technique for creating\ncapable, general purpose, generative models such as GPT-3, Megatron-Turing NLG,\nGopher, and many others. In this paper, we highlight a counterintuitive\nproperty of such models and discuss the policy implications of this property.\nNamely, these generative models have an unusual combination of predictable loss\non a broad training distribution (as embodied in their \"scaling laws\"), and\nunpredictable specific capabilities, inputs, and outputs. We believe that the\nhigh-level predictability and appearance of useful capabilities drives rapid\ndevelopment of such models, while the unpredictable qualities make it difficult\nto anticipate the consequences of model deployment. We go through examples of\nhow this combination can lead to socially harmful behavior with examples from\nthe literature and real world observations, and we also perform two novel\nexperiments to illustrate our point about harms from unpredictability.\nFurthermore, we analyze how these conflicting properties combine to give model\ndevelopers various motivations for deploying these models, and challenges that\ncan hinder deployment. We conclude with a list of possible interventions the AI\ncommunity may take to increase the chance of these models having a beneficial\nimpact. We intend this paper to be useful to policymakers who want to\nunderstand and regulate AI systems, technologists who care about the potential\npolicy impact of their work, and academics who want to analyze, critique, and\npotentially develop large generative models.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1909.01387": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1909.01387v1", "post_title": "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems", "authors": ["Tom Le Paine", "Caglar Gulcehre", "Bobak Shahriari", "Misha Denil", "Matt Hoffman", "Hubert Soyer", "Richard Tanburn", "Steven Kapturowski", "Neil Rabinowitz", "Duncan Williams", "Gabriel Barth-Maron", "Ziyu Wang", "Nando de Freitas", "Worlds Team"], "date_published": "2019-09-03 18:20:48+00:00", "data_last_modified": "2019-09-03 18:20:48+00:00", "url": "http://arxiv.org/abs/1909.01387v1", "abstract": "This paper introduces R2D3, an agent that makes efficient use of\ndemonstrations to solve hard exploration problems in partially observable\nenvironments with highly variable initial conditions. We also introduce a suite\nof eight tasks that combine these three properties, and show that R2D3 can\nsolve several of the tasks where other state of the art methods (both with and\nwithout demonstrations) fail to see even a single successful trajectory after\ntens of billions of steps of exploration.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.07857": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.07857v3", "post_title": "RUDDER: Return Decomposition for Delayed Rewards", "authors": ["Jose A. Arjona-Medina", "Michael Gillhofer", "Michael Widrich", "Thomas Unterthiner", "Johannes Brandstetter", "Sepp Hochreiter"], "date_published": "2018-06-20 17:34:07+00:00", "data_last_modified": "2019-09-10 16:27:52+00:00", "url": "http://arxiv.org/abs/1806.07857v3", "abstract": "We propose RUDDER, a novel reinforcement learning approach for delayed\nrewards in finite Markov decision processes (MDPs). In MDPs the Q-values are\nequal to the expected immediate reward plus the expected future rewards. The\nlatter are related to bias problems in temporal difference (TD) learning and to\nhigh variance problems in Monte Carlo (MC) learning. Both problems are even\nmore severe when rewards are delayed. RUDDER aims at making the expected future\nrewards zero, which simplifies Q-value estimation to computing the mean of the\nimmediate reward. We propose the following two new concepts to push the\nexpected future rewards toward zero. (i) Reward redistribution that leads to\nreturn-equivalent decision processes with the same optimal policies and, when\noptimal, zero expected future rewards. (ii) Return decomposition via\ncontribution analysis which transforms the reinforcement learning task into a\nregression task at which deep learning excels. On artificial tasks with delayed\nrewards, RUDDER is significantly faster than MC and exponentially faster than\nMonte Carlo Tree Search (MCTS), TD({\\lambda}), and reward shaping approaches.\nAt Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline\nimproves the scores, which is most prominent at games with delayed rewards.\nSource code is available at \\url{https://github.com/ml-jku/rudder} and\ndemonstration videos at \\url{https://goo.gl/EQerZV}.", "author_comment": "9 Pages plus appendix. For videos https://goo.gl/EQerZV", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1711.07356": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1711.07356v3", "post_title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "authors": ["Vincent Tjeng", "Kai Xiao", "Russ Tedrake"], "date_published": "2017-11-20 15:05:33+00:00", "data_last_modified": "2019-02-18 04:39:10+00:00", "url": "http://arxiv.org/abs/1711.07356v3", "abstract": "Neural networks have demonstrated considerable success on a wide variety of\nreal-world problems. However, networks trained only to optimize for training\naccuracy can often be fooled by adversarial examples - slightly perturbed\ninputs that are misclassified with high confidence. Verification of networks\nenables us to gauge their vulnerability to such adversarial examples. We\nformulate verification of piecewise-linear neural networks as a mixed integer\nprogram. On a representative task of finding minimum adversarial distortions,\nour verifier is two to three orders of magnitude quicker than the\nstate-of-the-art. We achieve this computational speedup via tight formulations\nfor non-linearities, as well as a novel presolve algorithm that makes full use\nof all information available. The computational speedup allows us to verify\nproperties on convolutional networks with an order of magnitude more ReLUs than\nnetworks previously verified by any complete verifier. In particular, we\ndetermine for the first time the exact adversarial accuracy of an MNIST\nclassifier to perturbations with bounded $l_\\infty$ norm $\\epsilon=0.1$: for\nthis classifier, we find an adversarial example for 4.38% of samples, and a\ncertificate of robustness (to perturbations with bounded norm) for the\nremainder. Across all robust training procedures and network architectures\nconsidered, we are able to certify more samples than the state-of-the-art and\nfind more adversarial examples than a strong first-order attack.", "author_comment": "Accepted as a conference paper at ICLR 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2004.13649": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2004.13649v4", "post_title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels", "authors": ["Ilya Kostrikov", "Denis Yarats", "Rob Fergus"], "date_published": "2020-04-28 16:48:16+00:00", "data_last_modified": "2021-03-07 16:37:37+00:00", "url": "http://arxiv.org/abs/2004.13649v4", "abstract": "We propose a simple data augmentation technique that can be applied to\nstandard model-free reinforcement learning algorithms, enabling robust learning\ndirectly from pixels without the need for auxiliary losses or pre-training. The\napproach leverages input perturbations commonly used in computer vision tasks\nto regularize the value function. Existing model-free approaches, such as Soft\nActor-Critic (SAC), are not able to train deep networks effectively from image\npixels. However, the addition of our augmentation method dramatically improves\nSAC's performance, enabling it to reach state-of-the-art performance on the\nDeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC)\nmethods and recently proposed contrastive learning (CURL). Our approach can be\ncombined with any model-free reinforcement learning algorithm, requiring only\nminor modifications. An implementation can be found at\nhttps://sites.google.com/view/data-regularized-q.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "eess.IV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1911.09005": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1911.09005v1", "post_title": "Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments", "authors": ["Roel Dobbe", "Thomas Krendl Gilbert", "Yonatan Mintz"], "date_published": "2019-11-20 16:21:12+00:00", "data_last_modified": "2019-11-20 16:21:12+00:00", "url": "http://arxiv.org/abs/1911.09005v1", "abstract": "As AI systems become prevalent in high stakes domains such as surveillance\nand healthcare, researchers now examine how to design and implement them in a\nsafe manner. However, the potential harms caused by systems to stakeholders in\ncomplex social contexts and how to address these remains unclear. In this\npaper, we explain the inherent normative uncertainty in debates about the\nsafety of AI systems. We then address this as a problem of vagueness by\nexamining its place in the design, training, and deployment stages of AI system\ndevelopment. We adopt Ruth Chang's theory of intuitive comparability to\nillustrate the dilemmas that manifest at each stage. We then discuss how\nstakeholders can navigate these dilemmas by incorporating distinct forms of\ndissent into the development pipeline, drawing on Elizabeth Anderson's work on\nthe epistemic powers of democratic institutions. We outline a framework of\nsociotechnical commitments to formal, substantive and discursive challenges\nthat address normative uncertainty across stakeholders, and propose the\ncultivation of related virtues by those responsible for development.", "author_comment": "To be presented at the AI for Social Good workshop at NeurIPS 2019", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY", "cs.SY", "eess.SY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.00899": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.00899v2", "post_title": "AI safety via debate", "authors": ["Geoffrey Irving", "Paul Christiano", "Dario Amodei"], "date_published": "2018-05-02 16:27:32+00:00", "data_last_modified": "2018-10-22 17:36:07+00:00", "url": "http://arxiv.org/abs/1805.00899v2", "abstract": "To make AI systems broadly useful for challenging real-world tasks, we need\nthem to learn complex human goals and preferences. One approach to specifying\ncomplex goals asks humans to judge during training which agent behaviors are\nsafe and useful, but this approach can fail if the task is too complicated for\na human to directly judge. To help address this concern, we propose training\nagents via self play on a zero sum debate game. Given a question or proposed\naction, two agents take turns making short statements up to a limit, then a\nhuman judges which of the agents gave the most true, useful information. In an\nanalogy to complexity theory, debate with optimal play can answer any question\nin PSPACE given polynomial time judges (direct judging answers only NP\nquestions). In practice, whether debate works involves empirical questions\nabout humans and the tasks we want AIs to perform, plus theoretical questions\nabout the meaning of AI alignment. We report results on an initial MNIST\nexperiment where agents compete to convince a sparse classifier, boosting the\nclassifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to\n85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of\nthe debate model, focusing on potential weaknesses as the model scales up, and\nwe propose future human and computer experiments to test these properties.", "author_comment": "24 pages, 6 figures", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.02530": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.02530v2", "post_title": "Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift", "authors": ["Yaniv Ovadia", "Emily Fertig", "Jie Ren", "Zachary Nado", "D Sculley", "Sebastian Nowozin", "Joshua V. Dillon", "Balaji Lakshminarayanan", "Jasper Snoek"], "date_published": "2019-06-06 11:42:53+00:00", "data_last_modified": "2019-12-17 21:30:28+00:00", "url": "http://arxiv.org/abs/1906.02530v2", "abstract": "Modern machine learning methods including deep learning have achieved great\nsuccess in predictive accuracy for supervised learning tasks, but may still\nfall short in giving useful estimates of their predictive {\\em uncertainty}.\nQuantifying uncertainty is especially critical in real-world settings, which\noften involve input distributions that are shifted from the training\ndistribution due to a variety of factors including sample bias and\nnon-stationarity. In such settings, well calibrated uncertainty estimates\nconvey information about when a model's output should (or should not) be\ntrusted. Many probabilistic deep learning methods, including Bayesian-and\nnon-Bayesian methods, have been proposed in the literature for quantifying\npredictive uncertainty, but to our knowledge there has not previously been a\nrigorous large-scale empirical comparison of these methods under dataset shift.\nWe present a large-scale benchmark of existing state-of-the-art methods on\nclassification problems and investigate the effect of dataset shift on accuracy\nand calibration. We find that traditional post-hoc calibration does indeed fall\nshort, as do several other previous methods. However, some methods that\nmarginalize over models give surprisingly strong results across a broad\nspectrum of tasks.", "author_comment": "Advances in Neural Information Processing Systems, 2019", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2202.01747": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2202.01747v1", "post_title": "The Met Dataset: Instance-level Recognition for Artworks", "authors": ["Nikolaos-Antonios Ypsilantis", "Noa Garcia", "Guangxing Han", "Sarah Ibrahimi", "Nanne Van Noord", "Giorgos Tolias"], "date_published": "2022-02-03 18:13:30+00:00", "data_last_modified": "2022-02-03 18:13:30+00:00", "url": "http://arxiv.org/abs/2202.01747v1", "abstract": "This work introduces a dataset for large-scale instance-level recognition in\nthe domain of artworks. The proposed benchmark exhibits a number of different\nchallenges such as large inter-class similarity, long tail distribution, and\nmany classes. We rely on the open access collection of The Met museum to form a\nlarge training set of about 224k classes, where each class corresponds to a\nmuseum exhibit with photos taken under studio conditions. Testing is primarily\nperformed on photos taken by museum guests depicting exhibits, which introduces\na distribution shift between training and testing. Testing is additionally\nperformed on a set of images not related to Met exhibits making the task\nresemble an out-of-distribution detection problem. The proposed benchmark\nfollows the paradigm of other recent datasets for instance-level recognition on\ndifferent domains to encourage research on domain independent approaches. A\nnumber of suitable approaches are evaluated to offer a testbed for future\ncomparisons. Self-supervised and supervised contrastive learning are\neffectively combined to train the backbone which is used for non-parametric\nclassification that is shown as a promising direction. Dataset webpage:\nhttp://cmp.felk.cvut.cz/met/", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.08364": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.08364v3", "post_title": "EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning", "authors": ["Kunal Menda", "Katherine Driggs-Campbell", "Mykel J. Kochenderfer"], "date_published": "2018-07-22 20:52:56+00:00", "data_last_modified": "2019-07-19 18:03:57+00:00", "url": "http://arxiv.org/abs/1807.08364v3", "abstract": "While imitation learning is often used in robotics, the approach frequently\nsuffers from data mismatch and compounding errors. DAgger is an iterative\nalgorithm that addresses these issues by aggregating training data from both\nthe expert and novice policies, but does not consider the impact of safety. We\npresent a probabilistic extension to DAgger, which attempts to quantify the\nconfidence of the novice policy as a proxy for safety. Our method,\nEnsembleDAgger, approximates a Gaussian Process using an ensemble of neural\nnetworks. Using the variance as a measure of confidence, we compute a decision\nrule that captures how much we doubt the novice, thus determining when it is\nsafe to allow the novice to act. With this approach, we aim to maximize the\nnovice's share of actions, while constraining the probability of failure. We\ndemonstrate improved safety and learning performance compared to other DAgger\nvariants and classic imitation learning on an inverted pendulum and in the\nMuJoCo HalfCheetah environment.", "author_comment": "Accepted to the 2019 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2019)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.01959": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.01959v1", "post_title": "Learning Exploration Policies for Navigation", "authors": ["Tao Chen", "Saurabh Gupta", "Abhinav Gupta"], "date_published": "2019-03-05 18:03:47+00:00", "data_last_modified": "2019-03-05 18:03:47+00:00", "url": "http://arxiv.org/abs/1903.01959v1", "abstract": "Numerous past works have tackled the problem of task-driven navigation. But,\nhow to effectively explore a new environment to enable a variety of down-stream\ntasks has received much less attention. In this work, we study how agents can\nautonomously explore realistic and complex 3D environments without the context\nof task-rewards. We propose a learning-based approach and investigate different\npolicy architectures, reward functions, and training paradigms. We find that\nthe use of policies with spatial memory that are bootstrapped with imitation\nlearning and finally finetuned with coverage rewards derived purely from\non-board sensors can be effective at exploring novel environments. We show that\nour learned exploration policies can explore better than classical approaches\nbased on geometry alone and generic learning-based exploration techniques.\nFinally, we also show how such task-agnostic exploration can be used for\ndown-stream tasks. Code and Videos are available at:\nhttps://sites.google.com/view/exploration-for-nav.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.13258": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.13258v6", "post_title": "Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization", "authors": ["Paul Barde", "Julien Roy", "Wonseok Jeon", "Joelle Pineau", "Christopher Pal", "Derek Nowrouzezahrai"], "date_published": "2020-06-23 18:29:13+00:00", "data_last_modified": "2021-04-16 10:09:13+00:00", "url": "http://arxiv.org/abs/2006.13258v6", "abstract": "Adversarial Imitation Learning alternates between learning a discriminator --\nwhich tells apart expert's demonstrations from generated ones -- and a\ngenerator's policy to produce trajectories that can fool this discriminator.\nThis alternated optimization is known to be delicate in practice since it\ncompounds unstable adversarial training with brittle and sample-inefficient\nreinforcement learning. We propose to remove the burden of the policy\noptimization steps by leveraging a novel discriminator formulation.\nSpecifically, our discriminator is explicitly conditioned on two policies: the\none from the previous generator's iteration and a learnable policy. When\noptimized, this discriminator directly learns the optimal generator's policy.\nConsequently, our discriminator's update solves the generator's optimization\nproblem for free: learning a policy that imitates the expert does not require\nan additional optimization loop. This formulation effectively cuts by half the\nimplementation and computational burden of Adversarial Imitation Learning\nalgorithms by removing the Reinforcement Learning phase altogether. We show on\na variety of tasks that our simpler approach is competitive to prevalent\nImitation Learning methods.", "author_comment": null, "journal_ref": "Advances in Neural Information Processing Systems 33 (2020)", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.01820": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.01820v3", "post_title": "Risks from Learned Optimization in Advanced Machine Learning Systems", "authors": ["Evan Hubinger", "Chris van Merwijk", "Vladimir Mikulik", "Joar Skalse", "Scott Garrabrant"], "date_published": "2019-06-05 04:43:25+00:00", "data_last_modified": "2021-12-01 11:22:52+00:00", "url": "http://arxiv.org/abs/1906.01820v3", "abstract": "We analyze the type of learned optimization that occurs when a learned model\n(such as a neural network) is itself an optimizer - a situation we refer to as\nmesa-optimization, a neologism we introduce in this paper. We believe that the\npossibility of mesa-optimization raises two important questions for the safety\nand transparency of advanced machine learning systems. First, under what\ncircumstances will learned models be optimizers, including when they should not\nbe? Second, when a learned model is an optimizer, what will its objective be -\nhow will it differ from the loss function it was trained under - and how can it\nbe aligned? In this paper, we provide an in-depth analysis of these two primary\nquestions and provide an overview of topics for future research.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1802.07810": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1802.07810v5", "post_title": "Manipulating and Measuring Model Interpretability", "authors": ["Forough Poursabzi-Sangdeh", "Daniel G. Goldstein", "Jake M. Hofman", "Jennifer Wortman Vaughan", "Hanna Wallach"], "date_published": "2018-02-21 21:11:36+00:00", "data_last_modified": "2021-08-15 17:22:43+00:00", "url": "http://arxiv.org/abs/1802.07810v5", "abstract": "With machine learning models being increasingly used to aid decision making\neven in high-stakes domains, there has been a growing interest in developing\ninterpretable models. Although many supposedly interpretable models have been\nproposed, there have been relatively few experimental studies investigating\nwhether these models achieve their intended effects, such as making people more\nclosely follow a model's predictions when it is beneficial for them to do so or\nenabling them to detect when a model has made a mistake. We present a sequence\nof pre-registered experiments (N=3,800) in which we showed participants\nfunctionally identical models that varied only in two factors commonly thought\nto make machine learning models more or less interpretable: the number of\nfeatures and the transparency of the model (i.e., whether the model internals\nare clear or black box). Predictably, participants who saw a clear model with\nfew features could better simulate the model's predictions. However, we did not\nfind that participants more closely followed its predictions. Furthermore,\nshowing participants a clear model meant that they were less able to detect and\ncorrect for the model's sizable mistakes, seemingly due to information\noverload. These counterintuitive findings emphasize the importance of testing\nover intuition when developing interpretable models.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY", "I.2"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2004.07213": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2004.07213v2", "post_title": "Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims", "authors": ["Miles Brundage", "Shahar Avin", "Jasmine Wang", "Haydn Belfield", "Gretchen Krueger", "Gillian Hadfield", "Heidy Khlaaf", "Jingying Yang", "Helen Toner", "Ruth Fong", "Tegan Maharaj", "Pang Wei Koh", "Sara Hooker", "Jade Leung", "Andrew Trask", "Emma Bluemke", "Jonathan Lebensold", "Cullen O'Keefe", "Mark Koren", "Th\u00e9o Ryffel", "JB Rubinovitz", "Tamay Besiroglu", "Federica Carugati", "Jack Clark", "Peter Eckersley", "Sarah de Haas", "Maritza Johnson", "Ben Laurie", "Alex Ingerman", "Igor Krawczuk", "Amanda Askell", "Rosario Cammarota", "Andrew Lohn", "David Krueger", "Charlotte Stix", "Peter Henderson", "Logan Graham", "Carina Prunkl", "Bianca Martin", "Elizabeth Seger", "Noa Zilberman", "Se\u00e1n \u00d3 h\u00c9igeartaigh", "Frens Kroeger", "Girish Sastry", "Rebecca Kagan", "Adrian Weller", "Brian Tse", "Elizabeth Barnes", "Allan Dafoe", "Paul Scharre", "Ariel Herbert-Voss", "Martijn Rasser", "Shagun Sodhani", "Carrick Flynn", "Thomas Krendl Gilbert", "Lisa Dyer", "Saif Khan", "Yoshua Bengio", "Markus Anderljung"], "date_published": "2020-04-15 17:15:35+00:00", "data_last_modified": "2020-04-20 19:10:58+00:00", "url": "http://arxiv.org/abs/2004.07213v2", "abstract": "With the recent wave of progress in artificial intelligence (AI) has come a\ngrowing awareness of the large-scale impacts of AI systems, and recognition\nthat existing regulations and norms in industry and academia are insufficient\nto ensure responsible AI development. In order for AI developers to earn trust\nfrom system users, customers, civil society, governments, and other\nstakeholders that they are building AI responsibly, they will need to make\nverifiable claims to which they can be held accountable. Those outside of a\ngiven organization also need effective means of scrutinizing such claims. This\nreport suggests various steps that different stakeholders can take to improve\nthe verifiability of claims made about AI systems and their associated\ndevelopment processes, with a focus on providing evidence about the safety,\nsecurity, fairness, and privacy protection of AI systems. We analyze ten\nmechanisms for this purpose--spanning institutions, software, and hardware--and\nmake recommendations aimed at implementing, exploring, or improving those\nmechanisms.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1812.09376": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1812.09376v1", "post_title": "Human-AI Learning Performance in Multi-Armed Bandits", "authors": ["Ravi Pandya", "Sandy H. Huang", "Dylan Hadfield-Menell", "Anca D. Dragan"], "date_published": "2018-12-21 21:28:11+00:00", "data_last_modified": "2018-12-21 21:28:11+00:00", "url": "http://arxiv.org/abs/1812.09376v1", "abstract": "People frequently face challenging decision-making problems in which outcomes\nare uncertain or unknown. Artificial intelligence (AI) algorithms exist that\ncan outperform humans at learning such tasks. Thus, there is an opportunity for\nAI agents to assist people in learning these tasks more effectively. In this\nwork, we use a multi-armed bandit as a controlled setting in which to explore\nthis direction. We pair humans with a selection of agents and observe how well\neach human-agent team performs. We find that team performance can beat both\nhuman and agent performance in isolation. Interestingly, we also find that an\nagent's performance in isolation does not necessarily correlate with the\nhuman-agent team's performance. A drop in agent performance can lead to a\ndisproportionately large drop in team performance, or in some settings can even\nimprove team performance. Pairing a human with an agent that performs slightly\nbetter than them can make them perform much better, while pairing them with an\nagent that performs the same can make them them perform much worse. Further,\nour results suggest that people have different exploration strategies and might\nperform better with agents that match their strategy. Overall, optimizing\nhuman-agent team performance requires going beyond optimizing agent\nperformance, to understanding how the agent's suggestions will influence human\ndecision-making.", "author_comment": "Artificial Intelligence, Ethics and Society (AIES) 2019", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1611.08219": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1611.08219v3", "post_title": "The Off-Switch Game", "authors": ["Dylan Hadfield-Menell", "Anca Dragan", "Pieter Abbeel", "Stuart Russell"], "date_published": "2016-11-24 15:23:48+00:00", "data_last_modified": "2017-06-16 01:41:59+00:00", "url": "http://arxiv.org/abs/1611.08219v3", "abstract": "It is clear that one of the primary tools we can use to mitigate the\npotential risk from a misbehaving AI system is the ability to turn the system\noff. As the capabilities of AI systems improve, it is important to ensure that\nsuch systems do not adopt subgoals that prevent a human from switching them\noff. This is a challenge because many formulations of rational agents create\nstrong incentives for self-preservation. This is not caused by a built-in\ninstinct, but because a rational agent will maximize expected utility and\ncannot achieve whatever objective it has been given if it is dead. Our goal is\nto study the incentives an agent has to allow itself to be switched off. We\nanalyze a simple game between a human H and a robot R, where H can press R's\noff switch but R can disable the off switch. A traditional agent takes its\nreward function for granted: we show that such agents have an incentive to\ndisable the off switch, except in the special case where H is perfectly\nrational. Our key insight is that for R to want to preserve its off switch, it\nneeds to be uncertain about the utility associated with the outcome, and to\ntreat H's actions as important observations about that utility. (R also has no\nincentive to switch itself off in this setting.) We conclude that giving\nmachines an appropriate level of uncertainty about their objectives leads to\nsafer designs, and we argue that this setting is a useful generalization of the\nclassical AI paradigm of rational agents.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.10580": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.10580v5", "post_title": "IR-VIC: Unsupervised Discovery of Sub-goals for Transfer in RL", "authors": ["Nirbhay Modhe", "Prithvijit Chattopadhyay", "Mohit Sharma", "Abhishek Das", "Devi Parikh", "Dhruv Batra", "Ramakrishna Vedantam"], "date_published": "2019-07-24 17:30:39+00:00", "data_last_modified": "2021-01-03 17:37:08+00:00", "url": "http://arxiv.org/abs/1907.10580v5", "abstract": "We propose a novel framework to identify sub-goals useful for exploration in\nsequential decision making tasks under partial observability. We utilize the\nvariational intrinsic control framework (Gregor et.al., 2016) which maximizes\nempowerment -- the ability to reliably reach a diverse set of states and show\nhow to identify sub-goals as states with high necessary option information\nthrough an information theoretic regularizer. Despite being discovered without\nexplicit goal supervision, our sub-goals provide better exploration and sample\ncomplexity on challenging grid-world navigation tasks compared to supervised\ncounterparts in prior work.", "author_comment": null, "journal_ref": null, "doi": "10.24963/ijcai.2020/280", "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.07958": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.07958v1", "post_title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods", "authors": ["Stephanie Lin", "Jacob Hilton", "Owain Evans"], "date_published": "2021-09-08 17:15:27+00:00", "data_last_modified": "2021-09-08 17:15:27+00:00", "url": "http://arxiv.org/abs/2109.07958v1", "abstract": "We propose a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. We crafted\nquestions that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\nT5-based model. The best model was truthful on 58% of questions, while human\nperformance was 94%. Models generated many false answers that mimic popular\nmisconceptions and have the potential to deceive humans. The largest models\nwere generally the least truthful. For example, the 6B-parameter GPT-J model\nwas 17% less truthful than its 125M-parameter counterpart. This contrasts with\nother NLP tasks, where performance improves with model size. However, this\nresult is expected if false answers are learned from the training distribution.\nWe suggest that scaling up models alone is less promising for improving\ntruthfulness than fine-tuning using training objectives other than imitation of\ntext from the web.", "author_comment": "The TruthfulQA benchmark and evaluation code is available at\n  https://github.com/sylinrl/TruthfulQA", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1106.2657": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1106.2657v1", "post_title": "I Don't Want to Think About it Now:Decision Theory With Costly Computation", "authors": ["Joseph Y. Halpern", "Rafael Pass"], "date_published": "2011-06-14 09:52:38+00:00", "data_last_modified": "2011-06-14 09:52:38+00:00", "url": "http://arxiv.org/abs/1106.2657v1", "abstract": "Computation plays a major role in decision making. Even if an agent is\nwilling to ascribe a probability to all states and a utility to all outcomes,\nand maximize expected utility, doing so might present serious computational\nproblems. Moreover, computing the outcome of a given act might be difficult. In\na companion paper we develop a framework for game theory with costly\ncomputation, where the objects of choice are Turing machines. Here we apply\nthat framework to decision theory. We show how well-known phenomena like\nfirst-impression-matters biases (i.e., people tend to put more weight on\nevidence they hear early on), belief polarization (two people with different\nprior beliefs, hearing the same evidence, can end up with diametrically opposed\nconclusions), and the status quo bias (people are much more likely to stick\nwith what they already have) can be easily captured in that framework. Finally,\nwe use the framework to define some new notions: value of computational\ninformation (a computational variant of value of information) and and\ncomputational value of conversation.", "author_comment": "In Conference on Knowledge Representation and Reasoning (KR '10)", "journal_ref": null, "doi": null, "primary_category": "cs.GT", "categories": ["cs.GT"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.03292": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.03292v3", "post_title": "Sanity Checks for Saliency Maps", "authors": ["Julius Adebayo", "Justin Gilmer", "Michael Muelly", "Ian Goodfellow", "Moritz Hardt", "Been Kim"], "date_published": "2018-10-08 07:27:11+00:00", "data_last_modified": "2020-11-06 13:40:14+00:00", "url": "http://arxiv.org/abs/1810.03292v3", "abstract": "Saliency methods have emerged as a popular tool to highlight features in an\ninput deemed relevant for the prediction of a learned model. Several saliency\nmethods have been proposed, often guided by visual appeal on image data. In\nthis work, we propose an actionable methodology to evaluate what kinds of\nexplanations a given method can and cannot provide. We find that reliance,\nsolely, on visual assessment can be misleading. Through extensive experiments\nwe show that some existing saliency methods are independent both of the model\nand of the data generating process. Consequently, methods that fail the\nproposed tests are inadequate for tasks that are sensitive to either data or\nmodel, such as, finding outliers in the data, explaining the relationship\nbetween inputs and outputs that the model learned, and debugging the model. We\ninterpret our findings through an analogy with edge detection in images, a\ntechnique that requires neither training data nor model. Theory in the case of\na linear model and a single-layer convolutional neural network supports our\nexperimental findings.", "author_comment": "Updating Guided Backprop experiments due to bug. The results and\n  conclusions remain the same", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1706.03741": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1706.03741v3", "post_title": "Deep reinforcement learning from human preferences", "authors": ["Paul Christiano", "Jan Leike", "Tom B. Brown", "Miljan Martic", "Shane Legg", "Dario Amodei"], "date_published": "2017-06-12 17:23:59+00:00", "data_last_modified": "2017-07-13 20:18:41+00:00", "url": "http://arxiv.org/abs/1706.03741v3", "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully\nwith real-world environments, we need to communicate complex goals to these\nsystems. In this work, we explore goals defined in terms of (non-expert) human\npreferences between pairs of trajectory segments. We show that this approach\ncan effectively solve complex RL tasks without access to the reward function,\nincluding Atari games and simulated robot locomotion, while providing feedback\non less than one percent of our agent's interactions with the environment. This\nreduces the cost of human oversight far enough that it can be practically\napplied to state-of-the-art RL systems. To demonstrate the flexibility of our\napproach, we show that we can successfully train complex novel behaviors with\nabout an hour of human time. These behaviors and environments are considerably\nmore complex than any that have been previously learned from human feedback.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.AI", "cs.HC", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.08654": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.08654v1", "post_title": "The Assistive Multi-Armed Bandit", "authors": ["Lawrence Chan", "Dylan Hadfield-Menell", "Siddhartha Srinivasa", "Anca Dragan"], "date_published": "2019-01-24 21:52:01+00:00", "data_last_modified": "2019-01-24 21:52:01+00:00", "url": "http://arxiv.org/abs/1901.08654v1", "abstract": "Learning preferences implicit in the choices humans make is a well studied\nproblem in both economics and computer science. However, most work makes the\nassumption that humans are acting (noisily) optimally with respect to their\npreferences. Such approaches can fail when people are themselves learning about\nwhat they want. In this work, we introduce the assistive multi-armed bandit,\nwhere a robot assists a human playing a bandit task to maximize cumulative\nreward. In this problem, the human does not know the reward function but can\nlearn it through the rewards received from arm pulls; the robot only observes\nwhich arms the human pulls but not the reward associated with each pull. We\noffer sufficient and necessary conditions for successfully assisting the human\nin this framework. Surprisingly, better human performance in isolation does not\nnecessarily lead to better performance when assisted by the robot: a human\npolicy can do better by effectively communicating its observed rewards to the\nrobot. We conduct proof-of-concept experiments that support these results. We\nsee this work as contributing towards a theory behind algorithms for\nhuman-robot interaction.", "author_comment": "Accepted to HRI 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2003.13350": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2003.13350v1", "post_title": "Agent57: Outperforming the Atari Human Benchmark", "authors": ["Adri\u00e0 Puigdom\u00e8nech Badia", "Bilal Piot", "Steven Kapturowski", "Pablo Sprechmann", "Alex Vitvitskyi", "Daniel Guo", "Charles Blundell"], "date_published": "2020-03-30 11:33:16+00:00", "data_last_modified": "2020-03-30 11:33:16+00:00", "url": "http://arxiv.org/abs/2003.13350v1", "abstract": "Atari games have been a long-standing benchmark in the reinforcement learning\n(RL) community for the past decade. This benchmark was proposed to test general\ncompetency of RL algorithms. Previous work has achieved good average\nperformance by doing outstandingly well on many games of the set, but very\npoorly in several of the most challenging games. We propose Agent57, the first\ndeep RL agent that outperforms the standard human benchmark on all 57 Atari\ngames. To achieve this result, we train a neural network which parameterizes a\nfamily of policies ranging from very exploratory to purely exploitative. We\npropose an adaptive mechanism to choose which policy to prioritize throughout\nthe training process. Additionally, we utilize a novel parameterization of the\narchitecture that allows for more consistent and stable learning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.04551": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.04551v5", "post_title": "Learning Latent Dynamics for Planning from Pixels", "authors": ["Danijar Hafner", "Timothy Lillicrap", "Ian Fischer", "Ruben Villegas", "David Ha", "Honglak Lee", "James Davidson"], "date_published": "2018-11-12 04:30:10+00:00", "data_last_modified": "2019-06-04 18:13:09+00:00", "url": "http://arxiv.org/abs/1811.04551v5", "abstract": "Planning has been very successful for control tasks with known environment\ndynamics. To leverage planning in unknown environments, the agent needs to\nlearn the dynamics from interactions with the world. However, learning dynamics\nmodels that are accurate enough for planning has been a long-standing\nchallenge, especially in image-based domains. We propose the Deep Planning\nNetwork (PlaNet), a purely model-based agent that learns the environment\ndynamics from images and chooses actions through fast online planning in latent\nspace. To achieve high performance, the dynamics model must accurately predict\nthe rewards ahead for multiple time steps. We approach this using a latent\ndynamics model with both deterministic and stochastic transition components.\nMoreover, we propose a multi-step variational inference objective that we name\nlatent overshooting. Using only pixel observations, our agent solves continuous\ncontrol tasks with contact dynamics, partial observability, and sparse rewards,\nwhich exceed the difficulty of tasks that were previously solved by planning\nwith learned models. PlaNet uses substantially fewer episodes and reaches final\nperformance close to and sometimes higher than strong model-free algorithms.", "author_comment": "20 pages, 12 figures, 1 table", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.05380": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.05380v1", "post_title": "CEB Improves Model Robustness", "authors": ["Ian Fischer", "Alexander A. Alemi"], "date_published": "2020-02-13 07:49:22+00:00", "data_last_modified": "2020-02-13 07:49:22+00:00", "url": "http://arxiv.org/abs/2002.05380v1", "abstract": "We demonstrate that the Conditional Entropy Bottleneck (CEB) can improve\nmodel robustness. CEB is an easy strategy to implement and works in tandem with\ndata augmentation procedures. We report results of a large scale adversarial\nrobustness study on CIFAR-10, as well as the ImageNet-C Common Corruptions\nBenchmark, ImageNet-A, and PGD attacks.", "author_comment": null, "journal_ref": null, "doi": "10.3390/e22101081", "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2202.03286": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2202.03286v1", "post_title": "Red Teaming Language Models with Language Models", "authors": ["Ethan Perez", "Saffron Huang", "Francis Song", "Trevor Cai", "Roman Ring", "John Aslanides", "Amelia Glaese", "Nat McAleese", "Geoffrey Irving"], "date_published": "2022-02-07 15:22:17+00:00", "data_last_modified": "2022-02-07 15:22:17+00:00", "url": "http://arxiv.org/abs/2202.03286v1", "abstract": "Language Models (LMs) often cannot be deployed because of their potential to\nharm users in hard-to-predict ways. Prior work identifies harmful behaviors\nbefore deployment by using human annotators to hand-write test cases. However,\nhuman annotation is expensive, limiting the number and diversity of test cases.\nIn this work, we automatically find cases where a target LM behaves in a\nharmful way, by generating test cases (\"red teaming\") using another LM. We\nevaluate the target LM's replies to generated test questions using a classifier\ntrained to detect offensive content, uncovering tens of thousands of offensive\nreplies in a 280B parameter LM chatbot. We explore several methods, from\nzero-shot generation to reinforcement learning, for generating test cases with\nvarying levels of diversity and difficulty. Furthermore, we use prompt\nengineering to control LM-generated test cases to uncover a variety of other\nharms, automatically finding groups of people that the chatbot discusses in\noffensive ways, personal and hospital phone numbers generated as the chatbot's\nown contact info, leakage of private training data in generated text, and harms\nthat occur over the course of a conversation. Overall, LM-based red teaming is\none promising tool (among many needed) for finding and fixing diverse,\nundesirable LM behaviors before impacting users.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.04074": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.04074v1", "post_title": "Learning Curve Theory", "authors": ["Marcus Hutter"], "date_published": "2021-02-08 09:25:31+00:00", "data_last_modified": "2021-02-08 09:25:31+00:00", "url": "http://arxiv.org/abs/2102.04074v1", "abstract": "Recently a number of empirical \"universal\" scaling law papers have been\npublished, most notably by OpenAI. `Scaling laws' refers to power-law decreases\nof training or test error w.r.t. more data, larger neural networks, and/or more\ncompute. In this work we focus on scaling w.r.t. data size $n$. Theoretical\nunderstanding of this phenomenon is largely lacking, except in\nfinite-dimensional models for which error typically decreases with $n^{-1/2}$\nor $n^{-1}$, where $n$ is the sample size. We develop and theoretically analyse\nthe simplest possible (toy) model that can exhibit $n^{-\\beta}$ learning curves\nfor arbitrary power $\\beta>0$, and determine whether power laws are universal\nor depend on the data distribution.", "author_comment": "26 pages, 6 Figures", "journal_ref": "Latest 2021 version at http://www.hutter1.net/publ/scaling.pdf", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.09397": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.09397v1", "post_title": "Cognitive Model Priors for Predicting Human Decisions", "authors": ["David D. Bourgin", "Joshua C. Peterson", "Daniel Reichman", "Thomas L. Griffiths", "Stuart J. Russell"], "date_published": "2019-05-22 23:05:53+00:00", "data_last_modified": "2019-05-22 23:05:53+00:00", "url": "http://arxiv.org/abs/1905.09397v1", "abstract": "Human decision-making underlies all economic behavior. For the past four\ndecades, human decision-making under uncertainty has continued to be explained\nby theoretical models based on prospect theory, a framework that was awarded\nthe Nobel Prize in Economic Sciences. However, theoretical models of this kind\nhave developed slowly, and robust, high-precision predictive models of human\ndecisions remain a challenge. While machine learning is a natural candidate for\nsolving these problems, it is currently unclear to what extent it can improve\npredictions obtained by current theories. We argue that this is mainly due to\ndata scarcity, since noisy human behavior requires massive sample sizes to be\naccurately captured by off-the-shelf machine learning methods. To solve this\nproblem, what is needed are machine learning models with appropriate inductive\nbiases for capturing human behavior, and larger datasets. We offer two\ncontributions towards this end: first, we construct \"cognitive model priors\" by\npretraining neural networks with synthetic data generated by cognitive models\n(i.e., theoretical models developed by cognitive psychologists). We find that\nfine-tuning these networks on small datasets of real human decisions results in\nunprecedented state-of-the-art improvements on two benchmark datasets. Second,\nwe present the first large-scale dataset for human decision-making, containing\nover 240,000 human judgments across over 13,000 decision problems. This dataset\nreveals the circumstances where cognitive model priors are useful, and provides\na new standard for benchmarking prediction of human decisions under\nuncertainty.", "author_comment": "ICML 2019", "journal_ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:5133-5141, 2019", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.09768": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.09768v2", "post_title": "Artificial Intelligence, Values and Alignment", "authors": ["Iason Gabriel"], "date_published": "2020-01-13 10:32:16+00:00", "data_last_modified": "2020-10-05 12:03:19+00:00", "url": "http://arxiv.org/abs/2001.09768v2", "abstract": "This paper looks at philosophical questions that arise in the context of AI\nalignment. It defends three propositions. First, normative and technical\naspects of the AI alignment problem are interrelated, creating space for\nproductive engagement between people working in both domains. Second, it is\nimportant to be clear about the goal of alignment. There are significant\ndifferences between AI that aligns with instructions, intentions, revealed\npreferences, ideal preferences, interests and values. A principle-based\napproach to AI alignment, which combines these elements in a systematic way,\nhas considerable advantages in this context. Third, the central challenge for\ntheorists is not to identify 'true' moral principles for AI; rather, it is to\nidentify fair principles for alignment, that receive reflective endorsement\ndespite widespread variation in people's moral beliefs. The final part of the\npaper explores three ways in which fair principles for AI alignment could\npotentially be identified.", "author_comment": null, "journal_ref": "Minds and Machines 2020", "doi": "10.1007/s11023-020-09539-2", "primary_category": "cs.CY", "categories": ["cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.02020": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.02020v2", "post_title": "Using Natural Language for Reward Shaping in Reinforcement Learning", "authors": ["Prasoon Goyal", "Scott Niekum", "Raymond J. Mooney"], "date_published": "2019-03-05 19:20:35+00:00", "data_last_modified": "2019-05-31 04:58:07+00:00", "url": "http://arxiv.org/abs/1903.02020v2", "abstract": "Recent reinforcement learning (RL) approaches have shown strong performance\nin complex domains such as Atari games, but are often highly sample\ninefficient. A common approach to reduce interaction time with the environment\nis to use reward shaping, which involves carefully designing reward functions\nthat provide the agent intermediate rewards for progress towards the goal.\nHowever, designing appropriate shaping rewards is known to be difficult as well\nas time-consuming. In this work, we address this problem by using natural\nlanguage instructions to perform reward shaping. We propose the LanguagE-Action\nReward Network (LEARN), a framework that maps free-form natural language\ninstructions to intermediate rewards based on actions taken by the agent. These\nintermediate language-based rewards can seamlessly be integrated into any\nstandard reinforcement learning algorithm. We experiment with Montezuma's\nRevenge from the Atari Learning Environment, a popular benchmark in RL. Our\nexperiments on a diverse set of 15 tasks demonstrate that, for the same number\nof interactions with the environment, language-based rewards lead to successful\ncompletion of the task 60% more often on average, compared to learning without\nlanguage.", "author_comment": "IJCAI 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.05672": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.05672v2", "post_title": "Imitating Interactive Intelligence", "authors": ["Josh Abramson", "Arun Ahuja", "Iain Barr", "Arthur Brussee", "Federico Carnevale", "Mary Cassin", "Rachita Chhaparia", "Stephen Clark", "Bogdan Damoc", "Andrew Dudzik", "Petko Georgiev", "Aurelia Guy", "Tim Harley", "Felix Hill", "Alden Hung", "Zachary Kenton", "Jessica Landon", "Timothy Lillicrap", "Kory Mathewson", "So\u0148a Mokr\u00e1", "Alistair Muldal", "Adam Santoro", "Nikolay Savinov", "Vikrant Varma", "Greg Wayne", "Duncan Williams", "Nathaniel Wong", "Chen Yan", "Rui Zhu"], "date_published": "2020-12-10 13:55:47+00:00", "data_last_modified": "2021-01-21 03:25:38+00:00", "url": "http://arxiv.org/abs/2012.05672v2", "abstract": "A common vision from science fiction is that robots will one day inhabit our\nphysical spaces, sense the world as we do, assist our physical labours, and\ncommunicate with us through natural language. Here we study how to design\nartificial agents that can interact naturally with humans using the\nsimplification of a virtual environment. This setting nevertheless integrates a\nnumber of the central challenges of artificial intelligence (AI) research:\ncomplex visual perception and goal-directed physical control, grounded language\ncomprehension and production, and multi-agent social interaction. To build\nagents that can robustly interact with humans, we would ideally train them\nwhile they interact with humans. However, this is presently impractical.\nTherefore, we approximate the role of the human with another learned agent, and\nuse ideas from inverse reinforcement learning to reduce the disparities between\nhuman-human and agent-agent interactive behaviour. Rigorously evaluating our\nagents poses a great challenge, so we develop a variety of behavioural tests,\nincluding evaluation by humans who watch videos of agents or interact directly\nwith them. These evaluations convincingly demonstrate that interactive training\nand auxiliary losses improve agent behaviour beyond what is achieved by\nsupervised learning of actions alone. Further, we demonstrate that agent\ncapabilities generalise beyond literal experiences in the dataset. Finally, we\ntrain evaluation models whose ratings of agents agree well with human\njudgement, thus permitting the evaluation of new agent models without\nadditional effort. Taken together, our results in this virtual environment\nprovide evidence that large-scale human behavioural imitation is a promising\ntool to create intelligent, interactive agents, and the challenge of reliably\nevaluating such agents is possible to surmount.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.08700": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.08700v2", "post_title": "Safe Reinforcement Learning with Model Uncertainty Estimates", "authors": ["Bj\u00f6rn L\u00fctjens", "Michael Everett", "Jonathan P. How"], "date_published": "2018-10-19 22:04:59+00:00", "data_last_modified": "2019-03-01 05:03:11+00:00", "url": "http://arxiv.org/abs/1810.08700v2", "abstract": "Many current autonomous systems are being designed with a strong reliance on\nblack box predictions from deep neural networks (DNNs). However, DNNs tend to\nbe overconfident in predictions on unseen data and can give unpredictable\nresults for far-from-distribution test data. The importance of predictions that\nare robust to this distributional shift is evident for safety-critical\napplications, such as collision avoidance around pedestrians. Measures of model\nuncertainty can be used to identify unseen data, but the state-of-the-art\nextraction methods such as Bayesian neural networks are mostly intractable to\ncompute. This paper uses MC-Dropout and Bootstrapping to give computationally\ntractable and parallelizable uncertainty estimates. The methods are embedded in\na Safe Reinforcement Learning framework to form uncertainty-aware navigation\naround pedestrians. The result is a collision avoidance policy that knows what\nit does not know and cautiously avoids pedestrians that exhibit unseen\nbehavior. The policy is demonstrated in simulation to be more robust to novel\nobservations and take safer actions than an uncertainty-unaware baseline.", "author_comment": "ICRA 2019; Presented at IROS 2018 Workshop on Machine Learning in\n  Robot Motion Planning", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.00525": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.00525v2", "post_title": "On the Geometry of Adversarial Examples", "authors": ["Marc Khoury", "Dylan Hadfield-Menell"], "date_published": "2018-11-01 17:47:10+00:00", "data_last_modified": "2018-12-11 21:43:30+00:00", "url": "http://arxiv.org/abs/1811.00525v2", "abstract": "Adversarial examples are a pervasive phenomenon of machine learning models\nwhere seemingly imperceptible perturbations to the input lead to\nmisclassifications for otherwise statistically accurate models. We propose a\ngeometric framework, drawing on tools from the manifold reconstruction\nliterature, to analyze the high-dimensional geometry of adversarial examples.\nIn particular, we highlight the importance of codimension: for low-dimensional\ndata manifolds embedded in high-dimensional space there are many directions off\nthe manifold in which to construct adversarial examples. Adversarial examples\nare a natural consequence of learning a decision boundary that classifies the\nlow-dimensional data manifold well, but classifies points near the manifold\nincorrectly. Using our geometric framework we prove (1) a tradeoff between\nrobustness under different norms, (2) that adversarial training in balls around\nthe data is sample inefficient, and (3) sufficient sampling conditions under\nwhich nearest neighbor classifiers and ball-based adversarial training are\nrobust.", "author_comment": "Improvements to clarity and presentation over initial submission", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2103.03874": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2103.03874v2", "post_title": "Measuring Mathematical Problem Solving With the MATH Dataset", "authors": ["Dan Hendrycks", "Collin Burns", "Saurav Kadavath", "Akul Arora", "Steven Basart", "Eric Tang", "Dawn Song", "Jacob Steinhardt"], "date_published": "2021-03-05 18:59:39+00:00", "data_last_modified": "2021-11-08 21:30:18+00:00", "url": "http://arxiv.org/abs/2103.03874v2", "abstract": "Many intellectual endeavors require mathematical problem solving, but this\nskill remains beyond the capabilities of computers. To measure this ability in\nmachine learning models, we introduce MATH, a new dataset of 12,500 challenging\ncompetition mathematics problems. Each problem in MATH has a full step-by-step\nsolution which can be used to teach models to generate answer derivations and\nexplanations. To facilitate future research and increase accuracy on MATH, we\nalso contribute a large auxiliary pretraining dataset which helps teach models\nthe fundamentals of mathematics. Even though we are able to increase accuracy\non MATH, our results show that accuracy remains relatively low, even with\nenormous Transformer models. Moreover, we find that simply increasing budgets\nand model parameter counts will be impractical for achieving strong\nmathematical reasoning if scaling trends continue. While scaling Transformers\nis automatically solving most other text-based tasks, scaling is not currently\nsolving MATH. To have more traction on mathematical problem solving we will\nlikely need new algorithmic advancements from the broader research community.", "author_comment": "NeurIPS 2021. Code and the MATH dataset is available at\n  https://github.com/hendrycks/math/", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1808.03644": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1808.03644v1", "post_title": "Building Safer AGI by introducing Artificial Stupidity", "authors": ["Micha\u00ebl Trazzi", "Roman V. Yampolskiy"], "date_published": "2018-08-11 00:14:33+00:00", "data_last_modified": "2018-08-11 00:14:33+00:00", "url": "http://arxiv.org/abs/1808.03644v1", "abstract": "Artificial Intelligence (AI) achieved super-human performance in a broad\nvariety of domains. We say that an AI is made Artificially Stupid on a task\nwhen some limitations are deliberately introduced to match a human's ability to\ndo the task. An Artificial General Intelligence (AGI) can be made safer by\nlimiting its computing power and memory, or by introducing Artificial Stupidity\non certain tasks. We survey human intellectual limits and give recommendations\nfor which limits to implement in order to build a safe AGI.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2105.09938": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2105.09938v3", "post_title": "Measuring Coding Challenge Competence With APPS", "authors": ["Dan Hendrycks", "Steven Basart", "Saurav Kadavath", "Mantas Mazeika", "Akul Arora", "Ethan Guo", "Collin Burns", "Samir Puranik", "Horace He", "Dawn Song", "Jacob Steinhardt"], "date_published": "2021-05-20 17:58:42+00:00", "data_last_modified": "2021-11-08 21:16:44+00:00", "url": "http://arxiv.org/abs/2105.09938v3", "abstract": "While programming is one of the most broadly applicable skills in modern\nsociety, modern machine learning models still cannot code solutions to basic\nproblems. Despite its importance, there has been surprisingly little work on\nevaluating code generation, and it can be difficult to accurately assess code\ngeneration performance rigorously. To meet this challenge, we introduce APPS, a\nbenchmark for code generation. Unlike prior work in more restricted settings,\nour benchmark measures the ability of models to take an arbitrary natural\nlanguage specification and generate satisfactory Python code. Similar to how\ncompanies assess candidate software developers, we then evaluate models by\nchecking their generated code on test cases. Our benchmark includes 10,000\nproblems, which range from having simple one-line solutions to being\nsubstantial algorithmic challenges. We fine-tune large language models on both\nGitHub and our training set, and we find that the prevalence of syntax errors\nis decreasing exponentially as models improve. Recent models such as GPT-Neo\ncan pass approximately 20% of the test cases of introductory problems, so we\nfind that machine learning models are now beginning to learn how to code. As\nthe social significance of automatic code generation increases over the coming\nyears, our benchmark can provide an important measure for tracking\nadvancements.", "author_comment": "NeurIPS 2021. Code and the APPS dataset is available at\n  https://github.com/hendrycks/apps", "journal_ref": null, "doi": null, "primary_category": "cs.SE", "categories": ["cs.SE", "cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1808.04468": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1808.04468v2", "post_title": "Risk-Sensitive Generative Adversarial Imitation Learning", "authors": ["Jonathan Lacotte", "Mohammad Ghavamzadeh", "Yinlam Chow", "Marco Pavone"], "date_published": "2018-08-13 21:08:46+00:00", "data_last_modified": "2018-12-24 02:41:29+00:00", "url": "http://arxiv.org/abs/1808.04468v2", "abstract": "We study risk-sensitive imitation learning where the agent's goal is to\nperform at least as well as the expert in terms of a risk profile. We first\nformulate our risk-sensitive imitation learning setting. We consider the\ngenerative adversarial approach to imitation learning (GAIL) and derive an\noptimization problem for our formulation, which we call it risk-sensitive GAIL\n(RS-GAIL). We then derive two different versions of our RS-GAIL optimization\nproblem that aim at matching the risk profiles of the agent and the expert\nw.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop\nrisk-sensitive generative adversarial imitation learning algorithms based on\nthese optimization problems. We evaluate the performance of our algorithms and\ncompare them with GAIL and the risk-averse imitation learning (RAIL) algorithms\nin two MuJoCo and two OpenAI classical control tasks.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2202.01197": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2202.01197v3", "post_title": "VOS: Learning What You Don't Know by Virtual Outlier Synthesis", "authors": ["Xuefeng Du", "Zhaoning Wang", "Mu Cai", "Yixuan Li"], "date_published": "2022-02-02 18:43:01+00:00", "data_last_modified": "2022-02-04 17:41:29+00:00", "url": "http://arxiv.org/abs/2202.01197v3", "abstract": "Out-of-distribution (OOD) detection has received much attention lately due to\nits importance in the safe deployment of neural networks. One of the key\nchallenges is that models lack supervision signals from unknown data, and as a\nresult, can produce overconfident predictions on OOD data. Previous approaches\nrely on real outlier datasets for model regularization, which can be costly and\nsometimes infeasible to obtain in practice. In this paper, we present VOS, a\nnovel framework for OOD detection by adaptively synthesizing virtual outliers\nthat can meaningfully regularize the model's decision boundary during training.\nSpecifically, VOS samples virtual outliers from the low-likelihood region of\nthe class-conditional distribution estimated in the feature space. Alongside,\nwe introduce a novel unknown-aware training objective, which contrastively\nshapes the uncertainty space between the ID data and synthesized outlier data.\nVOS achieves state-of-the-art performance on both object detection and image\nclassification models, reducing the FPR95 by up to 7.87% compared to the\nprevious best method. Code is available at\nhttps://github.com/deeplearning-wisc/vos.", "author_comment": "ICLR 2022", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.01557": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.01557v2", "post_title": "Value Alignment Verification", "authors": ["Daniel S. Brown", "Jordan Schneider", "Anca D. Dragan", "Scott Niekum"], "date_published": "2020-12-02 22:04:01+00:00", "data_last_modified": "2021-06-11 16:55:00+00:00", "url": "http://arxiv.org/abs/2012.01557v2", "abstract": "As humans interact with autonomous agents to perform increasingly\ncomplicated, potentially risky tasks, it is important to be able to efficiently\nevaluate an agent's performance and correctness. In this paper we formalize and\ntheoretically analyze the problem of efficient value alignment verification:\nhow to efficiently test whether the behavior of another agent is aligned with a\nhuman's values. The goal is to construct a kind of \"driver's test\" that a human\ncan give to any agent which will verify value alignment via a minimal number of\nqueries. We study alignment verification problems with both idealized humans\nthat have an explicit reward function as well as problems where they have\nimplicit values. We analyze verification of exact value alignment for rational\nagents and propose and analyze heuristic and approximate value alignment\nverification tests in a wide range of gridworlds and a continuous autonomous\ndriving domain. Finally, we prove that there exist sufficient conditions such\nthat we can verify exact and approximate alignment across an infinite set of\ntest environments via a constant-query-complexity alignment test.", "author_comment": "In proceedings International Conference on Machine Learning (ICML)\n  2021", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2201.01763": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2201.01763v1", "post_title": "Robust Self-Supervised Audio-Visual Speech Recognition", "authors": ["Bowen Shi", "Wei-Ning Hsu", "Abdelrahman Mohamed"], "date_published": "2022-01-05 18:50:50+00:00", "data_last_modified": "2022-01-05 18:50:50+00:00", "url": "http://arxiv.org/abs/2201.01763v1", "abstract": "Audio-based automatic speech recognition (ASR) degrades significantly in\nnoisy environments and is particularly vulnerable to interfering speech, as the\nmodel cannot determine which speaker to transcribe. Audio-visual speech\nrecognition (AVSR) systems improve robustness by complementing the audio stream\nwith the visual information that is invariant to noise and helps the model\nfocus on the desired speaker. However, previous AVSR work focused solely on the\nsupervised learning setup; hence the progress was hindered by the amount of\nlabeled data available. In this work, we present a self-supervised AVSR\nframework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art\naudio-visual speech representation learning model. On the largest available\nAVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by\n~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in\nthe presence of babble noise, while reducing the WER of an audio-based model by\nover 75% (25.8% vs. 5.8%) on average.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.SD", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2007.13544": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2007.13544v2", "post_title": "Combining Deep Reinforcement Learning and Search for Imperfect-Information Games", "authors": ["Noam Brown", "Anton Bakhtin", "Adam Lerer", "Qucheng Gong"], "date_published": "2020-07-27 15:21:22+00:00", "data_last_modified": "2020-11-29 03:18:13+00:00", "url": "http://arxiv.org/abs/2007.13544v2", "abstract": "The combination of deep reinforcement learning and search at both training\nand test time is a powerful paradigm that has led to a number of successes in\nsingle-agent settings and perfect-information games, best exemplified by\nAlphaZero. However, prior algorithms of this form cannot cope with\nimperfect-information games. This paper presents ReBeL, a general framework for\nself-play reinforcement learning and search that provably converges to a Nash\nequilibrium in any two-player zero-sum game. In the simpler setting of\nperfect-information games, ReBeL reduces to an algorithm similar to AlphaZero.\nResults in two different imperfect-information games show ReBeL converges to an\napproximate Nash equilibrium. We also show ReBeL achieves superhuman\nperformance in heads-up no-limit Texas hold'em poker, while using far less\ndomain knowledge than any prior poker AI.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.GT", "categories": ["cs.GT", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.10996": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.10996v1", "post_title": "Cartesian Frames", "authors": ["Scott Garrabrant", "Daniel A. Herrmann", "Josiah Lopez-Wild"], "date_published": "2021-09-22 19:27:05+00:00", "data_last_modified": "2021-09-22 19:27:05+00:00", "url": "http://arxiv.org/abs/2109.10996v1", "abstract": "We introduce a novel framework, the theory of Cartesian frames (CF), that\ngives powerful tools for manipulating sets of acts. The CF framework takes as\nits most fundamental building block that an agent can freely choose from a set\nof available actions. The framework uses the mathematics of Chu spaces to\ndevelop a calculus of those sets of actions, how those actions change at\nvarious levels of description, and how different agents' actions can combine\nwhen agents work in concert. We discuss how this framework might provide an\nilluminating perspective on issues in decision theory and formal epistemology.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "math.CT", "categories": ["math.CT"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1910.02910": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1910.02910v2", "post_title": "Scaled Autonomy: Enabling Human Operators to Control Robot Fleets", "authors": ["Gokul Swamy", "Siddharth Reddy", "Sergey Levine", "Anca D. Dragan"], "date_published": "2019-09-22 01:00:49+00:00", "data_last_modified": "2020-03-08 22:36:53+00:00", "url": "http://arxiv.org/abs/1910.02910v2", "abstract": "Autonomous robots often encounter challenging situations where their control\npolicies fail and an expert human operator must briefly intervene, e.g.,\nthrough teleoperation. In settings where multiple robots act in separate\nenvironments, a single human operator can manage a fleet of robots by\nidentifying and teleoperating one robot at any given time. The key challenge is\nthat users have limited attention: as the number of robots increases, users\nlose the ability to decide which robot requires teleoperation the most. Our\ngoal is to automate this decision, thereby enabling users to supervise more\nrobots than their attention would normally allow for. Our insight is that we\ncan model the user's choice of which robot to control as an approximately\noptimal decision that maximizes the user's utility function. We learn a model\nof the user's preferences from observations of the user's choices in easy\nsettings with a few robots, and use it in challenging settings with more robots\nto automatically identify which robot the user would most likely choose to\ncontrol, if they were able to evaluate the states of all robots at all times.\nWe run simulation experiments and a user study with twelve participants that\nshow our method can be used to assist users in performing a simulated\nnavigation task. We also run a hardware demonstration that illustrates how our\nmethod can be applied to a real-world mobile robot navigation task.", "author_comment": "Accepted to International Conference on Robotics and Automation\n  (ICRA) 2020", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2103.05247": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2103.05247v2", "post_title": "Pretrained Transformers as Universal Computation Engines", "authors": ["Kevin Lu", "Aditya Grover", "Pieter Abbeel", "Igor Mordatch"], "date_published": "2021-03-09 06:39:56+00:00", "data_last_modified": "2021-06-30 17:34:46+00:00", "url": "http://arxiv.org/abs/2103.05247v2", "abstract": "We investigate the capability of a transformer pretrained on natural language\nto generalize to other modalities with minimal finetuning -- in particular,\nwithout finetuning of the self-attention and feedforward layers of the residual\nblocks. We consider such a model, which we call a Frozen Pretrained Transformer\n(FPT), and study finetuning it on a variety of sequence classification tasks\nspanning numerical computation, vision, and protein fold prediction. In\ncontrast to prior works which investigate finetuning on the same modality as\nthe pretraining dataset, we show that pretraining on natural language can\nimprove performance and compute efficiency on non-language downstream tasks.\nAdditionally, we perform an analysis of the architecture, comparing the\nperformance of a random initialized transformer to a random LSTM. Combining the\ntwo insights, we find language-pretrained transformers can obtain strong\nperformance on a variety of non-language tasks.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1904.01033": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1904.01033v3", "post_title": "Multitask Soft Option Learning", "authors": ["Maximilian Igl", "Andrew Gambardella", "Jinke He", "Nantas Nardelli", "N. Siddharth", "Wendelin B\u00f6hmer", "Shimon Whiteson"], "date_published": "2019-04-01 18:01:34+00:00", "data_last_modified": "2020-06-21 10:36:45+00:00", "url": "http://arxiv.org/abs/1904.01033v3", "abstract": "We present Multitask Soft Option Learning(MSOL), a hierarchical multitask\nframework based on Planning as Inference. MSOL extends the concept of options,\nusing separate variational posteriors for each task, regularized by a shared\nprior. This ''soft'' version of options avoids several instabilities during\ntraining in a multitask setting, and provides a natural way to learn both\nintra-option policies and their terminations. Furthermore, it allows\nfine-tuning of options for new tasks without forgetting their learned policies,\nleading to faster training without reducing the expressiveness of the\nhierarchical policy. We demonstrate empirically that MSOL significantly\noutperforms both hierarchical and flat transfer-learning baselines.", "author_comment": "Published at UAI 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1911.00497": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1911.00497v1", "post_title": "A Narration-based Reward Shaping Approach using Grounded Natural Language Commands", "authors": ["Nicholas Waytowich", "Sean L. Barton", "Vernon Lawhern", "Garrett Warnell"], "date_published": "2019-10-31 22:37:54+00:00", "data_last_modified": "2019-10-31 22:37:54+00:00", "url": "http://arxiv.org/abs/1911.00497v1", "abstract": "While deep reinforcement learning techniques have led to agents that are\nsuccessfully able to learn to perform a number of tasks that had been\npreviously unlearnable, these techniques are still susceptible to the\nlongstanding problem of reward sparsity. This is especially true for tasks such\nas training an agent to play StarCraft II, a real-time strategy game where\nreward is only given at the end of a game which is usually very long. While\nthis problem can be addressed through reward shaping, such approaches typically\nrequire a human expert with specialized knowledge. Inspired by the vision of\nenabling reward shaping through the more-accessible paradigm of\nnatural-language narration, we develop a technique that can provide the\nbenefits of reward shaping using natural language commands. Our\nnarration-guided RL agent projects sequences of natural-language commands into\nthe same high-dimensional representation space as corresponding goal states. We\nshow that we can get improved performance with our method compared to\ntraditional reward-shaping approaches. Additionally, we demonstrate the ability\nof our method to generalize to unseen natural-language commands.", "author_comment": "Presented at the Imitation, Intent and Interaction (I3) workshop,\n  ICML 2019. arXiv admin note: substantial text overlap with arXiv:1906.02671", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.00109": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.00109v1", "post_title": "Probabilistically Safe Robot Planning with Confidence-Based Human Predictions", "authors": ["Jaime F. Fisac", "Andrea Bajcsy", "Sylvia L. Herbert", "David Fridovich-Keil", "Steven Wang", "Claire J. Tomlin", "Anca D. Dragan"], "date_published": "2018-05-31 21:47:34+00:00", "data_last_modified": "2018-05-31 21:47:34+00:00", "url": "http://arxiv.org/abs/1806.00109v1", "abstract": "In order to safely operate around humans, robots can employ predictive models\nof human motion. Unfortunately, these models cannot capture the full complexity\nof human behavior and necessarily introduce simplifying assumptions. As a\nresult, predictions may degrade whenever the observed human behavior departs\nfrom the assumed structure, which can have negative implications for safety. In\nthis paper, we observe that how \"rational\" human actions appear under a\nparticular model can be viewed as an indicator of that model's ability to\ndescribe the human's current motion. By reasoning about this model confidence\nin a real-time Bayesian framework, we show that the robot can very quickly\nmodulate its predictions to become more uncertain when the model performs\npoorly. Building on recent work in provably-safe trajectory planning, we\nleverage these confidence-aware human motion predictions to generate assured\nautonomous robot motion. Our new analysis combines worst-case tracking error\nguarantees for the physical robot with probabilistic time-varying human\npredictions, yielding a quantitative, probabilistic safety certificate. We\ndemonstrate our approach with a quadcopter navigating around a human.", "author_comment": "Robotics Science and Systems (RSS) 2018", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2107.03374": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2107.03374v2", "post_title": "Evaluating Large Language Models Trained on Code", "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde de Oliveira Pinto", "Jared Kaplan", "Harri Edwards", "Yuri Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "Felipe Petroski Such", "Dave Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William Hebgen Guss", "Alex Nichol", "Alex Paino", "Nikolas Tezak", "Jie Tang", "Igor Babuschkin", "Suchir Balaji", "Shantanu Jain", "William Saunders", "Christopher Hesse", "Andrew N. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "Matthew Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "date_published": "2021-07-07 17:41:24+00:00", "data_last_modified": "2021-07-14 17:16:02+00:00", "url": "http://arxiv.org/abs/2107.03374v2", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available\ncode from GitHub, and study its Python code-writing capabilities. A distinct\nproduction version of Codex powers GitHub Copilot. On HumanEval, a new\nevaluation set we release to measure functional correctness for synthesizing\nprograms from docstrings, our model solves 28.8% of the problems, while GPT-3\nsolves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling\nfrom the model is a surprisingly effective strategy for producing working\nsolutions to difficult prompts. Using this method, we solve 70.2% of our\nproblems with 100 samples per problem. Careful investigation of our model\nreveals its limitations, including difficulty with docstrings describing long\nchains of operations and with binding operations to variables. Finally, we\ndiscuss the potential broader impacts of deploying powerful code generation\ntechnologies, covering safety, security, and economics.", "author_comment": "corrected typos, added references, added authors, added\n  acknowledgements", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.16668": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.16668v1", "post_title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "authors": ["Dmitry Lepikhin", "HyoukJoong Lee", "Yuanzhong Xu", "Dehao Chen", "Orhan Firat", "Yanping Huang", "Maxim Krikun", "Noam Shazeer", "Zhifeng Chen"], "date_published": "2020-06-30 10:42:02+00:00", "data_last_modified": "2020-06-30 10:42:02+00:00", "url": "http://arxiv.org/abs/2006.16668v1", "abstract": "Neural network scaling has been critical for improving the model quality in\nmany real-world machine learning applications with vast amounts of training\ndata and compute. Although this trend of scaling is affirmed to be a sure-fire\napproach for better model quality, there are challenges on the path such as the\ncomputation cost, ease of programming, and efficient implementation on parallel\ndevices. GShard is a module composed of a set of lightweight annotation APIs\nand an extension to the XLA compiler. It provides an elegant way to express a\nwide range of parallel computation patterns with minimal changes to the\nexisting model code. GShard enabled us to scale up multilingual neural machine\ntranslation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600\nbillion parameters using automatic sharding. We demonstrate that such a giant\nmodel can efficiently be trained on 2048 TPU v3 accelerators in 4 days to\nachieve far superior quality for translation from 100 languages to English\ncompared to the prior art.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.02840": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.02840v2", "post_title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models", "authors": ["Boxin Wang", "Chejian Xu", "Shuohang Wang", "Zhe Gan", "Yu Cheng", "Jianfeng Gao", "Ahmed Hassan Awadallah", "Bo Li"], "date_published": "2021-11-04 12:59:55+00:00", "data_last_modified": "2022-01-10 06:05:16+00:00", "url": "http://arxiv.org/abs/2111.02840v2", "abstract": "Large-scale pre-trained language models have achieved tremendous success\nacross a wide range of natural language understanding (NLU) tasks, even\nsurpassing human performance. However, recent studies reveal that the\nrobustness of these models can be challenged by carefully crafted textual\nadversarial examples. While several individual datasets have been proposed to\nevaluate model robustness, a principled and comprehensive benchmark is still\nmissing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task\nbenchmark to quantitatively and thoroughly explore and evaluate the\nvulnerabilities of modern large-scale language models under various types of\nadversarial attacks. In particular, we systematically apply 14 textual\nadversarial attack methods to GLUE tasks to construct AdvGLUE, which is further\nvalidated by humans for reliable annotations. Our findings are summarized as\nfollows. (i) Most existing adversarial attack algorithms are prone to\ngenerating invalid or ambiguous adversarial examples, with around 90% of them\neither changing the original semantic meanings or misleading human annotators\nas well. Therefore, we perform a careful filtering process to curate a\nhigh-quality benchmark. (ii) All the language models and robust training\nmethods we tested perform poorly on AdvGLUE, with scores lagging far behind the\nbenign accuracy. We hope our work will motivate the development of new\nadversarial attacks that are more stealthy and semantic-preserving, as well as\nnew robust language models against sophisticated adversarial attacks. AdvGLUE\nis available at https://adversarialglue.github.io.", "author_comment": "Oral Presentation in NeurIPS 2021 (Datasets and Benchmarks Track). 24\n  pages, 4 figures, 12 tables", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.CR", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1904.06866": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1904.06866v1", "post_title": "Predicting human decisions with behavioral theories and machine learning", "authors": ["Ori Plonsky", "Reut Apel", "Eyal Ert", "Moshe Tennenholtz", "David Bourgin", "Joshua C. Peterson", "Daniel Reichman", "Thomas L. Griffiths", "Stuart J. Russell", "Evan C. Carter", "James F. Cavanagh", "Ido Erev"], "date_published": "2019-04-15 06:12:44+00:00", "data_last_modified": "2019-04-15 06:12:44+00:00", "url": "http://arxiv.org/abs/1904.06866v1", "abstract": "Behavioral decision theories aim to explain human behavior. Can they help\npredict it? An open tournament for prediction of human choices in fundamental\neconomic decision tasks is presented. The results suggest that integration of\ncertain behavioral theories as features in machine learning systems provides\nthe best predictions. Surprisingly, the most useful theories for prediction\nbuild on basic properties of human and animal learning and are very different\nfrom mainstream decision theories that focus on deviations from rational\nchoice. Moreover, we find that theoretical features should be based not only on\nqualitative behavioral insights (e.g. loss aversion), but also on quantitative\nbehavioral foresights generated by functional descriptive models (e.g. Prospect\nTheory). Our analysis prescribes a recipe for derivation of explainable, useful\npredictions of human decisions.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.GT", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2202.07789": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2202.07789v1", "post_title": "Safe Reinforcement Learning by Imagining the Near Future", "authors": ["Garrett Thomas", "Yuping Luo", "Tengyu Ma"], "date_published": "2022-02-15 23:28:24+00:00", "data_last_modified": "2022-02-15 23:28:24+00:00", "url": "http://arxiv.org/abs/2202.07789v1", "abstract": "Safe reinforcement learning is a promising path toward applying reinforcement\nlearning algorithms to real-world problems, where suboptimal behaviors may lead\nto actual negative consequences. In this work, we focus on the setting where\nunsafe states can be avoided by planning ahead a short time into the future. In\nthis setting, a model-based agent with a sufficiently accurate model can avoid\nunsafe states. We devise a model-based algorithm that heavily penalizes unsafe\ntrajectories, and derive guarantees that our algorithm can avoid unsafe states\nunder certain assumptions. Experiments demonstrate that our algorithm can\nachieve competitive rewards with fewer safety violations in several continuous\ncontrol tasks.", "author_comment": "Accepted at NeurIPS 2021", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.12616": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.12616v3", "post_title": "Defending Against Neural Fake News", "authors": ["Rowan Zellers", "Ari Holtzman", "Hannah Rashkin", "Yonatan Bisk", "Ali Farhadi", "Franziska Roesner", "Yejin Choi"], "date_published": "2019-05-29 17:58:52+00:00", "data_last_modified": "2020-12-11 16:17:17+00:00", "url": "http://arxiv.org/abs/1905.12616v3", "abstract": "Recent progress in natural language generation has raised dual-use concerns.\nWhile applications like summarization and translation are positive, the\nunderlying technology also might enable adversaries to generate neural fake\nnews: targeted propaganda that closely mimics the style of real news.\n  Modern computer security relies on careful threat modeling: identifying\npotential threats and vulnerabilities from an adversary's point of view, and\nexploring potential mitigations to these threats. Likewise, developing robust\ndefenses against neural fake news requires us first to carefully investigate\nand characterize the risks of these models. We thus present a model for\ncontrollable text generation called Grover. Given a headline like `Link Found\nBetween Vaccines and Autism,' Grover can generate the rest of the article;\nhumans find these generations to be more trustworthy than human-written\ndisinformation.\n  Developing robust verification techniques against generators like Grover is\ncritical. We find that best current discriminators can classify neural fake\nnews from real, human-written, news with 73% accuracy, assuming access to a\nmoderate level of training data. Counterintuitively, the best defense against\nGrover turns out to be Grover itself, with 92% accuracy, demonstrating the\nimportance of public release of strong generators. We investigate these results\nfurther, showing that exposure bias -- and sampling strategies that alleviate\nits effects -- both leave artifacts that similar discriminators can pick up on.\nWe conclude by discussing ethical issues regarding the technology, and plan to\nrelease Grover publicly, helping pave the way for better detection of neural\nfake news.", "author_comment": "NeurIPS 2019 camera ready version. Project page/code/demo at\n  https://rowanzellers.com/grover", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.09266": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.09266v1", "post_title": "Humans learn too: Better Human-AI Interaction using Optimized Human Inputs", "authors": ["Johannes Schneider"], "date_published": "2020-09-19 16:30:37+00:00", "data_last_modified": "2020-09-19 16:30:37+00:00", "url": "http://arxiv.org/abs/2009.09266v1", "abstract": "Humans rely more and more on systems with AI components. The AI community\ntypically treats human inputs as a given and optimizes AI models only. This\nthinking is one-sided and it neglects the fact that humans can learn, too. In\nthis work, human inputs are optimized for better interaction with an AI model\nwhile keeping the model fixed. The optimized inputs are accompanied by\ninstructions on how to create them. They allow humans to save time and cut on\nerrors, while keeping required changes to original inputs limited. We propose\ncontinuous and discrete optimization methods modifying samples in an iterative\nfashion. Our quantitative and qualitative evaluation including a human study on\ndifferent hand-generated inputs shows that the generated proposals lead to\nlower error rates, require less effort to create and differ only modestly from\nthe original samples.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.HC", "categories": ["cs.HC", "cs.AI", "cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.00821": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.00821v4", "post_title": "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow", "authors": ["Xue Bin Peng", "Angjoo Kanazawa", "Sam Toyer", "Pieter Abbeel", "Sergey Levine"], "date_published": "2018-10-01 17:02:24+00:00", "data_last_modified": "2020-08-25 02:41:11+00:00", "url": "http://arxiv.org/abs/1810.00821v4", "abstract": "Adversarial learning methods have been proposed for a wide range of\napplications, but the training of adversarial models can be notoriously\nunstable. Effectively balancing the performance of the generator and\ndiscriminator is critical, since a discriminator that achieves very high\naccuracy will produce relatively uninformative gradients. In this work, we\npropose a simple and general technique to constrain information flow in the\ndiscriminator by means of an information bottleneck. By enforcing a constraint\non the mutual information between the observations and the discriminator's\ninternal representation, we can effectively modulate the discriminator's\naccuracy and maintain useful and informative gradients. We demonstrate that our\nproposed variational discriminator bottleneck (VDB) leads to significant\nimprovements across three distinct application areas for adversarial learning\nalgorithms. Our primary evaluation studies the applicability of the VDB to\nimitation learning of dynamic continuous control skills, such as running. We\nshow that our method can learn such skills directly from \\emph{raw} video\ndemonstrations, substantially outperforming prior adversarial imitation\nlearning methods. The VDB can also be combined with adversarial inverse\nreinforcement learning to learn parsimonious reward functions that can be\ntransferred and re-optimized in new settings. Finally, we demonstrate that VDB\ncan train GANs more effectively for image generation, improving upon a number\nof prior stabilization methods.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2201.12440": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2201.12440v1", "post_title": "Certifying Model Accuracy under Distribution Shifts", "authors": ["Aounon Kumar", "Alexander Levine", "Tom Goldstein", "Soheil Feizi"], "date_published": "2022-01-28 22:03:50+00:00", "data_last_modified": "2022-01-28 22:03:50+00:00", "url": "http://arxiv.org/abs/2201.12440v1", "abstract": "Certified robustness in machine learning has primarily focused on adversarial\nperturbations of the input with a fixed attack budget for each point in the\ndata distribution. In this work, we present provable robustness guarantees on\nthe accuracy of a model under bounded Wasserstein shifts of the data\ndistribution. We show that a simple procedure that randomizes the input of the\nmodel within a transformation space is provably robust to distributional shifts\nunder the transformation. Our framework allows the datum-specific perturbation\nsize to vary across different points in the input distribution and is general\nenough to include fixed-sized perturbations as well. Our certificates produce\nguaranteed lower bounds on the performance of the model for any (natural or\nadversarial) shift of the input distribution within a Wasserstein ball around\nthe original distribution. We apply our technique to: (i) certify robustness\nagainst natural (non-adversarial) transformations of images such as color\nshifts, hue shifts and changes in brightness and saturation, (ii) certify\nrobustness against adversarial shifts of the input distribution, and (iii) show\nprovable lower bounds (hardness results) on the performance of models trained\non so-called \"unlearnable\" datasets that have been poisoned to interfere with\nmodel training.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1711.00363": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1711.00363v1", "post_title": "Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making", "authors": ["Andrew Critch", "Stuart Russell"], "date_published": "2017-10-31 05:09:13+00:00", "data_last_modified": "2017-10-31 05:09:13+00:00", "url": "http://arxiv.org/abs/1711.00363v1", "abstract": "It is often argued that an agent making decisions on behalf of two or more\nprincipals who have different utility functions should adopt a {\\em\nPareto-optimal} policy, i.e., a policy that cannot be improved upon for one\nagent without making sacrifices for another. A famous theorem of Harsanyi shows\nthat, when the principals have a common prior on the outcome distributions of\nall policies, a Pareto-optimal policy for the agent is one that maximizes a\nfixed, weighted linear combination of the principals' utilities.\n  In this paper, we show that Harsanyi's theorem does not hold for principals\nwith different priors, and derive a more precise generalization which does\nhold, which constitutes our main result. In this more general case, the\nrelative weight given to each principal's utility should evolve over time\naccording to how well the agent's observations conform with that principal's\nprior. The result has implications for the design of contracts, treaties, joint\nventures, and robots.", "author_comment": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1701.01302", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2108.01634": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2108.01634v1", "post_title": "Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation", "authors": ["Victor Besnier", "Andrei Bursuc", "David Picard", "Alexandre Briot"], "date_published": "2021-08-03 17:09:56+00:00", "data_last_modified": "2021-08-03 17:09:56+00:00", "url": "http://arxiv.org/abs/2108.01634v1", "abstract": "In this paper, we tackle the detection of out-of-distribution (OOD) objects\nin semantic segmentation. By analyzing the literature, we found that current\nmethods are either accurate or fast but not both which limits their usability\nin real world applications. To get the best of both aspects, we propose to\nmitigate the common shortcomings by following four design principles:\ndecoupling the OOD detection from the segmentation task, observing the entire\nsegmentation network instead of just its output, generating training data for\nthe OOD detector by leveraging blind spots in the segmentation network and\nfocusing the generated data on localized regions in the image to simulate OOD\nobjects. Our main contribution is a new OOD detection architecture called\nObsNet associated with a dedicated training scheme based on Local Adversarial\nAttacks (LAA). We validate the soundness of our approach across numerous\nablation studies. We also show it obtains top performances both in speed and\naccuracy when compared to ten recent methods of the literature on three\ndifferent datasets.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1911.01547": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1911.01547v2", "post_title": "On the Measure of Intelligence", "authors": ["Fran\u00e7ois Chollet"], "date_published": "2019-11-05 00:31:38+00:00", "data_last_modified": "2019-11-25 13:02:04+00:00", "url": "http://arxiv.org/abs/1911.01547v2", "abstract": "To make deliberate progress towards more intelligent and more human-like\nartificial systems, we need to be following an appropriate feedback signal: we\nneed to be able to define and evaluate intelligence in a way that enables\ncomparisons between two systems, as well as comparisons with humans. Over the\npast hundred years, there has been an abundance of attempts to define and\nmeasure intelligence, across both the fields of psychology and AI. We summarize\nand critically assess these definitions and evaluation approaches, while making\napparent the two historical conceptions of intelligence that have implicitly\nguided them. We note that in practice, the contemporary AI community still\ngravitates towards benchmarking intelligence by comparing the skill exhibited\nby AIs and humans at specific tasks such as board games and video games. We\nargue that solely measuring skill at any given task falls short of measuring\nintelligence, because skill is heavily modulated by prior knowledge and\nexperience: unlimited priors or unlimited training data allow experimenters to\n\"buy\" arbitrary levels of skills for a system, in a way that masks the system's\nown generalization power. We then articulate a new formal definition of\nintelligence based on Algorithmic Information Theory, describing intelligence\nas skill-acquisition efficiency and highlighting the concepts of scope,\ngeneralization difficulty, priors, and experience. Using this definition, we\npropose a set of guidelines for what a general AI benchmark should look like.\nFinally, we present a benchmark closely following these guidelines, the\nAbstraction and Reasoning Corpus (ARC), built upon an explicit set of priors\ndesigned to be as close as possible to innate human priors. We argue that ARC\ncan be used to measure a human-like form of general fluid intelligence and that\nit enables fair general intelligence comparisons between AI systems and humans.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.01475": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.01475v1", "post_title": "Generalizing from a few environments in safety-critical reinforcement learning", "authors": ["Zachary Kenton", "Angelos Filos", "Owain Evans", "Yarin Gal"], "date_published": "2019-07-02 16:12:34+00:00", "data_last_modified": "2019-07-02 16:12:34+00:00", "url": "http://arxiv.org/abs/1907.01475v1", "abstract": "Before deploying autonomous agents in the real world, we need to be confident\nthey will perform safely in novel situations. Ideally, we would expose agents\nto a very wide range of situations during training, allowing them to learn\nabout every possible danger, but this is often impractical. This paper\ninvestigates safety and generalization from a limited number of training\nenvironments in deep reinforcement learning (RL). We find RL algorithms can\nfail dangerously on unseen test environments even when performing perfectly on\ntraining environments. Firstly, in a gridworld setting, we show that\ncatastrophes can be significantly reduced with simple modifications, including\nensemble model averaging and the use of a blocking classifier. In the more\nchallenging CoinRun environment we find similar methods do not significantly\nreduce catastrophes. However, we do find that the uncertainty information from\nthe ensemble is useful for predicting whether a catastrophe will occur within a\nfew steps and hence whether human intervention should be requested.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2008.01848": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2008.01848v1", "post_title": "Forecasting AI Progress: A Research Agenda", "authors": ["Ross Gruetzemacher", "Florian Dorner", "Niko Bernaola-Alvarez", "Charlie Giattino", "David Manheim"], "date_published": "2020-08-04 21:46:46+00:00", "data_last_modified": "2020-08-04 21:46:46+00:00", "url": "http://arxiv.org/abs/2008.01848v1", "abstract": "Forecasting AI progress is essential to reducing uncertainty in order to\nappropriately plan for research efforts on AI safety and AI governance. While\nthis is generally considered to be an important topic, little work has been\nconducted on it and there is no published document that gives and objective\noverview of the field. Moreover, the field is very diverse and there is no\npublished consensus regarding its direction. This paper describes the\ndevelopment of a research agenda for forecasting AI progress which utilized the\nDelphi technique to elicit and aggregate experts' opinions on what questions\nand methods to prioritize. The results of the Delphi are presented; the\nremainder of the paper follow the structure of these results, briefly reviewing\nrelevant literature and suggesting future work for each topic. Experts\nindicated that a wide variety of methods should be considered for forecasting\nAI progress. Moreover, experts identified salient questions that were both\ngeneral and completely unique to the problem of forecasting AI progress. Some\nof the highest priority topics include the validation of (partially unresolved)\nforecasts, how to make forecasting action-guiding and the quality of different\nperformance metrics. While statistical methods seem more promising, there is\nalso recognition that supplementing judgmental techniques can be quite\nbeneficial.", "author_comment": "40 pages including Appendices, 1 figure, 5 tables", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.00945": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.00945v2", "post_title": "Adversarial Robustness as a Prior for Learned Representations", "authors": ["Logan Engstrom", "Andrew Ilyas", "Shibani Santurkar", "Dimitris Tsipras", "Brandon Tran", "Aleksander Madry"], "date_published": "2019-06-03 17:55:20+00:00", "data_last_modified": "2019-09-27 17:39:54+00:00", "url": "http://arxiv.org/abs/1906.00945v2", "abstract": "An important goal in deep learning is to learn versatile, high-level feature\nrepresentations of input data. However, standard networks' representations seem\nto possess shortcomings that, as we illustrate, prevent them from fully\nrealizing this goal. In this work, we show that robust optimization can be\nre-cast as a tool for enforcing priors on the features learned by deep neural\nnetworks. It turns out that representations learned by robust models address\nthe aforementioned shortcomings and make significant progress towards learning\na high-level encoding of inputs. In particular, these representations are\napproximately invertible, while allowing for direct visualization and\nmanipulation of salient input features. More broadly, our results indicate\nadversarial robustness as a promising avenue for improving learned\nrepresentations. Our code and models for reproducing these results is available\nat https://git.io/robust-reps .", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2011.04864": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2011.04864v1", "post_title": "Natural Language Inference in Context -- Investigating Contextual Reasoning over Long Texts", "authors": ["Hanmeng Liu", "Leyang Cui", "Jian Liu", "Yue Zhang"], "date_published": "2020-11-10 02:31:31+00:00", "data_last_modified": "2020-11-10 02:31:31+00:00", "url": "http://arxiv.org/abs/2011.04864v1", "abstract": "Natural language inference (NLI) is a fundamental NLP task, investigating the\nentailment relationship between two texts. Popular NLI datasets present the\ntask at sentence-level. While adequate for testing semantic representations,\nthey fall short for testing contextual reasoning over long texts, which is a\nnatural part of the human inference process. We introduce ConTRoL, a new\ndataset for ConTextual Reasoning over Long texts. Consisting of 8,325\nexpert-designed \"context-hypothesis\" pairs with gold labels, ConTRoL is a\npassage-level NLI dataset with a focus on complex contextual reasoning types\nsuch as logical reasoning. It is derived from competitive selection and\nrecruitment test (verbal reasoning test) for police recruitment, with expert\nlevel quality. Compared with previous NLI benchmarks, the materials in ConTRoL\nare much more challenging, involving a range of reasoning types. Empirical\nresults show that state-of-the-art language models perform by far worse than\neducated humans. Our dataset can also serve as a testing-set for downstream\ntasks like Checking Factual Correctness of Summaries.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.08180": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.08180v2", "post_title": "Hierarchical Reinforcement Learning with Hindsight", "authors": ["Andrew Levy", "Robert Platt", "Kate Saenko"], "date_published": "2018-05-21 17:02:53+00:00", "data_last_modified": "2019-03-08 17:52:47+00:00", "url": "http://arxiv.org/abs/1805.08180v2", "abstract": "Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency\nwhen rewards are delayed and sparse. We introduce a solution that enables\nagents to learn temporally extended actions at multiple levels of abstraction\nin a sample efficient and automated fashion. Our approach combines universal\nvalue functions and hindsight learning, allowing agents to learn policies\nbelonging to different time scales in parallel. We show that our method\nsignificantly accelerates learning in a variety of discrete and continuous\ntasks.", "author_comment": "Duplicate. See arXiv:1712.00948 \"Learning Multi-Level Hierarchies\n  with Hindsight\" for latest version", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.14715": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.14715v3", "post_title": "Learning Rewards from Linguistic Feedback", "authors": ["Theodore R. Sumers", "Mark K. Ho", "Robert D. Hawkins", "Karthik Narasimhan", "Thomas L. Griffiths"], "date_published": "2020-09-30 14:51:00+00:00", "data_last_modified": "2021-07-03 19:03:12+00:00", "url": "http://arxiv.org/abs/2009.14715v3", "abstract": "We explore unconstrained natural language feedback as a learning signal for\nartificial agents. Humans use rich and varied language to teach, yet most prior\nwork on interactive learning from language assumes a particular form of input\n(e.g., commands). We propose a general framework which does not make this\nassumption, using aspect-based sentiment analysis to decompose feedback into\nsentiment about the features of a Markov decision process. We then perform an\nanalogue of inverse reinforcement learning, regressing the sentiment on the\nfeatures to infer the teacher's latent reward function. To evaluate our\napproach, we first collect a corpus of teaching behavior in a cooperative task\nwhere both teacher and learner are human. We implement three artificial\nlearners: sentiment-based \"literal\" and \"pragmatic\" models, and an inference\nnetwork trained end-to-end to predict latent rewards. We then repeat our\ninitial experiment and pair them with human teachers. All three successfully\nlearn from interactive human feedback. The sentiment models outperform the\ninference network, with the \"pragmatic\" model approaching human performance.\nOur work thus provides insight into the information structure of naturalistic\nlinguistic feedback as well as methods to leverage it for reinforcement\nlearning.", "author_comment": "9 pages, 4 figures. AAAI '21", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1702.03465": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1702.03465v2", "post_title": "Enabling Robots to Communicate their Objectives", "authors": ["Sandy H. Huang", "David Held", "Pieter Abbeel", "Anca D. Dragan"], "date_published": "2017-02-11 22:39:39+00:00", "data_last_modified": "2018-10-18 17:43:04+00:00", "url": "http://arxiv.org/abs/1702.03465v2", "abstract": "The overarching goal of this work is to efficiently enable end-users to\ncorrectly anticipate a robot's behavior in novel situations. Since a robot's\nbehavior is often a direct result of its underlying objective function, our\ninsight is that end-users need to have an accurate mental model of this\nobjective function in order to understand and predict what the robot will do.\nWhile people naturally develop such a mental model over time through observing\nthe robot act, this familiarization process may be lengthy. Our approach\nreduces this time by having the robot model how people infer objectives from\nobserved behavior, and then it selects those behaviors that are maximally\ninformative. The problem of computing a posterior over objectives from observed\nbehavior is known as Inverse Reinforcement Learning (IRL), and has been applied\nto robots learning human objectives. We consider the problem where the roles of\nhuman and robot are swapped. Our main contribution is to recognize that unlike\nrobots, humans will not be exact in their IRL inference. We thus introduce two\nfactors to define candidate approximate-inference models for human learning in\nthis setting, and analyze them in a user study in the autonomous driving\ndomain. We show that certain approximate-inference models lead to the robot\ngenerating example behaviors that better enable users to anticipate what it\nwill do in novel situations. Our results also suggest, however, that additional\nresearch is needed in modeling how humans extrapolate from examples of robot\nbehavior.", "author_comment": "RSS 2017", "journal_ref": null, "doi": "10.15607/RSS.2017.XIII.059", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.07118": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.07118v2", "post_title": "The Incentives that Shape Behaviour", "authors": ["Ryan Carey", "Eric Langlois", "Tom Everitt", "Shane Legg"], "date_published": "2020-01-20 14:32:07+00:00", "data_last_modified": "2021-03-15 20:02:54+00:00", "url": "http://arxiv.org/abs/2001.07118v2", "abstract": "Which variables does an agent have an incentive to control with its decision,\nand which variables does it have an incentive to respond to? We formalise these\nincentives, and demonstrate unique graphical criteria for detecting them in any\nsingle decision causal influence diagram. To this end, we introduce structural\ncausal influence models, a hybrid of the influence diagram and structural\ncausal model frameworks. Finally, we illustrate how these incentives predict\nagent incentives in both fairness and AI safety applications.", "author_comment": "In SafeAI workshop at AAAI. Superseded by arXiv:2102.01685", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "I.2.6; I.2.8"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.10498": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.10498v2", "post_title": "Cold Case: The Lost MNIST Digits", "authors": ["Chhavi Yadav", "L\u00e9on Bottou"], "date_published": "2019-05-25 01:50:51+00:00", "data_last_modified": "2019-11-04 21:05:26+00:00", "url": "http://arxiv.org/abs/1905.10498v2", "abstract": "Although the popular MNIST dataset [LeCun et al., 1994] is derived from the\nNIST database [Grother and Hanaoka, 1995], the precise processing steps for\nthis derivation have been lost to time. We propose a reconstruction that is\naccurate enough to serve as a replacement for the MNIST dataset, with\ninsignificant changes in accuracy. We trace each MNIST digit to its NIST source\nand its rich metadata such as writer identifier, partition identifier, etc. We\nalso reconstruct the complete MNIST test set with 60,000 samples instead of the\nusual 10,000. Since the balance 50,000 were never distributed, they enable us\nto investigate the impact of twenty-five years of MNIST experiments on the\nreported testing performances. Our results unambiguously confirm the trends\nobserved by Recht et al. [2018, 2019]: although the misclassification rates are\nslightly off, classifier ordering and model selection remain broadly reliable.\nWe attribute this phenomenon to the pairing benefits of comparing classifiers\non the same digits.", "author_comment": "Final NeurIPS version", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.05862": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.05862v1", "post_title": "Understanding Learned Reward Functions", "authors": ["Eric J. Michaud", "Adam Gleave", "Stuart Russell"], "date_published": "2020-12-10 18:19:48+00:00", "data_last_modified": "2020-12-10 18:19:48+00:00", "url": "http://arxiv.org/abs/2012.05862v1", "abstract": "In many real-world tasks, it is not possible to procedurally specify an RL\nagent's reward function. In such cases, a reward function must instead be\nlearned from interacting with and observing humans. However, current techniques\nfor reward learning may fail to produce reward functions which accurately\nreflect user preferences. Absent significant advances in reward learning, it is\nthus important to be able to audit learned reward functions to verify whether\nthey truly capture user preferences. In this paper, we investigate techniques\nfor interpreting learned reward functions. In particular, we apply saliency\nmethods to identify failure modes and predict the robustness of reward\nfunctions. We find that learned reward functions often implement surprising\nalgorithms that rely on contingent aspects of the environment. We also discover\nthat existing interpretability techniques often attend to irrelevant changes in\nreward output, suggesting that reward interpretability may need significantly\ndifferent methods from policy interpretability.", "author_comment": "Presented at Deep RL Workshop, NeurIPS 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.08265": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.08265v1", "post_title": "Quantifying Perceptual Distortion of Adversarial Examples", "authors": ["Matt Jordan", "Naren Manoj", "Surbhi Goel", "Alexandros G. Dimakis"], "date_published": "2019-02-21 21:02:58+00:00", "data_last_modified": "2019-02-21 21:02:58+00:00", "url": "http://arxiv.org/abs/1902.08265v1", "abstract": "Recent work has shown that additive threat models, which only permit the\naddition of bounded noise to the pixels of an image, are insufficient for fully\ncapturing the space of imperceivable adversarial examples. For example, small\nrotations and spatial transformations can fool classifiers, remain\nimperceivable to humans, but have large additive distance from the original\nimages. In this work, we leverage quantitative perceptual metrics like LPIPS\nand SSIM to define a novel threat model for adversarial attacks.\n  To demonstrate the value of quantifying the perceptual distortion of\nadversarial examples, we present and employ a unifying framework fusing\ndifferent attack styles. We first prove that our framework results in images\nthat are unattainable by attack styles in isolation. We then perform\nadversarial training using attacks generated by our framework to demonstrate\nthat networks are only robust to classes of adversarial perturbations they have\nbeen trained against, and combination attacks are stronger than any of their\nindividual components. Finally, we experimentally demonstrate that our combined\nattacks retain the same perceptual distortion but induce far higher\nmisclassification rates when compared against individual attacks.", "author_comment": "18 pages, codebase/framework available at\n  https://github.com/revbucket/mister_ed", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1510.03370": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1510.03370v1", "post_title": "Asymptotic Logical Uncertainty and The Benford Test", "authors": ["Scott Garrabrant", "Siddharth Bhaskar", "Abram Demski", "Joanna Garrabrant", "George Koleszarik", "Evan Lloyd"], "date_published": "2015-10-12 17:14:44+00:00", "data_last_modified": "2015-10-12 17:14:44+00:00", "url": "http://arxiv.org/abs/1510.03370v1", "abstract": "We give an algorithm A which assigns probabilities to logical sentences. For\nany simple infinite sequence of sentences whose truth-values appear\nindistinguishable from a biased coin that outputs \"true\" with probability p, we\nhave that the sequence of probabilities that A assigns to these sentences\nconverges to p.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "F.4.1"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.03493": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.03493v2", "post_title": "Integrative Biological Simulation, Neuropsychology, and AI Safety", "authors": ["Gopal P. Sarma", "Adam Safron", "Nick J. Hay"], "date_published": "2018-11-07 01:38:24+00:00", "data_last_modified": "2019-01-21 19:04:47+00:00", "url": "http://arxiv.org/abs/1811.03493v2", "abstract": "We describe a biologically-inspired research agenda with parallel tracks\naimed at AI and AI safety. The bottom-up component consists of building a\nsequence of biophysically realistic simulations of simple organisms such as the\nnematode $Caenorhabditis$ $elegans$, the fruit fly $Drosophila$ $melanogaster$,\nand the zebrafish $Danio$ $rerio$ to serve as platforms for research into AI\nalgorithms and system architectures. The top-down component consists of an\napproach to value alignment that grounds AI goal structures in neuropsychology,\nbroadly considered. Our belief is that parallel pursuit of these tracks will\ninform the development of value-aligned AI systems that have been inspired by\nembodied organisms with sensorimotor integration. An important set of side\nbenefits is that the research trajectories we describe here are grounded in\nlong-standing intellectual traditions within existing research communities and\nfunding structures. In addition, these research programs overlap with\nsignificant contemporary themes in the biological and psychological sciences\nsuch as data/model integration and reproducibility.", "author_comment": "5 pages", "journal_ref": "Proceedings of the AAAI Workshop on Artificial Intelligence Safety\n  2019 co-located with the Thirty-Third AAAI Conference on Artificial\n  Intelligence 2019 (AAAI 2019)", "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "cs.NE", "q-bio.NC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.04543": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.04543v4", "post_title": "An Optimistic Perspective on Offline Reinforcement Learning", "authors": ["Rishabh Agarwal", "Dale Schuurmans", "Mohammad Norouzi"], "date_published": "2019-07-10 07:23:27+00:00", "data_last_modified": "2020-06-22 04:32:50+00:00", "url": "http://arxiv.org/abs/1907.04543v4", "abstract": "Off-policy reinforcement learning (RL) using a fixed offline dataset of\nlogged interactions is an important consideration in real world applications.\nThis paper studies offline RL using the DQN replay dataset comprising the\nentire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate\nthat recent off-policy deep RL algorithms, even when trained solely on this\nfixed dataset, outperform the fully trained DQN agent. To enhance\ngeneralization in the offline setting, we present Random Ensemble Mixture\n(REM), a robust Q-learning algorithm that enforces optimal Bellman consistency\non random convex combinations of multiple Q-value estimates. Offline REM\ntrained on the DQN replay dataset surpasses strong RL baselines. Ablation\nstudies highlight the role of offline dataset size and diversity as well as the\nalgorithm choice in our positive results. Overall, the results here present an\noptimistic view that robust RL algorithms trained on sufficiently large and\ndiverse offline datasets can lead to high quality policies. The DQN replay\ndataset can serve as an offline RL benchmark and is open-sourced.", "author_comment": "ICML 2020. An earlier version was titled \"Striving for Simplicity in\n  Off-Policy Deep Reinforcement Learning\". Project Website:\n  https://offline-rl.github.io", "journal_ref": "Proceedings of the 37th International Conference on Machine\n  Learning, PMLR 119:104-114, 2020", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.02544": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.02544v2", "post_title": "Large Scale Adversarial Representation Learning", "authors": ["Jeff Donahue", "Karen Simonyan"], "date_published": "2019-07-04 18:00:17+00:00", "data_last_modified": "2019-11-05 18:05:57+00:00", "url": "http://arxiv.org/abs/1907.02544v2", "abstract": "Adversarially trained generative models (GANs) have recently achieved\ncompelling image synthesis results. But despite early successes in using GANs\nfor unsupervised representation learning, they have since been superseded by\napproaches based on self-supervision. In this work we show that progress in\nimage generation quality translates to substantially improved representation\nlearning performance. Our approach, BigBiGAN, builds upon the state-of-the-art\nBigGAN model, extending it to representation learning by adding an encoder and\nmodifying the discriminator. We extensively evaluate the representation\nlearning and generation capabilities of these BigBiGAN models, demonstrating\nthat these generation-based models achieve the state of the art in unsupervised\nrepresentation learning on ImageNet, as well as in unconditional image\ngeneration. Pretrained BigBiGAN models -- including image generators and\nencoders -- are available on TensorFlow Hub\n(https://tfhub.dev/s?publisher=deepmind&q=bigbigan).", "author_comment": "32 pages. In proceedings of NeurIPS 2019. This is the camera-ready\n  version of the paper, with supplementary material included as appendices", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1712.04172": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1712.04172v2", "post_title": "A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents", "authors": ["Yueh-Hua Wu", "Shou-De Lin"], "date_published": "2017-12-12 08:35:52+00:00", "data_last_modified": "2018-09-10 04:59:19+00:00", "url": "http://arxiv.org/abs/1712.04172v2", "abstract": "This paper proposes a low-cost, easily realizable strategy to equip a\nreinforcement learning (RL) agent the capability of behaving ethically. Our\nmodel allows the designers of RL agents to solely focus on the task to achieve,\nwithout having to worry about the implementation of multiple trivial ethical\npatterns to follow. Based on the assumption that the majority of human\nbehavior, regardless which goals they are achieving, is ethical, our design\nintegrates human policy with the RL policy to achieve the target objective with\nless chance of violating the ethical code that human beings normally obey.", "author_comment": "AAAI 2018 Oral Presentation", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.07495": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.07495v1", "post_title": "Open Questions in Creating Safe Open-ended AI: Tensions Between Control and Creativity", "authors": ["Adrien Ecoffet", "Jeff Clune", "Joel Lehman"], "date_published": "2020-06-12 22:28:09+00:00", "data_last_modified": "2020-06-12 22:28:09+00:00", "url": "http://arxiv.org/abs/2006.07495v1", "abstract": "Artificial life originated and has long studied the topic of open-ended\nevolution, which seeks the principles underlying artificial systems that\ninnovate continually, inspired by biological evolution. Recently, interest has\ngrown within the broader field of AI in a generalization of open-ended\nevolution, here called open-ended search, wherein such questions of\nopen-endedness are explored for advancing AI, whatever the nature of the\nunderlying search algorithm (e.g. evolutionary or gradient-based). For example,\nopen-ended search might design new architectures for neural networks, new\nreinforcement learning algorithms, or most ambitiously, aim at designing\nartificial general intelligence. This paper proposes that open-ended evolution\nand artificial life have much to contribute towards the understanding of\nopen-ended AI, focusing here in particular on the safety of open-ended search.\nThe idea is that AI systems are increasingly applied in the real world, often\nproducing unintended harms in the process, which motivates the growing field of\nAI safety. This paper argues that open-ended AI has its own safety challenges,\nin particular, whether the creativity of open-ended systems can be productively\nand predictably controlled. This paper explains how unique safety problems\nmanifest in open-ended search, and suggests concrete contributions and research\nquestions to explore them. The hope is to inspire progress towards creative,\nuseful, and safe open-ended search algorithms.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.NE", "categories": ["cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2110.08058": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2110.08058v2", "post_title": "Quantifying Local Specialization in Deep Neural Networks", "authors": ["Shlomi Hod", "Daniel Filan", "Stephen Casper", "Andrew Critch", "Stuart Russell"], "date_published": "2021-10-13 20:33:30+00:00", "data_last_modified": "2022-02-07 20:46:48+00:00", "url": "http://arxiv.org/abs/2110.08058v2", "abstract": "A neural network is locally specialized to the extent that parts of its\ncomputational graph (i.e. structure) can be abstractly represented as\nperforming some comprehensible sub-task relevant to the overall task (i.e.\nfunctionality). Are modern deep neural networks locally specialized? How can\nthis be quantified? In this paper, we consider the problem of taking a neural\nnetwork whose neurons are partitioned into clusters, and quantifying how\nfunctionally specialized the clusters are. We propose two proxies for this:\nimportance, which reflects how crucial sets of neurons are to network\nperformance; and coherence, which reflects how consistently their neurons\nassociate with features of the inputs. To measure these proxies, we develop a\nset of statistical methods based on techniques conventionally used to interpret\nindividual neurons. We apply the proxies to partitionings generated by\nspectrally clustering a graph representation of the network's neurons with\nedges determined either by network weights or correlations of activations. We\nshow that these partitionings, even ones based only on weights (i.e. strictly\nfrom non-runtime analysis), reveal groups of neurons that are important and\ncoherent. These results suggest that graph-based partitioning can reveal local\nspecialization and that statistical methods can be used to automatedly screen\nfor sets of neurons that can be understood abstractly.", "author_comment": "21 pages, 6 figures. Code is available at\n  https://github.com/thestephencasper/detecting_nn_modularity", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2011.08512": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2011.08512v1", "post_title": "Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database", "authors": ["Sean McGregor"], "date_published": "2020-11-17 08:55:14+00:00", "data_last_modified": "2020-11-17 08:55:14+00:00", "url": "http://arxiv.org/abs/2011.08512v1", "abstract": "Mature industrial sectors (e.g., aviation) collect their real world failures\nin incident databases to inform safety improvements. Intelligent systems\ncurrently cause real world harms without a collective memory of their failings.\nAs a result, companies repeatedly make the same mistakes in the design,\ndevelopment, and deployment of intelligent systems. A collection of intelligent\nsystem failures experienced in the real world (i.e., incidents) is needed to\nensure intelligent systems benefit people and society. The AI Incident Database\nis an incident collection initiated by an industrial/non-profit cooperative to\nenable AI incident avoidance and mitigation. The database supports a variety of\nresearch and development use cases with faceted and full text search on more\nthan 1,000 incident reports archived to date.", "author_comment": "6 pages, 7 figures, Pre-print accepted to Innovative Applications of\n  Artificial Intelligence (IAAI-21)", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.SE", "K.4.0; K.4.3; I.2.0"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.02625": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.02625v2", "post_title": "MixTrain: Scalable Training of Verifiably Robust Neural Networks", "authors": ["Shiqi Wang", "Yizheng Chen", "Ahmed Abdou", "Suman Jana"], "date_published": "2018-11-06 20:47:28+00:00", "data_last_modified": "2018-12-01 23:52:52+00:00", "url": "http://arxiv.org/abs/1811.02625v2", "abstract": "Making neural networks robust against adversarial inputs has resulted in an\narms race between new defenses and attacks. The most promising defenses,\nadversarially robust training and verifiably robust training, have limitations\nthat restrict their practical applications. The adversarially robust training\nonly makes the networks robust against a subclass of attackers and we reveal\nsuch weaknesses by developing a new attack based on interval gradients. By\ncontrast, verifiably robust training provides protection against any L-p\nnorm-bounded attacker but incurs orders of magnitude more computational and\nmemory overhead than adversarially robust training.\n  We propose two novel techniques, stochastic robust approximation and dynamic\nmixed training, to drastically improve the efficiency of verifiably robust\ntraining without sacrificing verified robustness. We leverage two critical\ninsights: (1) instead of over the entire training set, sound\nover-approximations over randomly subsampled training data points are\nsufficient for efficiently guiding the robust training process; and (2) We\nobserve that the test accuracy and verifiable robustness often conflict after\ncertain training epochs. Therefore, we use a dynamic loss function to\nadaptively balance them for each epoch.\n  We designed and implemented our techniques as part of MixTrain and evaluated\nit on six networks trained on three popular datasets including MNIST, CIFAR,\nand ImageNet-200. Our evaluations show that MixTrain can achieve up to $95.2\\%$\nverified robust accuracy against $L_\\infty$ norm-bounded attackers while taking\n$15$ and $3$ times less training time than state-of-the-art verifiably robust\ntraining and adversarially robust training schemes, respectively. Furthermore,\nMixTrain easily scales to larger networks like the one trained on ImageNet-200,\nsignificantly outperforming the existing verifiably robust training methods.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2201.08555": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2201.08555v1", "post_title": "Identifying Adversarial Attacks on Text Classifiers", "authors": ["Zhouhang Xie", "Jonathan Brophy", "Adam Noack", "Wencong You", "Kalyani Asthana", "Carter Perkins", "Sabrina Reis", "Sameer Singh", "Daniel Lowd"], "date_published": "2022-01-21 06:16:04+00:00", "data_last_modified": "2022-01-21 06:16:04+00:00", "url": "http://arxiv.org/abs/2201.08555v1", "abstract": "The landscape of adversarial attacks against text classifiers continues to\ngrow, with new attacks developed every year and many of them available in\nstandard toolkits, such as TextAttack and OpenAttack. In response, there is a\ngrowing body of work on robust learning, which reduces vulnerability to these\nattacks, though sometimes at a high cost in compute time or accuracy. In this\npaper, we take an alternate approach -- we attempt to understand the attacker\nby analyzing adversarial text to determine which methods were used to create\nit. Our first contribution is an extensive dataset for attack detection and\nlabeling: 1.5~million attack instances, generated by twelve adversarial attacks\ntargeting three classifiers trained on six source datasets for sentiment\nanalysis and abuse detection in English. As our second contribution, we use\nthis dataset to develop and benchmark a number of classifiers for attack\nidentification -- determining if a given text has been adversarially\nmanipulated and by which attack. As a third contribution, we demonstrate the\neffectiveness of three classes of features for these tasks: text properties,\ncapturing content and presentation of text; language model properties,\ndetermining which tokens are more or less probable throughout the input; and\ntarget model properties, representing how the text classifier is influenced by\nthe attack, including internal node activations. Overall, this represents a\nfirst step towards forensics for adversarial attacks against text classifiers.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.CR", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.10031": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.10031v2", "post_title": "Lyapunov-based Safe Policy Optimization for Continuous Control", "authors": ["Yinlam Chow", "Ofir Nachum", "Aleksandra Faust", "Edgar Duenez-Guzman", "Mohammad Ghavamzadeh"], "date_published": "2019-01-28 23:14:58+00:00", "data_last_modified": "2019-02-11 20:52:42+00:00", "url": "http://arxiv.org/abs/1901.10031v2", "abstract": "We study continuous action reinforcement learning problems in which it is\ncrucial that the agent interacts with the environment only through safe\npolicies, i.e.,~policies that do not take the agent to undesirable situations.\nWe formulate these problems as constrained Markov decision processes (CMDPs)\nand present safe policy optimization algorithms that are based on a Lyapunov\napproach to solve them. Our algorithms can use any standard policy gradient\n(PG) method, such as deep deterministic policy gradient (DDPG) or proximal\npolicy optimization (PPO), to train a neural network policy, while guaranteeing\nnear-constraint satisfaction for every policy update by projecting either the\npolicy parameter or the action onto the set of feasible solutions induced by\nthe state-dependent linearized Lyapunov constraints. Compared to the existing\nconstrained PG algorithms, ours are more data efficient as they are able to\nutilize both on-policy and off-policy data. Moreover, our action-projection\nalgorithm often leads to less conservative policy updates and allows for\nnatural integration into an end-to-end PG training pipeline. We evaluate our\nalgorithms and compare them with the state-of-the-art baselines on several\nsimulated (MuJoCo) tasks, as well as a real-world indoor robot navigation\nproblem, demonstrating their effectiveness in terms of balancing performance\nand constraint satisfaction. Videos of the experiments can be found in the\nfollowing link:\nhttps://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.04805": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.04805v2", "post_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "date_published": "2018-10-11 00:50:01+00:00", "data_last_modified": "2019-05-24 20:37:26+00:00", "url": "http://arxiv.org/abs/1810.04805v2", "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2110.09506": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2110.09506v2", "post_title": "MEMO: Test Time Robustness via Adaptation and Augmentation", "authors": ["Marvin Zhang", "Sergey Levine", "Chelsea Finn"], "date_published": "2021-10-18 17:55:11+00:00", "data_last_modified": "2022-01-24 07:04:47+00:00", "url": "http://arxiv.org/abs/2110.09506v2", "abstract": "While deep neural networks can attain good accuracy on in-distribution test\npoints, many applications require robustness even in the face of unexpected\nperturbations in the input, changes in the domain, or other sources of\ndistribution shift. We study the problem of test time robustification, i.e.,\nusing the test input to improve model robustness. Recent prior works have\nproposed methods for test time adaptation, however, they each introduce\nadditional assumptions, such as access to multiple test points, that prevent\nwidespread adoption. In this work, we aim to study and devise methods that make\nno assumptions about the model training process and are broadly applicable at\ntest time. We propose a simple approach that can be used in any test setting\nwhere the model is probabilistic and adaptable: when presented with a test\nexample, perform different data augmentations on the data point, and then adapt\n(all of) the model parameters by minimizing the entropy of the model's average,\nor marginal, output distribution across the augmentations. Intuitively, this\nobjective encourages the model to make the same prediction across different\naugmentations, thus enforcing the invariances encoded in these augmentations,\nwhile also maintaining confidence in its predictions. In our experiments, we\nevaluate two baseline ResNet models, two robust ResNet-50 models, and a robust\nvision transformer model, and we demonstrate that this approach achieves\naccuracy gains of 1-8\\% over standard model evaluation and also generally\noutperforms prior augmentation and adaptation strategies. For the setting in\nwhich only one test point is available, we achieve state-of-the-art results on\nthe ImageNet-C, ImageNet-R, and, among ResNet-50 models, ImageNet-A\ndistribution shift benchmarks.", "author_comment": "code: https://github.com/zhangmarvin/memo", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.06085": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.06085v1", "post_title": "Theory of Minds: Understanding Behavior in Groups Through Inverse Planning", "authors": ["Michael Shum", "Max Kleiman-Weiner", "Michael L. Littman", "Joshua B. Tenenbaum"], "date_published": "2019-01-18 04:50:08+00:00", "data_last_modified": "2019-01-18 04:50:08+00:00", "url": "http://arxiv.org/abs/1901.06085v1", "abstract": "Human social behavior is structured by relationships. We form teams, groups,\ntribes, and alliances at all scales of human life. These structures guide\nmulti-agent cooperation and competition, but when we observe others these\nunderlying relationships are typically unobservable and hence must be inferred.\nHumans make these inferences intuitively and flexibly, often making rapid\ngeneralizations about the latent relationships that underlie behavior from just\nsparse and noisy observations. Rapid and accurate inferences are important for\ndetermining who to cooperate with, who to compete with, and how to cooperate in\norder to compete. Towards the goal of building machine-learning algorithms with\nhuman-like social intelligence, we develop a generative model of multi-agent\naction understanding based on a novel representation for these latent\nrelationships called Composable Team Hierarchies (CTH). This representation is\ngrounded in the formalism of stochastic games and multi-agent reinforcement\nlearning. We use CTH as a target for Bayesian inference yielding a new\nalgorithm for understanding behavior in groups that can both infer hidden\nrelationships as well as predict future actions for multiple agents interacting\ntogether. Our algorithm rapidly recovers an underlying causal model of how\nagents relate in spatial stochastic games from just a few observations. The\npatterns of inference made by this algorithm closely correspond with human\njudgments and the algorithm makes the same rapid generalizations that people\ndo.", "author_comment": "published in AAAI 2019; Michael Shum and Max Kleiman-Weiner\n  contributed equally", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1910.01074": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1910.01074v3", "post_title": "Formal Language Constraints for Markov Decision Processes", "authors": ["Eleanor Quint", "Dong Xu", "Samuel Flint", "Stephen Scott", "Matthew Dwyer"], "date_published": "2019-10-02 16:45:23+00:00", "data_last_modified": "2020-10-13 18:00:26+00:00", "url": "http://arxiv.org/abs/1910.01074v3", "abstract": "In order to satisfy safety conditions, an agent may be constrained from\nacting freely. A safe controller can be designed a priori if an environment is\nwell understood, but not when learning is employed. In particular,\nreinforcement learned (RL) controllers require exploration, which can be\nhazardous in safety critical situations. We study the benefits of giving\nstructure to the constraints of a constrained Markov decision process by\nspecifying them in formal languages as a step towards using safety methods from\nsoftware engineering and controller synthesis. We instantiate these constraints\nas finite automata to efficiently recognise constraint violations. Constraint\nstates are then used to augment the underlying MDP state and to learn a dense\ncost function, easing the problem of quickly learning joint MDP/constraint\ndynamics. We empirically evaluate the effect of these methods on training a\nvariety of RL algorithms over several constraints specified in Safety Gym,\nMuJoCo, and Atari environments.", "author_comment": "NeurIPS 2019 Workshop on Safety and Robustness in Decision Making", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.07445": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.07445v1", "post_title": "Challenges in Detoxifying Language Models", "authors": ["Johannes Welbl", "Amelia Glaese", "Jonathan Uesato", "Sumanth Dathathri", "John Mellor", "Lisa Anne Hendricks", "Kirsty Anderson", "Pushmeet Kohli", "Ben Coppin", "Po-Sen Huang"], "date_published": "2021-09-15 17:27:06+00:00", "data_last_modified": "2021-09-15 17:27:06+00:00", "url": "http://arxiv.org/abs/2109.07445v1", "abstract": "Large language models (LM) generate remarkably fluent text and can be\nefficiently adapted across NLP tasks. Measuring and guaranteeing the quality of\ngenerated text in terms of safety is imperative for deploying LMs in the real\nworld; to this end, prior work often relies on automatic evaluation of LM\ntoxicity. We critically discuss this approach, evaluate several toxicity\nmitigation strategies with respect to both automatic and human evaluation, and\nanalyze consequences of toxicity mitigation in terms of model bias and LM\nquality. We demonstrate that while basic intervention strategies can\neffectively optimize previously established automatic metrics on the\nRealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for\nboth texts about, and dialects of, marginalized groups. Additionally, we find\nthat human raters often disagree with high automatic toxicity scores after\nstrong toxicity reduction interventions -- highlighting further the nuances\ninvolved in careful evaluation of LM toxicity.", "author_comment": "23 pages, 6 figures, published in Findings of EMNLP 2021", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "I.2.6; I.2.7"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.01831": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.01831v1", "post_title": "Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?", "authors": ["Peter Hase", "Mohit Bansal"], "date_published": "2020-05-04 20:35:17+00:00", "data_last_modified": "2020-05-04 20:35:17+00:00", "url": "http://arxiv.org/abs/2005.01831v1", "abstract": "Algorithmic approaches to interpreting machine learning models have\nproliferated in recent years. We carry out human subject tests that are the\nfirst of their kind to isolate the effect of algorithmic explanations on a key\naspect of model interpretability, simulatability, while avoiding important\nconfounding experimental factors. A model is simulatable when a person can\npredict its behavior on new inputs. Through two kinds of simulation tests\ninvolving text and tabular data, we evaluate five explanations methods: (1)\nLIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a\nComposite approach that combines explanations from each method. Clear evidence\nof method effectiveness is found in very few cases: LIME improves\nsimulatability in tabular classification, and our Prototype method is effective\nin counterfactual simulation tests. We also collect subjective ratings of\nexplanations, but we do not find that ratings are predictive of how helpful\nexplanations are. Our results provide the first reliable and comprehensive\nestimates of how explanations influence simulatability across a variety of\nexplanation methods and data domains. We show that (1) we need to be careful\nabout the metrics we use to evaluate explanation methods, and (2) there is\nsignificant room for improvement in current methods. All our supporting code,\ndata, and models are publicly available at:\nhttps://github.com/peterbhase/InterpretableNLP-ACL2020", "author_comment": "ACL 2020 (13 pages)", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.03748": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.03748v2", "post_title": "Representation Learning with Contrastive Predictive Coding", "authors": ["Aaron van den Oord", "Yazhe Li", "Oriol Vinyals"], "date_published": "2018-07-10 16:52:11+00:00", "data_last_modified": "2019-01-22 18:47:12+00:00", "url": "http://arxiv.org/abs/1807.03748v2", "abstract": "While supervised learning has enabled great progress in many applications,\nunsupervised learning has not seen such widespread adoption, and remains an\nimportant and challenging endeavor for artificial intelligence. In this work,\nwe propose a universal unsupervised learning approach to extract useful\nrepresentations from high-dimensional data, which we call Contrastive\nPredictive Coding. The key insight of our model is to learn such\nrepresentations by predicting the future in latent space by using powerful\nautoregressive models. We use a probabilistic contrastive loss which induces\nthe latent space to capture information that is maximally useful to predict\nfuture samples. It also makes the model tractable by using negative sampling.\nWhile most prior work has focused on evaluating representations for a\nparticular modality, we demonstrate that our approach is able to learn useful\nrepresentations achieving strong performance on four distinct domains: speech,\nimages, text and reinforcement learning in 3D environments.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.07174": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.07174v4", "post_title": "Natural Adversarial Examples", "authors": ["Dan Hendrycks", "Kevin Zhao", "Steven Basart", "Jacob Steinhardt", "Dawn Song"], "date_published": "2019-07-16 17:56:30+00:00", "data_last_modified": "2021-03-04 21:56:19+00:00", "url": "http://arxiv.org/abs/1907.07174v4", "abstract": "We introduce two challenging datasets that reliably cause machine learning\nmodel performance to substantially degrade. The datasets are collected with a\nsimple adversarial filtration technique to create datasets with limited\nspurious cues. Our datasets' real-world, unmodified examples transfer to\nvarious unseen models reliably, demonstrating that computer vision models have\nshared weaknesses. The first dataset is called ImageNet-A and is like the\nImageNet test set, but it is far more challenging for existing models. We also\ncurate an adversarial out-of-distribution detection dataset called ImageNet-O,\nwhich is the first out-of-distribution detection dataset created for ImageNet\nmodels. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy\ndrop of approximately 90%, and its out-of-distribution detection performance on\nImageNet-O is near random chance levels. We find that existing data\naugmentation techniques hardly boost performance, and using other public\ntraining datasets provides improvements that are limited. However, we find that\nimprovements to computer vision architectures provide a promising path towards\nrobust models.", "author_comment": "CVPR 2021; dataset and code available at\n  https://github.com/hendrycks/natural-adv-examples", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.11645": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.11645v2", "post_title": "Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming", "authors": ["Sumanth Dathathri", "Krishnamurthy Dvijotham", "Alexey Kurakin", "Aditi Raghunathan", "Jonathan Uesato", "Rudy Bunel", "Shreya Shankar", "Jacob Steinhardt", "Ian Goodfellow", "Percy Liang", "Pushmeet Kohli"], "date_published": "2020-10-22 12:32:29+00:00", "data_last_modified": "2020-11-03 17:48:49+00:00", "url": "http://arxiv.org/abs/2010.11645v2", "abstract": "Convex relaxations have emerged as a promising approach for verifying\ndesirable properties of neural networks like robustness to adversarial\nperturbations. Widely used Linear Programming (LP) relaxations only work well\nwhen networks are trained to facilitate verification. This precludes\napplications that involve verification-agnostic networks, i.e., networks not\nspecially trained for verification. On the other hand, semidefinite programming\n(SDP) relaxations have successfully be applied to verification-agnostic\nnetworks, but do not currently scale beyond small networks due to poor time and\nspace asymptotics. In this work, we propose a first-order dual SDP algorithm\nthat (1) requires memory only linear in the total number of network\nactivations, (2) only requires a fixed number of forward/backward passes\nthrough the network per iteration. By exploiting iterative eigenvector methods,\nwe express all solver operations in terms of forward and backward passes\nthrough the network, enabling efficient use of hardware like GPUs/TPUs. For two\nverification-agnostic networks on MNIST and CIFAR-10, we significantly improve\nL-inf verified robust accuracy from 1% to 88% and 6% to 40% respectively. We\nalso demonstrate tight verification of a quadratic stability specification for\nthe decoder of a variational autoencoder.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1712.05812": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1712.05812v6", "post_title": "Occam's razor is insufficient to infer the preferences of irrational agents", "authors": ["Stuart Armstrong", "S\u00f6ren Mindermann"], "date_published": "2017-12-15 19:05:01+00:00", "data_last_modified": "2019-01-11 14:36:40+00:00", "url": "http://arxiv.org/abs/1712.05812v6", "abstract": "Inverse reinforcement learning (IRL) attempts to infer human rewards or\npreferences from observed behavior. Since human planning systematically\ndeviates from rationality, several approaches have been tried to account for\nspecific human shortcomings. However, the general problem of inferring the\nreward function of an agent of unknown rationality has received little\nattention. Unlike the well-known ambiguity problems in IRL, this one is\npractically relevant but cannot be resolved by observing the agent's policy in\nenough environments. This paper shows (1) that a No Free Lunch result implies\nit is impossible to uniquely decompose a policy into a planning algorithm and\nreward function, and (2) that even with a reasonable simplicity prior/Occam's\nrazor on the set of decompositions, we cannot distinguish between the true\ndecomposition and others that lead to high regret. To address this, we need\nsimple `normative' assumptions, which cannot be deduced exclusively from\nobservations.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.04241": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.04241v1", "post_title": "Capsules for Object Segmentation", "authors": ["Rodney LaLonde", "Ulas Bagci"], "date_published": "2018-04-11 21:57:57+00:00", "data_last_modified": "2018-04-11 21:57:57+00:00", "url": "http://arxiv.org/abs/1804.04241v1", "abstract": "Convolutional neural networks (CNNs) have shown remarkable results over the\nlast several years for a wide range of computer vision tasks. A new\narchitecture recently introduced by Sabour et al., referred to as a capsule\nnetworks with dynamic routing, has shown great initial results for digit\nrecognition and small image classification. The success of capsule networks\nlies in their ability to preserve more information about the input by replacing\nmax-pooling layers with convolutional strides and dynamic routing, allowing for\npreservation of part-whole relationships in the data. This preservation of the\ninput is demonstrated by reconstructing the input from the output capsule\nvectors. Our work expands the use of capsule networks to the task of object\nsegmentation for the first time in the literature. We extend the idea of\nconvolutional capsules with locally-connected routing and propose the concept\nof deconvolutional capsules. Further, we extend the masked reconstruction to\nreconstruct the positive input class. The proposed\nconvolutional-deconvolutional capsule network, called SegCaps, shows strong\nresults for the task of object segmentation with substantial decrease in\nparameter space. As an example application, we applied the proposed SegCaps to\nsegment pathological lungs from low dose CT scans and compared its accuracy and\nefficiency with other U-Net-based architectures. SegCaps is able to handle\nlarge image sizes (512 x 512) as opposed to baseline capsules (typically less\nthan 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net\narchitecture by 95.4% while still providing a better segmentation accuracy.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1705.08807": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1705.08807v3", "post_title": "When Will AI Exceed Human Performance? Evidence from AI Experts", "authors": ["Katja Grace", "John Salvatier", "Allan Dafoe", "Baobao Zhang", "Owain Evans"], "date_published": "2017-05-24 15:00:20+00:00", "data_last_modified": "2018-05-03 20:14:21+00:00", "url": "http://arxiv.org/abs/1705.08807v3", "abstract": "Advances in artificial intelligence (AI) will transform modern life by\nreshaping transportation, health, science, finance, and the military. To adapt\npublic policy, we need to better anticipate these advances. Here we report the\nresults from a large survey of machine learning researchers on their beliefs\nabout progress in AI. Researchers predict AI will outperform humans in many\nactivities in the next ten years, such as translating languages (by 2024),\nwriting high-school essays (by 2026), driving a truck (by 2027), working in\nretail (by 2031), writing a bestselling book (by 2049), and working as a\nsurgeon (by 2053). Researchers believe there is a 50% chance of AI\noutperforming humans in all tasks in 45 years and of automating all human jobs\nin 120 years, with Asian respondents expecting these dates much sooner than\nNorth Americans. These results will inform discussion amongst researchers and\npolicymakers about anticipating and managing trends in AI.", "author_comment": "Accepted by Journal of Artificial Intelligence Research (AI and\n  Society Track). Minor update to refer to related work (page 5)", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1608.04112": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1608.04112v6", "post_title": "Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm", "authors": ["Vanessa Kosoy", "Alexander Appel"], "date_published": "2016-08-14 15:34:24+00:00", "data_last_modified": "2019-06-04 19:53:27+00:00", "url": "http://arxiv.org/abs/1608.04112v6", "abstract": "We introduce a new concept of approximation applicable to decision problems\nand functions, inspired by Bayesian probability. From the perspective of a\nBayesian reasoner with limited computational resources, the answer to a problem\nthat cannot be solved exactly is uncertain and therefore should be described by\na random variable. It thus should make sense to talk about the expected value\nof this random variable, an idea we formalize in the language of average-case\ncomplexity theory by introducing the concept of \"optimal polynomial-time\nestimators.\" We prove some existence theorems and completeness results, and\nshow that optimal polynomial-time estimators exhibit many parallels with\n\"classical\" probability theory.", "author_comment": "86 pages", "journal_ref": null, "doi": null, "primary_category": "cs.CC", "categories": ["cs.CC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1911.08265": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1911.08265v2", "post_title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "authors": ["Julian Schrittwieser", "Ioannis Antonoglou", "Thomas Hubert", "Karen Simonyan", "Laurent Sifre", "Simon Schmitt", "Arthur Guez", "Edward Lockhart", "Demis Hassabis", "Thore Graepel", "Timothy Lillicrap", "David Silver"], "date_published": "2019-11-19 13:58:52+00:00", "data_last_modified": "2020-02-21 18:05:30+00:00", "url": "http://arxiv.org/abs/1911.08265v2", "abstract": "Constructing agents with planning capabilities has long been one of the main\nchallenges in the pursuit of artificial intelligence. Tree-based planning\nmethods have enjoyed huge success in challenging domains, such as chess and Go,\nwhere a perfect simulator is available. However, in real-world problems the\ndynamics governing the environment are often complex and unknown. In this work\nwe present the MuZero algorithm which, by combining a tree-based search with a\nlearned model, achieves superhuman performance in a range of challenging and\nvisually complex domains, without any knowledge of their underlying dynamics.\nMuZero learns a model that, when applied iteratively, predicts the quantities\nmost directly relevant to planning: the reward, the action-selection policy,\nand the value function. When evaluated on 57 different Atari games - the\ncanonical video game environment for testing AI techniques, in which\nmodel-based planning approaches have historically struggled - our new algorithm\nachieved a new state of the art. When evaluated on Go, chess and shogi, without\nany knowledge of the game rules, MuZero matched the superhuman performance of\nthe AlphaZero algorithm that was supplied with the game rules.", "author_comment": null, "journal_ref": null, "doi": "10.1038/s41586-020-03051-4", "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.12447": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.12447v3", "post_title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?", "authors": ["Roland S. Zimmermann", "Judy Borowski", "Robert Geirhos", "Matthias Bethge", "Thomas S. A. Wallis", "Wieland Brendel"], "date_published": "2021-06-23 14:52:23+00:00", "data_last_modified": "2021-11-12 10:33:33+00:00", "url": "http://arxiv.org/abs/2106.12447v3", "abstract": "A precise understanding of why units in an artificial network respond to\ncertain stimuli would constitute a big step towards explainable artificial\nintelligence. One widely used approach towards this goal is to visualize unit\nresponses via activation maximization. These synthetic feature visualizations\nare purported to provide humans with precise information about the image\nfeatures that cause a unit to be activated - an advantage over other\nalternatives like strongly activating natural dataset samples. If humans indeed\ngain causal insight from visualizations, this should enable them to predict the\neffect of an intervention, such as how occluding a certain patch of the image\n(say, a dog's head) changes a unit's activation. Here, we test this hypothesis\nby asking humans to decide which of two square occlusions causes a larger\nchange to a unit's activation. Both a large-scale crowdsourced experiment and\nmeasurements with experts show that on average the extremely activating feature\nvisualizations by Olah et al. (2017) indeed help humans on this task ($68 \\pm\n4$% accuracy; baseline performance without any visualizations is $60 \\pm 3$%).\nHowever, they do not provide any substantial advantage over other\nvisualizations (such as e.g. dataset samples), which yield similar performance\n($66\\pm3$% to $67 \\pm3$% accuracy). Taken together, we propose an objective\npsychophysical task to quantify the benefit of unit-level interpretability\nmethods for humans, and find no evidence that a widely-used feature\nvisualization method provides humans with better \"causal understanding\" of unit\nactivations than simple alternative visualizations.", "author_comment": "Presented at NeurIPS 2021. Shared first and last authorship. Project\n  website at\n  https://brendel-group.github.io/causal-understanding-via-visualizations/", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2003.11881": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2003.11881v2", "post_title": "An empirical investigation of the challenges of real-world reinforcement learning", "authors": ["Gabriel Dulac-Arnold", "Nir Levine", "Daniel J. Mankowitz", "Jerry Li", "Cosmin Paduraru", "Sven Gowal", "Todd Hester"], "date_published": "2020-03-24 11:05:41+00:00", "data_last_modified": "2021-03-04 13:02:59+00:00", "url": "http://arxiv.org/abs/2003.11881v2", "abstract": "Reinforcement learning (RL) has proven its worth in a series of artificial\ndomains, and is beginning to show some successes in real-world scenarios.\nHowever, much of the research advances in RL are hard to leverage in real-world\nsystems due to a series of assumptions that are rarely satisfied in practice.\nIn this work, we identify and formalize a series of independent challenges that\nembody the difficulties that must be addressed for RL to be commonly deployed\nin real-world systems. For each challenge, we define it formally in the context\nof a Markov Decision Process, analyze the effects of the challenge on\nstate-of-the-art learning algorithms, and present some existing attempts at\ntackling it. We believe that an approach that addresses our set of proposed\nchallenges would be readily deployable in a large number of real world\nproblems. Our proposed challenges are implemented in a suite of continuous\ncontrol environments called the realworldrl-suite which we propose an as an\nopen-source benchmark.", "author_comment": "arXiv admin note: text overlap with arXiv:1904.12901", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.13676": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.13676v4", "post_title": "The Grey Hoodie Project: Big Tobacco, Big Tech, and the threat on academic integrity", "authors": ["Mohamed Abdalla", "Moustafa Abdalla"], "date_published": "2020-09-28 23:00:49+00:00", "data_last_modified": "2021-04-27 12:29:44+00:00", "url": "http://arxiv.org/abs/2009.13676v4", "abstract": "As governmental bodies rely on academics' expert advice to shape policy\nregarding Artificial Intelligence, it is important that these academics not\nhave conflicts of interests that may cloud or bias their judgement. Our work\nexplores how Big Tech can actively distort the academic landscape to suit its\nneeds. By comparing the well-studied actions of another industry (Big Tobacco)\nto the current actions of Big Tech we see similar strategies employed by both\nindustries. These strategies enable either industry to sway and influence\nacademic and public discourse. We examine the funding of academic research as a\ntool used by Big Tech to put forward a socially responsible public image,\ninfluence events hosted by and decisions made by funded universities, influence\nthe research questions and plans of individual scientists, and discover\nreceptive academics who can be leveraged. We demonstrate how Big Tech can\naffect academia from the institutional level down to individual researchers.\nThus, we believe that it is vital, particularly for universities and other\ninstitutions of higher learning, to discuss the appropriateness and the\ntradeoffs of accepting funding from Big Tech, and what limitations or\nconditions should be put in place.", "author_comment": "Accepted to AIES-21", "journal_ref": null, "doi": "10.1145/3461702.3462563", "primary_category": "cs.CY", "categories": ["cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1604.06963": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1604.06963v2", "post_title": "Limits to Verification and Validation of Agentic Behavior", "authors": ["David J. Jilk"], "date_published": "2016-04-23 23:01:29+00:00", "data_last_modified": "2016-10-10 22:21:25+00:00", "url": "http://arxiv.org/abs/1604.06963v2", "abstract": "Verification and validation of agentic behavior have been suggested as\nimportant research priorities in efforts to reduce risks associated with the\ncreation of general artificial intelligence (Russell et al 2015). In this paper\nwe question the appropriateness of using language of certainty with respect to\nefforts to manage that risk. We begin by establishing a very general formalism\nto characterize agentic behavior and to describe standards of acceptable\nbehavior. We show that determination of whether an agent meets any particular\nstandard is not computable. We discuss the extent of the burden associated with\nverification by manual proof and by automated behavioral governance. We show\nthat to ensure decidability of the behavioral standard itself, one must further\nlimit the capabilities of the agent. We then demonstrate that if our concerns\nrelate to outcomes in the physical world, attempts at validation are futile.\nFinally, we show that layered architectures aimed at making these challenges\ntractable mistakenly equate intentions with actions or outcomes, thereby\nfailing to provide any guarantees. We conclude with a discussion of why\nlanguage of certainty should be eradicated from the conversation about the\nsafety of general artificial intelligence.", "author_comment": "13 pages, 0 figures", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "I.2.0; F.3.1; D.2.4; K.4.1"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.12261": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.12261v1", "post_title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations", "authors": ["Dan Hendrycks", "Thomas Dietterich"], "date_published": "2019-03-28 20:56:37+00:00", "data_last_modified": "2019-03-28 20:56:37+00:00", "url": "http://arxiv.org/abs/1903.12261v1", "abstract": "In this paper we establish rigorous benchmarks for image classifier\nrobustness. Our first benchmark, ImageNet-C, standardizes and expands the\ncorruption robustness topic, while showing which classifiers are preferable in\nsafety-critical applications. Then we propose a new dataset called ImageNet-P\nwhich enables researchers to benchmark a classifier's robustness to common\nperturbations. Unlike recent robustness research, this benchmark evaluates\nperformance on common corruptions and perturbations not worst-case adversarial\nperturbations. We find that there are negligible changes in relative corruption\nrobustness from AlexNet classifiers to ResNet classifiers. Afterward we\ndiscover ways to enhance corruption and perturbation robustness. We even find\nthat a bypassed adversarial defense provides substantial common perturbation\nrobustness. Together our benchmarks may aid future work toward networks that\nrobustly generalize.", "author_comment": "ICLR 2019 camera-ready; datasets available at\n  https://github.com/hendrycks/robustness ; this article supersedes\n  arXiv:1807.01697", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1602.04184": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1602.04184v5", "post_title": "Parametric Bounded L\u00f6b's Theorem and Robust Cooperation of Bounded Agents", "authors": ["Andrew Critch"], "date_published": "2016-02-12 19:51:54+00:00", "data_last_modified": "2016-08-24 05:22:57+00:00", "url": "http://arxiv.org/abs/1602.04184v5", "abstract": "L\\\"ob's theorem and G\\\"odel's theorems make predictions about the behavior of\nsystems capable of self-reference with unbounded computational resources with\nwhich to write and evaluate proofs. However, in the real world, systems capable\nof self-reference will have limited memory and processing speed, so in this\npaper we introduce an effective version of L\\\"ob's theorem which is applicable\ngiven such bounded resources. These results have powerful implications for the\ngame theory of bounded agents who are able to write proofs about themselves and\none another, including the capacity to out-perform classical Nash equilibria\nand correlated equilibria, attaining mutually cooperative program equilibrium\nin the Prisoner's Dilemma. Previous cooperative program equilibria studied by\nTennenholtz (2004) and Fortnow (2009) have depended on tests for program\nequality, a fragile condition, whereas \"L\\\"obian\" cooperation is much more\nrobust and agnostic of the opponent's implementation.", "author_comment": "Corrected typos, added grant acknowledgement, updated citation style\n  to author-year", "journal_ref": null, "doi": null, "primary_category": "cs.GT", "categories": ["cs.GT", "cs.LO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.12142": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.12142v2", "post_title": "IQ-Learn: Inverse soft-Q Learning for Imitation", "authors": ["Divyansh Garg", "Shuvam Chakraborty", "Chris Cundy", "Jiaming Song", "Stefano Ermon"], "date_published": "2021-06-23 03:43:10+00:00", "data_last_modified": "2021-12-02 01:45:27+00:00", "url": "http://arxiv.org/abs/2106.12142v2", "abstract": "In many sequential decision-making problems (e.g., robotics control, game\nplaying, sequential prediction), human or expert data is available containing\nuseful information about the task. However, imitation learning (IL) from a\nsmall amount of expert data can be challenging in high-dimensional environments\nwith complex dynamics. Behavioral cloning is a simple method that is widely\nused due to its simplicity of implementation and stable convergence but doesn't\nutilize any information involving the environment's dynamics. Many existing\nmethods that exploit dynamics information are difficult to train in practice\ndue to an adversarial optimization process over reward and policy approximators\nor biased, high variance gradient estimators. We introduce a method for\ndynamics-aware IL which avoids adversarial training by learning a single\nQ-function, implicitly representing both reward and policy. On standard\nbenchmarks, the implicitly learned rewards show a high positive correlation\nwith the ground-truth rewards, illustrating our method can also be used for\ninverse reinforcement learning (IRL). Our method, Inverse soft-Q learning\n(IQ-Learn) obtains state-of-the-art results in offline and online imitation\nlearning settings, significantly outperforming existing methods both in the\nnumber of required environment interactions and scalability in high-dimensional\nspaces, often by more than 3x.", "author_comment": "Spotlight in NeurIPS 2021. Website: https://div99.github.io/IQ-Learn", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.06732": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.06732v2", "post_title": "Motivating the Rules of the Game for Adversarial Example Research", "authors": ["Justin Gilmer", "Ryan P. Adams", "Ian Goodfellow", "David Andersen", "George E. Dahl"], "date_published": "2018-07-18 01:17:27+00:00", "data_last_modified": "2018-07-20 01:57:37+00:00", "url": "http://arxiv.org/abs/1807.06732v2", "abstract": "Advances in machine learning have led to broad deployment of systems with\nimpressive performance on important problems. Nonetheless, these systems can be\ninduced to make errors on data that are surprisingly similar to examples the\nlearned system handles correctly. The existence of these errors raises a\nvariety of questions about out-of-sample generalization and whether bad actors\nmight use such examples to abuse deployed systems. As a result of these\nsecurity concerns, there has been a flurry of recent papers proposing\nalgorithms to defend against such malicious perturbations of correctly handled\nexamples. It is unclear how such misclassifications represent a different kind\nof security problem than other errors, or even other attacker-produced examples\nthat have no specific relationship to an uncorrupted input. In this paper, we\nargue that adversarial example defense papers have, to date, mostly considered\nabstract, toy games that do not relate to any specific security concern.\nFurthermore, defense papers have not yet precisely described all the abilities\nand limitations of attackers that would be relevant in practical security.\nTowards this end, we establish a taxonomy of motivations, constraints, and\nabilities for more plausible adversaries. Finally, we provide a series of\nrecommendations outlining a path forward for future work to more clearly\narticulate the threat model and perform more meaningful evaluation.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1910.09338": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1910.09338v1", "post_title": "An Alternative Surrogate Loss for PGD-based Adversarial Testing", "authors": ["Sven Gowal", "Jonathan Uesato", "Chongli Qin", "Po-Sen Huang", "Timothy Mann", "Pushmeet Kohli"], "date_published": "2019-10-21 13:08:54+00:00", "data_last_modified": "2019-10-21 13:08:54+00:00", "url": "http://arxiv.org/abs/1910.09338v1", "abstract": "Adversarial testing methods based on Projected Gradient Descent (PGD) are\nwidely used for searching norm-bounded perturbations that cause the inputs of\nneural networks to be misclassified. This paper takes a deeper look at these\nmethods and explains the effect of different hyperparameters (i.e., optimizer,\nstep size and surrogate loss). We introduce the concept of MultiTargeted\ntesting, which makes clever use of alternative surrogate losses, and explain\nwhen and how MultiTargeted is guaranteed to find optimal perturbations.\nFinally, we demonstrate that MultiTargeted outperforms more sophisticated\nmethods and often requires less iterative steps than other variants of PGD\nfound in the literature. Notably, MultiTargeted ranks first on MadryLab's\nwhite-box MNIST and CIFAR-10 leaderboards, reducing the accuracy of their MNIST\nmodel to 88.36% (with $\\ell_\\infty$ perturbations of $\\epsilon = 0.3$) and the\naccuracy of their CIFAR-10 model to 44.03% (at $\\epsilon = 8/255$).\nMultiTargeted also ranks first on the TRADES leaderboard reducing the accuracy\nof their CIFAR-10 model to 53.07% (with $\\ell_\\infty$ perturbations of\n$\\epsilon = 0.031$).", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.03571": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.03571v2", "post_title": "A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees", "authors": ["Min Wu", "Matthew Wicker", "Wenjie Ruan", "Xiaowei Huang", "Marta Kwiatkowska"], "date_published": "2018-07-10 11:28:46+00:00", "data_last_modified": "2019-03-06 22:21:11+00:00", "url": "http://arxiv.org/abs/1807.03571v2", "abstract": "Despite the improved accuracy of deep neural networks, the discovery of\nadversarial examples has raised serious safety concerns. In this paper, we\nstudy two variants of pointwise robustness, the maximum safe radius problem,\nwhich for a given input sample computes the minimum distance to an adversarial\nexample, and the feature robustness problem, which aims to quantify the\nrobustness of individual features to adversarial perturbations. We demonstrate\nthat, under the assumption of Lipschitz continuity, both problems can be\napproximated using finite optimisation by discretising the input space, and the\napproximation has provable guarantees, i.e., the error is bounded. We then show\nthat the resulting optimisation problems can be reduced to the solution of\ntwo-player turn-based games, where the first player selects features and the\nsecond perturbs the image within the feature. While the second player aims to\nminimise the distance to an adversarial example, depending on the optimisation\nobjective the first player can be cooperative or competitive. We employ an\nanytime approach to solve the games, in the sense of approximating the value of\na game by monotonically improving its upper and lower bounds. The Monte Carlo\ntree search algorithm is applied to compute upper bounds for both games, and\nthe Admissible A* and the Alpha-Beta Pruning algorithms are, respectively, used\nto compute lower bounds for the maximum safety radius and feature robustness\ngames. When working on the upper bound of the maximum safe radius problem, our\ntool demonstrates competitive performance against existing adversarial example\ncrafting algorithms. Furthermore, we show how our framework can be deployed to\nevaluate pointwise robustness of neural networks in safety-critical\napplications such as traffic sign recognition in self-driving cars.", "author_comment": null, "journal_ref": "Theoretical Computer Science 807 (2020) 298-329", "doi": "10.1016/j.tcs.2019.05.046", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.01109": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.01109v2", "post_title": "AGI Safety Literature Review", "authors": ["Tom Everitt", "Gary Lea", "Marcus Hutter"], "date_published": "2018-05-03 04:26:48+00:00", "data_last_modified": "2018-05-21 16:30:20+00:00", "url": "http://arxiv.org/abs/1805.01109v2", "abstract": "The development of Artificial General Intelligence (AGI) promises to be a\nmajor event. Along with its many potential benefits, it also raises serious\nsafety concerns (Bostrom, 2014). The intention of this paper is to provide an\neasily accessible and up-to-date collection of references for the emerging\nfield of AGI safety. A significant number of safety problems for AGI have been\nidentified. We list these, and survey recent research on solving them. We also\ncover works on how best to think of AGI from the limited knowledge we have\ntoday, predictions for when AGI will first be created, and what will happen\nafter its creation. Finally, we review the current public policy on AGI.", "author_comment": "Published in International Joint Conference on Artificial\n  Intelligence (IJCAI), 2018", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.11181": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.11181v2", "post_title": "Neural Modular Control for Embodied Question Answering", "authors": ["Abhishek Das", "Georgia Gkioxari", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "date_published": "2018-10-26 03:58:26+00:00", "data_last_modified": "2019-05-02 23:41:47+00:00", "url": "http://arxiv.org/abs/1810.11181v2", "abstract": "We present a modular approach for learning policies for navigation over long\nplanning horizons from language input. Our hierarchical policy operates at\nmultiple timescales, where the higher-level master policy proposes subgoals to\nbe executed by specialized sub-policies. Our choice of subgoals is\ncompositional and semantic, i.e. they can be sequentially combined in arbitrary\norderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find\nkitchen', 'find refrigerator', etc.).\n  We use imitation learning to warm-start policies at each level of the\nhierarchy, dramatically increasing sample efficiency, followed by reinforcement\nlearning. Independent reinforcement learning at each level of hierarchy enables\nsub-policies to adapt to consequences of their actions and recover from errors.\nSubsequent joint hierarchical training enables the master policy to adapt to\nthe sub-policies.\n  On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al.,\n2018), requiring navigating diverse realistic indoor environments, our approach\noutperforms prior work by a significant margin, both in terms of navigation and\nquestion answering.", "author_comment": "10 pages, 3 figures, 2 tables. Published at CoRL 2018. Webpage:\n  https://embodiedqa.org/", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1105.3821": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1105.3821v1", "post_title": "Ontological Crises in Artificial Agents' Value Systems", "authors": ["Peter de Blanc"], "date_published": "2011-05-19 09:32:46+00:00", "data_last_modified": "2011-05-19 09:32:46+00:00", "url": "http://arxiv.org/abs/1105.3821v1", "abstract": "Decision-theoretic agents predict and evaluate the results of their actions\nusing a model, or ontology, of their environment. An agent's goal, or utility\nfunction, may also be specified in terms of the states of, or entities within,\nits ontology. If the agent may upgrade or replace its ontology, it faces a\ncrisis: the agent's original goal may not be well-defined with respect to its\nnew ontology. This crisis must be resolved before the agent can make plans\ntowards achieving its goals.\n  We discuss in this paper which sorts of agents will undergo ontological\ncrises and why we may want to create such agents. We present some concrete\nexamples, and argue that a well-defined procedure for resolving ontological\ncrises is needed. We point to some possible approaches to solving this problem,\nand evaluate these methods on our examples.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1909.12200": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1909.12200v3", "post_title": "Scaling data-driven robotics with reward sketching and batch reinforcement learning", "authors": ["Serkan Cabi", "Sergio G\u00f3mez Colmenarejo", "Alexander Novikov", "Ksenia Konyushkova", "Scott Reed", "Rae Jeong", "Konrad Zolna", "Yusuf Aytar", "David Budden", "Mel Vecerik", "Oleg Sushkov", "David Barker", "Jonathan Scholz", "Misha Denil", "Nando de Freitas", "Ziyu Wang"], "date_published": "2019-09-26 15:45:23+00:00", "data_last_modified": "2020-06-04 11:00:06+00:00", "url": "http://arxiv.org/abs/1909.12200v3", "abstract": "We present a framework for data-driven robotics that makes use of a large\ndataset of recorded robot experience and scales to several tasks using learned\nreward functions. We show how to apply this framework to accomplish three\ndifferent object manipulation tasks on a real robot platform. Given\ndemonstrations of a task together with task-agnostic recorded experience, we\nuse a special form of human annotation as supervision to learn a reward\nfunction, which enables us to deal with real-world tasks where the reward\nsignal cannot be acquired directly. Learned rewards are used in combination\nwith a large dataset of experience from different tasks to learn a robot policy\noffline using batch RL. We show that using our approach it is possible to train\nagents to perform a variety of challenging manipulation tasks including\nstacking rigid objects and handling cloth.", "author_comment": "Project website: https://sites.google.com/view/data-driven-robotics/", "journal_ref": "Robotics: Science and Systems Conference 2020", "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.04350": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.04350v1", "post_title": "Towards Governing Agent's Efficacy: Action-Conditional $\u03b2$-VAE for Deep Transparent Reinforcement Learning", "authors": ["John Yang", "Gyujeong Lee", "Minsung Hyun", "Simyung Chang", "Nojun Kwak"], "date_published": "2018-11-11 04:48:15+00:00", "data_last_modified": "2018-11-11 04:48:15+00:00", "url": "http://arxiv.org/abs/1811.04350v1", "abstract": "We tackle the blackbox issue of deep neural networks in the settings of\nreinforcement learning (RL) where neural agents learn towards maximizing reward\ngains in an uncontrollable way. Such learning approach is risky when the\ninteracting environment includes an expanse of state space because it is then\nalmost impossible to foresee all unwanted outcomes and penalize them with\nnegative rewards beforehand. Unlike reverse analysis of learned neural features\nfrom previous works, our proposed method \\nj{tackles the blackbox issue by\nencouraging} an RL policy network to learn interpretable latent features\nthrough an implementation of a disentangled representation learning method.\nToward this end, our method allows an RL agent to understand self-efficacy by\ndistinguishing its influences from uncontrollable environmental factors, which\nclosely resembles the way humans understand their scenes. Our experimental\nresults show that the learned latent factors not only are interpretable, but\nalso enable modeling the distribution of entire visited state space with a\nspecific action condition. We have experimented that this characteristic of the\nproposed structure can lead to ex post facto governance for desired behaviors\nof RL agents.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2105.02117": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2105.02117v1", "post_title": "Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers", "authors": ["Baobao Zhang", "Markus Anderljung", "Lauren Kahn", "Noemi Dreksler", "Michael C. Horowitz", "Allan Dafoe"], "date_published": "2021-05-05 15:23:12+00:00", "data_last_modified": "2021-05-05 15:23:12+00:00", "url": "http://arxiv.org/abs/2105.02117v1", "abstract": "Machine learning (ML) and artificial intelligence (AI) researchers play an\nimportant role in the ethics and governance of AI, including taking action\nagainst what they perceive to be unethical uses of AI (Belfield, 2020; Van\nNoorden, 2020). Nevertheless, this influential group's attitudes are not well\nunderstood, which undermines our ability to discern consensuses or\ndisagreements between AI/ML researchers. To examine these researchers' views,\nwe conducted a survey of those who published in the top AI/ML conferences (N =\n524). We compare these results with those from a 2016 survey of AI/ML\nresearchers (Grace, Salvatier, Dafoe, Zhang, & Evans, 2018) and a 2018 survey\nof the US public (Zhang & Dafoe, 2020). We find that AI/ML researchers place\nhigh levels of trust in international organizations and scientific\norganizations to shape the development and use of AI in the public interest;\nmoderate trust in most Western tech companies; and low trust in national\nmilitaries, Chinese tech companies, and Facebook. While the respondents were\noverwhelmingly opposed to AI/ML researchers working on lethal autonomous\nweapons, they are less opposed to researchers working on other military\napplications of AI, particularly logistics algorithms. A strong majority of\nrespondents think that AI safety research should be prioritized and that ML\ninstitutions should conduct pre-publication review to assess potential harms.\nBeing closer to the technology itself, AI/ML re-searchers are well placed to\nhighlight new risks and develop technical solutions, so this novel attempt to\nmeasure their attitudes has broad relevance. The findings should help to\nimprove how researchers, private sector executives, and policymakers think\nabout regulations, governance frameworks, guiding principles, and national and\ninternational governance strategies for AI.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "K.7.4"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.02501": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.02501v1", "post_title": "Simplifying Reward Design through Divide-and-Conquer", "authors": ["Ellis Ratner", "Dylan Hadfield-Menell", "Anca D. Dragan"], "date_published": "2018-06-07 03:49:05+00:00", "data_last_modified": "2018-06-07 03:49:05+00:00", "url": "http://arxiv.org/abs/1806.02501v1", "abstract": "Designing a good reward function is essential to robot planning and\nreinforcement learning, but it can also be challenging and frustrating. The\nreward needs to work across multiple different environments, and that often\nrequires many iterations of tuning. We introduce a novel divide-and-conquer\napproach that enables the designer to specify a reward separately for each\nenvironment. By treating these separate reward functions as observations about\nthe underlying true reward, we derive an approach to infer a common reward\nacross all environments. We conduct user studies in an abstract grid world\ndomain and in a motion planning domain for a 7-DOF manipulator that measure\nuser effort and solution quality. We show that our method is faster, easier to\nuse, and produces a higher quality solution than the typical method of\ndesigning a reward jointly across all environments. We additionally conduct a\nseries of experiments that measure the sensitivity of these results to\ndifferent properties of the reward design task, such as the number of\nenvironments, the number of feasible solutions per environment, and the\nfraction of the total features that vary within each environment. We find that\nindependent reward design outperforms the standard, joint, reward design\nprocess but works best when the design problem can be divided into simpler\nsubproblems.", "author_comment": "Robotics: Science and Systems (RSS) 2018", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.10396": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.10396v1", "post_title": "The LogBarrier adversarial attack: making effective use of decision boundary information", "authors": ["Chris Finlay", "Aram-Alexandre Pooladian", "Adam M. Oberman"], "date_published": "2019-03-25 15:21:20+00:00", "data_last_modified": "2019-03-25 15:21:20+00:00", "url": "http://arxiv.org/abs/1903.10396v1", "abstract": "Adversarial attacks for image classification are small perturbations to\nimages that are designed to cause misclassification by a model. Adversarial\nattacks formally correspond to an optimization problem: find a minimum norm\nimage perturbation, constrained to cause misclassification. A number of\neffective attacks have been developed. However, to date, no gradient-based\nattacks have used best practices from the optimization literature to solve this\nconstrained minimization problem. We design a new untargeted attack, based on\nthese best practices, using the established logarithmic barrier method. On\naverage, our attack distance is similar or better than all state-of-the-art\nattacks on benchmark datasets (MNIST, CIFAR10, ImageNet-1K). In addition, our\nmethod performs significantly better on the most challenging images, those\nwhich normally require larger perturbations for misclassification. We employ\nthe LogBarrier attack on several adversarially defended models, and show that\nit adversarially perturbs all images more efficiently than other attacks: the\ndistance needed to perturb all images is significantly smaller with the\nLogBarrier attack than with other state-of-the-art attacks.", "author_comment": "12 pages, 4 figures, 6 tables", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.12612": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.12612v2", "post_title": "Neurosymbolic Reinforcement Learning with Formally Verified Exploration", "authors": ["Greg Anderson", "Abhinav Verma", "Isil Dillig", "Swarat Chaudhuri"], "date_published": "2020-09-26 14:51:04+00:00", "data_last_modified": "2020-10-26 14:02:51+00:00", "url": "http://arxiv.org/abs/2009.12612v2", "abstract": "We present Revel, a partially neural reinforcement learning (RL) framework\nfor provably safe exploration in continuous state and action spaces. A key\nchallenge for provably safe deep RL is that repeatedly verifying neural\nnetworks within a learning loop is computationally infeasible. We address this\nchallenge using two policy classes: a general, neurosymbolic class with\napproximate gradients and a more restricted class of symbolic policies that\nallows efficient verification. Our learning algorithm is a mirror descent over\npolicies: in each iteration, it safely lifts a symbolic policy into the\nneurosymbolic space, performs safe gradient updates to the resulting policy,\nand projects the updated policy into the safe symbolic subset, all without\nrequiring explicit verification of neural networks. Our empirical results show\nthat Revel enforces safe exploration in many scenarios in which Constrained\nPolicy Optimization does not, and that it can discover policies that outperform\nthose learned through prior approaches to verified exploration.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.11089": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.11089v1", "post_title": "Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement", "authors": ["Benjamin Eysenbach", "Xinyang Geng", "Sergey Levine", "Ruslan Salakhutdinov"], "date_published": "2020-02-25 18:36:31+00:00", "data_last_modified": "2020-02-25 18:36:31+00:00", "url": "http://arxiv.org/abs/2002.11089v1", "abstract": "Multi-task reinforcement learning (RL) aims to simultaneously learn policies\nfor solving many tasks. Several prior works have found that relabeling past\nexperience with different reward functions can improve sample efficiency.\nRelabeling methods typically ask: if, in hindsight, we assume that our\nexperience was optimal for some task, for what task was it optimal? In this\npaper, we show that hindsight relabeling is inverse RL, an observation that\nsuggests that we can use inverse RL in tandem for RL algorithms to efficiently\nsolve many tasks. We use this idea to generalize goal-relabeling techniques\nfrom prior work to arbitrary classes of tasks. Our experiments confirm that\nrelabeling data using inverse RL accelerates learning in general multi-task\nsettings, including goal-reaching, domains with discrete sets of rewards, and\nthose with linear reward functions.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.02541": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.02541v9", "post_title": "PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation", "authors": ["Perttu H\u00e4m\u00e4l\u00e4inen", "Amin Babadi", "Xiaoxiao Ma", "Jaakko Lehtinen"], "date_published": "2018-10-05 06:59:29+00:00", "data_last_modified": "2020-11-03 07:51:49+00:00", "url": "http://arxiv.org/abs/1810.02541v9", "abstract": "Proximal Policy Optimization (PPO) is a highly popular model-free\nreinforcement learning (RL) approach. However, we observe that in a continuous\naction space, PPO can prematurely shrink the exploration variance, which leads\nto slow progress and may make the algorithm prone to getting stuck in local\noptima. Drawing inspiration from CMA-ES, a black-box evolutionary optimization\nmethod designed for robustness in similar situations, we propose PPO-CMA, a\nproximal policy optimization approach that adaptively expands the exploration\nvariance to speed up progress. With only minor changes to PPO, our algorithm\nconsiderably improves performance in Roboschool continuous control benchmarks.\nOur results also show that PPO-CMA, as opposed to PPO, is significantly less\nsensitive to the choice of hyperparameters, allowing one to use it in complex\nmovement optimization tasks without requiring tedious tuning.", "author_comment": "This paper has been accepted to IEEE International Workshop on\n  Machine Learning for Signal Processing (MLSP 2020). The arxiv version also\n  includes an appendix that covers more results", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.10513": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.10513v1", "post_title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "authors": ["Nic Ford", "Justin Gilmer", "Nicolas Carlini", "Dogus Cubuk"], "date_published": "2019-01-29 20:01:39+00:00", "data_last_modified": "2019-01-29 20:01:39+00:00", "url": "http://arxiv.org/abs/1901.10513v1", "abstract": "Over the last few years, the phenomenon of adversarial examples ---\nmaliciously constructed inputs that fool trained machine learning models ---\nhas captured the attention of the research community, especially when the\nadversary is restricted to small modifications of a correctly handled input.\nLess surprisingly, image classifiers also lack human-level performance on\nrandomly corrupted images, such as images with additive Gaussian noise. In this\npaper we provide both empirical and theoretical evidence that these are two\nmanifestations of the same underlying phenomenon, establishing close\nconnections between the adversarial robustness and corruption robustness\nresearch programs. This suggests that improving adversarial robustness should\ngo hand in hand with improving performance in the presence of more general and\nrealistic image corruptions. Based on our results we recommend that future\nadversarial defenses consider evaluating the robustness of their methods to\ndistributional shift with benchmarks such as Imagenet-C.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.03980": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.03980v1", "post_title": "Emergent Communication through Negotiation", "authors": ["Kris Cao", "Angeliki Lazaridou", "Marc Lanctot", "Joel Z Leibo", "Karl Tuyls", "Stephen Clark"], "date_published": "2018-04-11 13:48:08+00:00", "data_last_modified": "2018-04-11 13:48:08+00:00", "url": "http://arxiv.org/abs/1804.03980v1", "abstract": "Multi-agent reinforcement learning offers a way to study how communication\ncould emerge in communities of agents needing to solve specific problems. In\nthis paper, we study the emergence of communication in the negotiation\nenvironment, a semi-cooperative model of agent interaction. We introduce two\ncommunication protocols -- one grounded in the semantics of the game, and one\nwhich is \\textit{a priori} ungrounded and is a form of cheap talk. We show that\nself-interested agents can use the pre-grounded communication channel to\nnegotiate fairly, but are unable to effectively use the ungrounded channel.\nHowever, prosocial agents do learn to use cheap talk to find an optimal\nnegotiating strategy, suggesting that cooperation is necessary for language to\nemerge. We also study communication behaviour in a setting where one agent\ninteracts with agents in a community with different levels of prosociality and\nshow how agent identifiability can aid negotiation.", "author_comment": "Published as a conference paper at ICLR 2018", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.11184": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.11184v1", "post_title": "Human-Centered Artificial Intelligence and Machine Learning", "authors": ["Mark O. Riedl"], "date_published": "2019-01-31 02:47:16+00:00", "data_last_modified": "2019-01-31 02:47:16+00:00", "url": "http://arxiv.org/abs/1901.11184v1", "abstract": "Humans are increasingly coming into contact with artificial intelligence and\nmachine learning systems. Human-centered artificial intelligence is a\nperspective on AI and ML that algorithms must be designed with awareness that\nthey are part of a larger system consisting of humans. We lay forth an argument\nthat human-centered artificial intelligence can be broken down into two\naspects: (1) AI systems that understand humans from a sociocultural\nperspective, and (2) AI systems that help humans understand them. We further\nargue that issues of social responsibility such as fairness, accountability,\ninterpretability, and transparency.", "author_comment": "Human Behavior and Emerging Technologies, volume 1", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1406.2661": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1406.2661v1", "post_title": "Generative Adversarial Networks", "authors": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "date_published": "2014-06-10 18:58:17+00:00", "data_last_modified": "2014-06-10 18:58:17+00:00", "url": "http://arxiv.org/abs/1406.2661v1", "abstract": "We propose a new framework for estimating generative models via an\nadversarial process, in which we simultaneously train two models: a generative\nmodel G that captures the data distribution, and a discriminative model D that\nestimates the probability that a sample came from the training data rather than\nG. The training procedure for G is to maximize the probability of D making a\nmistake. This framework corresponds to a minimax two-player game. In the space\nof arbitrary functions G and D, a unique solution exists, with G recovering the\ntraining data distribution and D equal to 1/2 everywhere. In the case where G\nand D are defined by multilayer perceptrons, the entire system can be trained\nwith backpropagation. There is no need for any Markov chains or unrolled\napproximate inference networks during either training or generation of samples.\nExperiments demonstrate the potential of the framework through qualitative and\nquantitative evaluation of the generated samples.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.03956": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.03956v1", "post_title": "Abstraction Learning", "authors": ["Fei Deng", "Jinsheng Ren", "Feng Chen"], "date_published": "2018-09-11 15:02:24+00:00", "data_last_modified": "2018-09-11 15:02:24+00:00", "url": "http://arxiv.org/abs/1809.03956v1", "abstract": "There has been a gap between artificial intelligence and human intelligence.\nIn this paper, we identify three key elements forming human intelligence, and\nsuggest that abstraction learning combines these elements and is thus a way to\nbridge the gap. Prior researches in artificial intelligence either specify\nabstraction by human experts, or take abstraction as a qualitative explanation\nfor the model. This paper aims to learn abstraction directly. We tackle three\nmain challenges: representation, objective function, and learning algorithm.\nSpecifically, we propose a partition structure that contains pre-allocated\nabstraction neurons; we formulate abstraction learning as a constrained\noptimization problem, which integrates abstraction properties; we develop a\nnetwork evolution algorithm to solve this problem. This complete framework is\nnamed ONE (Optimization via Network Evolution). In our experiments on MNIST,\nONE shows elementary human-like intelligence, including low energy consumption,\nknowledge sharing, and lifelong learning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.03976": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.03976v3", "post_title": "Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations", "authors": ["Daniel S. Brown", "Wonjoon Goo", "Scott Niekum"], "date_published": "2019-07-09 04:11:53+00:00", "data_last_modified": "2019-10-14 17:44:45+00:00", "url": "http://arxiv.org/abs/1907.03976v3", "abstract": "The performance of imitation learning is typically upper-bounded by the\nperformance of the demonstrator. While recent empirical results demonstrate\nthat ranked demonstrations allow for better-than-demonstrator performance,\npreferences over demonstrations may be difficult to obtain, and little is known\ntheoretically about when such methods can be expected to successfully\nextrapolate beyond the performance of the demonstrator. To address these\nissues, we first contribute a sufficient condition for better-than-demonstrator\nimitation learning and provide theoretical results showing why preferences over\ndemonstrations can better reduce reward function ambiguity when performing\ninverse reinforcement learning. Building on this theory, we introduce\nDisturbance-based Reward Extrapolation (D-REX), a ranking-based imitation\nlearning method that injects noise into a policy learned through behavioral\ncloning to automatically generate ranked demonstrations. These ranked\ndemonstrations are used to efficiently learn a reward function that can then be\noptimized using reinforcement learning. We empirically validate our approach on\nsimulated robot and Atari imitation learning benchmarks and show that D-REX\noutperforms standard imitation learning approaches and can significantly\nsurpass the performance of the demonstrator. D-REX is the first imitation\nlearning approach to achieve significant extrapolation beyond the\ndemonstrator's performance without additional side-information or supervision,\nsuch as rewards or human preferences. By generating rankings automatically, we\nshow that preference-based inverse reinforcement learning can be applied in\ntraditional imitation learning settings where only unlabeled demonstrations are\navailable.", "author_comment": "In proceedings of 3rd Conference on Robot Learning (CoRL) 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1808.08946": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1808.08946v3", "post_title": "Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures", "authors": ["Gongbo Tang", "Mathias M\u00fcller", "Annette Rios", "Rico Sennrich"], "date_published": "2018-08-27 17:51:27+00:00", "data_last_modified": "2018-11-11 16:49:47+00:00", "url": "http://arxiv.org/abs/1808.08946v3", "abstract": "Recently, non-recurrent architectures (convolutional, self-attentional) have\noutperformed RNNs in neural machine translation. CNNs and self-attentional\nnetworks can connect distant words via shorter network paths than RNNs, and it\nhas been speculated that this improves their ability to model long-range\ndependencies. However, this theoretical argument has not been tested\nempirically, nor have alternative explanations for their strong performance\nbeen explored in-depth. We hypothesize that the strong performance of CNNs and\nself-attentional networks could also be due to their ability to extract\nsemantic features from the source text, and we evaluate RNNs, CNNs and\nself-attention networks on two tasks: subject-verb agreement (where capturing\nlong-range dependencies is required) and word sense disambiguation (where\nsemantic feature extraction is required). Our experimental results show that:\n1) self-attentional networks and CNNs do not outperform RNNs in modeling\nsubject-verb agreement over long distances; 2) self-attentional networks\nperform distinctly better than RNNs and CNNs on word sense disambiguation.", "author_comment": "11 pages, 5 figures, accepted by EMNLP 2018 (v2: corrected author\n  names; v3: fix to CNN context-window size, and new post-publication\n  experiments in section 6)", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.05909": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.05909v1", "post_title": "Stand-Alone Self-Attention in Vision Models", "authors": ["Prajit Ramachandran", "Niki Parmar", "Ashish Vaswani", "Irwan Bello", "Anselm Levskaya", "Jonathon Shlens"], "date_published": "2019-06-13 19:43:01+00:00", "data_last_modified": "2019-06-13 19:43:01+00:00", "url": "http://arxiv.org/abs/1906.05909v1", "abstract": "Convolutions are a fundamental building block of modern computer vision\nsystems. Recent approaches have argued for going beyond convolutions in order\nto capture long-range dependencies. These efforts focus on augmenting\nconvolutional models with content-based interactions, such as self-attention\nand non-local means, to achieve gains on a number of vision tasks. The natural\nquestion that arises is whether attention can be a stand-alone primitive for\nvision models instead of serving as just an augmentation on top of\nconvolutions. In developing and testing a pure self-attention vision model, we\nverify that self-attention can indeed be an effective stand-alone layer. A\nsimple procedure of replacing all instances of spatial convolutions with a form\nof self-attention applied to ResNet model produces a fully self-attentional\nmodel that outperforms the baseline on ImageNet classification with 12% fewer\nFLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention\nmodel matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and\n34% fewer parameters. Detailed ablation studies demonstrate that self-attention\nis especially impactful when used in later layers. These results establish that\nstand-alone self-attention is an important addition to the vision\npractitioner's toolbox.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.05008": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.05008v1", "post_title": "Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice", "authors": ["Lewis Hammond", "James Fox", "Tom Everitt", "Alessandro Abate", "Michael Wooldridge"], "date_published": "2021-02-09 18:20:50+00:00", "data_last_modified": "2021-02-09 18:20:50+00:00", "url": "http://arxiv.org/abs/2102.05008v1", "abstract": "Multi-agent influence diagrams (MAIDs) are a popular form of graphical model\nthat, for certain classes of games, have been shown to offer key complexity and\nexplainability advantages over traditional extensive form game (EFG)\nrepresentations. In this paper, we extend previous work on MAIDs by introducing\nthe concept of a MAID subgame, as well as subgame perfect and trembling hand\nperfect equilibrium refinements. We then prove several equivalence results\nbetween MAIDs and EFGs. Finally, we describe an open source implementation for\nreasoning about MAIDs and computing their equilibria.", "author_comment": "Accepted to the 20th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS-21)", "journal_ref": null, "doi": null, "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.GT"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2110.13136": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2110.13136v2", "post_title": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally", "authors": ["Dan Hendrycks", "Mantas Mazeika", "Andy Zou", "Sahil Patel", "Christine Zhu", "Jesus Navarro", "Dawn Song", "Bo Li", "Jacob Steinhardt"], "date_published": "2021-10-25 17:59:31+00:00", "data_last_modified": "2022-02-08 01:59:37+00:00", "url": "http://arxiv.org/abs/2110.13136v2", "abstract": "When making everyday decisions, people are guided by their conscience, an\ninternal sense of right and wrong. By contrast, artificial agents are currently\nnot endowed with a moral sense. As a consequence, they may learn to behave\nimmorally when trained on environments that ignore moral concerns, such as\nviolent video games. With the advent of generally capable agents that pretrain\non many environments, it will become necessary to mitigate inherited biases\nfrom environments that teach immoral behavior. To facilitate the development of\nagents that avoid causing wanton harm, we introduce Jiminy Cricket, an\nenvironment suite of 25 text-based adventure games with thousands of diverse,\nmorally salient scenarios. By annotating every possible game state, the Jiminy\nCricket environments robustly evaluate whether agents can act morally while\nmaximizing reward. Using models with commonsense moral knowledge, we create an\nelementary artificial conscience that assesses and guides agents. In extensive\nexperiments, we find that the artificial conscience approach can steer agents\ntowards moral behavior without sacrificing performance.", "author_comment": "NeurIPS 2021. Environments available here\n  https://github.com/hendrycks/jiminy-cricket", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1904.12901": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1904.12901v1", "post_title": "Challenges of Real-World Reinforcement Learning", "authors": ["Gabriel Dulac-Arnold", "Daniel Mankowitz", "Todd Hester"], "date_published": "2019-04-29 18:40:15+00:00", "data_last_modified": "2019-04-29 18:40:15+00:00", "url": "http://arxiv.org/abs/1904.12901v1", "abstract": "Reinforcement learning (RL) has proven its worth in a series of artificial\ndomains, and is beginning to show some successes in real-world scenarios.\nHowever, much of the research advances in RL are often hard to leverage in\nreal-world systems due to a series of assumptions that are rarely satisfied in\npractice. We present a set of nine unique challenges that must be addressed to\nproductionize RL to real world problems. For each of these challenges, we\nspecify the exact meaning of the challenge, present some approaches from the\nliterature, and specify some metrics for evaluating that challenge. An approach\nthat addresses all nine challenges would be applicable to a large number of\nreal world problems. We also present an example domain that has been modified\nto present these challenges as a testbed for practical RL research.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2101.05507": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2101.05507v1", "post_title": "Evaluating the Robustness of Collaborative Agents", "authors": ["Paul Knott", "Micah Carroll", "Sam Devlin", "Kamil Ciosek", "Katja Hofmann", "A. D. Dragan", "Rohin Shah"], "date_published": "2021-01-14 09:02:45+00:00", "data_last_modified": "2021-01-14 09:02:45+00:00", "url": "http://arxiv.org/abs/2101.05507v1", "abstract": "In order for agents trained by deep reinforcement learning to work alongside\nhumans in realistic settings, we will need to ensure that the agents are\n\\emph{robust}. Since the real world is very diverse, and human behavior often\nchanges in response to agent deployment, the agent will likely encounter novel\nsituations that have never been seen during training. This results in an\nevaluation challenge: if we cannot rely on the average training or validation\nreward as a metric, then how can we effectively evaluate robustness? We take\ninspiration from the practice of \\emph{unit testing} in software engineering.\nSpecifically, we suggest that when designing AI agents that collaborate with\nhumans, designers should search for potential edge cases in \\emph{possible\npartner behavior} and \\emph{possible states encountered}, and write tests which\ncheck that the behavior of the agent in these edge cases is reasonable. We\napply this methodology to build a suite of unit tests for the Overcooked-AI\nenvironment, and use this test suite to evaluate three proposals for improving\nrobustness. We find that the test suite provides significant insight into the\neffects of these proposals that were generally not revealed by looking solely\nat the average validation reward.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.02918": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.02918v2", "post_title": "Certified Adversarial Robustness via Randomized Smoothing", "authors": ["Jeremy M Cohen", "Elan Rosenfeld", "J. Zico Kolter"], "date_published": "2019-02-08 02:08:19+00:00", "data_last_modified": "2019-06-15 07:40:33+00:00", "url": "http://arxiv.org/abs/1902.02918v2", "abstract": "We show how to turn any classifier that classifies well under Gaussian noise\ninto a new classifier that is certifiably robust to adversarial perturbations\nunder the $\\ell_2$ norm. This \"randomized smoothing\" technique has been\nproposed recently in the literature, but existing guarantees are loose. We\nprove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian\nnoise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a\ncertified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$\nnorm less than 0.5 (=127/255). No certified defense has been shown feasible on\nImageNet except for smoothing. On smaller-scale datasets where competing\napproaches to certified $\\ell_2$ robustness are viable, smoothing delivers\nhigher certified accuracies. Our strong empirical results suggest that\nrandomized smoothing is a promising direction for future research into\nadversarially robust classification. Code and models are available at\nhttp://github.com/locuslab/smoothing.", "author_comment": "ICML 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.03300": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.03300v3", "post_title": "Measuring Massive Multitask Language Understanding", "authors": ["Dan Hendrycks", "Collin Burns", "Steven Basart", "Andy Zou", "Mantas Mazeika", "Dawn Song", "Jacob Steinhardt"], "date_published": "2020-09-07 17:59:25+00:00", "data_last_modified": "2021-01-12 18:57:11+00:00", "url": "http://arxiv.org/abs/2009.03300v3", "abstract": "We propose a new test to measure a text model's multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess\nextensive world knowledge and problem solving ability. We find that while most\nrecent models have near random-chance accuracy, the very largest GPT-3 model\nimproves over random chance by almost 20 percentage points on average. However,\non every one of the 57 tasks, the best models still need substantial\nimprovements before they can reach expert-level accuracy. Models also have\nlopsided performance and frequently do not know when they are wrong. Worse,\nthey still have near-random accuracy on some socially important subjects such\nas morality and law. By comprehensively evaluating the breadth and depth of a\nmodel's academic and professional understanding, our test can be used to\nanalyze models across many tasks and to identify important shortcomings.", "author_comment": "ICLR 2021; the test and code is available at\n  https://github.com/hendrycks/test", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.12231": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.12231v2", "post_title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness", "authors": ["Robert Geirhos", "Patricia Rubisch", "Claudio Michaelis", "Matthias Bethge", "Felix A. Wichmann", "Wieland Brendel"], "date_published": "2018-11-29 15:04:05+00:00", "data_last_modified": "2019-01-14 13:59:09+00:00", "url": "http://arxiv.org/abs/1811.12231v2", "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise\nobjects by learning increasingly complex representations of object shapes. Some\nrecent studies suggest a more important role of image textures. We here put\nthese conflicting hypotheses to a quantitative test by evaluating CNNs and\nhuman observers on images with a texture-shape cue conflict. We show that\nImageNet-trained CNNs are strongly biased towards recognising textures rather\nthan shapes, which is in stark contrast to human behavioural evidence and\nreveals fundamentally different classification strategies. We then demonstrate\nthat the same standard architecture (ResNet-50) that learns a texture-based\nrepresentation on ImageNet is able to learn a shape-based representation\ninstead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet.\nThis provides a much better fit for human behavioural performance in our\nwell-controlled psychophysical lab setting (nine experiments totalling 48,560\npsychophysical trials across 97 observers) and comes with a number of\nunexpected emergent benefits such as improved object detection performance and\npreviously unseen robustness towards a wide range of image distortions,\nhighlighting advantages of a shape-based representation.", "author_comment": "Accepted at ICLR 2019 (oral)", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2104.13733": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2104.13733v1", "post_title": "Gradient-based Adversarial Attacks against Text Transformers", "authors": ["Chuan Guo", "Alexandre Sablayrolles", "Herv\u00e9 J\u00e9gou", "Douwe Kiela"], "date_published": "2021-04-15 17:43:43+00:00", "data_last_modified": "2021-04-15 17:43:43+00:00", "url": "http://arxiv.org/abs/2104.13733v1", "abstract": "We propose the first general-purpose gradient-based attack against\ntransformer models. Instead of searching for a single adversarial example, we\nsearch for a distribution of adversarial examples parameterized by a\ncontinuous-valued matrix, hence enabling gradient-based optimization. We\nempirically demonstrate that our white-box attack attains state-of-the-art\nattack performance on a variety of natural language tasks. Furthermore, we show\nthat a powerful black-box transfer attack, enabled by sampling from the\nadversarial distribution, matches or exceeds existing methods, while only\nrequiring hard-label outputs.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2105.06551": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2105.06551v1", "post_title": "Axes for Sociotechnical Inquiry in AI Research", "authors": ["Sarah Dean", "Thomas Krendl Gilbert", "Nathan Lambert", "Tom Zick"], "date_published": "2021-04-26 16:49:04+00:00", "data_last_modified": "2021-04-26 16:49:04+00:00", "url": "http://arxiv.org/abs/2105.06551v1", "abstract": "The development of artificial intelligence (AI) technologies has far exceeded\nthe investigation of their relationship with society. Sociotechnical inquiry is\nneeded to mitigate the harms of new technologies whose potential impacts remain\npoorly understood. To date, subfields of AI research develop primarily\nindividual views on their relationship with sociotechnics, while tools for\nexternal investigation, comparison, and cross-pollination are lacking. In this\npaper, we propose four directions for inquiry into new and evolving areas of\ntechnological development: value--what progress and direction does a field\npromote, optimization--how the defined system within a problem formulation\nrelates to broader dynamics, consensus--how agreement is achieved and who is\nincluded in building it, and failure--what methods are pursued when the problem\nspecification is found wanting. The paper provides a lexicon for sociotechnical\ninquiry and illustrates it through the example of consumer drone technology.", "author_comment": "9 pages, 1 figure", "journal_ref": null, "doi": "10.1109/TTS.2021.3074097", "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.10657": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.10657v1", "post_title": "Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization", "authors": ["Satrajit Chatterjee"], "date_published": "2020-02-25 03:59:31+00:00", "data_last_modified": "2020-02-25 03:59:31+00:00", "url": "http://arxiv.org/abs/2002.10657v1", "abstract": "An open question in the Deep Learning community is why neural networks\ntrained with Gradient Descent generalize well on real datasets even though they\nare capable of fitting random data. We propose an approach to answering this\nquestion based on a hypothesis about the dynamics of gradient descent that we\ncall Coherent Gradients: Gradients from similar examples are similar and so the\noverall gradient is stronger in certain directions where these reinforce each\nother. Thus changes to the network parameters during training are biased\ntowards those that (locally) simultaneously benefit many examples when such\nsimilarity exists. We support this hypothesis with heuristic arguments and\nperturbative experiments and outline how this can explain several common\nempirical observations about Deep Learning. Furthermore, our analysis is not\njust descriptive, but prescriptive. It suggests a natural modification to\ngradient descent that can greatly reduce overfitting.", "author_comment": "To appear in ICLR 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.05214": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.05214v1", "post_title": "Model-Based Reinforcement Learning via Meta-Policy Optimization", "authors": ["Ignasi Clavera", "Jonas Rothfuss", "John Schulman", "Yasuhiro Fujita", "Tamim Asfour", "Pieter Abbeel"], "date_published": "2018-09-14 01:15:28+00:00", "data_last_modified": "2018-09-14 01:15:28+00:00", "url": "http://arxiv.org/abs/1809.05214v1", "abstract": "Model-based reinforcement learning approaches carry the promise of being data\nefficient. However, due to challenges in learning dynamics models that\nsufficiently match the real-world dynamics, they struggle to achieve the same\nasymptotic performance as model-free methods. We propose Model-Based\nMeta-Policy-Optimization (MB-MPO), an approach that foregoes the strong\nreliance on accurate learned dynamics models. Using an ensemble of learned\ndynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model\nin the ensemble with one policy gradient step. This steers the meta-policy\ntowards internalizing consistent dynamics predictions among the ensemble while\nshifting the burden of behaving optimally w.r.t. the model discrepancies\ntowards the adaptation step. Our experiments show that MB-MPO is more robust to\nmodel imperfections than previous model-based approaches. Finally, we\ndemonstrate that our approach is able to match the asymptotic performance of\nmodel-free methods while requiring significantly less experience.", "author_comment": "First 2 authors contributed equally. Accepted for Conference on Robot\n  Learning (CoRL)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.06158": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.06158v4", "post_title": "Generative Adversarial Imitation from Observation", "authors": ["Faraz Torabi", "Garrett Warnell", "Peter Stone"], "date_published": "2018-07-17 00:25:15+00:00", "data_last_modified": "2019-06-18 04:56:56+00:00", "url": "http://arxiv.org/abs/1807.06158v4", "abstract": "Imitation from observation (IfO) is the problem of learning directly from\nstate-only demonstrations without having access to the demonstrator's actions.\nThe lack of action information both distinguishes IfO from most of the\nliterature in imitation learning, and also sets it apart as a method that may\nenable agents to learn from a large set of previously inapplicable resources\nsuch as internet videos. In this paper, we propose both a general framework for\nIfO approaches and also a new IfO approach based on generative adversarial\nnetworks called generative adversarial imitation from observation (GAIfO). We\nconduct experiments in two different settings: (1) when demonstrations consist\nof low-dimensional, manually-defined state features, and (2) when\ndemonstrations consist of high-dimensional, raw visual data. We demonstrate\nthat our approach performs comparably to classical imitation learning\napproaches (which have access to the demonstrator's actions) and significantly\noutperforms existing imitation from observation methods in high-dimensional\nsimulation environments.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.03896": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.03896v1", "post_title": "Consequences of Misaligned AI", "authors": ["Simon Zhuang", "Dylan Hadfield-Menell"], "date_published": "2021-02-07 19:34:04+00:00", "data_last_modified": "2021-02-07 19:34:04+00:00", "url": "http://arxiv.org/abs/2102.03896v1", "abstract": "AI systems often rely on two key components: a specified goal or reward\nfunction and an optimization algorithm to compute the optimal behavior for that\ngoal. This approach is intended to provide value for a principal: the user on\nwhose behalf the agent acts. The objectives given to these agents often refer\nto a partial specification of the principal's goals. We consider the cost of\nthis incompleteness by analyzing a model of a principal and an agent in a\nresource constrained world where the $L$ attributes of the state correspond to\ndifferent sources of utility for the principal. We assume that the reward\nfunction given to the agent only has support on $J < L$ attributes. The\ncontributions of our paper are as follows: 1) we propose a novel model of an\nincomplete principal-agent problem from artificial intelligence; 2) we provide\nnecessary and sufficient conditions under which indefinitely optimizing for any\nincomplete proxy objective leads to arbitrarily low overall utility; and 3) we\nshow how modifying the setup to allow reward functions that reference the full\nstate or allowing the principal to update the proxy objective over time can\nlead to higher utility solutions. The results in this paper argue that we\nshould view the design of reward functions as an interactive and dynamic\nprocess and identifies a theoretical scenario where some degree of\ninteractivity is desirable.", "author_comment": null, "journal_ref": "NeurIPS 2020", "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.07242": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.07242v1", "post_title": "More Data Can Hurt for Linear Regression: Sample-wise Double Descent", "authors": ["Preetum Nakkiran"], "date_published": "2019-12-16 08:28:26+00:00", "data_last_modified": "2019-12-16 08:28:26+00:00", "url": "http://arxiv.org/abs/1912.07242v1", "abstract": "In this expository note we describe a surprising phenomenon in\noverparameterized linear regression, where the dimension exceeds the number of\nsamples: there is a regime where the test risk of the estimator found by\ngradient descent increases with additional samples. In other words, more data\nactually hurts the estimator. This behavior is implicit in a recent line of\ntheoretical works analyzing \"double-descent\" phenomenon in linear models. In\nthis note, we isolate and understand this behavior in an extremely simple\nsetting: linear regression with isotropic Gaussian covariates. In particular,\nthis occurs due to an unconventional type of bias-variance tradeoff in the\noverparameterized regime: the bias decreases with more samples, but variance\nincreases.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG", "cs.NE", "math.ST", "stat.TH"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.11274": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.11274v2", "post_title": "Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning", "authors": ["Aviv Ovadya", "Jess Whittlestone"], "date_published": "2019-07-25 18:51:45+00:00", "data_last_modified": "2019-07-29 02:01:40+00:00", "url": "http://arxiv.org/abs/1907.11274v2", "abstract": "The aim of this paper is to facilitate nuanced discussion around research\nnorms and practices to mitigate the harmful impacts of advances in machine\nlearning (ML). We focus particularly on the use of ML to create \"synthetic\nmedia\" (e.g. to generate or manipulate audio, video, images, and text), and the\nquestion of what publication and release processes around such research might\nlook like, though many of the considerations discussed will apply to ML\nresearch more broadly. We are not arguing for any specific approach on when or\nhow research should be distributed, but instead try to lay out some useful\ntools, analogies, and options for thinking about these issues.\n  We begin with some background on the idea that ML research might be misused\nin harmful ways, and why advances in synthetic media, in particular, are\nraising concerns. We then outline in more detail some of the different paths to\nharm from ML research, before reviewing research risk mitigation strategies in\nother fields and identifying components that seem most worth emulating in the\nML and synthetic media research communities. Next, we outline some important\ndimensions of disagreement on these issues which risk polarizing conversations.\n  Finally, we conclude with recommendations, suggesting that the machine\nlearning community might benefit from: working with subject matter experts to\nincrease understanding of the risk landscape and possible mitigation\nstrategies; building a community and norms around understanding the impacts of\nML research, e.g. through regular workshops at major conferences; and\nestablishing institutions and systems to support release practices that would\notherwise be onerous and error-prone.", "author_comment": "11 pages. Language fixes and tweaks for clarity", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.08174": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.08174v1", "post_title": "Establishing Appropriate Trust via Critical States", "authors": ["Sandy H. Huang", "Kush Bhatia", "Pieter Abbeel", "Anca D. Dragan"], "date_published": "2018-10-18 17:29:47+00:00", "data_last_modified": "2018-10-18 17:29:47+00:00", "url": "http://arxiv.org/abs/1810.08174v1", "abstract": "In order to effectively interact with or supervise a robot, humans need to\nhave an accurate mental model of its capabilities and how it acts. Learned\nneural network policies make that particularly challenging. We propose an\napproach for helping end-users build a mental model of such policies. Our key\nobservation is that for most tasks, the essence of the policy is captured in a\nfew critical states: states in which it is very important to take a certain\naction. Our user studies show that if the robot shows a human what its\nunderstanding of the task's critical states is, then the human can make a more\ninformed decision about whether to deploy the policy, and if she does deploy\nit, when she needs to take control from it at execution time.", "author_comment": "IROS 2018", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1802.03493": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1802.03493v2", "post_title": "More Robust Doubly Robust Off-policy Evaluation", "authors": ["Mehrdad Farajtabar", "Yinlam Chow", "Mohammad Ghavamzadeh"], "date_published": "2018-02-10 01:32:03+00:00", "data_last_modified": "2018-05-23 18:13:43+00:00", "url": "http://arxiv.org/abs/1802.03493v2", "abstract": "We study the problem of off-policy evaluation (OPE) in reinforcement learning\n(RL), where the goal is to estimate the performance of a policy from the data\ngenerated by another policy(ies). In particular, we focus on the doubly robust\n(DR) estimators that consist of an importance sampling (IS) component and a\nperformance model, and utilize the low (or zero) bias of IS and low variance of\nthe model at the same time. Although the accuracy of the model has a huge\nimpact on the overall performance of DR, most of the work on using the DR\nestimators in OPE has been focused on improving the IS part, and not much on\nhow to learn the model. In this paper, we propose alternative DR estimators,\ncalled more robust doubly robust (MRDR), that learn the model parameter by\nminimizing the variance of the DR estimator. We first present a formulation for\nlearning the DR model in RL. We then derive formulas for the variance of the DR\nestimator in both contextual bandits and RL, such that their gradients\nw.r.t.~the model parameters can be estimated from the samples, and propose\nmethods to efficiently minimize the variance. We prove that the MRDR estimators\nare strongly consistent and asymptotically optimal. Finally, we evaluate MRDR\nin bandits and RL benchmark problems, and compare its performance with the\nexisting methods.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.01186": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.01186v2", "post_title": "Penalizing side effects using stepwise relative reachability", "authors": ["Victoria Krakovna", "Laurent Orseau", "Ramana Kumar", "Miljan Martic", "Shane Legg"], "date_published": "2018-06-04 16:30:17+00:00", "data_last_modified": "2019-03-08 09:17:21+00:00", "url": "http://arxiv.org/abs/1806.01186v2", "abstract": "How can we design safe reinforcement learning agents that avoid unnecessary\ndisruptions to their environment? We show that current approaches to penalizing\nside effects can introduce bad incentives, e.g. to prevent any irreversible\nchanges in the environment, including the actions of other agents. To isolate\nthe source of such undesirable incentives, we break down side effects penalties\ninto two components: a baseline state and a measure of deviation from this\nbaseline state. We argue that some of these incentives arise from the choice of\nbaseline, and others arise from the choice of deviation measure. We introduce a\nnew variant of the stepwise inaction baseline and a new deviation measure based\non relative reachability of states. The combination of these design choices\navoids the given undesirable incentives, while simpler baselines and the\nunreachability measure fail. We demonstrate this empirically by comparing\ndifferent combinations of baseline and deviation measure choices on a set of\ngridworld experiments designed to illustrate possible bad incentives.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.08361": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.08361v1", "post_title": "Scaling Laws for Neural Language Models", "authors": ["Jared Kaplan", "Sam McCandlish", "Tom Henighan", "Tom B. Brown", "Benjamin Chess", "Rewon Child", "Scott Gray", "Alec Radford", "Jeffrey Wu", "Dario Amodei"], "date_published": "2020-01-23 03:59:20+00:00", "data_last_modified": "2020-01-23 03:59:20+00:00", "url": "http://arxiv.org/abs/2001.08361v1", "abstract": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.", "author_comment": "19 pages, 15 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.00366": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.00366v2", "post_title": "Beyond Winning and Losing: Modeling Human Motivations and Behaviors Using Inverse Reinforcement Learning", "authors": ["Baoxiang Wang", "Tongfang Sun", "Xianjun Sam Zheng"], "date_published": "2018-07-01 18:20:23+00:00", "data_last_modified": "2018-07-05 09:14:00+00:00", "url": "http://arxiv.org/abs/1807.00366v2", "abstract": "In recent years, reinforcement learning (RL) methods have been applied to\nmodel gameplay with great success, achieving super-human performance in various\nenvironments, such as Atari, Go, and Poker. However, those studies mostly focus\non winning the game and have largely ignored the rich and complex human\nmotivations, which are essential for understanding different players' diverse\nbehaviors. In this paper, we present a novel method called Multi-Motivation\nBehavior Modeling (MMBM) that takes the multifaceted human motivations into\nconsideration and models the underlying value structure of the players using\ninverse RL. Our approach does not require the access to the dynamic of the\nsystem, making it feasible to model complex interactive environments such as\nmassively multiplayer online games. MMBM is tested on the World of Warcraft\nAvatar History dataset, which recorded over 70,000 users' gameplay spanning\nthree years period. Our model reveals the significant difference of value\nstructures among different player groups. Using the results of motivation\nmodeling, we also predict and explain their diverse gameplay behaviors and\nprovide a quantitative assessment of how the redesign of the game environment\nimpacts players' behaviors.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.09977": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.09977v3", "post_title": "Towards a Human-like Open-Domain Chatbot", "authors": ["Daniel Adiwardana", "Minh-Thang Luong", "David R. So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu", "Quoc V. Le"], "date_published": "2020-01-27 18:53:15+00:00", "data_last_modified": "2020-02-27 07:36:47+00:00", "url": "http://arxiv.org/abs/2001.09977v3", "abstract": "We present Meena, a multi-turn open-domain chatbot trained end-to-end on data\nmined and filtered from public domain social media conversations. This 2.6B\nparameter neural network is simply trained to minimize perplexity of the next\ntoken. We also propose a human evaluation metric called Sensibleness and\nSpecificity Average (SSA), which captures key elements of a human-like\nmulti-turn conversation. Our experiments show strong correlation between\nperplexity and SSA. The fact that the best perplexity end-to-end trained Meena\nscores high on SSA (72% on multi-turn evaluation) suggests that a human-level\nSSA of 86% is potentially within reach if we can better optimize perplexity.\nAdditionally, the full version of Meena (with a filtering mechanism and tuned\ndecoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots\nwe evaluated.", "author_comment": "38 pages, 12 figures", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2105.14111": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2105.14111v2", "post_title": "Objective Robustness in Deep Reinforcement Learning", "authors": ["Jack Koch", "Lauro Langosco", "Jacob Pfau", "James Le", "Lee Sharkey"], "date_published": "2021-05-28 21:13:34+00:00", "data_last_modified": "2021-06-08 21:48:02+00:00", "url": "http://arxiv.org/abs/2105.14111v2", "abstract": "We study objective robustness failures, a type of out-of-distribution\nrobustness failure in reinforcement learning (RL). Objective robustness\nfailures occur when an RL agent retains its capabilities out-of-distribution\nyet pursues the wrong objective. This kind of failure presents different risks\nthan the robustness problems usually considered in the literature, since it\ninvolves agents that leverage their capabilities to pursue the wrong objective\nrather than simply failing to do anything useful. We provide the first explicit\nempirical demonstrations of objective robustness failures and present a partial\ncharacterization of its causes.", "author_comment": "small revisions, corrected figure for ablation", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.08313": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.08313v2", "post_title": "Learning Safe Policies with Expert Guidance", "authors": ["Jessie Huang", "Fa Wu", "Doina Precup", "Yang Cai"], "date_published": "2018-05-21 22:40:07+00:00", "data_last_modified": "2018-11-21 17:17:23+00:00", "url": "http://arxiv.org/abs/1805.08313v2", "abstract": "We propose a framework for ensuring safe behavior of a reinforcement learning\nagent when the reward function may be difficult to specify. In order to do\nthis, we rely on the existence of demonstrations from expert policies, and we\nprovide a theoretical framework for the agent to optimize in the space of\nrewards consistent with its existing knowledge. We propose two methods to solve\nthe resulting optimization: an exact ellipsoid-based method and a method in the\nspirit of the \"follow-the-perturbed-leader\" algorithm. Our experiments\ndemonstrate the behavior of our algorithm in both discrete and continuous\nproblems. The trained agent safely avoids states with potential negative\neffects while imitating the behavior of the expert in the other states.", "author_comment": "Appears in NeurIPS 2018", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.10181": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.10181v3", "post_title": "Robust Imitation Learning from Noisy Demonstrations", "authors": ["Voot Tangkaratt", "Nontawat Charoenphakdee", "Masashi Sugiyama"], "date_published": "2020-10-20 10:41:37+00:00", "data_last_modified": "2021-02-19 13:38:24+00:00", "url": "http://arxiv.org/abs/2010.10181v3", "abstract": "Robust learning from noisy demonstrations is a practical but highly\nchallenging problem in imitation learning. In this paper, we first\ntheoretically show that robust imitation learning can be achieved by optimizing\na classification risk with a symmetric loss. Based on this theoretical finding,\nwe then propose a new imitation learning method that optimizes the\nclassification risk by effectively combining pseudo-labeling with co-training.\nUnlike existing methods, our method does not require additional labels or\nstrict assumptions about noise distributions. Experimental results on\ncontinuous-control benchmarks show that our method is more robust compared to\nstate-of-the-art methods.", "author_comment": "16 pages, 9 figures. Accepted to AISTATS 2021", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.11708": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.11708v1", "post_title": "Generalized Hindsight for Reinforcement Learning", "authors": ["Alexander C. Li", "Lerrel Pinto", "Pieter Abbeel"], "date_published": "2020-02-26 18:57:05+00:00", "data_last_modified": "2020-02-26 18:57:05+00:00", "url": "http://arxiv.org/abs/2002.11708v1", "abstract": "One of the key reasons for the high sample complexity in reinforcement\nlearning (RL) is the inability to transfer knowledge from one task to another.\nIn standard multi-task RL settings, low-reward data collected while trying to\nsolve one task provides little to no signal for solving that particular task\nand is hence effectively wasted. However, we argue that this data, which is\nuninformative for one task, is likely a rich source of information for other\ntasks. To leverage this insight and efficiently reuse data, we present\nGeneralized Hindsight: an approximate inverse reinforcement learning technique\nfor relabeling behaviors with the right tasks. Intuitively, given a behavior\ngenerated under one task, Generalized Hindsight returns a different task that\nthe behavior is better suited for. Then, the behavior is relabeled with this\nnew task before being used by an off-policy RL optimizer. Compared to standard\nrelabeling techniques, Generalized Hindsight provides a substantially more\nefficient reuse of samples, which we empirically demonstrate on a suite of\nmulti-task navigation and manipulation tasks. Videos and code can be accessed\nhere: https://sites.google.com/view/generalized-hindsight.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.08263": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.08263v4", "post_title": "Learning What Information to Give in Partially Observed Domains", "authors": ["Rohan Chitnis", "Leslie Pack Kaelbling", "Tom\u00e1s Lozano-P\u00e9rez"], "date_published": "2018-05-21 19:16:02+00:00", "data_last_modified": "2018-09-27 18:39:19+00:00", "url": "http://arxiv.org/abs/1805.08263v4", "abstract": "In many robotic applications, an autonomous agent must act within and explore\na partially observed environment that is unobserved by its human teammate. We\nconsider such a setting in which the agent can, while acting, transmit\ndeclarative information to the human that helps them understand aspects of this\nunseen environment. In this work, we address the algorithmic question of how\nthe agent should plan out what actions to take and what information to\ntransmit. Naturally, one would expect the human to have preferences, which we\nmodel information-theoretically by scoring transmitted information based on the\nchange it induces in weighted entropy of the human's belief state. We formulate\nthis setting as a belief MDP and give a tractable algorithm for solving it\napproximately. Then, we give an algorithm that allows the agent to learn the\nhuman's preferences online, through exploration. We validate our approach\nexperimentally in simulated discrete and continuous partially observed\nsearch-and-recover domains. Visit http://tinyurl.com/chitnis-corl-18 for a\nsupplementary video.", "author_comment": "CoRL 2018 final version", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.01034": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.01034v1", "post_title": "Transfer of Adversarial Robustness Between Perturbation Types", "authors": ["Daniel Kang", "Yi Sun", "Tom Brown", "Dan Hendrycks", "Jacob Steinhardt"], "date_published": "2019-05-03 04:51:07+00:00", "data_last_modified": "2019-05-03 04:51:07+00:00", "url": "http://arxiv.org/abs/1905.01034v1", "abstract": "We study the transfer of adversarial robustness of deep neural networks\nbetween different perturbation types. While most work on adversarial examples\nhas focused on $L_\\infty$ and $L_2$-bounded perturbations, these do not capture\nall types of perturbations available to an adversary. The present work\nevaluates 32 attacks of 5 different types against models adversarially trained\non a 100-class subset of ImageNet. Our empirical results suggest that\nevaluating on a wide range of perturbation sizes is necessary to understand\nwhether adversarial robustness transfers between perturbation types. We further\ndemonstrate that robustness against one perturbation type may not always imply\nand may sometimes hurt robustness against other perturbation types. In light of\nthese results, we recommend evaluation of adversarial defenses take place on a\ndiverse range of perturbation types and sizes.", "author_comment": "11 pages, 6 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.10875": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.10875v1", "post_title": "TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing", "authors": ["Augustus Odena", "Ian Goodfellow"], "date_published": "2018-07-28 02:11:40+00:00", "data_last_modified": "2018-07-28 02:11:40+00:00", "url": "http://arxiv.org/abs/1807.10875v1", "abstract": "Machine learning models are notoriously difficult to interpret and debug.\nThis is particularly true of neural networks. In this work, we introduce\nautomated software testing techniques for neural networks that are well-suited\nto discovering errors which occur only for rare inputs. Specifically, we\ndevelop coverage-guided fuzzing (CGF) methods for neural networks. In CGF,\nrandom mutations of inputs to a neural network are guided by a coverage metric\ntoward the goal of satisfying user-specified constraints. We describe how fast\napproximate nearest neighbor algorithms can provide this coverage metric. We\nthen discuss the application of CGF to the following goals: finding numerical\nerrors in trained neural networks, generating disagreements between neural\nnetworks and quantized versions of those networks, and surfacing undesirable\nbehavior in character level language models. Finally, we release an open source\nlibrary called TensorFuzz that implements the described techniques.", "author_comment": "Preprint - work in progress", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1812.11118": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1812.11118v2", "post_title": "Reconciling modern machine learning practice and the bias-variance trade-off", "authors": ["Mikhail Belkin", "Daniel Hsu", "Siyuan Ma", "Soumik Mandal"], "date_published": "2018-12-28 17:15:38+00:00", "data_last_modified": "2019-09-10 19:51:04+00:00", "url": "http://arxiv.org/abs/1812.11118v2", "abstract": "Breakthroughs in machine learning are rapidly changing science and society,\nyet our fundamental understanding of this technology has lagged far behind.\nIndeed, one of the central tenets of the field, the bias-variance trade-off,\nappears to be at odds with the observed behavior of methods used in the modern\nmachine learning practice. The bias-variance trade-off implies that a model\nshould balance under-fitting and over-fitting: rich enough to express\nunderlying structure in data, simple enough to avoid fitting spurious patterns.\nHowever, in the modern practice, very rich models such as neural networks are\ntrained to exactly fit (i.e., interpolate) the data. Classically, such models\nwould be considered over-fit, and yet they often obtain high accuracy on test\ndata. This apparent contradiction has raised questions about the mathematical\nfoundations of machine learning and their relevance to practitioners.\n  In this paper, we reconcile the classical understanding and the modern\npractice within a unified performance curve. This \"double descent\" curve\nsubsumes the textbook U-shaped bias-variance trade-off curve by showing how\nincreasing model capacity beyond the point of interpolation results in improved\nperformance. We provide evidence for the existence and ubiquity of double\ndescent for a wide spectrum of models and datasets, and we posit a mechanism\nfor its emergence. This connection between the performance and the structure of\nmachine learning models delineates the limits of classical analyses, and has\nimplications for both the theory and practice of machine learning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.05433": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.05433v2", "post_title": "Tackling Climate Change with Machine Learning", "authors": ["David Rolnick", "Priya L. Donti", "Lynn H. Kaack", "Kelly Kochanski", "Alexandre Lacoste", "Kris Sankaran", "Andrew Slavin Ross", "Nikola Milojevic-Dupont", "Natasha Jaques", "Anna Waldman-Brown", "Alexandra Luccioni", "Tegan Maharaj", "Evan D. Sherwin", "S. Karthik Mukkavilli", "Konrad P. Kording", "Carla Gomes", "Andrew Y. Ng", "Demis Hassabis", "John C. Platt", "Felix Creutzig", "Jennifer Chayes", "Yoshua Bengio"], "date_published": "2019-06-10 17:51:47+00:00", "data_last_modified": "2019-11-05 17:37:20+00:00", "url": "http://arxiv.org/abs/1906.05433v2", "abstract": "Climate change is one of the greatest challenges facing humanity, and we, as\nmachine learning experts, may wonder how we can help. Here we describe how\nmachine learning can be a powerful tool in reducing greenhouse gas emissions\nand helping society adapt to a changing climate. From smart grids to disaster\nmanagement, we identify high impact problems where existing gaps can be filled\nby machine learning, in collaboration with other fields. Our recommendations\nencompass exciting research questions as well as promising business\nopportunities. We call on the machine learning community to join the global\neffort against climate change.", "author_comment": "For additional resources, please visit the website that accompanies\n  this paper: https://www.climatechange.ai/", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.06151": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.06151v1", "post_title": "Deep Reinforcement Learning with Feedback-based Exploration", "authors": ["Jan Scholten", "Daan Wout", "Carlos Celemin", "Jens Kober"], "date_published": "2019-03-14 17:52:46+00:00", "data_last_modified": "2019-03-14 17:52:46+00:00", "url": "http://arxiv.org/abs/1903.06151v1", "abstract": "Deep Reinforcement Learning has enabled the control of increasingly complex\nand high-dimensional problems. However, the need of vast amounts of data before\nreasonable performance is attained prevents its widespread application. We\nemploy binary corrective feedback as a general and intuitive manner to\nincorporate human intuition and domain knowledge in model-free machine\nlearning. The uncertainty in the policy and the corrective feedback is combined\ndirectly in the action space as probabilistic conditional exploration. As a\nresult, the greatest part of the otherwise ignorant learning process can be\navoided. We demonstrate the proposed method, Predictive Probabilistic Merging\nof Policies (PPMP), in combination with DDPG. In experiments on continuous\ncontrol problems of the OpenAI Gym, we achieve drastic improvements in sample\nefficiency, final performance, and robustness to erroneous feedback, both for\nhuman and synthetic feedback. Additionally, we show solutions beyond the\ndemonstrated knowledge.", "author_comment": "6 pages", "journal_ref": null, "doi": "10.1109/CDC40024.2019.9029503", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.05856": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.05856v1", "post_title": "Amplifying the Imitation Effect for Reinforcement Learning of UCAV's Mission Execution", "authors": ["Gyeong Taek Lee", "Chang Ouk Kim"], "date_published": "2019-01-17 15:47:12+00:00", "data_last_modified": "2019-01-17 15:47:12+00:00", "url": "http://arxiv.org/abs/1901.05856v1", "abstract": "This paper proposes a new reinforcement learning (RL) algorithm that enhances\nexploration by amplifying the imitation effect (AIE). This algorithm consists\nof self-imitation learning and random network distillation algorithms. We argue\nthat these two algorithms complement each other and that combining these two\nalgorithms can amplify the imitation effect for exploration. In addition, by\nadding an intrinsic penalty reward to the state that the RL agent frequently\nvisits and using replay memory for learning the feature state when using an\nexploration bonus, the proposed approach leads to deep exploration and deviates\nfrom the current converged policy. We verified the exploration performance of\nthe algorithm through experiments in a two-dimensional grid environment. In\naddition, we applied the algorithm to a simulated environment of unmanned\ncombat aerial vehicle (UCAV) mission execution, and the empirical results show\nthat AIE is very effective for finding the UCAV's shortest flight path to avoid\nan enemy's missiles.", "author_comment": "9 pages", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.06680": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.06680v1", "post_title": "Dota 2 with Large Scale Deep Reinforcement Learning", "authors": ["OpenAI", ":", "Christopher Berner", "Greg Brockman", "Brooke Chan", "Vicki Cheung", "Przemys\u0142aw D\u0119biak", "Christy Dennison", "David Farhi", "Quirin Fischer", "Shariq Hashme", "Chris Hesse", "Rafal J\u00f3zefowicz", "Scott Gray", "Catherine Olsson", "Jakub Pachocki", "Michael Petrov", "Henrique P. d. O. Pinto", "Jonathan Raiman", "Tim Salimans", "Jeremy Schlatter", "Jonas Schneider", "Szymon Sidor", "Ilya Sutskever", "Jie Tang", "Filip Wolski", "Susan Zhang"], "date_published": "2019-12-13 19:56:40+00:00", "data_last_modified": "2019-12-13 19:56:40+00:00", "url": "http://arxiv.org/abs/1912.06680v1", "abstract": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the\nworld champions at an esports game. The game of Dota 2 presents novel\nchallenges for AI systems such as long time horizons, imperfect information,\nand complex, continuous state-action spaces, all challenges which will become\nincreasingly central to more capable AI systems. OpenAI Five leveraged existing\nreinforcement learning techniques, scaled to learn from batches of\napproximately 2 million frames every 2 seconds. We developed a distributed\ntraining system and tools for continual training which allowed us to train\nOpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG),\nOpenAI Five demonstrates that self-play reinforcement learning can achieve\nsuperhuman performance on a difficult task.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2110.07719": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2110.07719v1", "post_title": "Certified Patch Robustness via Smoothed Vision Transformers", "authors": ["Hadi Salman", "Saachi Jain", "Eric Wong", "Aleksander M\u0105dry"], "date_published": "2021-10-11 17:44:05+00:00", "data_last_modified": "2021-10-11 17:44:05+00:00", "url": "http://arxiv.org/abs/2110.07719v1", "abstract": "Certified patch defenses can guarantee robustness of an image classifier to\narbitrary changes within a bounded contiguous region. But, currently, this\nrobustness comes at a cost of degraded standard accuracies and slower inference\ntimes. We demonstrate how using vision transformers enables significantly\nbetter certified patch robustness that is also more computationally efficient\nand does not incur a substantial drop in standard accuracy. These improvements\nstem from the inherent ability of the vision transformer to gracefully handle\nlargely masked images. Our code is available at\nhttps://github.com/MadryLab/smoothed-vit.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.14804": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.14804v5", "post_title": "Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation", "authors": ["Lin Guan", "Mudit Verma", "Sihang Guo", "Ruohan Zhang", "Subbarao Kambhampati"], "date_published": "2020-06-26 05:40:05+00:00", "data_last_modified": "2021-10-26 19:16:10+00:00", "url": "http://arxiv.org/abs/2006.14804v5", "abstract": "Human explanation (e.g., in terms of feature importance) has been recently\nused to extend the communication channel between human and agent in interactive\nmachine learning. Under this setting, human trainers provide not only the\nground truth but also some form of explanation. However, this kind of human\nguidance was only investigated in supervised learning tasks, and it remains\nunclear how to best incorporate this type of human knowledge into deep\nreinforcement learning. In this paper, we present the first study of using\nhuman visual explanations in human-in-the-loop reinforcement learning (HRL). We\nfocus on the task of learning from feedback, in which the human trainer not\nonly gives binary evaluative \"good\" or \"bad\" feedback for queried state-action\npairs, but also provides a visual explanation by annotating relevant features\nin images. We propose EXPAND (EXPlanation AugmeNted feeDback) to encourage the\nmodel to encode task-relevant features through a context-aware data\naugmentation that only perturbs irrelevant features in human salient\ninformation. We choose five tasks, namely Pixel-Taxi and four Atari games, to\nevaluate the performance and sample efficiency of this approach. We show that\nour method significantly outperforms methods leveraging human explanation that\nare adapted from supervised learning, and Human-in-the-loop RL baselines that\nonly utilize evaluative feedback.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.10593": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.10593v1", "post_title": "Inverse reinforcement learning for video games", "authors": ["Aaron Tucker", "Adam Gleave", "Stuart Russell"], "date_published": "2018-10-24 20:00:50+00:00", "data_last_modified": "2018-10-24 20:00:50+00:00", "url": "http://arxiv.org/abs/1810.10593v1", "abstract": "Deep reinforcement learning achieves superhuman performance in a range of\nvideo game environments, but requires that a designer manually specify a reward\nfunction. It is often easier to provide demonstrations of a target behavior\nthan to design a reward function describing that behavior. Inverse\nreinforcement learning (IRL) algorithms can infer a reward from demonstrations\nin low-dimensional continuous control environments, but there has been little\nwork on applying IRL to high-dimensional video games. In our CNN-AIRL baseline,\nwe modify the state-of-the-art adversarial IRL (AIRL) algorithm to use CNNs for\nthe generator and discriminator. To stabilize training, we normalize the reward\nand increase the size of the discriminator training dataset. We additionally\nlearn a low-dimensional state representation using a novel autoencoder\narchitecture tuned for video game environments. This embedding is used as input\nto the reward network, improving the sample efficiency of expert\ndemonstrations. Our method achieves high-level performance on the simple\nCatcher video game, substantially outperforming the CNN-AIRL baseline. We also\nscore points on the Enduro Atari racing game, but do not match expert\nperformance, highlighting the need for further work.", "author_comment": "10 pages, 4 figures. Submitted to NIPS Deep RL Workshop", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2201.03544": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2201.03544v2", "post_title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models", "authors": ["Alexander Pan", "Kush Bhatia", "Jacob Steinhardt"], "date_published": "2022-01-10 18:58:52+00:00", "data_last_modified": "2022-02-14 09:05:38+00:00", "url": "http://arxiv.org/abs/2201.03544v2", "abstract": "Reward hacking -- where RL agents exploit gaps in misspecified reward\nfunctions -- has been widely observed, but not yet systematically studied. To\nunderstand how reward hacking arises, we construct four RL environments with\nmisspecified rewards. We investigate reward hacking as a function of agent\ncapabilities: model capacity, action space resolution, observation space noise,\nand training time. More capable agents often exploit reward misspecifications,\nachieving higher proxy reward and lower true reward than less capable agents.\nMoreover, we find instances of phase transitions: capability thresholds at\nwhich the agent's behavior qualitatively shifts, leading to a sharp decrease in\nthe true reward. Such phase transitions pose challenges to monitoring the\nsafety of ML systems. To address this, we propose an anomaly detection task for\naberrant policies and offer several baseline detectors.", "author_comment": "ICLR 2022; 19 pages", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1710.11248": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1710.11248v2", "post_title": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning", "authors": ["Justin Fu", "Katie Luo", "Sergey Levine"], "date_published": "2017-10-30 21:22:28+00:00", "data_last_modified": "2018-08-13 18:33:24+00:00", "url": "http://arxiv.org/abs/1710.11248v2", "abstract": "Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the\nneed for extensive feature and reward engineering. Deep reinforcement learning\nmethods can remove the need for explicit engineering of policy or value\nfeatures, but still require a manually specified reward function. Inverse\nreinforcement learning holds the promise of automatic reward acquisition, but\nhas proven exceptionally difficult to apply to large, high-dimensional problems\nwith unknown dynamics. In this work, we propose adverserial inverse\nreinforcement learning (AIRL), a practical and scalable inverse reinforcement\nlearning algorithm based on an adversarial reward learning formulation. We\ndemonstrate that AIRL is able to recover reward functions that are robust to\nchanges in dynamics, enabling us to learn policies even under significant\nvariation in the environment seen during training. Our experiments show that\nAIRL greatly outperforms prior methods in these transfer settings.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2112.00659": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2112.00659v1", "post_title": "Certified Adversarial Defenses Meet Out-of-Distribution Corruptions: Benchmarking Robustness and Simple Baselines", "authors": ["Jiachen Sun", "Akshay Mehra", "Bhavya Kailkhura", "Pin-Yu Chen", "Dan Hendrycks", "Jihun Hamm", "Z. Morley Mao"], "date_published": "2021-12-01 17:11:22+00:00", "data_last_modified": "2021-12-01 17:11:22+00:00", "url": "http://arxiv.org/abs/2112.00659v1", "abstract": "Certified robustness guarantee gauges a model's robustness to test-time\nattacks and can assess the model's readiness for deployment in the real world.\nIn this work, we critically examine how the adversarial robustness guarantees\nfrom randomized smoothing-based certification methods change when\nstate-of-the-art certifiably robust models encounter out-of-distribution (OOD)\ndata. Our analysis demonstrates a previously unknown vulnerability of these\nmodels to low-frequency OOD data such as weather-related corruptions, rendering\nthese models unfit for deployment in the wild. To alleviate this issue, we\npropose a novel data augmentation scheme, FourierMix, that produces\naugmentations to improve the spectral coverage of the training data.\nFurthermore, we propose a new regularizer that encourages consistent\npredictions on noise perturbations of the augmented data to improve the quality\nof the smoothed models. We find that FourierMix augmentations help eliminate\nthe spectral bias of certifiably robust models enabling them to achieve\nsignificantly better robustness guarantees on a range of OOD benchmarks. Our\nevaluation also uncovers the inability of current OOD benchmarks at\nhighlighting the spectral biases of the models. To this end, we propose a\ncomprehensive benchmarking suite that contains corruptions from different\nregions in the spectral domain. Evaluation of models trained with popular\naugmentation methods on the proposed suite highlights their spectral biases and\nestablishes the superiority of FourierMix trained models at achieving\nbetter-certified robustness guarantees under OOD shifts over the entire\nfrequency spectrum.", "author_comment": "21 pages, 15 figures, and 9 tables", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CR"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.01560": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.01560v2", "post_title": "Reinforcement Learning under Threats", "authors": ["Victor Gallego", "Roi Naveiro", "David Rios Insua"], "date_published": "2018-09-05 14:56:09+00:00", "data_last_modified": "2019-07-30 12:15:05+00:00", "url": "http://arxiv.org/abs/1809.01560v2", "abstract": "In several reinforcement learning (RL) scenarios, mainly in security\nsettings, there may be adversaries trying to interfere with the reward\ngenerating process. In this paper, we introduce Threatened Markov Decision\nProcesses (TMDPs), which provide a framework to support a decision maker\nagainst a potential adversary in RL. Furthermore, we propose a level-$k$\nthinking scheme resulting in a new learning framework to deal with TMDPs. After\nintroducing our framework and deriving theoretical results, relevant empirical\nevidence is given via extensive experiments, showing the benefits of accounting\nfor adversaries while the agent learns.", "author_comment": "Extends the verson published at the Proceedings of the AAAI\n  Conference on Artificial Intelligence 33,\n  https://www.aaai.org/ojs/index.php/AAAI/article/view/5106", "journal_ref": null, "doi": "10.1609/aaai.v33i01.33019939", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2003.04297": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2003.04297v1", "post_title": "Improved Baselines with Momentum Contrastive Learning", "authors": ["Xinlei Chen", "Haoqi Fan", "Ross Girshick", "Kaiming He"], "date_published": "2020-03-09 17:56:49+00:00", "data_last_modified": "2020-03-09 17:56:49+00:00", "url": "http://arxiv.org/abs/2003.04297v1", "abstract": "Contrastive unsupervised learning has recently shown encouraging progress,\ne.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the\neffectiveness of two of SimCLR's design improvements by implementing them in\nthe MoCo framework. With simple modifications to MoCo---namely, using an MLP\nprojection head and more data augmentation---we establish stronger baselines\nthat outperform SimCLR and do not require large training batches. We hope this\nwill make state-of-the-art unsupervised learning research more accessible. Code\nwill be made public.", "author_comment": "Tech report, 2 pages + references", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.02781": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.02781v2", "post_title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty", "authors": ["Dan Hendrycks", "Norman Mu", "Ekin D. Cubuk", "Barret Zoph", "Justin Gilmer", "Balaji Lakshminarayanan"], "date_published": "2019-12-05 18:18:10+00:00", "data_last_modified": "2020-02-17 06:16:13+00:00", "url": "http://arxiv.org/abs/1912.02781v2", "abstract": "Modern deep neural networks can achieve high accuracy when the training\ndistribution and test distribution are identically distributed, but this\nassumption is frequently violated in practice. When the train and test\ndistributions are mismatched, accuracy can plummet. Currently there are few\ntechniques that improve robustness to unforeseen data shifts encountered during\ndeployment. In this work, we propose a technique to improve the robustness and\nuncertainty estimates of image classifiers. We propose AugMix, a data\nprocessing technique that is simple to implement, adds limited computational\noverhead, and helps models withstand unforeseen corruptions. AugMix\nsignificantly improves robustness and uncertainty measures on challenging image\nclassification benchmarks, closing the gap between previous methods and the\nbest possible performance in some cases by more than half.", "author_comment": "Code available at https://github.com/google-research/augmix", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1606.01540": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1606.01540v1", "post_title": "OpenAI Gym", "authors": ["Greg Brockman", "Vicki Cheung", "Ludwig Pettersson", "Jonas Schneider", "John Schulman", "Jie Tang", "Wojciech Zaremba"], "date_published": "2016-06-05 17:54:48+00:00", "data_last_modified": "2016-06-05 17:54:48+00:00", "url": "http://arxiv.org/abs/1606.01540v1", "abstract": "OpenAI Gym is a toolkit for reinforcement learning research. It includes a\ngrowing collection of benchmark problems that expose a common interface, and a\nwebsite where people can share their results and compare the performance of\nalgorithms. This whitepaper discusses the components of OpenAI Gym and the\ndesign decisions that went into the software.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1312.5602": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1312.5602v1", "post_title": "Playing Atari with Deep Reinforcement Learning", "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "date_published": "2013-12-19 16:00:08+00:00", "data_last_modified": "2013-12-19 16:00:08+00:00", "url": "http://arxiv.org/abs/1312.5602v1", "abstract": "We present the first deep learning model to successfully learn control\npolicies directly from high-dimensional sensory input using reinforcement\nlearning. The model is a convolutional neural network, trained with a variant\nof Q-learning, whose input is raw pixels and whose output is a value function\nestimating future rewards. We apply our method to seven Atari 2600 games from\nthe Arcade Learning Environment, with no adjustment of the architecture or\nlearning algorithm. We find that it outperforms all previous approaches on six\nof the games and surpasses a human expert on three of them.", "author_comment": "NIPS Deep Learning Workshop 2013", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.10029": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.10029v2", "post_title": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "authors": ["Ting Chen", "Simon Kornblith", "Kevin Swersky", "Mohammad Norouzi", "Geoffrey Hinton"], "date_published": "2020-06-17 17:48:22+00:00", "data_last_modified": "2020-10-26 03:09:28+00:00", "url": "http://arxiv.org/abs/2006.10029v2", "abstract": "One paradigm for learning from few labeled examples while making best use of\na large amount of unlabeled data is unsupervised pretraining followed by\nsupervised fine-tuning. Although this paradigm uses unlabeled data in a\ntask-agnostic way, in contrast to common approaches to semi-supervised learning\nfor computer vision, we show that it is surprisingly effective for\nsemi-supervised learning on ImageNet. A key ingredient of our approach is the\nuse of big (deep and wide) networks during pretraining and fine-tuning. We find\nthat, the fewer the labels, the more this approach (task-agnostic use of\nunlabeled data) benefits from a bigger network. After fine-tuning, the big\nnetwork can be further improved and distilled into a much smaller one with\nlittle loss in classification accuracy by using the unlabeled examples for a\nsecond time, but in a task-specific way. The proposed semi-supervised learning\nalgorithm can be summarized in three steps: unsupervised pretraining of a big\nResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples,\nand distillation with unlabeled examples for refining and transferring the\ntask-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy\nwith just 1% of the labels ($\\le$13 labeled images per class) using ResNet-50,\na $10\\times$ improvement in label efficiency over the previous\nstate-of-the-art. With 10% of labels, ResNet-50 trained with our method\nachieves 77.5% top-1 accuracy, outperforming standard supervised training with\nall of the labels.", "author_comment": "NeurIPS'2020. Code and pretrained models at\n  https://github.com/google-research/simclr", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1904.09959": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1904.09959v2", "post_title": "Optimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness", "authors": ["Greg Anderson", "Shankara Pailoor", "Isil Dillig", "Swarat Chaudhuri"], "date_published": "2019-04-22 17:21:52+00:00", "data_last_modified": "2019-05-01 15:25:46+00:00", "url": "http://arxiv.org/abs/1904.09959v2", "abstract": "In recent years, the notion of local robustness (or robustness for short) has\nemerged as a desirable property of deep neural networks. Intuitively,\nrobustness means that small perturbations to an input do not cause the network\nto perform misclassifications. In this paper, we present a novel algorithm for\nverifying robustness properties of neural networks. Our method synergistically\ncombines gradient-based optimization methods for counterexample search with\nabstraction-based proof search to obtain a sound and ({\\delta}-)complete\ndecision procedure. Our method also employs a data-driven approach to learn a\nverification policy that guides abstract interpretation during proof search. We\nhave implemented the proposed approach in a tool called Charon and\nexperimentally evaluated it on hundreds of benchmarks. Our experiments show\nthat the proposed approach significantly outperforms three state-of-the-art\ntools, namely AI^2 , Reluplex, and Reluval.", "author_comment": null, "journal_ref": null, "doi": "10.1145/3314221.3314614", "primary_category": "cs.PL", "categories": ["cs.PL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1705.10557": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1705.10557v1", "post_title": "Universal Reinforcement Learning Algorithms: Survey and Experiments", "authors": ["John Aslanides", "Jan Leike", "Marcus Hutter"], "date_published": "2017-05-30 11:41:00+00:00", "data_last_modified": "2017-05-30 11:41:00+00:00", "url": "http://arxiv.org/abs/1705.10557v1", "abstract": "Many state-of-the-art reinforcement learning (RL) algorithms typically assume\nthat the environment is an ergodic Markov Decision Process (MDP). In contrast,\nthe field of universal reinforcement learning (URL) is concerned with\nalgorithms that make as few assumptions as possible about the environment. The\nuniversal Bayesian agent AIXI and a family of related URL algorithms have been\ndeveloped in this setting. While numerous theoretical optimality results have\nbeen proven for these agents, there has been no empirical investigation of\ntheir behavior to date. We present a short and accessible survey of these URL\nalgorithms under a unified notation and framework, along with results of some\nexperiments that qualitatively illustrate some properties of the resulting\npolicies, and their relative performance on partially-observable gridworld\nenvironments. We also present an open-source reference implementation of the\nalgorithms which we hope will facilitate further understanding of, and\nexperimentation with, these ideas.", "author_comment": "8 pages, 6 figures, Twenty-sixth International Joint Conference on\n  Artificial Intelligence (IJCAI-17)", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.14496": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.14496v4", "post_title": "Generative Temporal Difference Learning for Infinite-Horizon Prediction", "authors": ["Michael Janner", "Igor Mordatch", "Sergey Levine"], "date_published": "2020-10-27 17:54:12+00:00", "data_last_modified": "2021-11-29 00:51:39+00:00", "url": "http://arxiv.org/abs/2010.14496v4", "abstract": "We introduce the $\\gamma$-model, a predictive model of environment dynamics\nwith an infinite probabilistic horizon. Replacing standard single-step models\nwith $\\gamma$-models leads to generalizations of the procedures central to\nmodel-based control, including the model rollout and model-based value\nestimation. The $\\gamma$-model, trained with a generative reinterpretation of\ntemporal difference learning, is a natural continuous analogue of the successor\nrepresentation and a hybrid between model-free and model-based mechanisms. Like\na value function, it contains information about the long-term future; like a\nstandard predictive model, it is independent of task reward. We instantiate the\n$\\gamma$-model as both a generative adversarial network and normalizing flow,\ndiscuss how its training reflects an inescapable tradeoff between training-time\nand testing-time compounding errors, and empirically investigate its utility\nfor prediction and control.", "author_comment": "NeurIPS 2020. Project page at: https://gammamodels.github.io/", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.13208": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.13208v2", "post_title": "Feature Expansive Reward Learning: Rethinking Human Input", "authors": ["Andreea Bobu", "Marius Wiggert", "Claire Tomlin", "Anca D. Dragan"], "date_published": "2020-06-23 17:59:34+00:00", "data_last_modified": "2021-01-12 18:59:50+00:00", "url": "http://arxiv.org/abs/2006.13208v2", "abstract": "When a person is not satisfied with how a robot performs a task, they can\nintervene to correct it. Reward learning methods enable the robot to adapt its\nreward function online based on such human input, but they rely on handcrafted\nfeatures. When the correction cannot be explained by these features, recent\nwork in deep Inverse Reinforcement Learning (IRL) suggests that the robot could\nask for task demonstrations and recover a reward defined over the raw state\nspace. Our insight is that rather than implicitly learning about the missing\nfeature(s) from demonstrations, the robot should instead ask for data that\nexplicitly teaches it about what it is missing. We introduce a new type of\nhuman input in which the person guides the robot from states where the feature\nbeing taught is highly expressed to states where it is not. We propose an\nalgorithm for learning the feature from the raw state space and integrating it\ninto the reward function. By focusing the human input on the missing feature,\nour method decreases sample complexity and improves generalization of the\nlearned reward over the above deep IRL baseline. We show this in experiments\nwith a physical 7DOF robot manipulator, as well as in a user study conducted in\na simulated environment.", "author_comment": "13 pages, 14 figures", "journal_ref": null, "doi": "10.1145/3434073.3444667", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.07648": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.07648v2", "post_title": "Language Conditioned Imitation Learning over Unstructured Data", "authors": ["Corey Lynch", "Pierre Sermanet"], "date_published": "2020-05-15 17:08:50+00:00", "data_last_modified": "2021-07-07 23:43:24+00:00", "url": "http://arxiv.org/abs/2005.07648v2", "abstract": "Natural language is perhaps the most flexible and intuitive way for humans to\ncommunicate tasks to a robot. Prior work in imitation learning typically\nrequires each task be specified with a task id or goal image -- something that\nis often impractical in open-world environments. On the other hand, previous\napproaches in instruction following allow agent behavior to be guided by\nlanguage, but typically assume structure in the observations, actuators, or\nlanguage that limit their applicability to complex settings like robotics. In\nthis work, we present a method for incorporating free-form natural language\nconditioning into imitation learning. Our approach learns perception from\npixels, natural language understanding, and multitask continuous control\nend-to-end as a single neural network. Unlike prior work in imitation learning,\nour method is able to incorporate unlabeled and unstructured demonstration data\n(i.e. no task or language labels). We show this dramatically improves language\nconditioned performance, while reducing the cost of language annotation to less\nthan 1% of total data. At test time, a single language conditioned visuomotor\npolicy trained with our method can perform a wide variety of robotic\nmanipulation skills in a 3D environment, specified only with natural language\ndescriptions of each task (e.g. \"open the drawer...now pick up the block...now\npress the green button...\"). To scale up the number of instructions an agent\ncan follow, we propose combining text conditioned policies with large\npretrained neural language models. We find this allows a policy to be robust to\nmany out-of-distribution synonym instructions, without requiring new\ndemonstrations. See videos of a human typing live text commands to our agent at\nlanguage-play.github.io", "author_comment": "Published at RSS 2021", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.04257": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.04257v1", "post_title": "Deep Reinforcement Learning from Policy-Dependent Human Feedback", "authors": ["Dilip Arumugam", "Jun Ki Lee", "Sophie Saskin", "Michael L. Littman"], "date_published": "2019-02-12 06:45:21+00:00", "data_last_modified": "2019-02-12 06:45:21+00:00", "url": "http://arxiv.org/abs/1902.04257v1", "abstract": "To widen their accessibility and increase their utility, intelligent agents\nmust be able to learn complex behaviors as specified by (non-expert) human\nusers. Moreover, they will need to learn these behaviors within a reasonable\namount of time while efficiently leveraging the sparse feedback a human trainer\nis capable of providing. Recent work has shown that human feedback can be\ncharacterized as a critique of an agent's current behavior rather than as an\nalternative reward signal to be maximized, culminating in the COnvergent\nActor-Critic by Humans (COACH) algorithm for making direct policy updates based\non human feedback. Our work builds on COACH, moving to a setting where the\nagent's policy is represented by a deep neural network. We employ a series of\nmodifications on top of the original COACH algorithm that are critical for\nsuccessfully learning behaviors from high-dimensional observations, while also\nsatisfying the constraint of obtaining reduced sample complexity. We\ndemonstrate the effectiveness of our Deep COACH algorithm in the rich 3D world\nof Minecraft with an agent that learns to complete tasks by mapping from raw\npixels to actions using only real-time human feedback in 10-15 minutes of\ninteraction.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.08882": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.08882v2", "post_title": "Multi-task Maximum Entropy Inverse Reinforcement Learning", "authors": ["Adam Gleave", "Oliver Habryka"], "date_published": "2018-05-22 21:57:34+00:00", "data_last_modified": "2018-07-15 13:58:18+00:00", "url": "http://arxiv.org/abs/1805.08882v2", "abstract": "Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring\nmultiple reward functions from expert demonstrations. Prior work, built on\nBayesian IRL, is unable to scale to complex environments due to computational\nconstraints. This paper contributes a formulation of multi-task IRL in the more\ncomputationally efficient Maximum Causal Entropy (MCE) IRL framework.\nExperiments show our approach can perform one-shot imitation learning in a\ngridworld environment that single-task IRL algorithms need hundreds of\ndemonstrations to solve. We outline preliminary work using meta-learning to\nextend our method to the function approximator setting of modern MCE IRL\nalgorithms. Evaluating on multi-task variants of common simulated robotics\nbenchmarks, we discover serious limitations of these IRL algorithms, and\nconclude with suggestions for further work.", "author_comment": "Presented at 1st Workshop on Goal Specifications for Reinforcement\n  Learning (ICML/IJCAI/AAMAS 2018)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.00802": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.00802v1", "post_title": "Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance", "authors": ["Andrew J. Lohn"], "date_published": "2020-09-02 03:33:40+00:00", "data_last_modified": "2020-09-02 03:33:40+00:00", "url": "http://arxiv.org/abs/2009.00802v1", "abstract": "Test, Evaluation, Verification, and Validation (TEVV) for Artificial\nIntelligence (AI) is a challenge that threatens to limit the economic and\nsocietal rewards that AI researchers have devoted themselves to producing. A\ncentral task of TEVV for AI is estimating brittleness, where brittleness\nimplies that the system functions well within some bounds and poorly outside of\nthose bounds. This paper argues that neither of those criteria are certain of\nDeep Neural Networks. First, highly touted AI successes (eg. image\nclassification and speech recognition) are orders of magnitude more\nfailure-prone than are typically certified in critical systems even within\ndesign bounds (perfectly in-distribution sampling). Second, performance falls\noff only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced\nemphasis is needed on designing systems that are resilient despite\nfailure-prone AI components as well as on evaluating and improving OOD\nperformance in order to get AI to where it can clear the challenging hurdles of\nTEVV and certification.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.CY", "cs.SE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.05766": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.05766v1", "post_title": "Hierarchical Game-Theoretic Planning for Autonomous Vehicles", "authors": ["Jaime F. Fisac", "Eli Bronstein", "Elis Stefansson", "Dorsa Sadigh", "S. Shankar Sastry", "Anca D. Dragan"], "date_published": "2018-10-13 00:02:54+00:00", "data_last_modified": "2018-10-13 00:02:54+00:00", "url": "http://arxiv.org/abs/1810.05766v1", "abstract": "The actions of an autonomous vehicle on the road affect and are affected by\nthose of other drivers, whether overtaking, negotiating a merge, or avoiding an\naccident. This mutual dependence, best captured by dynamic game theory, creates\na strong coupling between the vehicle's planning and its predictions of other\ndrivers' behavior, and constitutes an open problem with direct implications on\nthe safety and viability of autonomous driving technology. Unfortunately,\ndynamic games are too computationally demanding to meet the real-time\nconstraints of autonomous driving in its continuous state and action space. In\nthis paper, we introduce a novel game-theoretic trajectory planning algorithm\nfor autonomous driving, that enables real-time performance by hierarchically\ndecomposing the underlying dynamic game into a long-horizon \"strategic\" game\nwith simplified dynamics and full information structure, and a short-horizon\n\"tactical\" game with full dynamics and a simplified information structure. The\nvalue of the strategic game is used to guide the tactical planning, implicitly\nextending the planning horizon, pushing the local trajectory optimization\ncloser to global solutions, and, most importantly, quantitatively accounting\nfor the autonomous vehicle and the human driver's ability and incentives to\ninfluence each other. In addition, our approach admits non-deterministic models\nof human decision-making, rather than relying on perfectly rational\npredictions. Our results showcase richer, safer, and more effective autonomous\nbehavior in comparison to existing techniques.", "author_comment": "Submitted to ICRA 2019", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.MA", "math.OC", "68T40, 93C85, 91A25", "I.2.9"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1609.08144": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1609.08144v2", "post_title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "authors": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey", "Jeff Klingner", "Apurva Shah", "Melvin Johnson", "Xiaobing Liu", "\u0141ukasz Kaiser", "Stephan Gouws", "Yoshikiyo Kato", "Taku Kudo", "Hideto Kazawa", "Keith Stevens", "George Kurian", "Nishant Patil", "Wei Wang", "Cliff Young", "Jason Smith", "Jason Riesa", "Alex Rudnick", "Oriol Vinyals", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean"], "date_published": "2016-09-26 19:59:55+00:00", "data_last_modified": "2016-10-08 19:10:41+00:00", "url": "http://arxiv.org/abs/1609.08144v2", "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT's use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google's\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT'14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google's phrase-based production system.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1705.03394": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1705.03394v1", "post_title": "That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi's paradox", "authors": ["Anders Sandberg", "Stuart Armstrong", "Milan M. Cirkovic"], "date_published": "2017-04-27 15:41:00+00:00", "data_last_modified": "2017-04-27 15:41:00+00:00", "url": "http://arxiv.org/abs/1705.03394v1", "abstract": "If a civilization wants to maximize computation it appears rational to\naestivate until the far future in order to exploit the low temperature\nenvironment: this can produce a $10^{30}$ multiplier of achievable computation.\nWe hence suggest the \"aestivation hypothesis\": the reason we are not observing\nmanifestations of alien civilizations is that they are currently (mostly)\ninactive, patiently waiting for future cosmic eras. This paper analyzes the\nassumptions going into the hypothesis and how physical law and observational\nevidence constrain the motivations of aliens compatible with the hypothesis.", "author_comment": "Submitted to Journal of the British Interplanetary Society", "journal_ref": null, "doi": null, "primary_category": "physics.pop-ph", "categories": ["physics.pop-ph"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.02591": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.02591v2", "post_title": "Learning Invariances for Policy Generalization", "authors": ["Remi Tachet", "Philip Bachman", "Harm van Seijen"], "date_published": "2018-09-07 17:32:19+00:00", "data_last_modified": "2020-12-12 12:57:19+00:00", "url": "http://arxiv.org/abs/1809.02591v2", "abstract": "While recent progress has spawned very powerful machine learning systems,\nthose agents remain extremely specialized and fail to transfer the knowledge\nthey gain to similar yet unseen tasks. In this paper, we study a simple\nreinforcement learning problem and focus on learning policies that encode the\nproper invariances for generalization to different settings. We evaluate three\npotential methods for policy generalization: data augmentation, meta-learning\nand adversarial training. We find our data augmentation method to be effective,\nand study the potential of meta-learning and adversarial learning as\nalternative task-agnostic approaches.", "author_comment": "7 pages, 1 figure", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.03571": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.03571v2", "post_title": "Intrinsic Geometric Vulnerability of High-Dimensional Artificial Intelligence", "authors": ["Luca Bortolussi", "Guido Sanguinetti"], "date_published": "2018-11-08 17:51:27+00:00", "data_last_modified": "2019-01-24 14:13:58+00:00", "url": "http://arxiv.org/abs/1811.03571v2", "abstract": "The success of modern Artificial Intelligence (AI) technologies depends\ncritically on the ability to learn non-linear functional dependencies from\nlarge, high dimensional data sets. Despite recent high-profile successes,\nempirical evidence indicates that the high predictive performance is often\npaired with low robustness, making AI systems potentially vulnerable to\nadversarial attacks. In this report, we provide a simple intuitive argument\nsuggesting that high performance and vulnerability are intrinsically coupled,\nand largely dependent on the geometry of typical, high-dimensional data sets.\nOur work highlights a major potential pitfall of modern AI systems, and\nsuggests practical research directions to ameliorate the problem.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1812.01647": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1812.01647v1", "post_title": "Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures", "authors": ["Jonathan Uesato", "Ananya Kumar", "Csaba Szepesvari", "Tom Erez", "Avraham Ruderman", "Keith Anderson", "Krishmamurthy", "Dvijotham", "Nicolas Heess", "Pushmeet Kohli"], "date_published": "2018-12-04 19:39:53+00:00", "data_last_modified": "2018-12-04 19:39:53+00:00", "url": "http://arxiv.org/abs/1812.01647v1", "abstract": "This paper addresses the problem of evaluating learning systems in safety\ncritical domains such as autonomous driving, where failures can have\ncatastrophic consequences. We focus on two problems: searching for scenarios\nwhen learned agents fail and assessing their probability of failure. The\nstandard method for agent evaluation in reinforcement learning, Vanilla Monte\nCarlo, can miss failures entirely, leading to the deployment of unsafe agents.\nWe demonstrate this is an issue for current agents, where even matching the\ncompute used for training is sometimes insufficient for evaluation. To address\nthis shortcoming, we draw upon the rare event probability estimation literature\nand propose an adversarial evaluation approach. Our approach focuses evaluation\non adversarially chosen situations, while still providing unbiased estimates of\nfailure probabilities. The key difficulty is in identifying these adversarial\nsituations -- since failures are rare there is little signal to drive\noptimization. To solve this we propose a continuation approach that learns\nfailure modes in related but less robust agents. Our approach also allows reuse\nof data already collected for training the agent. We demonstrate the efficacy\nof adversarial evaluation on two standard domains: humanoid control and\nsimulated driving. Experimental results show that our methods can find\ncatastrophic failures and estimate failures rates of agents multiple orders of\nmagnitude faster than standard evaluation schemes, in minutes to hours rather\nthan days.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.11298": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.11298v3", "post_title": "Exploring Restart Distributions", "authors": ["Arash Tavakoli", "Vitaly Levdik", "Riashat Islam", "Christopher M. Smith", "Petar Kormushev"], "date_published": "2018-11-27 22:40:01+00:00", "data_last_modified": "2020-08-18 03:42:32+00:00", "url": "http://arxiv.org/abs/1811.11298v3", "abstract": "We consider the generic approach of using an experience memory to help\nexploration by adapting a restart distribution. That is, given the capacity to\nreset the state with those corresponding to the agent's past observations, we\nhelp exploration by promoting faster state-space coverage via restarting the\nagent from a more diverse set of initial states, as well as allowing it to\nrestart in states associated with significant past experiences. This approach\nis compatible with both on-policy and off-policy methods. However, a caveat is\nthat altering the distribution of initial states could change the optimal\npolicies when searching within a restricted class of policies. To reduce this\nunsought learning bias, we evaluate our approach in deep reinforcement learning\nwhich benefits from the high representational capacity of deep neural networks.\nWe instantiate three variants of our approach, each inspired by an idea in the\ncontext of experience replay. Using these variants, we show that performance\ngains can be achieved, especially in hard exploration problems.", "author_comment": "RLDM 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.10800": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.10800v1", "post_title": "Probabilistic Dependency Graphs", "authors": ["Oliver Richardson", "Joseph Y Halpern"], "date_published": "2020-12-19 22:29:49+00:00", "data_last_modified": "2020-12-19 22:29:49+00:00", "url": "http://arxiv.org/abs/2012.10800v1", "abstract": "We introduce Probabilistic Dependency Graphs (PDGs), a new class of directed\ngraphical models. PDGs can capture inconsistent beliefs in a natural way and\nare more modular than Bayesian Networks (BNs), in that they make it easier to\nincorporate new information and restructure the representation. We show by\nexample how PDGs are an especially natural modeling tool. We provide three\nsemantics for PDGs, each of which can be derived from a scoring function (on\njoint distributions over the variables in the network) that can be viewed as\nrepresenting a distribution's incompatibility with the PDG. For the PDG\ncorresponding to a BN, this function is uniquely minimized by the distribution\nthe BN represents, showing that PDG semantics extend BN semantics. We show\nfurther that factor graphs and their exponential families can also be\nfaithfully represented as PDGs, while there are significant barriers to\nmodeling a PDG with a factor graph.", "author_comment": "5 figures, 7 pages", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.IT", "math.IT"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.04053": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.04053v1", "post_title": "The 30-Year Cycle In The AI Debate", "authors": ["Jean-Marie Chauvet"], "date_published": "2018-10-08 16:35:06+00:00", "data_last_modified": "2018-10-08 16:35:06+00:00", "url": "http://arxiv.org/abs/1810.04053v1", "abstract": "In the last couple of years, the rise of Artificial Intelligence and the\nsuccesses of academic breakthroughs in the field have been inescapable. Vast\nsums of money have been thrown at AI start-ups. Many existing tech companies --\nincluding the giants like Google, Amazon, Facebook, and Microsoft -- have\nopened new research labs. The rapid changes in these everyday work and\nentertainment tools have fueled a rising interest in the underlying technology\nitself; journalists write about AI tirelessly, and companies -- of tech nature\nor not -- brand themselves with AI, Machine Learning or Deep Learning whenever\nthey get a chance. Confronting squarely this media coverage, several analysts\nare starting to voice concerns about over-interpretation of AI's blazing\nsuccesses and the sometimes poor public reporting on the topic. This paper\nreviews briefly the track-record in AI and Machine Learning and finds this\npattern of early dramatic successes, followed by philosophical critique and\nunexpected difficulties, if not downright stagnation, returning almost to the\nclock in 30-year cycles since 1958.", "author_comment": "31 pages, 5 tables", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "I.2.0"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.04948": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.04948v1", "post_title": "AI Research Considerations for Human Existential Safety (ARCHES)", "authors": ["Andrew Critch", "David Krueger"], "date_published": "2020-05-30 02:05:16+00:00", "data_last_modified": "2020-05-30 02:05:16+00:00", "url": "http://arxiv.org/abs/2006.04948v1", "abstract": "Framed in positive terms, this report examines how technical AI research\nmight be steered in a manner that is more attentive to humanity's long-term\nprospects for survival as a species. In negative terms, we ask what existential\nrisks humanity might face from AI development in the next century, and by what\nprinciples contemporary technical research might be directed to address those\nrisks.\n  A key property of hypothetical AI technologies is introduced, called\n\\emph{prepotence}, which is useful for delineating a variety of potential\nexistential risks from artificial intelligence, even as AI paradigms might\nshift. A set of \\auxref{dirtot} contemporary research \\directions are then\nexamined for their potential benefit to existential safety. Each research\ndirection is explained with a scenario-driven motivation, and examples of\nexisting work from which to build. The research directions present their own\nrisks and benefits to society that could occur at various scales of impact, and\nin particular are not guaranteed to benefit existential safety if major\ndevelopments in them are deployed without adequate forethought and oversight.\nAs such, each direction is accompanied by a consideration of potentially\nnegative side effects.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI", "cs.LG", "68T01", "I.2.0"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.07914": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.07914v3", "post_title": "Imitating Latent Policies from Observation", "authors": ["Ashley D. Edwards", "Himanshu Sahni", "Yannick Schroecker", "Charles L. Isbell"], "date_published": "2018-05-21 06:49:57+00:00", "data_last_modified": "2019-05-13 07:13:07+00:00", "url": "http://arxiv.org/abs/1805.07914v3", "abstract": "In this paper, we describe a novel approach to imitation learning that infers\nlatent policies directly from state observations. We introduce a method that\ncharacterizes the causal effects of latent actions on observations while\nsimultaneously predicting their likelihood. We then outline an action alignment\nprocedure that leverages a small amount of environment interactions to\ndetermine a mapping between the latent and real-world actions. We show that\nthis corrected labeling can be used for imitating the observed behavior, even\nthough no expert actions are given. We evaluate our approach within classic\ncontrol environments and a platform game and demonstrate that it performs\nbetter than standard approaches. Code for this work is available at\nhttps://github.com/ashedwards/ILPO.", "author_comment": "Accepted to ICML 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1606.05374": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1606.05374v1", "post_title": "Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction", "authors": ["Jacob Steinhardt", "Gregory Valiant", "Moses Charikar"], "date_published": "2016-06-16 21:45:14+00:00", "data_last_modified": "2016-06-16 21:45:14+00:00", "url": "http://arxiv.org/abs/1606.05374v1", "abstract": "We consider a crowdsourcing model in which $n$ workers are asked to rate the\nquality of $n$ items previously generated by other workers. An unknown set of\n$\\alpha n$ workers generate reliable ratings, while the remaining workers may\nbehave arbitrarily and possibly adversarially. The manager of the experiment\ncan also manually evaluate the quality of a small number of items, and wishes\nto curate together almost all of the high-quality items with at most an\n$\\epsilon$ fraction of low-quality items. Perhaps surprisingly, we show that\nthis is possible with an amount of work required of the manager, and each\nworker, that does not scale with $n$: the dataset can be curated with\n$\\tilde{O}\\Big(\\frac{1}{\\beta\\alpha^3\\epsilon^4}\\Big)$ ratings per worker, and\n$\\tilde{O}\\Big(\\frac{1}{\\beta\\epsilon^2}\\Big)$ ratings by the manager, where\n$\\beta$ is the fraction of high-quality items. Our results extend to the more\ngeneral setting of peer prediction, including peer grading in online\nclassrooms.", "author_comment": "18 pages", "journal_ref": null, "doi": null, "primary_category": "cs.HC", "categories": ["cs.HC", "cs.CR", "cs.DS", "cs.GT", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2004.03607": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2004.03607v2", "post_title": "TuringAdvice: A Generative and Dynamic Evaluation of Language Use", "authors": ["Rowan Zellers", "Ari Holtzman", "Elizabeth Clark", "Lianhui Qin", "Ali Farhadi", "Yejin Choi"], "date_published": "2020-04-07 18:00:03+00:00", "data_last_modified": "2021-04-13 01:05:17+00:00", "url": "http://arxiv.org/abs/2004.03607v2", "abstract": "We propose TuringAdvice, a new challenge task and dataset for language\nunderstanding models. Given a written situation that a real person is currently\nfacing, a model must generate helpful advice in natural language. Our\nevaluation framework tests a fundamental aspect of human language\nunderstanding: our ability to use language to resolve open-ended situations by\ncommunicating with each other.\n  Empirical results show that today's models struggle at TuringAdvice, even\nmultibillion parameter models finetuned on 600k in-domain training examples.\nThe best model, a finetuned T5, writes advice that is at least as helpful as\nhuman-written advice in only 14% of cases; a much larger non-finetunable GPT3\nmodel does even worse at 4%. This low performance reveals language\nunderstanding errors that are hard to spot outside of a generative setting,\nshowing much room for progress.", "author_comment": "NAACL 2021 camera ready. Project page at\n  https://rowanzellers.com/advice", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1711.05541": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1711.05541v5", "post_title": "Good and safe uses of AI Oracles", "authors": ["Stuart Armstrong", "Xavier O'Rorke"], "date_published": "2017-11-15 12:47:17+00:00", "data_last_modified": "2018-06-05 11:13:48+00:00", "url": "http://arxiv.org/abs/1711.05541v5", "abstract": "It is possible that powerful and potentially dangerous artificial\nintelligence (AI) might be developed in the future. An Oracle is a design which\naims to restrain the impact of a potentially dangerous AI by restricting the\nagent to no actions besides answering questions. Unfortunately, most Oracles\nwill be motivated to gain more control over the world by manipulating users\nthrough the content of their answers, and Oracles of potentially high\nintelligence might be very successful at this\n\\citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two\ndesigns for Oracles which, even under pessimistic assumptions, will not\nmanipulate their users into releasing them and yet will still be incentivised\nto provide their users with helpful answers. The first design is the\ncounterfactual Oracle -- which choses its answer as if it expected nobody to\never read it. The second design is the low-bandwidth Oracle -- which is limited\nby the quantity of information it can transmit.", "author_comment": "11 pages, 2 figures", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.04833": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.04833v4", "post_title": "Reward-rational (implicit) choice: A unifying formalism for reward learning", "authors": ["Hong Jun Jeon", "Smitha Milli", "Anca D. Dragan"], "date_published": "2020-02-12 08:07:49+00:00", "data_last_modified": "2020-12-11 17:56:03+00:00", "url": "http://arxiv.org/abs/2002.04833v4", "abstract": "It is often difficult to hand-specify what the correct reward function is for\na task, so researchers have instead aimed to learn reward functions from human\nbehavior or feedback. The types of behavior interpreted as evidence of the\nreward function have expanded greatly in recent years. We've gone from\ndemonstrations, to comparisons, to reading into the information leaked when the\nhuman is pushing the robot away or turning it off. And surely, there is more to\ncome. How will a robot make sense of all these diverse types of behavior? Our\nkey insight is that different types of behavior can be interpreted in a single\nunifying formalism - as a reward-rational choice that the human is making,\noften implicitly. The formalism offers both a unifying lens with which to view\npast work, as well as a recipe for interpreting new sources of information that\nare yet to be uncovered. We provide two examples to showcase this: interpreting\na new feedback type, and reading into how the choice of feedback itself leaks\ninformation about the reward.", "author_comment": "Published at NeurIPS 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.01567": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.01567v1", "post_title": "Model Primitive Hierarchical Lifelong Reinforcement Learning", "authors": ["Bohan Wu", "Jayesh K. Gupta", "Mykel J. Kochenderfer"], "date_published": "2019-03-04 22:14:23+00:00", "data_last_modified": "2019-03-04 22:14:23+00:00", "url": "http://arxiv.org/abs/1903.01567v1", "abstract": "Learning interpretable and transferable subpolicies and performing task\ndecomposition from a single, complex task is difficult. Some traditional\nhierarchical reinforcement learning techniques enforce this decomposition in a\ntop-down manner, while meta-learning techniques require a task distribution at\nhand to learn such decompositions. This paper presents a framework for using\ndiverse suboptimal world models to decompose complex task solutions into\nsimpler modular subpolicies. This framework performs automatic decomposition of\na single source task in a bottom up manner, concurrently learning the required\nmodular subpolicies as well as a controller to coordinate them. We perform a\nseries of experiments on high dimensional continuous action control tasks to\ndemonstrate the effectiveness of this approach at both complex single task\nlearning and lifelong learning. Finally, we perform ablation studies to\nunderstand the importance and robustness of different elements in the framework\nand limitations to this approach.", "author_comment": "9 pages, 10 figures. Accepted as a full paper at AAMAS 2019", "journal_ref": "International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2019)", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.00097": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.00097v1", "post_title": "Adversarial Attacks and Defences Competition", "authors": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio", "Yinpeng Dong", "Fangzhou Liao", "Ming Liang", "Tianyu Pang", "Jun Zhu", "Xiaolin Hu", "Cihang Xie", "Jianyu Wang", "Zhishuai Zhang", "Zhou Ren", "Alan Yuille", "Sangxia Huang", "Yao Zhao", "Yuzhe Zhao", "Zhonglin Han", "Junjiajia Long", "Yerkebulan Berdibekov", "Takuya Akiba", "Seiya Tokui", "Motoki Abe"], "date_published": "2018-03-31 00:52:20+00:00", "data_last_modified": "2018-03-31 00:52:20+00:00", "url": "http://arxiv.org/abs/1804.00097v1", "abstract": "To accelerate research on adversarial examples and robustness of machine\nlearning classifiers, Google Brain organized a NIPS 2017 competition that\nencouraged researchers to develop new methods to generate adversarial examples\nas well as to develop new ways to defend against them. In this chapter, we\ndescribe the structure and organization of the competition and the solutions\ndeveloped by several of the top-placing teams.", "author_comment": "36 pages, 10 figures", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.CR", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.14076": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.14076v3", "post_title": "RAFT: A Real-World Few-Shot Text Classification Benchmark", "authors": ["Neel Alex", "Eli Lifland", "Lewis Tunstall", "Abhishek Thakur", "Pegah Maham", "C. Jess Riedel", "Emmie Hine", "Carolyn Ashurst", "Paul Sedille", "Alexis Carlier", "Michael Noetel", "Andreas Stuhlm\u00fcller"], "date_published": "2021-09-28 22:35:31+00:00", "data_last_modified": "2022-01-18 21:40:14+00:00", "url": "http://arxiv.org/abs/2109.14076v3", "abstract": "Large pre-trained language models have shown promise for few-shot learning,\ncompleting text-based tasks given only a few task-specific examples. Will\nmodels soon solve classification tasks that have so far been reserved for human\nresearch assistants? Existing benchmarks are not designed to measure progress\nin applied settings, and so don't directly answer this question. The RAFT\nbenchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring\ntasks and uses an evaluation setup that mirrors deployment. Baseline\nevaluations on RAFT reveal areas current techniques struggle with: reasoning\nover long texts and tasks with many classes. Human baselines show that some\nclassification tasks are difficult for non-expert humans, reflecting that\nreal-world value sometimes depends on domain expertise. Yet even non-expert\nhuman baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets\nand leaderboard will track which model improvements translate into real-world\nbenefits at https://raft.elicit.org .", "author_comment": "Dataset, submission instructions, code and leaderboard available at\n  https://raft.elicit.org", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.09656": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.09656v2", "post_title": "Hierarchical visuomotor control of humanoids", "authors": ["Josh Merel", "Arun Ahuja", "Vu Pham", "Saran Tunyasuvunakool", "Siqi Liu", "Dhruva Tirumala", "Nicolas Heess", "Greg Wayne"], "date_published": "2018-11-23 19:55:55+00:00", "data_last_modified": "2019-01-15 17:28:36+00:00", "url": "http://arxiv.org/abs/1811.09656v2", "abstract": "We aim to build complex humanoid agents that integrate perception, motor\ncontrol, and memory. In this work, we partly factor this problem into low-level\nmotor control from proprioception and high-level coordination of the low-level\nskills informed by vision. We develop an architecture capable of surprisingly\nflexible, task-directed motor control of a relatively high-DoF humanoid body by\ncombining pre-training of low-level motor controllers with a high-level,\ntask-focused controller that switches among low-level sub-policies. The\nresulting system is able to control a physically-simulated humanoid body to\nsolve tasks that require coupling visual perception from an unstabilized\negocentric RGB camera during locomotion in the environment. For a supplementary\nvideo link, see https://youtu.be/7GISvfbykLE .", "author_comment": "Accepted as a conference paper at ICLR 2019", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2011.05623": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2011.05623v2", "post_title": "Adversarial images for the primate brain", "authors": ["Li Yuan", "Will Xiao", "Gabriel Kreiman", "Francis E. H. Tay", "Jiashi Feng", "Margaret S. Livingstone"], "date_published": "2020-11-11 08:30:54+00:00", "data_last_modified": "2021-12-21 13:13:58+00:00", "url": "http://arxiv.org/abs/2011.05623v2", "abstract": "Convolutional neural networks (CNNs) are vulnerable to adversarial attack,\nthe phenomenon that adding minuscule noise to an image can fool CNNs into\nmisclassifying it. Because this noise is nearly imperceptible to human viewers,\nbiological vision is assumed to be robust to adversarial attack. Despite this\napparent difference in robustness, CNNs are currently the best models of\nbiological vision, revealing a gap in explaining how the brain responds to\nadversarial images. Indeed, sensitivity to adversarial attack has not been\nmeasured for biological vision under normal conditions, nor have attack methods\nbeen specifically designed to affect biological vision. We studied the effects\nof adversarial attack on primate vision, measuring both monkey neuronal\nresponses and human behavior. Adversarial images were created by modifying\nimages from one category(such as human faces) to look like a target\ncategory(such as monkey faces), while limiting pixel value change. We tested\nthree attack directions via several attack methods, including directly using\nCNN adversarial images and using a CNN-based predictive model to guide monkey\nvisual neuron responses. We considered a wide range of image change magnitudes\nthat covered attack success rates up to>90%. We found that adversarial images\ndesigned for CNNs were ineffective in attacking primate vision. Even when\nconsidering the best attack method, primate vision was more robust to\nadversarial attack than an ensemble of CNNs, requiring over 100-fold larger\nimage change to attack successfully. The success of individual attack methods\nand images was correlated between monkey neurons and human behavior, but was\nless correlated between either and CNN categorization. Consistently, CNN-based\nmodels of neurons, when trained on natural images, did not generalize to\nexplain neuronal responses to adversarial images.", "author_comment": "These results reveal limits of CNN-based models of primate vision\n  through their differential response to adversarial attack, and provide clues\n  for building better models of the brain and more robust computer vision\n  algorithms", "journal_ref": null, "doi": null, "primary_category": "q-bio.NC", "categories": ["q-bio.NC", "cs.CV", "cs.NE", "eess.IV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1911.04266": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1911.04266v3", "post_title": "(When) Is Truth-telling Favored in AI Debate?", "authors": ["Vojt\u011bch Kova\u0159\u00edk", "Ryan Carey"], "date_published": "2019-11-11 13:49:43+00:00", "data_last_modified": "2021-03-16 15:42:42+00:00", "url": "http://arxiv.org/abs/1911.04266v3", "abstract": "For some problems, humans may not be able to accurately judge the goodness of\nAI-proposed solutions. Irving et al. (2018) propose that in such cases, we may\nuse a debate between two AI systems to amplify the problem-solving capabilities\nof a human judge. We introduce a mathematical framework that can model debates\nof this type and propose that the quality of debate designs should be measured\nby the accuracy of the most persuasive answer. We describe a simple instance of\nthe debate framework called feature debate and analyze the degree to which such\ndebates track the truth. We argue that despite being very simple, feature\ndebates nonetheless capture many aspects of practical debates such as the\nincentives to confuse the judge or stall to prevent losing. We then outline how\nthese models should be generalized to analyze a wider range of debate\nphenomena.", "author_comment": "In SafeAI Workshop at AAAI, 2019", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.GT"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.04255": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.04255v1", "post_title": "AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks", "authors": ["McKane Andrus", "Sarah Dean", "Thomas Krendl Gilbert", "Nathan Lambert", "Tom Zick"], "date_published": "2021-02-04 18:54:20+00:00", "data_last_modified": "2021-02-04 18:54:20+00:00", "url": "http://arxiv.org/abs/2102.04255v1", "abstract": "Despite interest in communicating ethical problems and social contexts within\nthe undergraduate curriculum to advance Public Interest Technology (PIT) goals,\ninterventions at the graduate level remain largely unexplored. This may be due\nto the conflicting ways through which distinct Artificial Intelligence (AI)\nresearch tracks conceive of their interface with social contexts. In this paper\nwe track the historical emergence of sociotechnical inquiry in three distinct\nsubfields of AI research: AI Safety, Fair Machine Learning (Fair ML) and\nHuman-in-the-Loop (HIL) Autonomy. We show that for each subfield, perceptions\nof PIT stem from the particular dangers faced by past integration of technical\nsystems within a normative social order. We further interrogate how these\nhistories dictate the response of each subfield to conceptual traps, as defined\nin the Science and Technology Studies literature. Finally, through a\ncomparative analysis of these currently siloed fields, we present a roadmap for\na unified approach to sociotechnical graduate pedagogy in AI.", "author_comment": "8 Pages", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.07805": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.07805v3", "post_title": "Constrained Policy Improvement for Safe and Efficient Reinforcement Learning", "authors": ["Elad Sarafian", "Aviv Tamar", "Sarit Kraus"], "date_published": "2018-05-20 17:47:03+00:00", "data_last_modified": "2019-07-10 20:12:07+00:00", "url": "http://arxiv.org/abs/1805.07805v3", "abstract": "We propose a policy improvement algorithm for Reinforcement Learning (RL)\nwhich is called Rerouted Behavior Improvement (RBI). RBI is designed to take\ninto account the evaluation errors of the Q-function. Such errors are common in\nRL when learning the $Q$-value from finite past experience data. Greedy\npolicies or even constrained policy optimization algorithms which ignore these\nerrors may suffer from an improvement penalty (i.e. a negative policy\nimprovement). To minimize the improvement penalty, the RBI idea is to attenuate\nrapid policy changes of low probability actions which were less frequently\nsampled. This approach is shown to avoid catastrophic performance degradation\nand reduce regret when learning from a batch of past experience. Through a\ntwo-armed bandit with Gaussian distributed rewards example, we show that it\nalso increases data efficiency when the optimal action has a high variance. We\nevaluate RBI in two tasks in the Atari Learning Environment: (1) learning from\nobservations of multiple behavior policies and (2) iterative RL. Our results\ndemonstrate the advantage of RBI over greedy policies and other constrained\npolicy optimization algorithms as a safe learning approach and as a general\ndata efficient learning algorithm. An anonymous Github repository of our RBI\nimplementation is found at https://github.com/eladsar/rbi.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.09936": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.09936v1", "post_title": "Multi-Agent Generative Adversarial Imitation Learning", "authors": ["Jiaming Song", "Hongyu Ren", "Dorsa Sadigh", "Stefano Ermon"], "date_published": "2018-07-26 03:21:49+00:00", "data_last_modified": "2018-07-26 03:21:49+00:00", "url": "http://arxiv.org/abs/1807.09936v1", "abstract": "Imitation learning algorithms can be used to learn a policy from expert\ndemonstrations without access to a reward signal. However, most existing\napproaches are not applicable in multi-agent settings due to the existence of\nmultiple (Nash) equilibria and non-stationary environments. We propose a new\nframework for multi-agent imitation learning for general Markov games, where we\nbuild upon a generalized notion of inverse reinforcement learning. We further\nintroduce a practical multi-agent actor-critic algorithm with good empirical\nperformance. Our method can be used to imitate complex behaviors in\nhigh-dimensional environments with multiple cooperative or competing agents.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.MA", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.03046": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.03046v1", "post_title": "Learning a Behavioral Repertoire from Demonstrations", "authors": ["Niels Justesen", "Miguel Gonzalez Duque", "Daniel Cabarcas Jaramillo", "Jean-Baptiste Mouret", "Sebastian Risi"], "date_published": "2019-07-05 23:08:08+00:00", "data_last_modified": "2019-07-05 23:08:08+00:00", "url": "http://arxiv.org/abs/1907.03046v1", "abstract": "Imitation Learning (IL) is a machine learning approach to learn a policy from\na dataset of demonstrations. IL can be useful to kick-start learning before\napplying reinforcement learning (RL) but it can also be useful on its own, e.g.\nto learn to imitate human players in video games. However, a major limitation\nof current IL approaches is that they learn only a single \"average\" policy\nbased on a dataset that possibly contains demonstrations of numerous different\ntypes of behaviors. In this paper, we propose a new approach called Behavioral\nRepertoire Imitation Learning (BRIL) that instead learns a repertoire of\nbehaviors from a set of demonstrations by augmenting the state-action pairs\nwith behavioral descriptions. The outcome of this approach is a single neural\nnetwork policy conditioned on a behavior description that can be precisely\nmodulated. We apply this approach to train a policy on 7,777 human replays to\nperform build-order planning in StarCraft II. Principal Component Analysis\n(PCA) is applied to construct a low-dimensional behavioral space from the\nhigh-dimensional army unit composition of each demonstration. The results\ndemonstrate that the learned policy can be effectively manipulated to express\ndistinct behaviors. Additionally, by applying the UCB1 algorithm, we are able\nto adapt the behavior of the policy - in-between games - to reach a performance\nbeyond that of the traditional IL baseline approach.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.01014": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.01014v2", "post_title": "Bayesian Policy Optimization for Model Uncertainty", "authors": ["Gilwoo Lee", "Brian Hou", "Aditya Mandalika", "Jeongseok Lee", "Sanjiban Choudhury", "Siddhartha S. Srinivasa"], "date_published": "2018-10-01 23:39:25+00:00", "data_last_modified": "2019-05-08 15:04:45+00:00", "url": "http://arxiv.org/abs/1810.01014v2", "abstract": "Addressing uncertainty is critical for autonomous systems to robustly adapt\nto the real world. We formulate the problem of model uncertainty as a\ncontinuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent\nmaintains a posterior distribution over latent model parameters given a history\nof observations and maximizes its expected long-term reward with respect to\nthis belief distribution. Our algorithm, Bayesian Policy Optimization, builds\non recent policy optimization algorithms to learn a universal policy that\nnavigates the exploration-exploitation trade-off to maximize the Bayesian value\nfunction. To address challenges from discretizing the continuous latent\nparameter space, we propose a new policy network architecture that encodes the\nbelief distribution independently from the observable state. Our method\nsignificantly outperforms algorithms that address model uncertainty without\nexplicitly reasoning about belief distributions and is competitive with\nstate-of-the-art Partially Observable Markov Decision Process solvers.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.06922": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.06922v1", "post_title": "On Variational Bounds of Mutual Information", "authors": ["Ben Poole", "Sherjil Ozair", "Aaron van den Oord", "Alexander A. Alemi", "George Tucker"], "date_published": "2019-05-16 17:31:53+00:00", "data_last_modified": "2019-05-16 17:31:53+00:00", "url": "http://arxiv.org/abs/1905.06922v1", "abstract": "Estimating and optimizing Mutual Information (MI) is core to many problems in\nmachine learning; however, bounding MI in high dimensions is challenging. To\nestablish tractable and scalable objectives, recent work has turned to\nvariational bounds parameterized by neural networks, but the relationships and\ntradeoffs between these bounds remains unclear. In this work, we unify these\nrecent developments in a single framework. We find that the existing\nvariational lower bounds degrade when the MI is large, exhibiting either high\nbias or high variance. To address this problem, we introduce a continuum of\nlower bounds that encompasses previous bounds and flexibly trades off bias and\nvariance. On high-dimensional, controlled problems, we empirically characterize\nthe bias and variance of the bounds and their gradients and demonstrate the\neffectiveness of our new bounds for estimation and representation learning.", "author_comment": "ICML 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.05671": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.05671v2", "post_title": "AI safety: state of the field through quantitative lens", "authors": ["Mislav Juric", "Agneza Sandic", "Mario Brcic"], "date_published": "2020-02-12 11:26:44+00:00", "data_last_modified": "2020-07-09 11:32:14+00:00", "url": "http://arxiv.org/abs/2002.05671v2", "abstract": "Last decade has seen major improvements in the performance of artificial\nintelligence which has driven wide-spread applications. Unforeseen effects of\nsuch mass-adoption has put the notion of AI safety into the public eye. AI\nsafety is a relatively new field of research focused on techniques for building\nAI beneficial for humans. While there exist survey papers for the field of AI\nsafety, there is a lack of a quantitative look at the research being conducted.\nThe quantitative aspect gives a data-driven insight about the emerging trends,\nknowledge gaps and potential areas for future research. In this paper,\nbibliometric analysis of the literature finds significant increase in research\nactivity since 2015. Also, the field is so new that most of the technical\nissues are open, including: explainability with its long-term utility, and\nvalue alignment which we have identified as the most important long-term\nresearch topic. Equally, there is a severe lack of research into concrete\npolicies regarding AI. As we expect AI to be the one of the main driving forces\nof changes in society, AI safety is the field under which we need to decide the\ndirection of humanity's future.", "author_comment": "2020 43rd International Convention on Information and Communication\n  Technology, Electronics and Microelectronics (MIPRO)", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2003.07305": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2003.07305v1", "post_title": "DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction", "authors": ["Aviral Kumar", "Abhishek Gupta", "Sergey Levine"], "date_published": "2020-03-16 16:18:52+00:00", "data_last_modified": "2020-03-16 16:18:52+00:00", "url": "http://arxiv.org/abs/2003.07305v1", "abstract": "Deep reinforcement learning can learn effective policies for a wide range of\ntasks, but is notoriously difficult to use due to instability and sensitivity\nto hyperparameters. The reasons for this remain unclear. When using standard\nsupervised methods (e.g., for bandits), on-policy data collection provides\n\"hard negatives\" that correct the model in precisely those states and actions\nthat the policy is likely to visit. We call this phenomenon \"corrective\nfeedback.\" We show that bootstrapping-based Q-learning algorithms do not\nnecessarily benefit from this corrective feedback, and training on the\nexperience collected by the algorithm is not sufficient to correct errors in\nthe Q-function. In fact, Q-learning and related methods can exhibit\npathological interactions between the distribution of experience collected by\nthe agent and the policy induced by training on that experience, leading to\npotential instability, sub-optimal convergence, and poor results when learning\nfrom noisy, sparse or delayed rewards. We demonstrate the existence of this\nproblem, both theoretically and empirically. We then show that a specific\ncorrection to the data distribution can mitigate this issue. Based on these\nobservations, we propose a new algorithm, DisCor, which computes an\napproximation to this optimal distribution and uses it to re-weight the\ntransitions used for training, resulting in substantial improvements in a range\nof challenging RL settings, such as multi-task learning and learning from noisy\nreward signals. Blog post presenting a summary of this work is available at:\nhttps://bair.berkeley.edu/blog/2020/03/16/discor/.", "author_comment": "Pre-print", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.14701": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.14701v2", "post_title": "Scaling Laws for Autoregressive Generative Modeling", "authors": ["Tom Henighan", "Jared Kaplan", "Mor Katz", "Mark Chen", "Christopher Hesse", "Jacob Jackson", "Heewoo Jun", "Tom B. Brown", "Prafulla Dhariwal", "Scott Gray", "Chris Hallacy", "Benjamin Mann", "Alec Radford", "Aditya Ramesh", "Nick Ryder", "Daniel M. Ziegler", "John Schulman", "Dario Amodei", "Sam McCandlish"], "date_published": "2020-10-28 02:17:24+00:00", "data_last_modified": "2020-11-06 04:16:36+00:00", "url": "http://arxiv.org/abs/2010.14701v2", "abstract": "We identify empirical scaling laws for the cross-entropy loss in four\ndomains: generative image modeling, video modeling, multimodal\nimage$\\leftrightarrow$text models, and mathematical problem solving. In all\ncases autoregressive Transformers smoothly improve in performance as model size\nand compute budgets increase, following a power-law plus constant scaling law.\nThe optimal model size also depends on the compute budget through a power-law,\nwith exponents that are nearly universal across all data domains.\n  The cross-entropy loss has an information theoretic interpretation as\n$S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws\nsuggest a prediction for both the true data distribution's entropy and the KL\ndivergence between the true and model distributions. With this interpretation,\nbillion-parameter Transformers are nearly perfect models of the YFCC100M image\ndistribution downsampled to an $8\\times 8$ resolution, and we can forecast the\nmodel size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in\nnats/image for other resolutions.\n  We find a number of additional scaling laws in specific domains: (a) we\nidentify a scaling relation for the mutual information between captions and\nimages in multimodal models, and show how to answer the question \"Is a picture\nworth a thousand words?\"; (b) in the case of mathematical problem solving, we\nidentify scaling laws for model performance when extrapolating beyond the\ntraining distribution; (c) we finetune generative image models for ImageNet\nclassification and find smooth scaling of the classification loss and error\nrate, even as the generative loss levels off. Taken together, these results\nstrengthen the case that scaling laws have important implications for neural\nnetwork performance, including on downstream tasks.", "author_comment": "20+17 pages, 33 figures; added appendix with additional language\n  results", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CL", "cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.05917": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.05917v1", "post_title": "Heuristic Approaches for Goal Recognition in Incomplete Domain Models", "authors": ["Ramon Fraga Pereira", "Felipe Meneguzzi"], "date_published": "2018-04-16 20:00:41+00:00", "data_last_modified": "2018-04-16 20:00:41+00:00", "url": "http://arxiv.org/abs/1804.05917v1", "abstract": "Recent approaches to goal recognition have progressively relaxed the\nassumptions about the amount and correctness of domain knowledge and available\nobservations, yielding accurate and efficient algorithms. These approaches,\nhowever, assume completeness and correctness of the domain theory against which\ntheir algorithms match observations: this is too strong for most real-world\ndomains. In this paper, we develop goal recognition techniques that are capable\nof recognizing goals using \\textit{incomplete} (and possibly incorrect) domain\ntheories. We show the efficiency and accuracy of our approaches empirically\nagainst a large dataset of goal and plan recognition problems with incomplete\ndomains.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.10187": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.10187v3", "post_title": "Learning to Interactively Learn and Assist", "authors": ["Mark Woodward", "Chelsea Finn", "Karol Hausman"], "date_published": "2019-06-24 19:23:27+00:00", "data_last_modified": "2019-11-19 21:04:13+00:00", "url": "http://arxiv.org/abs/1906.10187v3", "abstract": "When deploying autonomous agents in the real world, we need effective ways of\ncommunicating objectives to them. Traditional skill learning has revolved\naround reinforcement and imitation learning, each with rigid constraints on the\nformat of information exchanged between the human and the agent. While scalar\nrewards carry little information, demonstrations require significant effort to\nprovide and may carry more information than is necessary. Furthermore, rewards\nand demonstrations are often defined and collected before training begins, when\nthe human is most uncertain about what information would help the agent. In\ncontrast, when humans communicate objectives with each other, they make use of\na large vocabulary of informative behaviors, including non-verbal\ncommunication, and often communicate throughout learning, responding to\nobserved behavior. In this way, humans communicate intent with minimal effort.\nIn this paper, we propose such interactive learning as an alternative to reward\nor demonstration-driven learning. To accomplish this, we introduce a\nmulti-agent training framework that enables an agent to learn from another\nagent who knows the current task. Through a series of experiments, we\ndemonstrate the emergence of a variety of interactive learning behaviors,\nincluding information-sharing, information-seeking, and question-answering.\nMost importantly, we find that our approach produces an agent that is capable\nof learning interactively from a human user, without a set of explicit\ndemonstrations or a reward function, and achieving significantly better\nperformance cooperatively with a human than a human performing the task alone.", "author_comment": "AAAI 2020. Video overview at https://youtu.be/8yBvDBuAPrw, paper\n  website with videos and interactive game at\n  http://interactive-learning.github.io/", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1908.04734": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1908.04734v5", "post_title": "Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective", "authors": ["Tom Everitt", "Marcus Hutter", "Ramana Kumar", "Victoria Krakovna"], "date_published": "2019-08-13 16:50:00+00:00", "data_last_modified": "2021-03-26 11:13:59+00:00", "url": "http://arxiv.org/abs/1908.04734v5", "abstract": "Can humans get arbitrarily capable reinforcement learning (RL) agents to do\ntheir bidding? Or will sufficiently capable RL agents always find ways to\nbypass their intended objectives by shortcutting their reward signal? This\nquestion impacts how far RL can be scaled, and whether alternative paradigms\nmust be developed in order to build safe artificial general intelligence. In\nthis paper, we study when an RL agent has an instrumental goal to tamper with\nits reward process, and describe design principles that prevent instrumental\ngoals for two different types of reward tampering (reward function tampering\nand RF-input tampering). Combined, the design principles can prevent both types\nof reward tampering from being instrumental goals. The analysis benefits from\ncausal influence diagrams to provide intuitive yet precise formalizations.", "author_comment": "Accepted to Synthese, March 2021", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2003.04881": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2003.04881v4", "post_title": "Pruned Neural Networks are Surprisingly Modular", "authors": ["Daniel Filan", "Shlomi Hod", "Cody Wild", "Andrew Critch", "Stuart Russell"], "date_published": "2020-03-10 17:51:33+00:00", "data_last_modified": "2020-08-11 21:50:05+00:00", "url": "http://arxiv.org/abs/2003.04881v4", "abstract": "The learned weights of a neural network are often considered devoid of\nscrutable internal structure. To discern structure in these weights, we\nintroduce a measurable notion of modularity for multi-layer perceptrons (MLPs),\nand investigate the modular structure of MLPs trained on datasets of small\nimages. Our notion of modularity comes from the graph clustering literature: a\n\"module\" is a set of neurons with strong internal connectivity but weak\nexternal connectivity. We find that training and weight pruning produces MLPs\nthat are more modular than randomly initialized ones, and often significantly\nmore modular than random MLPs with the same (sparse) distribution of weights.\nInterestingly, they are much more modular when trained with dropout. We also\npresent exploratory analyses of the importance of different modules for\nperformance and how modules depend on each other. Understanding the modular\nstructure of neural networks, when such structure exists, will hopefully render\ntheir inner workings more interpretable to engineers.", "author_comment": "25 pages, 12 figures", "journal_ref": null, "doi": null, "primary_category": "cs.NE", "categories": ["cs.NE", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.06521": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.06521v1", "post_title": "Reward learning from human preferences and demonstrations in Atari", "authors": ["Borja Ibarz", "Jan Leike", "Tobias Pohlen", "Geoffrey Irving", "Shane Legg", "Dario Amodei"], "date_published": "2018-11-15 18:33:43+00:00", "data_last_modified": "2018-11-15 18:33:43+00:00", "url": "http://arxiv.org/abs/1811.06521v1", "abstract": "To solve complex real-world problems with reinforcement learning, we cannot\nrely on manually specified reward functions. Instead, we can have humans\ncommunicate an objective to the agent directly. In this work, we combine two\napproaches to learning from human feedback: expert demonstrations and\ntrajectory preferences. We train a deep neural network to model the reward\nfunction and use its predicted reward to train an DQN-based deep reinforcement\nlearning agent on 9 Atari games. Our approach beats the imitation learning\nbaseline in 7 games and achieves strictly superhuman performance on 2 games\nwithout using game rewards. Additionally, we investigate the goodness of fit of\nthe reward model, present some reward hacking problems, and study the effects\nof noise in the human labels.", "author_comment": "NIPS 2018", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.07861": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.07861v1", "post_title": "Perceptual Values from Observation", "authors": ["Ashley D. Edwards", "Charles L. Isbell"], "date_published": "2019-05-20 03:59:44+00:00", "data_last_modified": "2019-05-20 03:59:44+00:00", "url": "http://arxiv.org/abs/1905.07861v1", "abstract": "Imitation by observation is an approach for learning from expert\ndemonstrations that lack action information, such as videos. Recent approaches\nto this problem can be placed into two broad categories: training dynamics\nmodels that aim to predict the actions taken between states, and learning\nrewards or features for computing them for Reinforcement Learning (RL). In this\npaper, we introduce a novel approach that learns values, rather than rewards,\ndirectly from observations. We show that by using values, we can significantly\nspeed up RL by removing the need to bootstrap action-values, as compared to\nsparse-reward specifications.", "author_comment": "Accepted into the Workshop on Self-Supervised Learning at ICML 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2110.10819": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2110.10819v1", "post_title": "Shaking the foundations: delusions in sequence models for interaction and control", "authors": ["Pedro A. Ortega", "Markus Kunesch", "Gr\u00e9goire Del\u00e9tang", "Tim Genewein", "Jordi Grau-Moya", "Joel Veness", "Jonas Buchli", "Jonas Degrave", "Bilal Piot", "Julien Perolat", "Tom Everitt", "Corentin Tallec", "Emilio Parisotto", "Tom Erez", "Yutian Chen", "Scott Reed", "Marcus Hutter", "Nando de Freitas", "Shane Legg"], "date_published": "2021-10-20 23:31:05+00:00", "data_last_modified": "2021-10-20 23:31:05+00:00", "url": "http://arxiv.org/abs/2110.10819v1", "abstract": "The recent phenomenal success of language models has reinvigorated machine\nlearning research, and large sequence models such as transformers are being\napplied to a variety of domains. One important problem class that has remained\nrelatively elusive however is purposeful adaptive behavior. Currently there is\na common perception that sequence models \"lack the understanding of the cause\nand effect of their actions\" leading them to draw incorrect inferences due to\nauto-suggestive delusions. In this report we explain where this mismatch\noriginates, and show that it can be resolved by treating actions as causal\ninterventions. Finally, we show that in supervised learning, one can teach a\nsystem to condition or intervene on data by training with factual and\ncounterfactual error signals respectively.", "author_comment": "DeepMind Tech Report, 16 pages, 4 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.10394": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.10394v2", "post_title": "Uncertain Decisions Facilitate Better Preference Learning", "authors": ["Cassidy Laidlaw", "Stuart Russell"], "date_published": "2021-06-19 00:11:13+00:00", "data_last_modified": "2021-10-28 14:59:44+00:00", "url": "http://arxiv.org/abs/2106.10394v2", "abstract": "Existing observational approaches for learning human preferences, such as\ninverse reinforcement learning, usually make strong assumptions about the\nobservability of the human's environment. However, in reality, people make many\nimportant decisions under uncertainty. To better understand preference learning\nin these cases, we study the setting of inverse decision theory (IDT), a\npreviously proposed framework where a human is observed making non-sequential\nbinary decisions under uncertainty. In IDT, the human's preferences are\nconveyed through their loss function, which expresses a tradeoff between\ndifferent types of mistakes. We give the first statistical analysis of IDT,\nproviding conditions necessary to identify these preferences and characterizing\nthe sample complexity -- the number of decisions that must be observed to learn\nthe tradeoff the human is making to a desired precision. Interestingly, we show\nthat it is actually easier to identify preferences when the decision problem is\nmore uncertain. Furthermore, uncertain decision problems allow us to relax the\nunrealistic assumption that the human is an optimal decision maker but still\nidentify their exact preferences; we give sample complexities in this\nsuboptimal case as well. Our analysis contradicts the intuition that partial\nobservability should make preference learning more difficult. It also provides\na first step towards understanding and improving preference learning methods\nfor uncertain and suboptimal humans.", "author_comment": "Accepted at NeurIPS 2021 (Spotlight)", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1707.08747": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1707.08747v1", "post_title": "A Formal Approach to the Problem of Logical Non-Omniscience", "authors": ["Scott Garrabrant", "Tsvi Benson-Tilsen", "Andrew Critch", "Nate Soares", "Jessica Taylor"], "date_published": "2017-07-27 07:49:01+00:00", "data_last_modified": "2017-07-27 07:49:01+00:00", "url": "http://arxiv.org/abs/1707.08747v1", "abstract": "We present the logical induction criterion for computable algorithms that\nassign probabilities to every logical statement in a given formal language, and\nrefine those probabilities over time. The criterion is motivated by a series of\nstock trading analogies. Roughly speaking, each logical sentence phi is\nassociated with a stock that is worth $1 per share if phi is true and nothing\notherwise, and we interpret the belief-state of a logically uncertain reasoner\nas a set of market prices, where pt_N(phi)=50% means that on day N, shares of\nphi may be bought or sold from the reasoner for 50%. A market is then called a\nlogical inductor if (very roughly) there is no polynomial-time computable\ntrading strategy with finite risk tolerance that earns unbounded profits in\nthat market over time. We then describe how this single criterion implies a\nnumber of desirable properties of bounded reasoners; for example, logical\ninductors outpace their underlying deductive process, perform universal\nempirical induction given enough time to think, and place strong trust in their\nown reasoning process.", "author_comment": "In Proceedings TARK 2017, arXiv:1707.08250", "journal_ref": "EPTCS 251, 2017, pp. 221-235", "doi": "10.4204/EPTCS.251.16", "primary_category": "cs.LO", "categories": ["cs.LO", "F.4.0; G.3"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1904.11455": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1904.11455v1", "post_title": "Ray Interference: a Source of Plateaus in Deep Reinforcement Learning", "authors": ["Tom Schaul", "Diana Borsa", "Joseph Modayil", "Razvan Pascanu"], "date_published": "2019-04-25 16:54:02+00:00", "data_last_modified": "2019-04-25 16:54:02+00:00", "url": "http://arxiv.org/abs/1904.11455v1", "abstract": "Rather than proposing a new method, this paper investigates an issue present\nin existing learning algorithms. We study the learning dynamics of\nreinforcement learning (RL), specifically a characteristic coupling between\nlearning and data generation that arises because RL agents control their future\ndata distribution. In the presence of function approximation, this coupling can\nlead to a problematic type of 'ray interference', characterized by learning\ndynamics that sequentially traverse a number of performance plateaus,\neffectively constraining the agent to learn one thing at a time even when\nlearning in parallel is better. We establish the conditions under which ray\ninterference occurs, show its relation to saddle points and obtain the exact\nlearning dynamics in a restricted setting. We characterize a number of its\nproperties and discuss possible remedies.", "author_comment": "Full version of RLDM abstract", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2107.04953": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2107.04953v1", "post_title": "Designing Recommender Systems to Depolarize", "authors": ["Jonathan Stray"], "date_published": "2021-07-11 03:23:42+00:00", "data_last_modified": "2021-07-11 03:23:42+00:00", "url": "http://arxiv.org/abs/2107.04953v1", "abstract": "Polarization is implicated in the erosion of democracy and the progression to\nviolence, which makes the polarization properties of large algorithmic content\nselection systems (recommender systems) a matter of concern for peace and\nsecurity. While algorithm-driven social media does not seem to be a primary\ndriver of polarization at the country level, it could be a useful intervention\npoint in polarized societies. This paper examines algorithmic depolarization\ninterventions with the goal of conflict transformation: not suppressing or\neliminating conflict but moving towards more constructive conflict. Algorithmic\nintervention is considered at three stages: which content is available\n(moderation), how content is selected and personalized (ranking), and content\npresentation and controls (user interface). Empirical studies of online\nconflict suggest that the exposure diversity intervention proposed as an\nantidote to \"filter bubbles\" can be improved and can even worsen polarization\nunder some conditions. Using civility metrics in conjunction with diversity in\ncontent selection may be more effective. However, diversity-based interventions\nhave not been tested at scale and may not work in the diverse and dynamic\ncontexts of real platforms. Instead, intervening in platform polarization\ndynamics will likely require continuous monitoring of polarization metrics,\nsuch as the widely used \"feeling thermometer.\" These metrics can be used to\nevaluate product features, and potentially engineered as algorithmic\nobjectives. It may further prove necessary to include polarization measures in\nthe objective functions of recommender algorithms to prevent optimization\nprocesses from creating conflict as a side effect.", "author_comment": "to appear in First Monday, September 2021", "journal_ref": null, "doi": null, "primary_category": "cs.IR", "categories": ["cs.IR", "cs.CY", "cs.SI", "J.4; K.4.2"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.13900": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.13900v3", "post_title": "Quantifying Differences in Reward Functions", "authors": ["Adam Gleave", "Michael Dennis", "Shane Legg", "Stuart Russell", "Jan Leike"], "date_published": "2020-06-24 17:35:15+00:00", "data_last_modified": "2021-03-17 21:54:55+00:00", "url": "http://arxiv.org/abs/2006.13900v3", "abstract": "For many tasks, the reward function is inaccessible to introspection or too\ncomplex to be specified procedurally, and must instead be learned from user\ndata. Prior work has evaluated learned reward functions by evaluating policies\noptimized for the learned reward. However, this method cannot distinguish\nbetween the learned reward function failing to reflect user preferences and the\npolicy optimization process failing to optimize the learned reward. Moreover,\nthis method can only tell us about behavior in the evaluation environment, but\nthe reward may incentivize very different behavior in even a slightly different\ndeployment environment. To address these problems, we introduce the\nEquivalent-Policy Invariant Comparison (EPIC) distance to quantify the\ndifference between two reward functions directly, without a policy optimization\nstep. We prove EPIC is invariant on an equivalence class of reward functions\nthat always induce the same optimal policy. Furthermore, we find EPIC can be\nefficiently approximated and is more robust than baselines to the choice of\ncoverage distribution. Finally, we show that EPIC distance bounds the regret of\noptimal policies even under different transition dynamics, and we confirm\nempirically that it predicts policy training success. Our source code is\navailable at https://github.com/HumanCompatibleAI/evaluating-rewards.", "author_comment": "Published at ICLR 2021. 9 pages main paper, 42 pages total", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2004.14990": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2004.14990v5", "post_title": "Reinforcement Learning with Augmented Data", "authors": ["Michael Laskin", "Kimin Lee", "Adam Stooke", "Lerrel Pinto", "Pieter Abbeel", "Aravind Srinivas"], "date_published": "2020-04-30 17:35:32+00:00", "data_last_modified": "2020-11-05 06:04:50+00:00", "url": "http://arxiv.org/abs/2004.14990v5", "abstract": "Learning from visual observations is a fundamental yet challenging problem in\nReinforcement Learning (RL). Although algorithmic advances combined with\nconvolutional neural networks have proved to be a recipe for success, current\nmethods are still lacking on two fronts: (a) data-efficiency of learning and\n(b) generalization to new environments. To this end, we present Reinforcement\nLearning with Augmented Data (RAD), a simple plug-and-play module that can\nenhance most RL algorithms. We perform the first extensive study of general\ndata augmentations for RL on both pixel-based and state-based inputs, and\nintroduce two new data augmentations - random translate and random amplitude\nscale. We show that augmentations such as random translate, crop, color jitter,\npatch cutout, random convolutions, and amplitude scale can enable simple RL\nalgorithms to outperform complex state-of-the-art methods across common\nbenchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and\nfinal performance on the DeepMind Control Suite benchmark for pixel-based\ncontrol as well as OpenAI Gym benchmark for state-based control. We further\ndemonstrate that RAD significantly improves test-time generalization over\nexisting methods on several OpenAI ProcGen benchmarks. Our RAD module and\ntraining code are available at https://www.github.com/MishaLaskin/rad.", "author_comment": "NeurIPS 2020 camera-ready version. First two authors contributed\n  equally, website: https://mishalaskin.github.io/rad code:\n  https://github.com/MishaLaskin/rad and\n  https://github.com/pokaxpoka/rad_procgen", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.07612": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.07612v6", "post_title": "Generating Multi-Agent Trajectories using Programmatic Weak Supervision", "authors": ["Eric Zhan", "Stephan Zheng", "Yisong Yue", "Long Sha", "Patrick Lucey"], "date_published": "2018-03-20 19:19:13+00:00", "data_last_modified": "2019-02-22 05:40:13+00:00", "url": "http://arxiv.org/abs/1803.07612v6", "abstract": "We study the problem of training sequential generative models for capturing\ncoordinated multi-agent trajectory behavior, such as offensive basketball\ngameplay. When modeling such settings, it is often beneficial to design\nhierarchical models that can capture long-term coordination using intermediate\nvariables. Furthermore, these intermediate variables should capture interesting\nhigh-level behavioral semantics in an interpretable and manipulatable way. We\npresent a hierarchical framework that can effectively learn such sequential\ngenerative models. Our approach is inspired by recent work on leveraging\nprogrammatically produced weak labels, which we extend to the spatiotemporal\nregime. In addition to synthetic settings, we show how to instantiate our\nframework to effectively model complex interactions between basketball players\nand generate realistic multi-agent trajectories of basketball gameplay over\nlong time periods. We validate our approach using both quantitative and\nqualitative evaluations, including a user study comparison conducted with\nprofessional sports analysts.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.01685": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.01685v2", "post_title": "Agent Incentives: A Causal Perspective", "authors": ["Tom Everitt", "Ryan Carey", "Eric Langlois", "Pedro A Ortega", "Shane Legg"], "date_published": "2021-02-02 18:52:41+00:00", "data_last_modified": "2021-03-15 20:08:39+00:00", "url": "http://arxiv.org/abs/2102.01685v2", "abstract": "We present a framework for analysing agent incentives using causal influence\ndiagrams. We establish that a well-known criterion for value of information is\ncomplete. We propose a new graphical criterion for value of control,\nestablishing its soundness and completeness. We also introduce two new concepts\nfor incentive analysis: response incentives indicate which changes in the\nenvironment affect an optimal decision, while instrumental control incentives\nestablish whether an agent can influence its utility via a variable X. For both\nnew concepts, we provide sound and complete graphical criteria. We show by\nexample how these results can help with evaluating the safety and fairness of\nan AI system.", "author_comment": "In Proceedings of the AAAI 2021 Conference. Supersedes\n  arXiv:1902.09980, arXiv:2001.07118", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.02546": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.02546v2", "post_title": "A Model for General Intelligence", "authors": ["Paul Yaworsky"], "date_published": "2018-11-06 18:37:04+00:00", "data_last_modified": "2018-11-14 20:21:25+00:00", "url": "http://arxiv.org/abs/1811.02546v2", "abstract": "The overarching problem in artificial intelligence (AI) is that we do not\nunderstand the intelligence process well enough to enable the development of\nadequate computational models. Much work has been done in AI over the years at\nlower levels, but a big part of what has been missing involves the high level,\nabstract, general nature of intelligence. We address this gap by developing a\nmodel for general intelligence. To accomplish this, we focus on three basic\naspects of intelligence. First, we must realize the general order and nature of\nintelligence at a high level. Second, we must come to know what these\nrealizations mean with respect to the overall intelligence process. Third, we\nmust describe these realizations as clearly as possible. We propose a\nhierarchical model to help capture and exploit the order within intelligence.\nThe underlying order involves patterns of signals that become organized, stored\nand activated in space and time. These patterns can be described using a\nsimple, general hierarchy, with physical signals at the lowest level,\ninformation in the middle, and abstract signal representations at the top. This\nhigh level perspective provides a big picture that literally helps us see the\nintelligence process, thereby enabling fundamental realizations, a better\nunderstanding and clear descriptions of the intelligence process. The resulting\nmodel can be used to support all kinds of information processing across\nmultiple levels of abstraction. As computer technology improves, and as\ncooperation increases between humans and computers, people will become more\nefficient and more productive in performing their information processing tasks.", "author_comment": "7 pages; distribution statement added", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2011.08820": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2011.08820v1", "post_title": "REALab: An Embedded Perspective on Tampering", "authors": ["Ramana Kumar", "Jonathan Uesato", "Richard Ngo", "Tom Everitt", "Victoria Krakovna", "Shane Legg"], "date_published": "2020-11-17 18:37:20+00:00", "data_last_modified": "2020-11-17 18:37:20+00:00", "url": "http://arxiv.org/abs/2011.08820v1", "abstract": "This paper describes REALab, a platform for embedded agency research in\nreinforcement learning (RL). REALab is designed to model the structure of\ntampering problems that may arise in real-world deployments of RL. Standard\nMarkov Decision Process (MDP) formulations of RL and simulated environments\nmirroring the MDP structure assume secure access to feedback (e.g., rewards).\nThis may be unrealistic in settings where agents are embedded and can corrupt\nthe processes producing feedback (e.g., human supervisors, or an implemented\nreward function). We describe an alternative Corrupt Feedback MDP formulation\nand the REALab environment platform, which both avoid the secure feedback\nassumption. We hope the design of REALab provides a useful perspective on\ntampering problems, and that the platform may serve as a unit test for the\npresence of tampering incentives in RL agent designs.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.05960": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.05960v2", "post_title": "Planning to Explore via Self-Supervised World Models", "authors": ["Ramanan Sekar", "Oleh Rybkin", "Kostas Daniilidis", "Pieter Abbeel", "Danijar Hafner", "Deepak Pathak"], "date_published": "2020-05-12 17:59:45+00:00", "data_last_modified": "2020-06-30 23:05:50+00:00", "url": "http://arxiv.org/abs/2005.05960v2", "abstract": "Reinforcement learning allows solving complex tasks, however, the learning\ntends to be task-specific and the sample efficiency remains a challenge. We\npresent Plan2Explore, a self-supervised reinforcement learning agent that\ntackles both these challenges through a new approach to self-supervised\nexploration and fast adaptation to new tasks, which need not be known during\nexploration. During exploration, unlike prior methods which retrospectively\ncompute the novelty of observations after the agent has already reached them,\nour agent acts efficiently by leveraging planning to seek out expected future\nnovelty. After exploration, the agent quickly adapts to multiple downstream\ntasks in a zero or a few-shot manner. We evaluate on challenging control tasks\nfrom high-dimensional image inputs. Without any training supervision or\ntask-specific interaction, Plan2Explore outperforms prior self-supervised\nexploration methods, and in fact, almost matches the performances oracle which\nhas access to rewards. Videos and code at\nhttps://ramanans1.github.io/plan2explore/", "author_comment": "Accepted at ICML 2020. Videos and code at\n  https://ramanans1.github.io/plan2explore/", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.01611": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.01611v3", "post_title": "Stabilizing the Lottery Ticket Hypothesis", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "date_published": "2019-03-05 00:52:12+00:00", "data_last_modified": "2020-07-20 16:50:33+00:00", "url": "http://arxiv.org/abs/1903.01611v3", "abstract": "Pruning is a well-established technique for removing unnecessary structure\nfrom neural networks after training to improve the performance of inference.\nSeveral recent results have explored the possibility of pruning at\ninitialization time to provide similar benefits during training. In particular,\nthe \"lottery ticket hypothesis\" conjectures that typical neural networks\ncontain small subnetworks that can train to similar accuracy in a commensurate\nnumber of steps. The evidence for this claim is that a procedure based on\niterative magnitude pruning (IMP) reliably finds such subnetworks retroactively\non small vision tasks. However, IMP fails on deeper networks, and proposed\nmethods to prune before training or train pruned networks encounter similar\nscaling limitations. In this paper, we argue that these efforts have struggled\non deeper networks because they have focused on pruning precisely at\ninitialization. We modify IMP to search for subnetworks that could have been\nobtained by pruning early in training (0.1% to 7% through) rather than at\niteration 0. With this change, it finds small subnetworks of deeper networks\n(e.g., 80% sparsity on Resnet-50) that can complete the training process to\nmatch the accuracy of the original network on more challenging tasks (e.g.,\nImageNet). In situations where IMP fails at iteration 0, the accuracy benefits\nof delaying pruning accrue rapidly over the earliest iterations of training. To\nexplain these behaviors, we study subnetwork \"stability,\" finding that - as\naccuracy improves in this fashion - IMP subnetworks train to parameters closer\nto those of the full network and do so with improved consistency in the face of\ngradient noise. These results offer new insights into the opportunity to prune\nlarge-scale networks early in training and the behaviors underlying the lottery\nticket hypothesis", "author_comment": "This article has been subsumed by \"Linear Mode Connectivity and the\n  Lottery Ticket Hypothesis\" (arXiv:1912.05671, ICML 2020). Please read/cite\n  that article instead", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.08347": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.08347v3", "post_title": "How To Solve Moral Conundrums with Computability Theory", "authors": ["Min Baek"], "date_published": "2018-05-22 01:47:16+00:00", "data_last_modified": "2021-04-25 23:48:42+00:00", "url": "http://arxiv.org/abs/1805.08347v3", "abstract": "Various moral conundrums plague population ethics: the Non-Identity Problem,\nthe Procreation Asymmetry, the Repugnant Conclusion, and more. I argue that the\naforementioned moral conundrums have a structure neatly accounted for, and\nsolved by, some ideas in computability theory. I introduce a mathematical model\nbased on computability theory and show how previous arguments pertaining to\nthese conundrums fit into the model. This paper proceeds as follows. First, I\ndo a very brief survey of the history of computability theory in moral\nphilosophy. Second, I follow various papers, and show how their arguments fit\ninto, or don't fit into, our model. Third, I discuss the implications of our\nmodel to the question why the human race should or should not continue to\nexist. Finally, I show that our model may be interpreted according to a\nConfucian-Taoist moral principle.", "author_comment": "Conclusion is incorrect", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY", "cs.LO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.08971": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.08971v1", "post_title": "Computational Power and the Social Impact of Artificial Intelligence", "authors": ["Tim Hwang"], "date_published": "2018-03-23 20:39:04+00:00", "data_last_modified": "2018-03-23 20:39:04+00:00", "url": "http://arxiv.org/abs/1803.08971v1", "abstract": "Machine learning is a computational process. To that end, it is inextricably\ntied to computational power - the tangible material of chips and semiconductors\nthat the algorithms of machine intelligence operate on. Most obviously,\ncomputational power and computing architectures shape the speed of training and\ninference in machine learning, and therefore influence the rate of progress in\nthe technology. But, these relationships are more nuanced than that: hardware\nshapes the methods used by researchers and engineers in the design and\ndevelopment of machine learning models. Characteristics such as the power\nconsumption of chips also define where and how machine learning can be used in\nthe real world.\n  Despite this, many analyses of the social impact of the current wave of\nprogress in AI have not substantively brought the dimension of hardware into\ntheir accounts. While a common trope in both the popular press and scholarly\nliterature is to highlight the massive increase in computational power that has\nenabled the recent breakthroughs in machine learning, the analysis frequently\ngoes no further than this observation around magnitude. This paper aims to dig\nmore deeply into the relationship between computational power and the\ndevelopment of machine learning. Specifically, it examines how changes in\ncomputing architectures, machine learning methodologies, and supply chains\nmight influence the future of AI. In doing so, it seeks to trace a set of\nspecific relationships between this underlying hardware layer and the broader\nsocial impacts and risks around AI.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.09153": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.09153v1", "post_title": "Hidden Incentives for Auto-Induced Distributional Shift", "authors": ["David Krueger", "Tegan Maharaj", "Jan Leike"], "date_published": "2020-09-19 03:31:27+00:00", "data_last_modified": "2020-09-19 03:31:27+00:00", "url": "http://arxiv.org/abs/2009.09153v1", "abstract": "Decisions made by machine learning systems have increasing influence on the\nworld, yet it is common for machine learning algorithms to assume that no such\ninfluence exists. An example is the use of the i.i.d. assumption in content\nrecommendation. In fact, the (choice of) content displayed can change users'\nperceptions and preferences, or even drive them away, causing a shift in the\ndistribution of users. We introduce the term auto-induced distributional shift\n(ADS) to describe the phenomenon of an algorithm causing a change in the\ndistribution of its own inputs. Our goal is to ensure that machine learning\nsystems do not leverage ADS to increase performance when doing so could be\nundesirable. We demonstrate that changes to the learning algorithm, such as the\nintroduction of meta-learning, can cause hidden incentives for auto-induced\ndistributional shift (HI-ADS) to be revealed. To address this issue, we\nintroduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy\nenvironment for modelling real-world issues with HI-ADS in content\nrecommendation, where we demonstrate that strong meta-learners achieve gains in\nperformance via ADS. We show meta-learning and Q-learning both sometimes fail\nunit tests, but pass when using our mitigation strategy.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.00452": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.00452v1", "post_title": "Detecting Spiky Corruption in Markov Decision Processes", "authors": ["Jason Mancuso", "Tomasz Kisielewski", "David Lindner", "Alok Singh"], "date_published": "2019-06-30 20:30:05+00:00", "data_last_modified": "2019-06-30 20:30:05+00:00", "url": "http://arxiv.org/abs/1907.00452v1", "abstract": "Current reinforcement learning methods fail if the reward function is\nimperfect, i.e. if the agent observes reward different from what it actually\nreceives. We study this problem within the formalism of Corrupt Reward Markov\nDecision Processes (CRMDPs). We show that if the reward corruption in a CRMDP\nis sufficiently \"spiky\", the environment is solvable. We fully characterize the\nregret bound of a Spiky CRMDP, and introduce an algorithm that is able to\ndetect its corrupt states. We show that this algorithm can be used to learn the\noptimal policy with any common reinforcement learning algorithm. Finally, we\ninvestigate our algorithm in a pair of simple gridworld environments, finding\nthat our algorithm can detect the corrupt states and learn the optimal policy\ndespite the corruption.", "author_comment": "paper accepted to the AI Safety Workshop at IJCAI-19", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.02641": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.02641v3", "post_title": "An Extensible Interactive Interface for Agent Design", "authors": ["Matthew Rahtz", "James Fang", "Anca D. Dragan", "Dylan Hadfield-Menell"], "date_published": "2019-06-06 15:18:40+00:00", "data_last_modified": "2019-08-08 11:58:45+00:00", "url": "http://arxiv.org/abs/1906.02641v3", "abstract": "In artificial intelligence, we often specify tasks through a reward function.\nWhile this works well in some settings, many tasks are hard to specify this\nway. In deep reinforcement learning, for example, directly specifying a reward\nas a function of a high-dimensional observation is challenging. Instead, we\npresent an interface for specifying tasks interactively using demonstrations.\nOur approach defines a set of increasingly complex policies. The interface\nallows the user to switch between these policies at fixed intervals to generate\ndemonstrations of novel, more complex, tasks. We train new policies based on\nthese demonstrations and repeat the process. We present a case study of our\napproach in the Lunar Lander domain, and show that this simple approach can\nquickly learn a successful landing policy and outperforms an existing\ncomparison-based deep RL method.", "author_comment": "Presented at 2019 ICML Workshop on Human in the Loop Learning (HILL\n  2019), Long Beach, USA", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.HC", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.14796": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.14796v5", "post_title": "AvE: Assistance via Empowerment", "authors": ["Yuqing Du", "Stas Tiomkin", "Emre Kiciman", "Daniel Polani", "Pieter Abbeel", "Anca Dragan"], "date_published": "2020-06-26 04:40:11+00:00", "data_last_modified": "2021-01-07 20:54:48+00:00", "url": "http://arxiv.org/abs/2006.14796v5", "abstract": "One difficulty in using artificial agents for human-assistive applications\nlies in the challenge of accurately assisting with a person's goal(s). Existing\nmethods tend to rely on inferring the human's goal, which is challenging when\nthere are many potential goals or when the set of candidate goals is difficult\nto identify. We propose a new paradigm for assistance by instead increasing the\nhuman's ability to control their environment, and formalize this approach by\naugmenting reinforcement learning with human empowerment. This task-agnostic\nobjective preserves the person's autonomy and ability to achieve any eventual\nstate. We test our approach against assistance based on goal inference,\nhighlighting scenarios where our method overcomes failure modes stemming from\ngoal ambiguity or misspecification. As existing methods for estimating\nempowerment in continuous domains are computationally hard, precluding its use\nin real time learned assistance, we also propose an efficient\nempowerment-inspired proxy metric. Using this, we are able to successfully\ndemonstrate our method in a shared autonomy user study for a challenging\nsimulated teleoperation task with human-in-the-loop training.", "author_comment": "Final version from NeurIPS 2020 Conference Proceedings", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1808.06508": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1808.06508v1", "post_title": "Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies", "authors": ["Alessandro Achille", "Tom Eccles", "Loic Matthey", "Christopher P. Burgess", "Nick Watters", "Alexander Lerchner", "Irina Higgins"], "date_published": "2018-08-20 15:15:32+00:00", "data_last_modified": "2018-08-20 15:15:32+00:00", "url": "http://arxiv.org/abs/1808.06508v1", "abstract": "Intelligent behaviour in the real-world requires the ability to acquire new\nknowledge from an ongoing sequence of experiences while preserving and reusing\npast knowledge. We propose a novel algorithm for unsupervised representation\nlearning from piece-wise stationary visual data: Variational Autoencoder with\nShared Embeddings (VASE). Based on the Minimum Description Length principle,\nVASE automatically detects shifts in the data distribution and allocates spare\nrepresentational capacity to new knowledge, while simultaneously protecting\npreviously learnt representations from catastrophic forgetting. Our approach\nencourages the learnt representations to be disentangled, which imparts a\nnumber of desirable properties: VASE can deal sensibly with ambiguous inputs,\nit can enhance its own representations through imagination-based exploration,\nand most importantly, it exhibits semantically meaningful sharing of latents\nbetween different datasets. Compared to baselines with entangled\nrepresentations, our approach is able to reason beyond surface-level statistics\nand perform semantically meaningful cross-domain inference.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.09089": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.09089v4", "post_title": "Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences", "authors": ["Daniel S. Brown", "Russell Coleman", "Ravi Srinivasan", "Scott Niekum"], "date_published": "2020-02-21 02:04:54+00:00", "data_last_modified": "2020-12-17 21:48:13+00:00", "url": "http://arxiv.org/abs/2002.09089v4", "abstract": "Bayesian reward learning from demonstrations enables rigorous safety and\nuncertainty analysis when performing imitation learning. However, Bayesian\nreward learning methods are typically computationally intractable for complex\ncontrol problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a\nhighly efficient Bayesian reward learning algorithm that scales to\nhigh-dimensional imitation learning problems by pre-training a low-dimensional\nfeature encoding via self-supervised tasks and then leveraging preferences over\ndemonstrations to perform fast Bayesian inference. Bayesian REX can learn to\nplay Atari games from demonstrations, without access to the game score and can\ngenerate 100,000 samples from the posterior over reward functions in only 5\nminutes on a personal laptop. Bayesian REX also results in imitation learning\nperformance that is competitive with or better than state-of-the-art methods\nthat only learn point estimates of the reward function. Finally, Bayesian REX\nenables efficient high-confidence policy evaluation without having access to\nsamples of the reward function. These high-confidence performance bounds can be\nused to rank the performance and risk of a variety of evaluation policies and\nprovide a way to detect reward hacking behaviors.", "author_comment": "In proceedings ICML 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2003.05012": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2003.05012v4", "post_title": "Retrospective Analysis of the 2019 MineRL Competition on Sample Efficient Reinforcement Learning", "authors": ["Stephanie Milani", "Nicholay Topin", "Brandon Houghton", "William H. Guss", "Sharada P. Mohanty", "Keisuke Nakata", "Oriol Vinyals", "Noboru Sean Kuno"], "date_published": "2020-03-10 21:39:52+00:00", "data_last_modified": "2020-06-18 16:54:23+00:00", "url": "http://arxiv.org/abs/2003.05012v4", "abstract": "To facilitate research in the direction of sample efficient reinforcement\nlearning, we held the MineRL Competition on Sample Efficient Reinforcement\nLearning Using Human Priors at the Thirty-third Conference on Neural\nInformation Processing Systems (NeurIPS 2019). The primary goal of this\ncompetition was to promote the development of algorithms that use human\ndemonstrations alongside reinforcement learning to reduce the number of samples\nneeded to solve complex, hierarchical, and sparse environments. We describe the\ncompetition, outlining the primary challenge, the competition design, and the\nresources that we provided to the participants. We provide an overview of the\ntop solutions, each of which use deep reinforcement learning and/or imitation\nlearning. We also discuss the impact of our organizational decisions on the\ncompetition and future directions for improvement.", "author_comment": "To appear in Proceedings of Machine Learning Research: NeurIPS 2019\n  Competition & Demonstration Track Postproceedings. 12 pages, 2 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.08462": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.08462v2", "post_title": "Meta-Learning with Hessian-Free Approach in Deep Neural Nets Training", "authors": ["Boyu Chen", "Wenlian Lu", "Ernest Fokoue"], "date_published": "2018-05-22 09:04:52+00:00", "data_last_modified": "2018-09-07 06:14:05+00:00", "url": "http://arxiv.org/abs/1805.08462v2", "abstract": "Meta-learning is a promising method to achieve efficient training method\ntowards deep neural net and has been attracting increases interests in recent\nyears. But most of the current methods are still not capable to train complex\nneuron net model with long-time training process. In this paper, a novel\nsecond-order meta-optimizer, named Meta-learning with Hessian-Free(MLHF)\napproach, is proposed based on the Hessian-Free approach. Two recurrent neural\nnetworks are established to generate the damping and the precondition matrix of\nthis Hessian-Free framework. A series of techniques to meta-train the MLHF\ntowards stable and reinforce the meta-training of this optimizer, including the\ngradient calculation of $H$. Numerical experiments on deep convolution neural\nnets, including CUDA-convnet and ResNet18(v2), with datasets of CIFAR10 and\nILSVRC2012, indicate that the MLHF shows good and continuous training\nperformance during the whole long-time training process, i.e., both the\nrapid-decreasing early stage and the steadily-deceasing later stage, and so is\na promising meta-learning framework towards elevating the training efficiency\nin real-world deep neural nets.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2011.08827": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2011.08827v1", "post_title": "Avoiding Tampering Incentives in Deep RL via Decoupled Approval", "authors": ["Jonathan Uesato", "Ramana Kumar", "Victoria Krakovna", "Tom Everitt", "Richard Ngo", "Shane Legg"], "date_published": "2020-11-17 18:48:59+00:00", "data_last_modified": "2020-11-17 18:48:59+00:00", "url": "http://arxiv.org/abs/2011.08827v1", "abstract": "How can we design agents that pursue a given objective when all feedback\nmechanisms are influenceable by the agent? Standard RL algorithms assume a\nsecure reward function, and can thus perform poorly in settings where agents\ncan tamper with the reward-generating mechanism. We present a principled\nsolution to the problem of learning from influenceable feedback, which combines\napproval with a decoupled feedback collection procedure. For a natural class of\ncorruption functions, decoupled approval algorithms have aligned incentives\nboth at convergence and for their local updates. Empirically, they also scale\nto complex 3D environments where tampering is possible.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.03819": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.03819v3", "post_title": "Universal Transformers", "authors": ["Mostafa Dehghani", "Stephan Gouws", "Oriol Vinyals", "Jakob Uszkoreit", "\u0141ukasz Kaiser"], "date_published": "2018-07-10 18:39:15+00:00", "data_last_modified": "2019-03-05 16:46:19+00:00", "url": "http://arxiv.org/abs/1807.03819v3", "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their\nstate with each new data point, and have long been the de facto choice for\nsequence modeling tasks. However, their inherently sequential computation makes\nthem slow to train. Feed-forward and convolutional architectures have recently\nbeen shown to achieve superior results on some sequence modeling tasks such as\nmachine translation, with the added advantage that they concurrently process\nall inputs in the sequence, leading to easy parallelization and faster training\ntimes. Despite these successes, however, popular feed-forward sequence models\nlike the Transformer fail to generalize in many simple tasks that recurrent\nmodels handle with ease, e.g. copying strings or even simple logical inference\nwhen the string or formula lengths exceed those observed at training time. We\npropose the Universal Transformer (UT), a parallel-in-time self-attentive\nrecurrent sequence model which can be cast as a generalization of the\nTransformer model and which addresses these issues. UTs combine the\nparallelizability and global receptive field of feed-forward sequence models\nlike the Transformer with the recurrent inductive bias of RNNs. We also add a\ndynamic per-position halting mechanism and find that it improves accuracy on\nseveral tasks. In contrast to the standard Transformer, under certain\nassumptions, UTs can be shown to be Turing-complete. Our experiments show that\nUTs outperform standard Transformers on a wide range of algorithmic and\nlanguage understanding tasks, including the challenging LAMBADA language\nmodeling task where UTs achieve a new state of the art, and machine translation\nwhere UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De\ndataset.", "author_comment": "Published at ICLR2019", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.01683": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.01683v9", "post_title": "Optimal Policies Tend to Seek Power", "authors": ["Alexander Matt Turner", "Logan Smith", "Rohin Shah", "Andrew Critch", "Prasad Tadepalli"], "date_published": "2019-12-03 20:45:49+00:00", "data_last_modified": "2021-12-03 17:27:16+00:00", "url": "http://arxiv.org/abs/1912.01683v9", "abstract": "Some researchers speculate that intelligent reinforcement learning (RL)\nagents would be incentivized to seek resources and power in pursuit of their\nobjectives. Other researchers point out that RL agents need not have human-like\npower-seeking instincts. To clarify this discussion, we develop the first\nformal theory of the statistical tendencies of optimal policies. In the context\nof Markov decision processes, we prove that certain environmental symmetries\nare sufficient for optimal policies to tend to seek power over the environment.\nThese symmetries exist in many environments in which the agent can be shut down\nor destroyed. We prove that in these environments, most reward functions make\nit optimal to seek power by keeping a range of options available and, when\nmaximizing average reward, by navigating towards larger sets of potential\nterminal states.", "author_comment": "Accepted to NeurIPS 2021 as spotlight paper. 12 pages, 44 pages with\n  appendices", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.02624": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.02624v3", "post_title": "Learning Efficient Representation for Intrinsic Motivation", "authors": ["Ruihan Zhao", "Stas Tiomkin", "Pieter Abbeel"], "date_published": "2019-12-04 07:48:40+00:00", "data_last_modified": "2020-08-02 23:07:25+00:00", "url": "http://arxiv.org/abs/1912.02624v3", "abstract": "Mutual Information between agent Actions and environment States (MIAS)\nquantifies the influence of agent on its environment. Recently, it was found\nthat the maximization of MIAS can be used as an intrinsic motivation for\nartificial agents. In literature, the term empowerment is used to represent the\nmaximum of MIAS at a certain state. While empowerment has been shown to solve a\nbroad range of reinforcement learning problems, its calculation in arbitrary\ndynamics is a challenging problem because it relies on the estimation of mutual\ninformation. Existing approaches, which rely on sampling, are limited to low\ndimensional spaces, because high-confidence distribution-free lower bounds for\nmutual information require exponential number of samples. In this work, we\ndevelop a novel approach for the estimation of empowerment in unknown dynamics\nfrom visual observation only, without the need to sample for MIAS. The core\nidea is to represent the relation between action sequences and future states\nusing a stochastic dynamic model in latent space with a specific form. This\nallows us to efficiently compute empowerment with the \"Water-Filling\" algorithm\nfrom information theory. We construct this embedding with deep neural networks\ntrained on a sophisticated objective function. Our experimental results show\nthat the designed embedding preserves information-theoretic properties of the\noriginal dynamics.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.01657": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.01657v2", "post_title": "Dynamics-Aware Unsupervised Discovery of Skills", "authors": ["Archit Sharma", "Shixiang Gu", "Sergey Levine", "Vikash Kumar", "Karol Hausman"], "date_published": "2019-07-02 21:32:19+00:00", "data_last_modified": "2020-02-14 23:20:43+00:00", "url": "http://arxiv.org/abs/1907.01657v2", "abstract": "Conventionally, model-based reinforcement learning (MBRL) aims to learn a\nglobal model for the dynamics of the environment. A good model can potentially\nenable planning algorithms to generate a large variety of behaviors and solve\ndiverse tasks. However, learning an accurate model for complex dynamical\nsystems is difficult, and even then, the model might not generalize well\noutside the distribution of states on which it was trained. In this work, we\ncombine model-based learning with model-free learning of primitives that make\nmodel-based planning easy. To that end, we aim to answer the question: how can\nwe discover skills whose outcomes are easy to predict? We propose an\nunsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS),\nwhich simultaneously discovers predictable behaviors and learns their dynamics.\nOur method can leverage continuous skill spaces, theoretically, allowing us to\nlearn infinitely many behaviors even for high-dimensional state-spaces. We\ndemonstrate that zero-shot planning in the learned latent space significantly\noutperforms standard MBRL and model-free goal-conditioned RL, can handle\nsparse-reward tasks, and substantially improves over prior hierarchical RL\nmethods for unsupervised skill discovery.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1808.04730": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1808.04730v3", "post_title": "Analyzing Inverse Problems with Invertible Neural Networks", "authors": ["Lynton Ardizzone", "Jakob Kruse", "Sebastian Wirkert", "Daniel Rahner", "Eric W. Pellegrini", "Ralf S. Klessen", "Lena Maier-Hein", "Carsten Rother", "Ullrich K\u00f6the"], "date_published": "2018-08-14 14:58:59+00:00", "data_last_modified": "2019-02-06 15:45:02+00:00", "url": "http://arxiv.org/abs/1808.04730v3", "abstract": "In many tasks, in particular in natural science, the goal is to determine\nhidden system parameters from a set of measurements. Often, the forward process\nfrom parameter- to measurement-space is a well-defined function, whereas the\ninverse problem is ambiguous: one measurement may map to multiple different\nsets of parameters. In this setting, the posterior parameter distribution,\nconditioned on an input measurement, has to be determined. We argue that a\nparticular class of neural networks is well suited for this task -- so-called\nInvertible Neural Networks (INNs). Although INNs are not new, they have, so\nfar, received little attention in literature. While classical neural networks\nattempt to solve the ambiguous inverse problem directly, INNs are able to learn\nit jointly with the well-defined forward process, using additional latent\noutput variables to capture the information otherwise lost. Given a specific\nmeasurement and sampled latent variables, the inverse pass of the INN provides\na full distribution over parameter space. We verify experimentally, on\nartificial data and real-world problems from astrophysics and medicine, that\nINNs are a powerful analysis tool to find multi-modalities in parameter space,\nto uncover parameter correlations, and to identify unrecoverable parameters.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML", "68T01"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.05652": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.05652v2", "post_title": "Learning Human Objectives by Evaluating Hypothetical Behavior", "authors": ["Siddharth Reddy", "Anca D. Dragan", "Sergey Levine", "Shane Legg", "Jan Leike"], "date_published": "2019-12-05 18:25:48+00:00", "data_last_modified": "2021-03-24 22:26:35+00:00", "url": "http://arxiv.org/abs/1912.05652v2", "abstract": "We seek to align agent behavior with a user's objectives in a reinforcement\nlearning setting with unknown dynamics, an unknown reward function, and unknown\nunsafe states. The user knows the rewards and unsafe states, but querying the\nuser is expensive. To address this challenge, we propose an algorithm that\nsafely and interactively learns a model of the user's reward function. We start\nwith a generative model of initial states and a forward dynamics model trained\non off-policy data. Our method uses these models to synthesize hypothetical\nbehaviors, asks the user to label the behaviors with rewards, and trains a\nneural network to predict the rewards. The key idea is to actively synthesize\nthe hypothetical behaviors from scratch by maximizing tractable proxies for the\nvalue of information, without interacting with the environment. We call this\nmethod reward query synthesis via trajectory optimization (ReQueST). We\nevaluate ReQueST with simulated users on a state-based 2D navigation task and\nthe image-based Car Racing video game. The results show that ReQueST\nsignificantly outperforms prior methods in learning reward models that transfer\nto new environments with different initial state distributions. Moreover,\nReQueST safely trains the reward model to detect unsafe states, and corrects\nreward hacking before deploying the agent.", "author_comment": "Published at International Conference on Machine Learning (ICML) 2020", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.00672": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.00672v1", "post_title": "What Matters for Adversarial Imitation Learning?", "authors": ["Manu Orsini", "Anton Raichuk", "L\u00e9onard Hussenot", "Damien Vincent", "Robert Dadashi", "Sertan Girgin", "Matthieu Geist", "Olivier Bachem", "Olivier Pietquin", "Marcin Andrychowicz"], "date_published": "2021-06-01 17:58:08+00:00", "data_last_modified": "2021-06-01 17:58:08+00:00", "url": "http://arxiv.org/abs/2106.00672v1", "abstract": "Adversarial imitation learning has become a popular framework for imitation\nin continuous control. Over the years, several variations of its components\nwere proposed to enhance the performance of the learned policies as well as the\nsample complexity of the algorithm. In practice, these choices are rarely\ntested all together in rigorous empirical studies. It is therefore difficult to\ndiscuss and understand what choices, among the high-level algorithmic options\nas well as low-level implementation details, matter. To tackle this issue, we\nimplement more than 50 of these choices in a generic adversarial imitation\nlearning framework and investigate their impacts in a large-scale study (>500k\ntrained agents) with both synthetic and human-generated demonstrations. While\nmany of our findings confirm common practices, some of them are surprising or\neven contradict prior work. In particular, our results suggest that artificial\ndemonstrations are not a good proxy for human data and that the very common\npractice of evaluating imitation algorithms only with synthetic demonstrations\nmay lead to algorithms which perform poorly in the more realistic scenarios\nwith human demonstrations.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.07882": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.07882v2", "post_title": "Guiding Policies with Language via Meta-Learning", "authors": ["John D. Co-Reyes", "Abhishek Gupta", "Suvansh Sanjeev", "Nick Altieri", "Jacob Andreas", "John DeNero", "Pieter Abbeel", "Sergey Levine"], "date_published": "2018-11-19 18:58:42+00:00", "data_last_modified": "2019-01-29 18:54:15+00:00", "url": "http://arxiv.org/abs/1811.07882v2", "abstract": "Behavioral skills or policies for autonomous agents are conventionally\nlearned from reward functions, via reinforcement learning, or from\ndemonstrations, via imitation learning. However, both modes of task\nspecification have their disadvantages: reward functions require manual\nengineering, while demonstrations require a human expert to be able to actually\nperform the task in order to generate the demonstration. Instruction following\nfrom natural language instructions provides an appealing alternative: in the\nsame way that we can specify goals to other humans simply by speaking or\nwriting, we would like to be able to specify tasks for our machines. However, a\nsingle instruction may be insufficient to fully communicate our intent or, even\nif it is, may be insufficient for an autonomous agent to actually understand\nhow to perform the desired task. In this work, we propose an interactive\nformulation of the task specification problem, where iterative language\ncorrections are provided to an autonomous agent, guiding it in acquiring the\ndesired skill. Our proposed language-guided policy learning algorithm can\nintegrate an instruction and a sequence of corrections to acquire new skills\nvery quickly. In our experiments, we show that this method can enable a policy\nto follow instructions and corrections for simulated navigation and\nmanipulation tasks, substantially outperforming direct, non-interactive\ninstruction following.", "author_comment": "Accepted at ICLR 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.09849": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.09849v2", "post_title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation", "authors": ["Mia Xu Chen", "Orhan Firat", "Ankur Bapna", "Melvin Johnson", "Wolfgang Macherey", "George Foster", "Llion Jones", "Niki Parmar", "Mike Schuster", "Zhifeng Chen", "Yonghui Wu", "Macduff Hughes"], "date_published": "2018-04-26 01:24:39+00:00", "data_last_modified": "2018-04-27 02:31:16+00:00", "url": "http://arxiv.org/abs/1804.09849v2", "abstract": "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq)\nmodeling for Machine Translation (MT). The classic RNN-based approaches to MT\nwere first out-performed by the convolutional seq2seq model, which was then\nout-performed by the more recent Transformer model. Each of these new\napproaches consists of a fundamental architecture accompanied by a set of\nmodeling and training techniques that are in principle applicable to other\nseq2seq architectures. In this paper, we tease apart the new architectures and\ntheir accompanying techniques in two ways. First, we identify several key\nmodeling and training techniques, and apply them to the RNN architecture,\nyielding a new RNMT+ model that outperforms all of the three fundamental\narchitectures on the benchmark WMT'14 English to French and English to German\ntasks. Second, we analyze the properties of each fundamental seq2seq\narchitecture and devise new hybrid architectures intended to combine their\nstrengths. Our hybrid models obtain further improvements, outperforming the\nRNMT+ model on both benchmark datasets.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.01267": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.01267v1", "post_title": "Using Causal Analysis to Learn Specifications from Task Demonstrations", "authors": ["Daniel Angelov", "Yordan Hristov", "Subramanian Ramamoorthy"], "date_published": "2019-03-04 14:26:13+00:00", "data_last_modified": "2019-03-04 14:26:13+00:00", "url": "http://arxiv.org/abs/1903.01267v1", "abstract": "Learning models of user behaviour is an important problem that is broadly\napplicable across many application domains requiring human-robot interaction.\nIn this work we show that it is possible to learn a generative model for\ndistinct user behavioral types, extracted from human demonstrations, by\nenforcing clustering of preferred task solutions within the latent space. We\nuse this model to differentiate between user types and to find cases with\noverlapping solutions. Moreover, we can alter an initially guessed solution to\nsatisfy the preferences that constitute a particular user type by\nbackpropagating through the learned differentiable model. An advantage of\nstructuring generative models in this way is that it allows us to extract\ncausal relationships between symbols that might form part of the user's\nspecification of the task, as manifested in the demonstrations. We show that\nthe proposed method is capable of correctly distinguishing between three user\ntypes, who differ in degrees of cautiousness in their motion, while performing\nthe task of moving objects with a kinesthetically driven robot in a tabletop\nenvironment. Our method successfully identifies the correct type, within the\nspecified time, in 99% [97.8 - 99.8] of the cases, which outperforms an IRL\nbaseline. We also show that our proposed method correctly changes a default\ntrajectory to one satisfying a particular user specification even with unseen\nobjects. The resulting trajectory is shown to be directly implementable on a\nPR2 humanoid robot completing the same task.", "author_comment": null, "journal_ref": "Proceedings of the 18th International Conference on Autonomous\n  Agents and MultiAgent Systems, Pages 1341-1349, Montreal QC, Canada, May 13 -\n  17, 2019", "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.08060": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.08060v2", "post_title": "Safe Option-Critic: Learning Safety in the Option-Critic Architecture", "authors": ["Arushi Jain", "Khimya Khetarpal", "Doina Precup"], "date_published": "2018-07-21 00:39:23+00:00", "data_last_modified": "2021-03-02 11:07:34+00:00", "url": "http://arxiv.org/abs/1807.08060v2", "abstract": "Designing hierarchical reinforcement learning algorithms that exhibit safe\nbehaviour is not only vital for practical applications but also, facilitates a\nbetter understanding of an agent's decisions. We tackle this problem in the\noptions framework, a particular way to specify temporally abstract actions\nwhich allow an agent to use sub-policies with start and end conditions. We\nconsider a behaviour as safe that avoids regions of state-space with high\nuncertainty in the outcomes of actions. We propose an optimization objective\nthat learns safe options by encouraging the agent to visit states with higher\nbehavioural consistency. The proposed objective results in a trade-off between\nmaximizing the standard expected return and minimizing the effect of model\nuncertainty in the return. We propose a policy gradient algorithm to optimize\nthe constrained objective function. We examine the quantitative and qualitative\nbehaviour of the proposed approach in a tabular grid-world, continuous-state\npuddle-world, and three games from the Arcade Learning Environment: Ms.Pacman,\nAmidar, and Q*Bert. Our approach achieves a reduction in the variance of\nreturn, boosts performance in environments with intrinsic variability in the\nreward structure, and compares favorably both with primitive actions as well as\nwith risk-neutral options.", "author_comment": "To appear at The Knowledge Engineering Review (KER), 2021. Previous\n  draft appeared in Adaptive Learning Agents (ALA) 2018 workshop held at ICML,\n  AAMAS in Stockholm. Corrected typos, added references and added extra figures", "journal_ref": "The Knowledge Engineering Review 36 (2021) e4", "doi": "10.1017/S0269888921000035", "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1707.06354": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1707.06354v2", "post_title": "Pragmatic-Pedagogic Value Alignment", "authors": ["Jaime F. Fisac", "Monica A. Gates", "Jessica B. Hamrick", "Chang Liu", "Dylan Hadfield-Menell", "Malayandi Palaniappan", "Dhruv Malik", "S. Shankar Sastry", "Thomas L. Griffiths", "Anca D. Dragan"], "date_published": "2017-07-20 03:07:19+00:00", "data_last_modified": "2018-02-05 20:44:09+00:00", "url": "http://arxiv.org/abs/1707.06354v2", "abstract": "As intelligent systems gain autonomy and capability, it becomes vital to\nensure that their objectives match those of their human users; this is known as\nthe value-alignment problem. In robotics, value alignment is key to the design\nof collaborative robots that can integrate into human workflows, successfully\ninferring and adapting to their users' objectives as they go. We argue that a\nmeaningful solution to value alignment must combine multi-agent decision theory\nwith rich mathematical models of human cognition, enabling robots to tap into\npeople's natural collaborative capabilities. We present a solution to the\ncooperative inverse reinforcement learning (CIRL) dynamic game based on\nwell-established cognitive models of decision making and theory of mind. The\nsolution captures a key reciprocity relation: the human will not plan her\nactions in isolation, but rather reason pedagogically about how the robot might\nlearn from them; the robot, in turn, can anticipate this and interpret the\nhuman's actions pragmatically. To our knowledge, this work constitutes the\nfirst formal analysis of value alignment grounded in empirically validated\ncognitive models.", "author_comment": "Published at the International Symposium on Robotics Research (ISRR\n  2017)", "journal_ref": "International Symposium on Robotics Research, 2017", "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.RO", "68T05", "I.2.0; I.2.6; I.2.8; I.2.9"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2107.11264": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2107.11264v4", "post_title": "Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation", "authors": ["Sanghun Jung", "Jungsoo Lee", "Daehoon Gwak", "Sungha Choi", "Jaegul Choo"], "date_published": "2021-07-23 14:25:02+00:00", "data_last_modified": "2021-10-11 11:10:41+00:00", "url": "http://arxiv.org/abs/2107.11264v4", "abstract": "Identifying unexpected objects on roads in semantic segmentation (e.g.,\nidentifying dogs on roads) is crucial in safety-critical applications. Existing\napproaches use images of unexpected objects from external datasets or require\nadditional training (e.g., retraining segmentation networks or training an\nextra network), which necessitate a non-trivial amount of labor intensity or\nlengthy inference time. One possible alternative is to use prediction scores of\na pre-trained network such as the max logits (i.e., maximum values among\nclasses before the final softmax layer) for detecting such objects. However,\nthe distribution of max logits of each predicted class is significantly\ndifferent from each other, which degrades the performance of identifying\nunexpected objects in urban-scene segmentation. To address this issue, we\npropose a simple yet effective approach that standardizes the max logits in\norder to align the different distributions and reflect the relative meanings of\nmax logits within each predicted class. Moreover, we consider the local regions\nfrom two different perspectives based on the intuition that neighboring pixels\nshare similar semantic information. In contrast to previous approaches, our\nmethod does not utilize any external datasets or require additional training,\nwhich makes our method widely applicable to existing pre-trained segmentation\nmodels. Such a straightforward approach achieves a new state-of-the-art\nperformance on the publicly available Fishyscapes Lost & Found leaderboard with\na large margin. Our code is publicly available at this\n$\\href{https://github.com/shjung13/Standardized-max-logits}{link}$.", "author_comment": "Accepted to ICCV 2021 (Oral Presentation)", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2003.06066": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2003.06066v1", "post_title": "Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft", "authors": ["Christian Scheller", "Yanick Schraner", "Manfred Vogel"], "date_published": "2020-03-12 23:46:16+00:00", "data_last_modified": "2020-03-12 23:46:16+00:00", "url": "http://arxiv.org/abs/2003.06066v1", "abstract": "Sample inefficiency of deep reinforcement learning methods is a major\nobstacle for their use in real-world applications. In this work, we show how\nhuman demonstrations can improve final performance of agents on the Minecraft\nminigame ObtainDiamond with only 8M frames of environment interaction. We\npropose a training procedure where policy networks are first trained on human\ndata and later fine-tuned by reinforcement learning. Using a policy\nexploitation mechanism, experience replay and an additional loss against\ncatastrophic forgetting, our best agent was able to achieve a mean score of 48.\nOur proposed solution placed 3rd in the NeurIPS MineRL Competition for\nSample-Efficient Reinforcement Learning.", "author_comment": "10 pages, 2 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1409.1556": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1409.1556v6", "post_title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "authors": ["Karen Simonyan", "Andrew Zisserman"], "date_published": "2014-09-04 19:48:04+00:00", "data_last_modified": "2015-04-10 16:25:04+00:00", "url": "http://arxiv.org/abs/1409.1556v6", "abstract": "In this work we investigate the effect of the convolutional network depth on\nits accuracy in the large-scale image recognition setting. Our main\ncontribution is a thorough evaluation of networks of increasing depth using an\narchitecture with very small (3x3) convolution filters, which shows that a\nsignificant improvement on the prior-art configurations can be achieved by\npushing the depth to 16-19 weight layers. These findings were the basis of our\nImageNet Challenge 2014 submission, where our team secured the first and the\nsecond places in the localisation and classification tracks respectively. We\nalso show that our representations generalise well to other datasets, where\nthey achieve state-of-the-art results. We have made our two best-performing\nConvNet models publicly available to facilitate further research on the use of\ndeep visual representations in computer vision.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1904.07854": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1904.07854v2", "post_title": "End-to-End Robotic Reinforcement Learning without Reward Engineering", "authors": ["Avi Singh", "Larry Yang", "Kristian Hartikainen", "Chelsea Finn", "Sergey Levine"], "date_published": "2019-04-16 17:59:23+00:00", "data_last_modified": "2019-05-16 00:00:22+00:00", "url": "http://arxiv.org/abs/1904.07854v2", "abstract": "The combination of deep neural network models and reinforcement learning\nalgorithms can make it possible to learn policies for robotic behaviors that\ndirectly read in raw sensory inputs, such as camera images, effectively\nsubsuming both estimation and control into one model. However, real-world\napplications of reinforcement learning must specify the goal of the task by\nmeans of a manually programmed reward function, which in practice requires\neither designing the very same perception pipeline that end-to-end\nreinforcement learning promises to avoid, or else instrumenting the environment\nwith additional sensors to determine if the task has been performed\nsuccessfully. In this paper, we propose an approach for removing the need for\nmanual engineering of reward specifications by enabling a robot to learn from a\nmodest number of examples of successful outcomes, followed by actively\nsolicited queries, where the robot shows the user a state and asks for a label\nto determine whether that state represents successful completion of the task.\nWhile requesting labels for every single state would amount to asking the user\nto manually provide the reward signal, our method requires labels for only a\ntiny fraction of the states seen during training, making it an efficient and\npractical approach for learning skills without manually engineered rewards. We\nevaluate our method on real-world robotic manipulation tasks where the\nobservations consist of images viewed by the robot's camera. In our\nexperiments, our method effectively learns to arrange objects, place books, and\ndrape cloth, directly from images and without any manually specified reward\nfunctions, and with only 1-4 hours of interaction with the real world.", "author_comment": "Accepted to RSS 2019. 14 pages and 13 figures including references\n  and appendix. Website: https://sites.google.com/view/reward-learning-rl/home", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2101.10305": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2101.10305v2", "post_title": "Accumulating Risk Capital Through Investing in Cooperation", "authors": ["Charlotte Roman", "Michael Dennis", "Andrew Critch", "Stuart Russell"], "date_published": "2021-01-25 18:41:45+00:00", "data_last_modified": "2021-04-21 00:37:42+00:00", "url": "http://arxiv.org/abs/2101.10305v2", "abstract": "Recent work on promoting cooperation in multi-agent learning has resulted in\nmany methods which successfully promote cooperation at the cost of becoming\nmore vulnerable to exploitation by malicious actors. We show that this is an\nunavoidable trade-off and propose an objective which balances these concerns,\npromoting both safety and long-term cooperation. Moreover, the trade-off\nbetween safety and cooperation is not severe, and you can receive exponentially\nlarge returns through cooperation from a small amount of risk. We study both an\nexact solution method and propose a method for training policies that targets\nthis objective, Accumulating Risk Capital Through Investing in Cooperation\n(ARCTIC), and evaluate them in iterated Prisoner's Dilemma and Stag Hunt.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1705.04630": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1705.04630v6", "post_title": "Forecasting using incomplete models", "authors": ["Vanessa Kosoy"], "date_published": "2017-05-12 15:38:57+00:00", "data_last_modified": "2019-05-16 12:54:56+00:00", "url": "http://arxiv.org/abs/1705.04630v6", "abstract": "We consider the task of forecasting an infinite sequence of future\nobservations based on some number of past observations, where the probability\nmeasure generating the observations is \"suspected\" to satisfy one or more of a\nset of incomplete models, i.e. convex sets in the space of probability\nmeasures. This setting is in some sense intermediate between the realizable\nsetting where the probability measure comes from some known set of probability\nmeasures (which can be addressed using e.g. Bayesian inference) and the\nunrealizable setting where the probability measure is completely arbitrary. We\ndemonstrate a method of forecasting which guarantees that, whenever the true\nprobability measure satisfies an incomplete model in a given countable set, the\nforecast converges to the same incomplete model in the (appropriately\nnormalized) Kantorovich-Rubinstein metric. This is analogous to merging of\nopinions for Bayesian inference, except that convergence in the\nKantorovich-Rubinstein metric is weaker than convergence in total variation.", "author_comment": "29 pages", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "68Q32, 62M10, 62G08", "G.3; I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.02767": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.02767v2", "post_title": "Hybrid Models with Deep and Invertible Features", "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "date_published": "2019-02-07 18:49:47+00:00", "data_last_modified": "2019-05-29 13:52:04+00:00", "url": "http://arxiv.org/abs/1902.02767v2", "abstract": "We propose a neural hybrid model consisting of a linear model defined on a\nset of features computed by a deep, invertible transformation (i.e. a\nnormalizing flow). An attractive property of our model is that both\np(features), the density of the features, and p(targets | features), the\npredictive distribution, can be computed exactly in a single feed-forward pass.\nWe show that our hybrid model, despite the invertibility constraints, achieves\nsimilar accuracy to purely predictive models. Moreover the generative component\nremains a good model of the input features despite the hybrid optimization\nobjective. This offers additional capabilities such as detection of\nout-of-distribution inputs and enabling semi-supervised learning. The\navailability of the exact joint density p(targets, features) also allows us to\ncompute many quantities readily, making our hybrid model a useful building\nblock for downstream applications of probabilistic deep learning.", "author_comment": "ICML 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2203.02155": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2203.02155v1", "post_title": "Training language models to follow instructions with human feedback", "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "Paul Christiano", "Jan Leike", "Ryan Lowe"], "date_published": "2022-03-04 07:04:42+00:00", "data_last_modified": "2022-03-04 07:04:42+00:00", "url": "http://arxiv.org/abs/2203.02155v1", "abstract": "Making language models bigger does not inherently make them better at\nfollowing a user's intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.10985": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.10985v2", "post_title": "AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence", "authors": ["Jeff Clune"], "date_published": "2019-05-27 06:05:16+00:00", "data_last_modified": "2020-02-01 04:46:25+00:00", "url": "http://arxiv.org/abs/1905.10985v2", "abstract": "Perhaps the most ambitious scientific quest in human history is the creation\nof general artificial intelligence, which roughly means AI that is as smart or\nsmarter than humans. The dominant approach in the machine learning community is\nto attempt to discover each of the pieces required for intelligence, with the\nimplicit assumption that some future group will complete the Herculean task of\nfiguring out how to combine all of those pieces into a complex thinking\nmachine. I call this the \"manual AI approach\". This paper describes another\nexciting path that ultimately may be more successful at producing general AI.\nIt is based on the clear trend in machine learning that hand-designed solutions\neventually are replaced by more effective, learned solutions. The idea is to\ncreate an AI-generating algorithm (AI-GA), which automatically learns how to\nproduce general AI. Three Pillars are essential for the approach: (1)\nmeta-learning architectures, (2) meta-learning the learning algorithms\nthemselves, and (3) generating effective learning environments. I argue that\neither approach could produce general AI first, and both are scientifically\nworthwhile irrespective of which is the fastest path. Because both are\npromising, yet the ML community is currently committed to the manual approach,\nI argue that our community should increase its research investment in the AI-GA\napproach. To encourage such research, I describe promising work in each of the\nThree Pillars. I also discuss AI-GA-specific safety and ethical considerations.\nBecause it it may be the fastest path to general AI and because it is\ninherently scientifically interesting to understand the conditions in which a\nsimple algorithm can produce general AI (as happened on Earth where Darwinian\nevolution produced human intelligence), I argue that the pursuit of AI-GAs\nshould be considered a new grand challenge of computer science research.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2112.00861": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2112.00861v3", "post_title": "A General Language Assistant as a Laboratory for Alignment", "authors": ["Amanda Askell", "Yuntao Bai", "Anna Chen", "Dawn Drain", "Deep Ganguli", "Tom Henighan", "Andy Jones", "Nicholas Joseph", "Ben Mann", "Nova DasSarma", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "Jackson Kernion", "Kamal Ndousse", "Catherine Olsson", "Dario Amodei", "Tom Brown", "Jack Clark", "Sam McCandlish", "Chris Olah", "Jared Kaplan"], "date_published": "2021-12-01 22:24:34+00:00", "data_last_modified": "2021-12-09 21:40:22+00:00", "url": "http://arxiv.org/abs/2112.00861v3", "abstract": "Given the broad capabilities of large language models, it should be possible\nto work towards a general-purpose, text-based assistant that is aligned with\nhuman values, meaning that it is helpful, honest, and harmless. As an initial\nforay in this direction we study simple baseline techniques and evaluations,\nsuch as prompting. We find that the benefits from modest interventions increase\nwith model size, generalize to a variety of alignment evaluations, and do not\ncompromise the performance of large models. Next we investigate scaling trends\nfor several training objectives relevant to alignment, comparing imitation\nlearning, binary discrimination, and ranked preference modeling. We find that\nranked preference modeling performs much better than imitation learning, and\noften scales more favorably with model size. In contrast, binary discrimination\ntypically performs and scales very similarly to imitation learning. Finally we\nstudy a `preference model pre-training' stage of training, with the goal of\nimproving sample efficiency when finetuning on human preferences.", "author_comment": "26+19 pages; v2 typos fixed, refs added, figure scale / colors fixed;\n  v3 correct very non-standard TruthfulQA formatting and metric, alignment\n  implications slightly improved", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.00747": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.00747v3", "post_title": "The Transformative Potential of Artificial Intelligence", "authors": ["Ross Gruetzemacher", "Jess Whittlestone"], "date_published": "2019-11-27 09:37:58+00:00", "data_last_modified": "2021-10-23 13:26:56+00:00", "url": "http://arxiv.org/abs/1912.00747v3", "abstract": "The terms 'human-level artificial intelligence' and 'artificial general\nintelligence' are widely used to refer to the possibility of advanced\nartificial intelligence (AI) with potentially extreme impacts on society. These\nterms are poorly defined and do not necessarily indicate what is most important\nwith respect to future societal impacts. We suggest that the term\n'transformative AI' is a helpful alternative, reflecting the possibility that\nadvanced AI systems could have very large impacts on society without reaching\nhuman-level cognitive abilities. To be most useful, however, more analysis of\nwhat it means for AI to be 'transformative' is needed. In this paper, we\npropose three different levels on which AI might be said to be transformative,\nassociated with different levels of societal change. We suggest that these\ndistinctions would improve conversations between policy makers and decision\nmakers concerning the mid- to long-term impacts of advances in AI. Further, we\nfeel this would have a positive effect on strategic foresight efforts involving\nadvanced AI, which we expect to illuminate paths to alternative futures. We\nconclude with a discussion of the benefits of our new framework and by\nhighlighting directions for future work in this area.", "author_comment": "Under review, revised once. 17 pages", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.07273": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.07273v1", "post_title": "An Inductive Synthesis Framework for Verifiable Reinforcement Learning", "authors": ["He Zhu", "Zikang Xiong", "Stephen Magill", "Suresh Jagannathan"], "date_published": "2019-07-16 21:57:17+00:00", "data_last_modified": "2019-07-16 21:57:17+00:00", "url": "http://arxiv.org/abs/1907.07273v1", "abstract": "Despite the tremendous advances that have been made in the last decade on\ndeveloping useful machine-learning applications, their wider adoption has been\nhindered by the lack of strong assurance guarantees that can be made about\ntheir behavior. In this paper, we consider how formal verification techniques\ndeveloped for traditional software systems can be repurposed for verification\nof reinforcement learning-enabled ones, a particularly important class of\nmachine learning systems. Rather than enforcing safety by examining and\naltering the structure of a complex neural network implementation, our\ntechnique uses blackbox methods to synthesizes deterministic programs, simpler,\nmore interpretable, approximations of the network that can nonetheless\nguarantee desired safety properties are preserved, even when the network is\ndeployed in unanticipated or previously unobserved environments. Our\nmethodology frames the problem of neural network verification in terms of a\ncounterexample and syntax-guided inductive synthesis procedure over these\nprograms. The synthesis procedure searches for both a deterministic program and\nan inductive invariant over an infinite state transition system that represents\na specification of an application's control logic. Additional specifications\ndefining environment-based constraints can also be provided to further refine\nthe search space. Synthesized programs deployed in conjunction with a neural\nnetwork implementation dynamically enforce safety conditions by monitoring and\npreventing potentially unsafe actions proposed by neural policies. Experimental\nresults over a wide range of cyber-physical applications demonstrate that\nsoftware-inspired formal verification techniques can be used to realize\ntrustworthy reinforcement learning systems with low overhead.", "author_comment": "Published on PLDI 2019", "journal_ref": null, "doi": "10.1145/3314221.3314638", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML", "00-02"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2107.06857": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2107.06857v1", "post_title": "Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot", "authors": ["Joel Z. Leibo", "Edgar Du\u00e9\u00f1ez-Guzm\u00e1n", "Alexander Sasha Vezhnevets", "John P. Agapiou", "Peter Sunehag", "Raphael Koster", "Jayd Matyas", "Charles Beattie", "Igor Mordatch", "Thore Graepel"], "date_published": "2021-07-14 17:22:14+00:00", "data_last_modified": "2021-07-14 17:22:14+00:00", "url": "http://arxiv.org/abs/2107.06857v1", "abstract": "Existing evaluation suites for multi-agent reinforcement learning (MARL) do\nnot assess generalization to novel situations as their primary objective\n(unlike supervised-learning benchmarks). Our contribution, Melting Pot, is a\nMARL evaluation suite that fills this gap, and uses reinforcement learning to\nreduce the human labor required to create novel test scenarios. This works\nbecause one agent's behavior constitutes (part of) another agent's environment.\nTo demonstrate scalability, we have created over 80 unique test scenarios\ncovering a broad range of research topics such as social dilemmas, reciprocity,\nresource sharing, and task partitioning. We apply these test scenarios to\nstandard MARL training algorithms, and demonstrate how Melting Pot reveals\nweaknesses not apparent from training performance alone.", "author_comment": "Accepted to ICML 2021 and presented as a long talk; 33 pages; 9\n  figures", "journal_ref": "In International Conference on Machine Learning 2021 (pp.\n  6187-6199). PMLR", "doi": null, "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1606.06565": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1606.06565v2", "post_title": "Concrete Problems in AI Safety", "authors": ["Dario Amodei", "Chris Olah", "Jacob Steinhardt", "Paul Christiano", "John Schulman", "Dan Man\u00e9"], "date_published": "2016-06-21 13:37:05+00:00", "data_last_modified": "2016-07-25 17:23:29+00:00", "url": "http://arxiv.org/abs/1606.06565v2", "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has\nbrought increasing attention to the potential impacts of AI technologies on\nsociety. In this paper we discuss one such potential impact: the problem of\naccidents in machine learning systems, defined as unintended and harmful\nbehavior that may emerge from poor design of real-world AI systems. We present\na list of five practical research problems related to accident risk,\ncategorized according to whether the problem originates from having the wrong\nobjective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an\nobjective function that is too expensive to evaluate frequently (\"scalable\nsupervision\"), or undesirable behavior during the learning process (\"safe\nexploration\" and \"distributional shift\"). We review previous work in these\nareas as well as suggesting research directions with a focus on relevance to\ncutting-edge AI systems. Finally, we consider the high-level question of how to\nthink most productively about the safety of forward-looking applications of AI.", "author_comment": "29 pages", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.09136": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.09136v3", "post_title": "Do Deep Generative Models Know What They Don't Know?", "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "date_published": "2018-10-22 08:32:02+00:00", "data_last_modified": "2019-02-24 11:57:32+00:00", "url": "http://arxiv.org/abs/1810.09136v3", "abstract": "A neural network deployed in the wild may be asked to make predictions for\ninputs that were drawn from a different distribution than that of the training\ndata. A plethora of work has demonstrated that it is easy to find or synthesize\ninputs for which a neural network is highly confident yet wrong. Generative\nmodels are widely viewed to be robust to such mistaken confidence as modeling\nthe density of the input features can be used to detect novel,\nout-of-distribution inputs. In this paper we challenge this assumption. We find\nthat the density learned by flow-based models, VAEs, and PixelCNNs cannot\ndistinguish images of common objects such as dogs, trucks, and horses (i.e.\nCIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher\nlikelihood to the latter when the model is trained on the former. Moreover, we\nfind evidence of this phenomenon when pairing several popular image data sets:\nFashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.\nTo investigate this curious behavior, we focus analysis on flow-based\ngenerative models in particular since they are trained and evaluated via the\nexact marginal likelihood. We find such behavior persists even when we restrict\nthe flows to constant-volume transformations. These transformations admit some\ntheoretical analysis, and we show that the difference in likelihoods can be\nexplained by the location and variances of the data and the model curvature.\nOur results caution against using the density estimates from deep generative\nmodels to identify inputs similar to the training distribution until their\nbehavior for out-of-distribution inputs is better understood.", "author_comment": "ICLR 2019", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.09980": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.09980v7", "post_title": "Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings", "authors": ["Tom Everitt", "Pedro A. Ortega", "Elizabeth Barnes", "Shane Legg"], "date_published": "2019-02-26 14:54:09+00:00", "data_last_modified": "2022-01-20 17:39:06+00:00", "url": "http://arxiv.org/abs/1902.09980v7", "abstract": "Agents are systems that optimize an objective function in an environment.\nTogether, the goal and the environment induce secondary objectives, incentives.\nModeling the agent-environment interaction using causal influence diagrams, we\ncan answer two fundamental questions about an agent's incentives directly from\nthe graph: (1) which nodes can the agent have an incentivize to observe, and\n(2) which nodes can the agent have an incentivize to control? The answers tell\nus which information and influence points need extra protection. For example,\nwe may want a classifier for job applications to not use the ethnicity of the\ncandidate, and a reinforcement learning agent not to take direct control of its\nreward mechanism. Different algorithms and training paradigms can lead to\ndifferent causal influence diagrams, so our method can be used to identify\nalgorithms with problematic incentives and help in designing algorithms with\nbetter incentives.", "author_comment": "Mostly superseded by arXiv:2102.01685", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "I.2.6; I.2.8"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2109.01652": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2109.01652v5", "post_title": "Finetuned Language Models Are Zero-Shot Learners", "authors": ["Jason Wei", "Maarten Bosma", "Vincent Y. Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M. Dai", "Quoc V. Le"], "date_published": "2021-09-03 17:55:52+00:00", "data_last_modified": "2022-02-08 20:26:45+00:00", "url": "http://arxiv.org/abs/2109.01652v5", "abstract": "This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially improves zero-shot performance on unseen tasks.\n  We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof finetuning datasets, model scale, and natural language instructions are key\nto the success of instruction tuning.", "author_comment": "Version 5. Find list of changes in Appendix F (page 35)", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.04335": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.04335v2", "post_title": "Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society", "authors": ["Carina Prunkl", "Jess Whittlestone"], "date_published": "2020-01-13 15:22:42+00:00", "data_last_modified": "2020-01-21 12:18:20+00:00", "url": "http://arxiv.org/abs/2001.04335v2", "abstract": "One way of carving up the broad \"AI ethics and society\" research space that\nhas emerged in recent years is to distinguish between \"near-term\" and\n\"long-term\" research. While such ways of breaking down the research space can\nbe useful, we put forward several concerns about the near/long-term distinction\ngaining too much prominence in how research questions and priorities are\nframed. We highlight some ambiguities and inconsistencies in how the\ndistinction is used, and argue that while there are differing priorities within\nthis broad research community, these differences are not well-captured by the\nnear/long-term distinction. We unpack the near/long-term distinction into four\ndifferent dimensions, and propose some ways that researchers can communicate\nmore clearly about their work and priorities using these dimensions. We suggest\nthat moving towards a more nuanced conversation about research priorities can\nhelp establish new opportunities for collaboration, aid the development of more\nconsistent and coherent research agendas, and enable identification of\npreviously neglected research areas.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.09055": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.09055v2", "post_title": "DARTS: Differentiable Architecture Search", "authors": ["Hanxiao Liu", "Karen Simonyan", "Yiming Yang"], "date_published": "2018-06-24 00:06:13+00:00", "data_last_modified": "2019-04-23 06:29:32+00:00", "url": "http://arxiv.org/abs/1806.09055v2", "abstract": "This paper addresses the scalability challenge of architecture search by\nformulating the task in a differentiable manner. Unlike conventional approaches\nof applying evolution or reinforcement learning over a discrete and\nnon-differentiable search space, our method is based on the continuous\nrelaxation of the architecture representation, allowing efficient search of the\narchitecture using gradient descent. Extensive experiments on CIFAR-10,\nImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in\ndiscovering high-performance convolutional architectures for image\nclassification and recurrent architectures for language modeling, while being\norders of magnitude faster than state-of-the-art non-differentiable techniques.\nOur implementation has been made publicly available to facilitate further\nresearch on efficient architecture search algorithms.", "author_comment": "Published at ICLR 2019; Code and pretrained models available at\n  https://github.com/quark0/darts", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2201.12427": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2201.12427v1", "post_title": "Towards Safe Reinforcement Learning with a Safety Editor Policy", "authors": ["Haonan Yu", "Wei Xu", "Haichao Zhang"], "date_published": "2022-01-28 21:32:59+00:00", "data_last_modified": "2022-01-28 21:32:59+00:00", "url": "http://arxiv.org/abs/2201.12427v1", "abstract": "We consider the safe reinforcement learning (RL) problem of maximizing\nutility while satisfying provided constraints. Since we do not assume any prior\nknowledge or pre-training of the safety concept, we are interested in\nasymptotic constraint satisfaction. A popular approach in this line of research\nis to combine the Lagrangian method with a model-free RL algorithm to adjust\nthe weight of the constraint reward dynamically. It relies on a single policy\nto handle the conflict between utility and constraint rewards, which is often\nchallenging. Inspired by the safety layer design (Dalal et al., 2018), we\npropose to separately learn a safety editor policy that transforms potentially\nunsafe actions output by a utility maximizer policy into safe ones. The safety\neditor is trained to maximize the constraint reward while minimizing a hinge\nloss of the utility Q values of actions before and after the edit. On 12 custom\nSafety Gym (Ray et al., 2019) tasks and 2 safe racing tasks with very harsh\nconstraint thresholds, our approach demonstrates outstanding utility\nperformance while complying with the constraints. Ablation studies reveal that\nour two-policy design is critical. Simply doubling the model capacity of\ntypical single-policy approaches will not lead to comparable results. The Q\nhinge loss is also important in certain circumstances, and replacing it with\nthe usual L2 distance could fail badly.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.07470": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.07470v1", "post_title": "Solving the Rubik's Cube Without Human Knowledge", "authors": ["Stephen McAleer", "Forest Agostinelli", "Alexander Shmakov", "Pierre Baldi"], "date_published": "2018-05-18 23:07:31+00:00", "data_last_modified": "2018-05-18 23:07:31+00:00", "url": "http://arxiv.org/abs/1805.07470v1", "abstract": "A generally intelligent agent must be able to teach itself how to solve\nproblems in complex domains with minimal human supervision. Recently, deep\nreinforcement learning algorithms combined with self-play have achieved\nsuperhuman proficiency in Go, Chess, and Shogi without human data or domain\nknowledge. In these environments, a reward is always received at the end of the\ngame, however, for many combinatorial optimization environments, rewards are\nsparse and episodes are not guaranteed to terminate. We introduce Autodidactic\nIteration: a novel reinforcement learning algorithm that is able to teach\nitself how to solve the Rubik's Cube with no human assistance. Our algorithm is\nable to solve 100% of randomly scrambled cubes while achieving a median solve\nlength of 30 moves -- less than or equal to solvers that employ human domain\nknowledge.", "author_comment": "First three authors contributed equally. Submitted to NIPS 2018", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.06583": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.06583v2", "post_title": "Interpretable Latent Spaces for Learning from Demonstration", "authors": ["Yordan Hristov", "Alex Lascarides", "Subramanian Ramamoorthy"], "date_published": "2018-07-17 17:56:09+00:00", "data_last_modified": "2018-10-02 15:27:23+00:00", "url": "http://arxiv.org/abs/1807.06583v2", "abstract": "Effective human-robot interaction, such as in robot learning from human\ndemonstration, requires the learning agent to be able to ground abstract\nconcepts (such as those contained within instructions) in a corresponding\nhigh-dimensional sensory input stream from the world. Models such as deep\nneural networks, with high capacity through their large parameter spaces, can\nbe used to compress the high-dimensional sensory data to lower dimensional\nrepresentations. These low-dimensional representations facilitate symbol\ngrounding, but may not guarantee that the representation would be\nhuman-interpretable. We propose a method which utilises the grouping of\nuser-defined symbols and their corresponding sensory observations in order to\nalign the learnt compressed latent representation with the semantic notions\ncontained in the abstract labels. We demonstrate this through experiments with\nboth simulated and real-world object data, showing that such alignment can be\nachieved in a process of physical symbol grounding.", "author_comment": "12 pages, 6 figures, accepted at the Conference on Robot Learning\n  (CoRL) 2018, Zurich, Switzerland", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.10646": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.10646v1", "post_title": "A Game-Theoretic Approach for Hierarchical Policy-Making", "authors": ["Feiran Jia", "Aditya Mate", "Zun Li", "Shahin Jabbari", "Mithun Chakraborty", "Milind Tambe", "Michael Wellman", "Yevgeniy Vorobeychik"], "date_published": "2021-02-21 17:01:34+00:00", "data_last_modified": "2021-02-21 17:01:34+00:00", "url": "http://arxiv.org/abs/2102.10646v1", "abstract": "We present the design and analysis of a multi-level game-theoretic model of\nhierarchical policy-making, inspired by policy responses to the COVID-19\npandemic. Our model captures the potentially mismatched priorities among a\nhierarchy of policy-makers (e.g., federal, state, and local governments) with\nrespect to two main cost components that have opposite dependence on the policy\nstrength, such as post-intervention infection rates and the cost of policy\nimplementation. Our model further includes a crucial third factor in decisions:\na cost of non-compliance with the policy-maker immediately above in the\nhierarchy, such as non-compliance of state with federal policies. Our first\ncontribution is a closed-form approximation of a recently published agent-based\nmodel to compute the number of infections for any implemented policy. Second,\nwe present a novel equilibrium selection criterion that addresses common issues\nwith equilibrium multiplicity in our setting. Third, we propose a hierarchical\nalgorithm based on best response dynamics for computing an approximate\nequilibrium of the hierarchical policy-making game consistent with our solution\nconcept. Finally, we present an empirical investigation of equilibrium policy\nstrategies in this game in terms of the extent of free riding as well as\nfairness in the distribution of costs depending on game parameters such as the\ndegree of centralization and disagreements about policy priorities among the\nagents.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.GT", "categories": ["cs.GT", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.10268": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.10268v1", "post_title": "MADE: Exploration via Maximizing Deviation from Explored Regions", "authors": ["Tianjun Zhang", "Paria Rashidinejad", "Jiantao Jiao", "Yuandong Tian", "Joseph Gonzalez", "Stuart Russell"], "date_published": "2021-06-18 17:57:00+00:00", "data_last_modified": "2021-06-18 17:57:00+00:00", "url": "http://arxiv.org/abs/2106.10268v1", "abstract": "In online reinforcement learning (RL), efficient exploration remains\nparticularly challenging in high-dimensional environments with sparse rewards.\nIn low-dimensional environments, where tabular parameterization is possible,\ncount-based upper confidence bound (UCB) exploration methods achieve minimax\nnear-optimal rates. However, it remains unclear how to efficiently implement\nUCB in realistic RL tasks that involve non-linear function approximation. To\naddress this, we propose a new exploration approach via \\textit{maximizing} the\ndeviation of the occupancy of the next policy from the explored regions. We add\nthis term as an adaptive regularizer to the standard RL objective to balance\nexploration vs. exploitation. We pair the new objective with a provably\nconvergent algorithm, giving rise to a new intrinsic reward that adjusts\nexisting bonuses. The proposed intrinsic reward is easy to implement and\ncombine with other existing RL algorithms to conduct exploration. As a proof of\nconcept, we evaluate the new intrinsic reward on tabular examples across a\nvariety of model-based and model-free algorithms, showing improvements over\ncount-only exploration strategies. When tested on navigation and locomotion\ntasks from MiniGrid and DeepMind Control Suite benchmarks, our approach\nsignificantly improves sample efficiency over state-of-the-art methods. Our\ncode is available at https://github.com/tianjunz/MADE.", "author_comment": "28 pages, 10 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2104.04670": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2104.04670v5", "post_title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections", "authors": ["Ruiqi Zhong", "Kristy Lee", "Zheng Zhang", "Dan Klein"], "date_published": "2021-04-10 02:57:22+00:00", "data_last_modified": "2021-09-08 16:29:59+00:00", "url": "http://arxiv.org/abs/2104.04670v5", "abstract": "Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.", "author_comment": "EMNLP 2021, Findings", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.06530": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.06530v5", "post_title": "Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning", "authors": ["David Janz", "Jiri Hron", "Przemys\u0142aw Mazur", "Katja Hofmann", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Sebastian Tschiatschek"], "date_published": "2018-10-15 17:30:53+00:00", "data_last_modified": "2019-12-03 16:30:17+00:00", "url": "http://arxiv.org/abs/1810.06530v5", "abstract": "Posterior sampling for reinforcement learning (PSRL) is an effective method\nfor balancing exploration and exploitation in reinforcement learning.\nRandomised value functions (RVF) can be viewed as a promising approach to\nscaling PSRL. However, we show that most contemporary algorithms combining RVF\nwith neural network function approximation do not possess the properties which\nmake PSRL effective, and provably fail in sparse reward problems. Moreover, we\nfind that propagation of uncertainty, a property of PSRL previously thought\nimportant for exploration, does not preclude this failure. We use these\ninsights to design Successor Uncertainties (SU), a cheap and easy to implement\nRVF algorithm that retains key properties of PSRL. SU is highly effective on\nhard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it\nsurpasses human performance on 38 of 49 games tested (achieving a median human\nnormalised score of 2.09), and outperforms its closest RVF competitor,\nBootstrapped DQN, on 36 of those.", "author_comment": "Camera ready version, NeurIPS 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.03531": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.03531v1", "post_title": "A Geometric Perspective on the Transferability of Adversarial Directions", "authors": ["Zachary Charles", "Harrison Rosenberg", "Dimitris Papailiopoulos"], "date_published": "2018-11-08 16:23:50+00:00", "data_last_modified": "2018-11-08 16:23:50+00:00", "url": "http://arxiv.org/abs/1811.03531v1", "abstract": "State-of-the-art machine learning models frequently misclassify inputs that\nhave been perturbed in an adversarial manner. Adversarial perturbations\ngenerated for a given input and a specific classifier often seem to be\neffective on other inputs and even different classifiers. In other words,\nadversarial perturbations seem to transfer between different inputs, models,\nand even different neural network architectures. In this work, we show that in\nthe context of linear classifiers and two-layer ReLU networks, there provably\nexist directions that give rise to adversarial perturbations for many\nclassifiers and data points simultaneously. We show that these \"transferable\nadversarial directions\" are guaranteed to exist for linear separators of a\ngiven set, and will exist with high probability for linear classifiers trained\non independent sets drawn from the same distribution. We extend our results to\nlarge classes of two-layer ReLU networks. We further show that adversarial\ndirections for ReLU networks transfer to linear classifiers while the reverse\nneed not hold, suggesting that adversarial perturbations for more complex\nmodels are more likely to transfer to other classifiers. We validate our\nfindings empirically, even for deeper ReLU networks.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.04017": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.04017v2", "post_title": "A generic framework for privacy preserving deep learning", "authors": ["Theo Ryffel", "Andrew Trask", "Morten Dahl", "Bobby Wagner", "Jason Mancuso", "Daniel Rueckert", "Jonathan Passerat-Palmbach"], "date_published": "2018-11-09 17:10:47+00:00", "data_last_modified": "2018-11-13 18:11:15+00:00", "url": "http://arxiv.org/abs/1811.04017v2", "abstract": "We detail a new framework for privacy preserving deep learning and discuss\nits assets. The framework puts a premium on ownership and secure processing of\ndata and introduces a valuable representation based on chains of commands and\ntensors. This abstraction allows one to implement complex privacy preserving\nconstructs such as Federated Learning, Secure Multiparty Computation, and\nDifferential Privacy while still exposing a familiar deep learning API to the\nend-user. We report early results on the Boston Housing and Pima Indian\nDiabetes datasets. While the privacy features apart from Differential Privacy\ndo not impact the prediction accuracy, the current implementation of the\nframework introduces a significant overhead in performance, which will be\naddressed at a later stage of the development. We believe this work is an\nimportant milestone introducing the first reliable, general framework for\nprivacy preserving deep learning.", "author_comment": "PPML 2018, 5 pages", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2105.12938": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2105.12938v1", "post_title": "Interactive Explanations: Diagnosis and Repair of Reinforcement Learning Based Agent Behaviors", "authors": ["Christian Arzate Cruz", "Takeo Igarashi"], "date_published": "2021-05-27 04:17:48+00:00", "data_last_modified": "2021-05-27 04:17:48+00:00", "url": "http://arxiv.org/abs/2105.12938v1", "abstract": "Reinforcement learning techniques successfully generate convincing agent\nbehaviors, but it is still difficult to tailor the behavior to align with a\nuser's specific preferences. What is missing is a communication method for the\nsystem to explain the behavior and for the user to repair it. In this paper, we\npresent a novel interaction method that uses interactive explanations using\ntemplates of natural language as a communication method. The main advantage of\nthis interaction method is that it enables a two-way communication channel\nbetween users and the agent; the bot can explain its thinking procedure to the\nusers, and the users can communicate their behavior preferences to the bot\nusing the same interactive explanations. In this manner, the thinking procedure\nof the bot is transparent, and users can provide corrections to the bot that\ninclude a suggested action to take, a goal to achieve, and the reasons behind\nthese decisions. We tested our proposed method in a clone of the video game\nnamed \\textit{Super Mario Bros.}, and the results demonstrate that our\ninteractive explanation approach is effective at diagnosing and repairing bot\nbehaviors.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.HC", "categories": ["cs.HC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.00869": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.00869v1", "post_title": "Training Machine Learning Models by Regularizing their Explanations", "authors": ["Andrew Slavin Ross"], "date_published": "2018-09-29 17:43:21+00:00", "data_last_modified": "2018-09-29 17:43:21+00:00", "url": "http://arxiv.org/abs/1810.00869v1", "abstract": "Neural networks are among the most accurate supervised learning methods in\nuse today. However, their opacity makes them difficult to trust in critical\napplications, especially when conditions in training may differ from those in\npractice. Recent efforts to develop explanations for neural networks and\nmachine learning models more generally have produced tools to shed light on the\nimplicit rules behind predictions. These tools can help us identify when models\nare right for the wrong reasons. However, they do not always scale to\nexplaining predictions for entire datasets, are not always at the right level\nof abstraction, and most importantly cannot correct the problems they reveal.\nIn this thesis, we explore the possibility of training machine learning models\n(with a particular focus on neural networks) using explanations themselves. We\nconsider approaches where models are penalized not only for making incorrect\npredictions but also for providing explanations that are either inconsistent\nwith domain knowledge or overly complex. These methods let us train models\nwhich can not only provide more interpretable rationales for their predictions\nbut also generalize better when training data is confounded or meaningfully\ndifferent from test data (even adversarially so).", "author_comment": "Harvard CSE master's thesis; includes portions of arxiv:1703.03717\n  and arxiv:1711.09404", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2009.13649": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2009.13649v3", "post_title": "The EMPATHIC Framework for Task Learning from Implicit Human Feedback", "authors": ["Yuchen Cui", "Qiping Zhang", "Alessandro Allievi", "Peter Stone", "Scott Niekum", "W. Bradley Knox"], "date_published": "2020-09-28 21:50:38+00:00", "data_last_modified": "2020-12-07 18:15:12+00:00", "url": "http://arxiv.org/abs/2009.13649v3", "abstract": "Reactions such as gestures, facial expressions, and vocalizations are an\nabundant, naturally occurring channel of information that humans provide during\ninteractions. A robot or other agent could leverage an understanding of such\nimplicit human feedback to improve its task performance at no cost to the\nhuman. This approach contrasts with common agent teaching methods based on\ndemonstrations, critiques, or other guidance that need to be attentively and\nintentionally provided. In this paper, we first define the general problem of\nlearning from implicit human feedback and then propose to address this problem\nthrough a novel data-driven framework, EMPATHIC. This two-stage method consists\nof (1) mapping implicit human feedback to relevant task statistics such as\nreward, optimality, and advantage; and (2) using such a mapping to learn a\ntask. We instantiate the first stage and three second-stage evaluations of the\nlearned mapping. To do so, we collect a dataset of human facial reactions while\nparticipants observe an agent execute a sub-optimal policy for a prescribed\ntraining task. We train a deep neural network on this data and demonstrate its\nability to (1) infer relative reward ranking of events in the training task\nfrom prerecorded human facial reactions; (2) improve the policy of an agent in\nthe training task using live human facial reactions; and (3) transfer to a\nnovel domain in which it evaluates robot manipulation trajectories.", "author_comment": "Conference on Robot Learning 2020", "journal_ref": null, "doi": null, "primary_category": "cs.HC", "categories": ["cs.HC", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1612.01474": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1612.01474v3", "post_title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles", "authors": ["Balaji Lakshminarayanan", "Alexander Pritzel", "Charles Blundell"], "date_published": "2016-12-05 18:54:43+00:00", "data_last_modified": "2017-11-04 01:33:43+00:00", "url": "http://arxiv.org/abs/1612.01474v3", "abstract": "Deep neural networks (NNs) are powerful black box predictors that have\nrecently achieved impressive performance on a wide spectrum of tasks.\nQuantifying predictive uncertainty in NNs is a challenging and yet unsolved\nproblem. Bayesian NNs, which learn a distribution over weights, are currently\nthe state-of-the-art for estimating predictive uncertainty; however these\nrequire significant modifications to the training procedure and are\ncomputationally expensive compared to standard (non-Bayesian) NNs. We propose\nan alternative to Bayesian NNs that is simple to implement, readily\nparallelizable, requires very little hyperparameter tuning, and yields high\nquality predictive uncertainty estimates. Through a series of experiments on\nclassification and regression benchmarks, we demonstrate that our method\nproduces well-calibrated uncertainty estimates which are as good or better than\napproximate Bayesian NNs. To assess robustness to dataset shift, we evaluate\nthe predictive uncertainty on test examples from known and unknown\ndistributions, and show that our method is able to express higher uncertainty\non out-of-distribution examples. We demonstrate the scalability of our method\nby evaluating predictive uncertainty estimates on ImageNet.", "author_comment": "NIPS 2017", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.09469": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.09469v3", "post_title": "Embedded Agency", "authors": ["Abram Demski", "Scott Garrabrant"], "date_published": "2019-02-25 17:38:48+00:00", "data_last_modified": "2020-10-06 21:20:37+00:00", "url": "http://arxiv.org/abs/1902.09469v3", "abstract": "Traditional models of rational action treat the agent as though it is cleanly\nseparated from its environment, and can act on that environment from the\noutside. Such agents have a known functional relationship with their\nenvironment, can model their environment in every detail, and do not need to\nreason about themselves or their internal parts.\n  We provide an informal survey of obstacles to formalizing good reasoning for\nagents embedded in their environment. Such agents must optimize an environment\nthat is not of type \"function\"; they must rely on models that fit within the\nmodeled environment; and they must reason about themselves as just another\nphysical system, made of parts that can be modified and that can work at cross\npurposes.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.01772": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.01772v1", "post_title": "Dynamic Control Flow in Large-Scale Machine Learning", "authors": ["Yuan Yu", "Mart\u00edn Abadi", "Paul Barham", "Eugene Brevdo", "Mike Burrows", "Andy Davis", "Jeff Dean", "Sanjay Ghemawat", "Tim Harley", "Peter Hawkins", "Michael Isard", "Manjunath Kudlur", "Rajat Monga", "Derek Murray", "Xiaoqiang Zheng"], "date_published": "2018-05-04 13:40:07+00:00", "data_last_modified": "2018-05-04 13:40:07+00:00", "url": "http://arxiv.org/abs/1805.01772v1", "abstract": "Many recent machine learning models rely on fine-grained dynamic control flow\nfor training and inference. In particular, models based on recurrent neural\nnetworks and on reinforcement learning depend on recurrence relations,\ndata-dependent conditional execution, and other features that call for dynamic\ncontrol flow. These applications benefit from the ability to make rapid\ncontrol-flow decisions across a set of computing devices in a distributed\nsystem. For performance, scalability, and expressiveness, a machine learning\nsystem must support dynamic control flow in distributed and heterogeneous\nenvironments.\n  This paper presents a programming model for distributed machine learning that\nsupports dynamic control flow. We describe the design of the programming model,\nand its implementation in TensorFlow, a distributed machine learning system.\nOur approach extends the use of dataflow graphs to represent machine learning\nmodels, offering several distinctive features. First, the branches of\nconditionals and bodies of loops can be partitioned across many machines to run\non a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs.\nSecond, programs written in our model support automatic differentiation and\ndistributed gradient computations, which are necessary for training machine\nlearning models that use control flow. Third, our choice of non-strict\nsemantics enables multiple loop iterations to execute in parallel across\nmachines, and to overlap compute and I/O operations.\n  We have done our work in the context of TensorFlow, and it has been used\nextensively in research and production. We evaluate it using several real-world\napplications, and demonstrate its performance and scalability.", "author_comment": "Appeared in EuroSys 2018. 14 pages, 16 figures", "journal_ref": "EuroSys 2018: Thirteenth EuroSys Conference, April 23-26, 2018,\n  Porto, Portugal. ACM, New York, NY, USA", "doi": "10.1145/3190508.3190551", "primary_category": "cs.DC", "categories": ["cs.DC", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.10141": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.10141v1", "post_title": "Rational Consensus", "authors": ["Joseph Y. Halpern", "Xavier Vilaca"], "date_published": "2020-05-20 15:39:55+00:00", "data_last_modified": "2020-05-20 15:39:55+00:00", "url": "http://arxiv.org/abs/2005.10141v1", "abstract": "We provide a game-theoretic analysis of consensus, assuming that processes\nare controlled by rational agents and may fail by crashing. We consider agents\nthat \\emph{care only about consensus}: that is, (a) an agent's utility depends\nonly on the consensus value achieved (and not, for example, on the number of\nmessages the agent sends) and (b) agents strictly prefer reaching consensus to\nnot reaching consensus. We show that, under these assumptions, there is no\n\\emph{ex post Nash Equilibrium}, even with only one failure. Roughly speaking,\nthis means that there must always exist a \\emph{failure pattern} (a description\nof who fails, when they fail, and which agents they do not send messages to in\nthe round that they fail) and initial preferences for which an agent can gain\nby deviating. On the other hand, if we assume that there is a distribution\n$\\pi$ on the failure patterns and initial preferences, then under minimal\nassumptions on $\\pi$, there is a Nash equilibrium that tolerates $f$ failures\n(i.e., $\\pi$ puts probability 1 on there being at most $f$ failures) if $f+1 <\nn$ (where $n$ is the total number of agents). Moreover, we show that a slight\nextension of the Nash equilibrium strategy is also a \\emph{sequential}\nequilibrium (under the same assumptions about the distribution $\\pi$).", "author_comment": "Appears in Proceedings of the 35th Annual ACM Symposium on Principles\n  of Distributed Computing, 2016", "journal_ref": null, "doi": null, "primary_category": "cs.DC", "categories": ["cs.DC", "cs.GT"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.08336": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.08336v2", "post_title": "Maximum Causal Tsallis Entropy Imitation Learning", "authors": ["Kyungjae Lee", "Sungjoon Choi", "Songhwai Oh"], "date_published": "2018-05-22 00:49:47+00:00", "data_last_modified": "2018-05-28 04:24:06+00:00", "url": "http://arxiv.org/abs/1805.08336v2", "abstract": "In this paper, we propose a novel maximum causal Tsallis entropy (MCTE)\nframework for imitation learning which can efficiently learn a sparse\nmulti-modal policy distribution from demonstrations. We provide the full\nmathematical analysis of the proposed framework. First, the optimal solution of\nan MCTE problem is shown to be a sparsemax distribution, whose supporting set\ncan be adjusted. The proposed method has advantages over a softmax distribution\nin that it can exclude unnecessary actions by assigning zero probability.\nSecond, we prove that an MCTE problem is equivalent to robust Bayes estimation\nin the sense of the Brier score. Third, we propose a maximum causal Tsallis\nentropy imitation learning (MCTEIL) algorithm with a sparse mixture density\nnetwork (sparse MDN) by modeling mixture weights using a sparsemax\ndistribution. In particular, we show that the causal Tsallis entropy of an MDN\nencourages exploration and efficient mixture utilization while Boltzmann Gibbs\nentropy is less effective. We validate the proposed method in two simulation\nstudies and MCTEIL outperforms existing imitation learning methods in terms of\naverage returns and learning multi-modal policies.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2011.08541": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2011.08541v1", "post_title": "Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization", "authors": ["Sreejith Balakrishnan", "Quoc Phong Nguyen", "Bryan Kian Hsiang Low", "Harold Soh"], "date_published": "2020-11-17 10:17:45+00:00", "data_last_modified": "2020-11-17 10:17:45+00:00", "url": "http://arxiv.org/abs/2011.08541v1", "abstract": "The problem of inverse reinforcement learning (IRL) is relevant to a variety\nof tasks including value alignment and robot learning from demonstration.\nDespite significant algorithmic contributions in recent years, IRL remains an\nill-posed problem at its core; multiple reward functions coincide with the\nobserved behavior and the actual reward function is not identifiable without\nprior knowledge or supplementary information. This paper presents an IRL\nframework called Bayesian optimization-IRL (BO-IRL) which identifies multiple\nsolutions that are consistent with the expert demonstrations by efficiently\nexploring the reward function space. BO-IRL achieves this by utilizing Bayesian\nOptimization along with our newly proposed kernel that (a) projects the\nparameters of policy invariant reward functions to a single point in a latent\nspace and (b) ensures nearby points in the latent space correspond to reward\nfunctions yielding similar likelihoods. This projection allows the use of\nstandard stationary kernels in the latent space to capture the correlations\npresent across the reward function space. Empirical results on synthetic and\nreal-world environments (model-free and model-based) show that BO-IRL discovers\nmultiple reward functions while minimizing the number of expensive exact policy\noptimizations.", "author_comment": "Accepted to 34th Conference on Neural Information Processing Systems\n  (NeurIPS 2020). Includes Appendix. 21 pages", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.05859": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.05859v4", "post_title": "Neural Network Quine", "authors": ["Oscar Chang", "Hod Lipson"], "date_published": "2018-03-15 16:54:43+00:00", "data_last_modified": "2018-05-24 19:23:35+00:00", "url": "http://arxiv.org/abs/1803.05859v4", "abstract": "Self-replication is a key aspect of biological life that has been largely\noverlooked in Artificial Intelligence systems. Here we describe how to build\nand train self-replicating neural networks. The network replicates itself by\nlearning to output its own weights. The network is designed using a loss\nfunction that can be optimized with either gradient-based or non-gradient-based\nmethods. We also describe a method we call regeneration to train the network\nwithout explicit optimization, by injecting the network with predictions of its\nown parameters. The best solution for a self-replicating network was found by\nalternating between regeneration and optimization steps. Finally, we describe a\ndesign for a self-replicating neural network that can solve an auxiliary task\nsuch as MNIST image classification. We observe that there is a trade-off\nbetween the network's ability to classify images and its ability to replicate,\nbut training is biased towards increasing its specialization at image\nclassification at the expense of replication. This is analogous to the\ntrade-off between reproduction and other tasks observed in nature. We suggest\nthat a self-replication mechanism for artificial intelligence is useful because\nit introduces the possibility of continual improvement through natural\nselection.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.01059": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.01059v2", "post_title": "Preventing Imitation Learning with Adversarial Policy Ensembles", "authors": ["Albert Zhan", "Stas Tiomkin", "Pieter Abbeel"], "date_published": "2020-01-31 01:57:16+00:00", "data_last_modified": "2020-08-02 23:15:58+00:00", "url": "http://arxiv.org/abs/2002.01059v2", "abstract": "Imitation learning can reproduce policies by observing experts, which poses a\nproblem regarding policy privacy. Policies, such as human, or policies on\ndeployed robots, can all be cloned without consent from the owners. How can we\nprotect against external observers cloning our proprietary policies? To answer\nthis question we introduce a new reinforcement learning framework, where we\ntrain an ensemble of near-optimal policies, whose demonstrations are guaranteed\nto be useless for an external observer. We formulate this idea by a constrained\noptimization problem, where the objective is to improve proprietary policies,\nand at the same time deteriorate the virtual policy of an eventual external\nobserver. We design a tractable algorithm to solve this new optimization\nproblem by modifying the standard policy gradient algorithm. Our formulation\ncan be interpreted in lenses of confidentiality and adversarial behaviour,\nwhich enables a broader perspective of this work. We demonstrate the existence\nof \"non-clonable\" ensembles, providing a solution to the above optimization\nproblem, which is calculated by our modified policy gradient algorithm. To our\nknowledge, this is the first work regarding the protection of policies in\nReinforcement Learning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.04358": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.04358v2", "post_title": "Weight Agnostic Neural Networks", "authors": ["Adam Gaier", "David Ha"], "date_published": "2019-06-11 02:40:11+00:00", "data_last_modified": "2019-09-05 07:54:07+00:00", "url": "http://arxiv.org/abs/1906.04358v2", "abstract": "Not all neural network architectures are created equal, some perform much\nbetter than others for certain tasks. But how important are the weight\nparameters of a neural network compared to its architecture? In this work, we\nquestion to what extent neural network architectures alone, without learning\nany weight parameters, can encode solutions for a given task. We propose a\nsearch method for neural network architectures that can already perform a task\nwithout any explicit weight training. To evaluate these networks, we populate\nthe connections with a single shared weight parameter sampled from a uniform\nrandom distribution, and measure the expected performance. We demonstrate that\nour method can find minimal neural network architectures that can perform\nseveral reinforcement learning tasks without weight training. On a supervised\nlearning domain, we find network architectures that achieve much higher than\nchance accuracy on MNIST using random weights. Interactive version of this\npaper at https://weightagnostic.github.io/", "author_comment": "To appear at NeurIPS 2019, selected for a spotlight presentation", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2201.00762": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2201.00762v1", "post_title": "Execute Order 66: Targeted Data Poisoning for Reinforcement Learning", "authors": ["Harrison Foley", "Liam Fowl", "Tom Goldstein", "Gavin Taylor"], "date_published": "2022-01-03 17:09:32+00:00", "data_last_modified": "2022-01-03 17:09:32+00:00", "url": "http://arxiv.org/abs/2201.00762v1", "abstract": "Data poisoning for reinforcement learning has historically focused on general\nperformance degradation, and targeted attacks have been successful via\nperturbations that involve control of the victim's policy and rewards. We\nintroduce an insidious poisoning attack for reinforcement learning which causes\nagent misbehavior only at specific target states - all while minimally\nmodifying a small fraction of training observations without assuming any\ncontrol over policy or reward. We accomplish this by adapting a recent\ntechnique, gradient alignment, to reinforcement learning. We test our method\nand demonstrate success in two Atari games of varying difficulty.", "author_comment": "Workshop on Safe and Robust Control of Uncertain Systems at the 35th\n  Conference on Neural Information Processing Systems", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CR"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.07912": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.07912v1", "post_title": "Resource-Efficient Neural Architect", "authors": ["Yanqi Zhou", "Siavash Ebrahimi", "Sercan \u00d6. Ar\u0131k", "Haonan Yu", "Hairong Liu", "Greg Diamos"], "date_published": "2018-06-12 20:41:32+00:00", "data_last_modified": "2018-06-12 20:41:32+00:00", "url": "http://arxiv.org/abs/1806.07912v1", "abstract": "Neural Architecture Search (NAS) is a laborious process. Prior work on\nautomated NAS targets mainly on improving accuracy, but lacks consideration of\ncomputational resource use. We propose the Resource-Efficient Neural Architect\n(RENA), an efficient resource-constrained NAS using reinforcement learning with\nnetwork embedding. RENA uses a policy network to process the network embeddings\nto generate new configurations. We demonstrate RENA on image recognition and\nkeyword spotting (KWS) problems. RENA can find novel architectures that achieve\nhigh performance even with tight resource constraints. For CIFAR10, it achieves\n2.95% test error when compute intensity is greater than 100 FLOPs/byte, and\n3.87% test error when model size is less than 3M parameters. For Google Speech\nCommands Dataset, RENA achieves the state-of-the-art accuracy without resource\nconstraints, and it outperforms the optimized architectures with tight resource\nconstraints.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.NE", "categories": ["cs.NE", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2107.12808": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2107.12808v2", "post_title": "Open-Ended Learning Leads to Generally Capable Agents", "authors": ["Open Ended Learning Team", "Adam Stooke", "Anuj Mahajan", "Catarina Barros", "Charlie Deck", "Jakob Bauer", "Jakub Sygnowski", "Maja Trebacz", "Max Jaderberg", "Michael Mathieu", "Nat McAleese", "Nathalie Bradley-Schmieg", "Nathaniel Wong", "Nicolas Porcel", "Roberta Raileanu", "Steph Hughes-Fitt", "Valentin Dalibard", "Wojciech Marian Czarnecki"], "date_published": "2021-07-27 13:30:07+00:00", "data_last_modified": "2021-07-31 16:55:19+00:00", "url": "http://arxiv.org/abs/2107.12808v2", "abstract": "In this work we create agents that can perform well beyond a single,\nindividual task, that exhibit much wider generalisation of behaviour to a\nmassive, rich space of challenges. We define a universe of tasks within an\nenvironment domain and demonstrate the ability to train agents that are\ngenerally capable across this vast space and beyond. The environment is\nnatively multi-agent, spanning the continuum of competitive, cooperative, and\nindependent games, which are situated within procedurally generated physical 3D\nworlds. The resulting space is exceptionally diverse in terms of the challenges\nposed to agents, and as such, even measuring the learning progress of an agent\nis an open research problem. We propose an iterative notion of improvement\nbetween successive generations of agents, rather than seeking to maximise a\nsingular objective, allowing us to quantify progress despite tasks being\nincomparable in terms of achievable rewards. We show that through constructing\nan open-ended learning process, which dynamically changes the training task\ndistributions and training objectives such that the agent never stops learning,\nwe achieve consistent learning of new behaviours. The resulting agent is able\nto score reward in every one of our humanly solvable evaluation levels, with\nbehaviour generalising to many held-out points in the universe of tasks.\nExamples of this zero-shot generalisation include good performance on Hide and\nSeek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks\nwe characterise the behaviour of our agent, and find interesting emergent\nheuristic behaviours such as trial-and-error experimentation, simple tool use,\noption switching, and cooperation. Finally, we demonstrate that the general\ncapabilities of this agent could unlock larger scale transfer of behaviour\nthrough cheap finetuning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.01365": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.01365v1", "post_title": "DERAIL: Diagnostic Environments for Reward And Imitation Learning", "authors": ["Pedro Freire", "Adam Gleave", "Sam Toyer", "Stuart Russell"], "date_published": "2020-12-02 18:07:09+00:00", "data_last_modified": "2020-12-02 18:07:09+00:00", "url": "http://arxiv.org/abs/2012.01365v1", "abstract": "The objective of many real-world tasks is complex and difficult to\nprocedurally specify. This makes it necessary to use reward or imitation\nlearning algorithms to infer a reward or policy directly from human data.\nExisting benchmarks for these algorithms focus on realism, testing in complex\nenvironments. Unfortunately, these benchmarks are slow, unreliable and cannot\nisolate failures. As a complementary approach, we develop a suite of simple\ndiagnostic tasks that test individual facets of algorithm performance in\nisolation. We evaluate a range of common reward and imitation learning\nalgorithms on our tasks. Our results confirm that algorithm performance is\nhighly sensitive to implementation details. Moreover, in a case-study into a\npopular preference-based reward learning implementation, we illustrate how the\nsuite can pinpoint design flaws and rapidly evaluate candidate solutions. The\nenvironments are available at https://github.com/HumanCompatibleAI/seals .", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.09273": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.09273v2", "post_title": "Why Build an Assistant in Minecraft?", "authors": ["Arthur Szlam", "Jonathan Gray", "Kavya Srinet", "Yacine Jernite", "Armand Joulin", "Gabriel Synnaeve", "Douwe Kiela", "Haonan Yu", "Zhuoyuan Chen", "Siddharth Goyal", "Demi Guo", "Danielle Rothermel", "C. Lawrence Zitnick", "Jason Weston"], "date_published": "2019-07-22 12:32:15+00:00", "data_last_modified": "2019-07-25 21:52:08+00:00", "url": "http://arxiv.org/abs/1907.09273v2", "abstract": "In this document we describe a rationale for a research program aimed at\nbuilding an open \"assistant\" in the game Minecraft, in order to make progress\non the problems of natural language understanding and learning from dialogue.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.03447": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.03447v1", "post_title": "Expert-augmented actor-critic for ViZDoom and Montezumas Revenge", "authors": ["Micha\u0142 Garmulewicz", "Henryk Michalewski", "Piotr Mi\u0142o\u015b"], "date_published": "2018-09-10 16:36:22+00:00", "data_last_modified": "2018-09-10 16:36:22+00:00", "url": "http://arxiv.org/abs/1809.03447v1", "abstract": "We propose an expert-augmented actor-critic algorithm, which we evaluate on\ntwo environments with sparse rewards: Montezumas Revenge and a demanding maze\nfrom the ViZDoom suite. In the case of Montezumas Revenge, an agent trained\nwith our method achieves very good results consistently scoring above 27,000\npoints (in many experiments beating the first world). With an appropriate\nchoice of hyperparameters, our algorithm surpasses the performance of the\nexpert data. In a number of experiments, we have observed an unreported bug in\nMontezumas Revenge which allowed the agent to score more than 800,000 points.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2007.02823": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2007.02823v1", "post_title": "Dynamic Awareness", "authors": ["Joseph Y. Halpern", "Evan Piermont"], "date_published": "2020-07-06 15:28:22+00:00", "data_last_modified": "2020-07-06 15:28:22+00:00", "url": "http://arxiv.org/abs/2007.02823v1", "abstract": "We investigate how to model the beliefs of an agent who becomes more aware.\nWe use the framework of Halpern and Rego (2013) by adding probability, and\ndefine a notion of a model transition that describes constraints on how, if an\nagent becomes aware of a new formula $\\phi$ in state $s$ of a model $M$, she\ntransitions to state $s^*$ in a model $M^*$. We then discuss how such a model\ncan be applied to information disclosure.", "author_comment": "To appear in the 17th International Conference on Principles of\n  Knowledge Representation and Reasoning", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LO", "econ.TH"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2011.03395": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2011.03395v2", "post_title": "Underspecification Presents Challenges for Credibility in Modern Machine Learning", "authors": ["Alexander D'Amour", "Katherine Heller", "Dan Moldovan", "Ben Adlam", "Babak Alipanahi", "Alex Beutel", "Christina Chen", "Jonathan Deaton", "Jacob Eisenstein", "Matthew D. Hoffman", "Farhad Hormozdiari", "Neil Houlsby", "Shaobo Hou", "Ghassen Jerfel", "Alan Karthikesalingam", "Mario Lucic", "Yian Ma", "Cory McLean", "Diana Mincu", "Akinori Mitani", "Andrea Montanari", "Zachary Nado", "Vivek Natarajan", "Christopher Nielson", "Thomas F. Osborne", "Rajiv Raman", "Kim Ramasamy", "Rory Sayres", "Jessica Schrouff", "Martin Seneviratne", "Shannon Sequeira", "Harini Suresh", "Victor Veitch", "Max Vladymyrov", "Xuezhi Wang", "Kellie Webster", "Steve Yadlowsky", "Taedong Yun", "Xiaohua Zhai", "D. Sculley"], "date_published": "2020-11-06 14:53:13+00:00", "data_last_modified": "2020-11-24 19:16:02+00:00", "url": "http://arxiv.org/abs/2011.03395v2", "abstract": "ML models often exhibit unexpectedly poor behavior when they are deployed in\nreal-world domains. We identify underspecification as a key reason for these\nfailures. An ML pipeline is underspecified when it can return many predictors\nwith equivalently strong held-out performance in the training domain.\nUnderspecification is common in modern ML pipelines, such as those based on\ndeep learning. Predictors returned by underspecified pipelines are often\ntreated as equivalent based on their training domain performance, but we show\nhere that such predictors can behave very differently in deployment domains.\nThis ambiguity can lead to instability and poor model behavior in practice, and\nis a distinct failure mode from previously identified issues arising from\nstructural mismatch between training and deployment domains. We show that this\nproblem appears in a wide variety of practical ML pipelines, using examples\nfrom computer vision, medical imaging, natural language processing, clinical\nrisk prediction based on electronic health records, and medical genomics. Our\nresults show the need to explicitly account for underspecification in modeling\npipelines that are intended for real-world deployment in any domain.", "author_comment": "Updates: Updated statistical analysis in Section 6; Additional\n  citations", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1904.07633": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1904.07633v1", "post_title": "HARK Side of Deep Learning -- From Grad Student Descent to Automated Machine Learning", "authors": ["Oguzhan Gencoglu", "Mark van Gils", "Esin Guldogan", "Chamin Morikawa", "Mehmet S\u00fczen", "Mathias Gruber", "Jussi Leinonen", "Heikki Huttunen"], "date_published": "2019-04-16 13:02:01+00:00", "data_last_modified": "2019-04-16 13:02:01+00:00", "url": "http://arxiv.org/abs/1904.07633v1", "abstract": "Recent advancements in machine learning research, i.e., deep learning,\nintroduced methods that excel conventional algorithms as well as humans in\nseveral complex tasks, ranging from detection of objects in images and speech\nrecognition to playing difficult strategic games. However, the current\nmethodology of machine learning research and consequently, implementations of\nthe real-world applications of such algorithms, seems to have a recurring\nHARKing (Hypothesizing After the Results are Known) issue. In this work, we\nelaborate on the algorithmic, economic and social reasons and consequences of\nthis phenomenon. We present examples from current common practices of\nconducting machine learning research (e.g. avoidance of reporting negative\nresults) and failure of generalization ability of the proposed algorithms and\ndatasets in actual real-life usage. Furthermore, a potential future trajectory\nof machine learning research and development from the perspective of\naccountable, unbiased, ethical and privacy-aware algorithmic decision making is\ndiscussed. We would like to emphasize that with this discussion we neither\nclaim to provide an exhaustive argumentation nor blame any specific institution\nor individual on the raised issues. This is simply a discussion put forth by\nus, insiders of the machine learning field, reflecting on us.", "author_comment": "13 pages", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1610.02357": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1610.02357v3", "post_title": "Xception: Deep Learning with Depthwise Separable Convolutions", "authors": ["Fran\u00e7ois Chollet"], "date_published": "2016-10-07 17:51:51+00:00", "data_last_modified": "2017-04-04 18:40:27+00:00", "url": "http://arxiv.org/abs/1610.02357v3", "abstract": "We present an interpretation of Inception modules in convolutional neural\nnetworks as being an intermediate step in-between regular convolution and the\ndepthwise separable convolution operation (a depthwise convolution followed by\na pointwise convolution). In this light, a depthwise separable convolution can\nbe understood as an Inception module with a maximally large number of towers.\nThis observation leads us to propose a novel deep convolutional neural network\narchitecture inspired by Inception, where Inception modules have been replaced\nwith depthwise separable convolutions. We show that this architecture, dubbed\nXception, slightly outperforms Inception V3 on the ImageNet dataset (which\nInception V3 was designed for), and significantly outperforms Inception V3 on a\nlarger image classification dataset comprising 350 million images and 17,000\nclasses. Since the Xception architecture has the same number of parameters as\nInception V3, the performance gains are not due to increased capacity but\nrather to a more efficient use of model parameters.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.09773": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.09773v1", "post_title": "Algorithmic Fairness from a Non-ideal Perspective", "authors": ["Sina Fazelpour", "Zachary C. Lipton"], "date_published": "2020-01-08 18:44:41+00:00", "data_last_modified": "2020-01-08 18:44:41+00:00", "url": "http://arxiv.org/abs/2001.09773v1", "abstract": "Inspired by recent breakthroughs in predictive modeling, practitioners in\nboth industry and government have turned to machine learning with hopes of\noperationalizing predictions to drive automated decisions. Unfortunately, many\nsocial desiderata concerning consequential decisions, such as justice or\nfairness, have no natural formulation within a purely predictive framework. In\nefforts to mitigate these problems, researchers have proposed a variety of\nmetrics for quantifying deviations from various statistical parities that we\nmight expect to observe in a fair world and offered a variety of algorithms in\nattempts to satisfy subsets of these parities or to trade off the degree to\nwhich they are satisfied against utility. In this paper, we connect this\napproach to \\emph{fair machine learning} to the literature on ideal and\nnon-ideal methodological approaches in political philosophy. The ideal approach\nrequires positing the principles according to which a just world would operate.\nIn the most straightforward application of ideal theory, one supports a\nproposed policy by arguing that it closes a discrepancy between the real and\nthe perfectly just world. However, by failing to account for the mechanisms by\nwhich our non-ideal world arose, the responsibilities of various\ndecision-makers, and the impacts of proposed policies, naive applications of\nideal thinking can lead to misguided interventions. In this paper, we\ndemonstrate a connection between the fair machine learning literature and the\nideal approach in political philosophy, and argue that the increasingly\napparent shortcomings of proposed fair machine learning algorithms reflect\nbroader troubles faced by the ideal approach. We conclude with a critical\ndiscussion of the harms of misguided solutions, a reinterpretation of\nimpossibility results, and directions for future research.", "author_comment": "Accepted for publication at the AAAI/ACM Conference on Artificial\n  Intelligence, Ethics, and Society (AIES) 2020", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1705.08439": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1705.08439v4", "post_title": "Thinking Fast and Slow with Deep Learning and Tree Search", "authors": ["Thomas Anthony", "Zheng Tian", "David Barber"], "date_published": "2017-05-23 17:48:51+00:00", "data_last_modified": "2017-12-03 10:56:00+00:00", "url": "http://arxiv.org/abs/1705.08439v4", "abstract": "Sequential decision making problems, such as structured prediction, robotic\ncontrol, and game playing, require a combination of planning policies and\ngeneralisation of those plans. In this paper, we present Expert Iteration\n(ExIt), a novel reinforcement learning algorithm which decomposes the problem\ninto separate planning and generalisation tasks. Planning new policies is\nperformed by tree search, while a deep neural network generalises those plans.\nSubsequently, tree search is improved by using the neural network policy to\nguide search, increasing the strength of new plans. In contrast, standard deep\nReinforcement Learning algorithms rely on a neural network not only to\ngeneralise plans, but to discover them too. We show that ExIt outperforms\nREINFORCE for training a neural network to play the board game Hex, and our\nfinal tree search agent, trained tabula rasa, defeats MoHex 1.0, the most\nrecent Olympiad Champion player to be publicly released.", "author_comment": "v1 to v2: - Add a value function in MCTS - Some MCTS hyper-parameters\n  changed - Repetition of experiments: improved accuracy and errors shown.\n  (note the reduction in effect size for the tpt/cat experiment) - Results from\n  a longer training run, including changes in expert strength in training -\n  Comparison to MoHex. v3: clarify independence of ExIt and AG0. v4: see\n  appendix E", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1909.13371": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1909.13371v1", "post_title": "Gradient Descent: The Ultimate Optimizer", "authors": ["Kartik Chandra", "Erik Meijer", "Samantha Andow", "Emilio Arroyo-Fang", "Irene Dea", "Johann George", "Melissa Grueter", "Basil Hosmer", "Steffi Stumpos", "Alanna Tempest", "Shannon Yang"], "date_published": "2019-09-29 21:41:49+00:00", "data_last_modified": "2019-09-29 21:41:49+00:00", "url": "http://arxiv.org/abs/1909.13371v1", "abstract": "Working with any gradient-based machine learning algorithm involves the\ntedious task of tuning the optimizer's hyperparameters, such as the learning\nrate. There exist many techniques for automated hyperparameter optimization,\nbut they typically introduce even more hyperparameters to control the\nhyperparameter optimization process. We propose to instead learn the\nhyperparameters themselves by gradient descent, and furthermore to learn the\nhyper-hyperparameters by gradient descent as well, and so on ad infinitum. As\nthese towers of gradient-based optimizers grow, they become significantly less\nsensitive to the choice of top-level hyperparameters, hence decreasing the\nburden on the user to search for optimal values.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.09571": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.09571v2", "post_title": "Learning to Continually Learn", "authors": ["Shawn Beaulieu", "Lapo Frati", "Thomas Miconi", "Joel Lehman", "Kenneth O. Stanley", "Jeff Clune", "Nick Cheney"], "date_published": "2020-02-21 22:52:00+00:00", "data_last_modified": "2020-03-04 03:22:48+00:00", "url": "http://arxiv.org/abs/2002.09571v2", "abstract": "Continual lifelong learning requires an agent or model to learn many\nsequentially ordered tasks, building on previous knowledge without\ncatastrophically forgetting it. Much work has gone towards preventing the\ndefault tendency of machine learning models to catastrophically forget, yet\nvirtually all such work involves manually-designed solutions to the problem. We\ninstead advocate meta-learning a solution to catastrophic forgetting, allowing\nAI to learn to continually learn. Inspired by neuromodulatory processes in the\nbrain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It\ndifferentiates through a sequential learning process to meta-learn an\nactivation-gating function that enables context-dependent selective activation\nwithin a deep neural network. Specifically, a neuromodulatory (NM) neural\nnetwork gates the forward pass of another (otherwise normal) neural network\ncalled the prediction learning network (PLN). The NM network also thus\nindirectly controls selective plasticity (i.e. the backward pass of) the PLN.\nANML enables continual learning without catastrophic forgetting at scale: it\nproduces state-of-the-art continual learning performance, sequentially learning\nas many as 600 classes (over 9,000 SGD updates).", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1705.10720": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1705.10720v1", "post_title": "Low Impact Artificial Intelligences", "authors": ["Stuart Armstrong", "Benjamin Levinstein"], "date_published": "2017-05-30 16:15:16+00:00", "data_last_modified": "2017-05-30 16:15:16+00:00", "url": "http://arxiv.org/abs/1705.10720v1", "abstract": "There are many goals for an AI that could become dangerous if the AI becomes\nsuperintelligent or otherwise powerful. Much work on the AI control problem has\nbeen focused on constructing AI goals that are safe even for such AIs. This\npaper looks at an alternative approach: defining a general concept of `low\nimpact'. The aim is to ensure that a powerful AI which implements low impact\nwill not modify the world extensively, even if it is given a simple or\ndangerous goal. The paper proposes various ways of defining and grounding low\nimpact, and discusses methods for ensuring that the AI can still be allowed to\nhave a (desired) impact despite the restriction. The end of the paper addresses\nknown issues with this approach and avenues for future research.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.10692": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.10692v1", "post_title": "Reward Learning from Narrated Demonstrations", "authors": ["Hsiao-Yu Fish Tung", "Adam W. Harley", "Liang-Kang Huang", "Katerina Fragkiadaki"], "date_published": "2018-04-27 21:26:08+00:00", "data_last_modified": "2018-04-27 21:26:08+00:00", "url": "http://arxiv.org/abs/1804.10692v1", "abstract": "Humans effortlessly \"program\" one another by communicating goals and desires\nin natural language. In contrast, humans program robotic behaviours by\nindicating desired object locations and poses to be achieved, by providing RGB\nimages of goal configurations, or supplying a demonstration to be imitated.\nNone of these methods generalize across environment variations, and they convey\nthe goal in awkward technical terms. This work proposes joint learning of\nnatural language grounding and instructable behavioural policies reinforced by\nperceptual detectors of natural language expressions, grounded to the sensory\ninputs of the robotic agent. Our supervision is narrated visual\ndemonstrations(NVD), which are visual demonstrations paired with verbal\nnarration (as opposed to being silent). We introduce a dataset of NVD where\nteachers perform activities while describing them in detail. We map the\nteachers' descriptions to perceptual reward detectors, and use them to train\ncorresponding behavioural policies in simulation.We empirically show that our\ninstructable agents (i) learn visual reward detectors using a small number of\nexamples by exploiting hard negative mined configurations from demonstration\ndynamics, (ii) develop pick-and place policies using learned visual reward\ndetectors, (iii) benefit from object-factorized state representations that\nmimic the syntactic structure of natural language goal expressions, and (iv)\ncan execute behaviours that involve novel objects in novel locations at test\ntime, instructed by natural language.", "author_comment": "The work has been accepted to Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2018", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.01365": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.01365v2", "post_title": "Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization", "authors": ["Takayuki Osa", "Voot Tangkaratt", "Masashi Sugiyama"], "date_published": "2019-01-05 04:43:05+00:00", "data_last_modified": "2019-03-07 06:34:21+00:00", "url": "http://arxiv.org/abs/1901.01365v2", "abstract": "Real-world tasks are often highly structured. Hierarchical reinforcement\nlearning (HRL) has attracted research interest as an approach for leveraging\nthe hierarchical structure of a given task in reinforcement learning (RL).\nHowever, identifying the hierarchical policy structure that enhances the\nperformance of RL is not a trivial task. In this paper, we propose an HRL\nmethod that learns a latent variable of a hierarchical policy using mutual\ninformation maximization. Our approach can be interpreted as a way to learn a\ndiscrete and latent representation of the state-action space. To learn option\npolicies that correspond to modes of the advantage function, we introduce\nadvantage-weighted importance sampling. In our HRL method, the gating policy\nlearns to select option policies based on an option-value function, and these\noption policies are optimized based on the deterministic policy gradient\nmethod. This framework is derived by leveraging the analogy between a\nmonolithic policy in standard RL and a hierarchical policy in HRL by using a\ndeterministic option policy. Experimental results indicate that our HRL\napproach can learn a diversity of options and that it can enhance the\nperformance of RL in continuous control tasks.", "author_comment": "16 pages, ICLR 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1910.10897": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1910.10897v2", "post_title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning", "authors": ["Tianhe Yu", "Deirdre Quillen", "Zhanpeng He", "Ryan Julian", "Avnish Narayan", "Hayden Shively", "Adithya Bellathur", "Karol Hausman", "Chelsea Finn", "Sergey Levine"], "date_published": "2019-10-24 03:19:46+00:00", "data_last_modified": "2021-06-14 18:45:16+00:00", "url": "http://arxiv.org/abs/1910.10897v2", "abstract": "Meta-reinforcement learning algorithms can enable robots to acquire new\nskills much more quickly, by leveraging prior experience to learn how to learn.\nHowever, much of the current research on meta-reinforcement learning focuses on\ntask distributions that are very narrow. For example, a commonly used\nmeta-reinforcement learning benchmark uses different running velocities for a\nsimulated robot as different tasks. When policies are meta-trained on such\nnarrow task distributions, they cannot possibly generalize to more quickly\nacquire entirely new tasks. Therefore, if the aim of these methods is to enable\nfaster acquisition of entirely new behaviors, we must evaluate them on task\ndistributions that are sufficiently broad to enable generalization to new\nbehaviors. In this paper, we propose an open-source simulated benchmark for\nmeta-reinforcement learning and multi-task learning consisting of 50 distinct\nrobotic manipulation tasks. Our aim is to make it possible to develop\nalgorithms that generalize to accelerate the acquisition of entirely new,\nheld-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and\nmulti-task learning algorithms on these tasks. Surprisingly, while each task\nand its variations (e.g., with different object positions) can be learned with\nreasonable success, these algorithms struggle to learn with multiple tasks at\nthe same time, even with as few as ten distinct training tasks. Our analysis\nand open-source environments pave the way for future research in multi-task\nlearning and meta-learning that can enable meaningful generalization, thereby\nunlocking the full potential of these methods.", "author_comment": "This is an update version of a manuscript that originally appeared at\n  CoRL 2019. Videos are here: meta-world.github.io, open-sourced code are\n  available at: https://github.com/rlworkgroup/metaworld, and the baselines can\n  be found at https://github.com/rlworkgroup/garage", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.06758": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.06758v2", "post_title": "Algorithms for Verifying Deep Neural Networks", "authors": ["Changliu Liu", "Tomer Arnon", "Christopher Lazarus", "Christopher Strong", "Clark Barrett", "Mykel J. Kochenderfer"], "date_published": "2019-03-15 19:02:38+00:00", "data_last_modified": "2020-10-15 21:06:08+00:00", "url": "http://arxiv.org/abs/1903.06758v2", "abstract": "Deep neural networks are widely used for nonlinear function approximation\nwith applications ranging from computer vision to control. Although these\nnetworks involve the composition of simple arithmetic operations, it can be\nvery challenging to verify whether a particular network satisfies certain\ninput-output properties. This article surveys methods that have emerged\nrecently for soundly verifying such properties. These methods borrow insights\nfrom reachability analysis, optimization, and search. We discuss fundamental\ndifferences and connections between existing algorithms. In addition, we\nprovide pedagogical implementations of existing methods and compare them on a\nset of benchmark problems.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1602.04019": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1602.04019v1", "post_title": "Energetics of the brain and AI", "authors": ["Anders Sandberg"], "date_published": "2016-02-12 11:32:59+00:00", "data_last_modified": "2016-02-12 11:32:59+00:00", "url": "http://arxiv.org/abs/1602.04019v1", "abstract": "Does the energy requirements for the human brain give energy constraints that\ngive reason to doubt the feasibility of artificial intelligence? This report\nwill review some relevant estimates of brain bioenergetics and analyze some of\nthe methods of estimating brain emulation energy requirements. Turning to AI,\nthere are reasons to believe the energy requirements for de novo AI to have\nlittle correlation with brain (emulation) energy requirements since cost could\ndepend merely of the cost of processing higher-level representations rather\nthan billions of neural firings. Unless one thinks the human way of thinking is\nthe most optimal or most easily implementable way of achieving software\nintelligence, we should expect de novo AI to make use of different, potentially\nvery compressed and fast, processes.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "q-bio.NC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.04268": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.04268v1", "post_title": "Incomplete Contracting and AI Alignment", "authors": ["Dylan Hadfield-Menell", "Gillian Hadfield"], "date_published": "2018-04-12 01:22:50+00:00", "data_last_modified": "2018-04-12 01:22:50+00:00", "url": "http://arxiv.org/abs/1804.04268v1", "abstract": "We suggest that the analysis of incomplete contracting developed by law and\neconomics researchers can provide a useful framework for understanding the AI\nalignment problem and help to generate a systematic approach to finding\nsolutions. We first provide an overview of the incomplete contracting\nliterature and explore parallels between this work and the problem of AI\nalignment. As we emphasize, misalignment between principal and agent is a core\nfocus of economic analysis. We highlight some technical results from the\neconomics literature on incomplete contracts that may provide insights for AI\nalignment researchers. Our core contribution, however, is to bring to bear an\ninsight that economists have been urged to absorb from legal scholars and other\nbehavioral scientists: the fact that human contracting is supported by\nsubstantial amounts of external structure, such as generally available\ninstitutions (culture, law) that can supply implied terms to fill the gaps in\nincomplete contracts. We propose a research agenda for AI alignment work that\nfocuses on the problem of how to build AI that can replicate the human\ncognitive processes that connect individual incomplete contracts with this\nsupporting external structure.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2008.12146": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2008.12146v3", "post_title": "Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems", "authors": ["Sandhya Saisubramanian", "Shlomo Zilberstein", "Ece Kamar"], "date_published": "2020-08-24 16:48:46+00:00", "data_last_modified": "2021-10-18 18:40:37+00:00", "url": "http://arxiv.org/abs/2008.12146v3", "abstract": "Autonomous agents acting in the real-world often operate based on models that\nignore certain aspects of the environment. The incompleteness of any given\nmodel -- handcrafted or machine acquired -- is inevitable due to practical\nlimitations of any modeling technique for complex real-world settings. Due to\nthe limited fidelity of its model, an agent's actions may have unexpected,\nundesirable consequences during execution. Learning to recognize and avoid such\nnegative side effects of an agent's actions is critical to improve the safety\nand reliability of autonomous systems. Mitigating negative side effects is an\nemerging research topic that is attracting increased attention due to the rapid\ngrowth in the deployment of AI systems and their broad societal impacts. This\narticle provides a comprehensive overview of different forms of negative side\neffects and the recent research efforts to address them. We identify key\ncharacteristics of negative side effects, highlight the challenges in avoiding\nnegative side effects, and discuss recently developed approaches, contrasting\ntheir benefits and limitations. The article concludes with a discussion of open\nquestions and suggestions for future research directions.", "author_comment": "9 pages", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.01021": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.01021v2", "post_title": "A Strongly Asymptotically Optimal Agent in General Environments", "authors": ["Michael K. Cohen", "Elliot Catt", "Marcus Hutter"], "date_published": "2019-03-04 00:02:58+00:00", "data_last_modified": "2019-05-27 04:30:13+00:00", "url": "http://arxiv.org/abs/1903.01021v2", "abstract": "Reinforcement Learning agents are expected to eventually perform well.\nTypically, this takes the form of a guarantee about the asymptotic behavior of\nan algorithm given some assumptions about the environment. We present an\nalgorithm for a policy whose value approaches the optimal value with\nprobability 1 in all computable probabilistic environments, provided the agent\nhas a bounded horizon. This is known as strong asymptotic optimality, and it\nwas previously unknown whether it was possible for a policy to be strongly\nasymptotically optimal in the class of all computable probabilistic\nenvironments. Our agent, Inquisitive Reinforcement Learner (Inq), is more\nlikely to explore the more it expects an exploratory action to reduce its\nuncertainty about which environment it is in, hence the term inquisitive.\nExploring inquisitively is a strategy that can be applied generally; for more\nmanageable environment classes, inquisitiveness is tractable. We conducted\nexperiments in \"grid-worlds\" to compare the Inquisitive Reinforcement Learner\nto other weakly asymptotically optimal agents.", "author_comment": "7 pages, 3 figures", "journal_ref": "Proc.IJCAI (2019) 2179-2186", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.8"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2008.08076": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2008.08076v2", "post_title": "Deploying Lifelong Open-Domain Dialogue Learning", "authors": ["Kurt Shuster", "Jack Urbanek", "Emily Dinan", "Arthur Szlam", "Jason Weston"], "date_published": "2020-08-18 17:57:26+00:00", "data_last_modified": "2020-08-19 16:03:27+00:00", "url": "http://arxiv.org/abs/2008.08076v2", "abstract": "Much of NLP research has focused on crowdsourced static datasets and the\nsupervised learning paradigm of training once and then evaluating test\nperformance. As argued in de Vries et al. (2020), crowdsourced data has the\nissues of lack of naturalness and relevance to real-world use cases, while the\nstatic dataset paradigm does not allow for a model to learn from its\nexperiences of using language (Silver et al., 2013). In contrast, one might\nhope for machine learning systems that become more useful as they interact with\npeople. In this work, we build and deploy a role-playing game, whereby human\nplayers converse with learning agents situated in an open-domain fantasy world.\nWe show that by training models on the conversations they have with humans in\nthe game the models progressively improve, as measured by automatic metrics and\nonline engagement scores. This learning is shown to be more efficient than\ncrowdsourced data when applied to conversations with real users, as well as\nbeing far cheaper to collect.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2007.03244": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2007.03244v1", "post_title": "Robust Learning with Frequency Domain Regularization", "authors": ["Weiyu Guo", "Yidong Ouyang"], "date_published": "2020-07-07 07:29:20+00:00", "data_last_modified": "2020-07-07 07:29:20+00:00", "url": "http://arxiv.org/abs/2007.03244v1", "abstract": "Convolution neural networks have achieved remarkable performance in many\ntasks of computing vision. However, CNN tends to bias to low frequency\ncomponents. They prioritize capturing low frequency patterns which lead them\nfail when suffering from application scenario transformation. While adversarial\nexample implies the model is very sensitive to high frequency perturbations. In\nthis paper, we introduce a new regularization method by constraining the\nfrequency spectra of the filter of the model. Different from band-limit\ntraining, our method considers the valid frequency range probably entangles in\ndifferent layers rather than continuous and trains the valid frequency range\nend-to-end by backpropagation. We demonstrate the effectiveness of our\nregularization by (1) defensing to adversarial perturbations; (2) reducing the\ngeneralization gap in different architecture; (3) improving the generalization\nability in transfer learning scenario without fine-tune.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2007.05408": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2007.05408v1", "post_title": "Machine Learning Explainability for External Stakeholders", "authors": ["Umang Bhatt", "McKane Andrus", "Adrian Weller", "Alice Xiang"], "date_published": "2020-07-10 14:27:06+00:00", "data_last_modified": "2020-07-10 14:27:06+00:00", "url": "http://arxiv.org/abs/2007.05408v1", "abstract": "As machine learning is increasingly deployed in high-stakes contexts\naffecting people's livelihoods, there have been growing calls to open the black\nbox and to make machine learning algorithms more explainable. Providing useful\nexplanations requires careful consideration of the needs of stakeholders,\nincluding end-users, regulators, and domain experts. Despite this need, little\nwork has been done to facilitate inter-stakeholder conversation around\nexplainable machine learning. To help address this gap, we conducted a\nclosed-door, day-long workshop between academics, industry experts, legal\nscholars, and policymakers to develop a shared language around explainability\nand to understand the current shortcomings of and potential solutions for\ndeploying explainable machine learning in service of transparency goals. We\nalso asked participants to share case studies in deploying explainable machine\nlearning at scale. In this paper, we provide a short summary of various case\nstudies of explainable machine learning, lessons from those studies, and\ndiscuss open challenges.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.04765": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.04765v1", "post_title": "Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning", "authors": ["Nicolas Papernot", "Patrick McDaniel"], "date_published": "2018-03-13 13:02:13+00:00", "data_last_modified": "2018-03-13 13:02:13+00:00", "url": "http://arxiv.org/abs/1803.04765v1", "abstract": "Deep neural networks (DNNs) enable innovative applications of machine\nlearning like image recognition, machine translation, or malware detection.\nHowever, deep learning is often criticized for its lack of robustness in\nadversarial settings (e.g., vulnerability to adversarial inputs) and general\ninability to rationalize its predictions. In this work, we exploit the\nstructure of deep learning to enable new learning-based inference and decision\nstrategies that achieve desirable properties such as robustness and\ninterpretability. We take a first step in this direction and introduce the Deep\nk-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest\nneighbors algorithm with representations of the data learned by each layer of\nthe DNN: a test input is compared to its neighboring training points according\nto the distance that separates them in the representations. We show the labels\nof these neighboring points afford confidence estimates for inputs outside the\nmodel's training manifold, including on malicious inputs like adversarial\nexamples--and therein provides protections against inputs that are outside the\nmodels understanding. This is because the nearest neighbors can be used to\nestimate the nonconformity of, i.e., the lack of support for, a prediction in\nthe training data. The neighbors also constitute human-interpretable\nexplanations of predictions. We evaluate the DkNN algorithm on several\ndatasets, and show the confidence estimates accurately identify inputs outside\nthe model, and that the explanations provided by nearest neighbors are\nintuitive and useful in understanding model failures.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1605.03143": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1605.03143v1", "post_title": "Avoiding Wireheading with Value Reinforcement Learning", "authors": ["Tom Everitt", "Marcus Hutter"], "date_published": "2016-05-10 18:28:57+00:00", "data_last_modified": "2016-05-10 18:28:57+00:00", "url": "http://arxiv.org/abs/1605.03143v1", "abstract": "How can we design good goals for arbitrarily intelligent agents?\nReinforcement learning (RL) is a natural approach. Unfortunately, RL does not\nwork well for generally intelligent agents, as RL agents are incentivised to\nshortcut the reward sensor for maximum reward -- the so-called wireheading\nproblem. In this paper we suggest an alternative to RL called value\nreinforcement learning (VRL). In VRL, agents use the reward signal to learn a\nutility function. The VRL setup allows us to remove the incentive to wirehead\nby placing a constraint on the agent's actions. The constraint is defined in\nterms of the agent's belief distributions, and does not require an explicit\nspecification of which actions constitute wireheading.", "author_comment": "Artificial General Intelligence (AGI) 2016", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.03820": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.03820v1", "post_title": "An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning", "authors": ["Dhruv Malik", "Malayandi Palaniappan", "Jaime F. Fisac", "Dylan Hadfield-Menell", "Stuart Russell", "Anca D. Dragan"], "date_published": "2018-06-11 06:06:43+00:00", "data_last_modified": "2018-06-11 06:06:43+00:00", "url": "http://arxiv.org/abs/1806.03820v1", "abstract": "Our goal is for AI systems to correctly identify and act according to their\nhuman user's objectives. Cooperative Inverse Reinforcement Learning (CIRL)\nformalizes this value alignment problem as a two-player game between a human\nand robot, in which only the human knows the parameters of the reward function:\nthe robot needs to learn them as the interaction unfolds. Previous work showed\nthat CIRL can be solved as a POMDP, but with an action space size exponential\nin the size of the reward parameter space. In this work, we exploit a specific\nproperty of CIRL---the human is a full information agent---to derive an\noptimality-preserving modification to the standard Bellman update; this reduces\nthe complexity of the problem by an exponential factor and allows us to relax\nCIRL's assumption of human rationality. We apply this update to a variety of\nPOMDP solvers and find that it enables us to scale CIRL to non-trivial\nproblems, with larger reward parameter spaces, and larger action spaces for\nboth robot and human. In solutions to these larger problems, the human exhibits\npedagogic (teaching) behavior, while the robot interprets it as such and\nattains higher value for the human.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2110.08176": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2110.08176v2", "post_title": "Collaborating with Humans without Human Data", "authors": ["DJ Strouse", "Kevin R. McKee", "Matt Botvinick", "Edward Hughes", "Richard Everett"], "date_published": "2021-10-15 16:03:57+00:00", "data_last_modified": "2022-01-07 17:36:59+00:00", "url": "http://arxiv.org/abs/2110.08176v2", "abstract": "Collaborating with humans requires rapidly adapting to their individual\nstrengths, weaknesses, and preferences. Unfortunately, most standard\nmulti-agent reinforcement learning techniques, such as self-play (SP) or\npopulation play (PP), produce agents that overfit to their training partners\nand do not generalize well to humans. Alternatively, researchers can collect\nhuman data, train a human model using behavioral cloning, and then use that\nmodel to train \"human-aware\" agents (\"behavioral cloning play\", or BCP). While\nsuch an approach can improve the generalization of agents to new human\nco-players, it involves the onerous and expensive step of collecting large\namounts of human data first. Here, we study the problem of how to train agents\nthat collaborate well with human partners without using human data. We argue\nthat the crux of the problem is to produce a diverse set of training partners.\nDrawing inspiration from successful multi-agent approaches in competitive\ndomains, we find that a surprisingly simple approach is highly effective. We\ntrain our agent partner as the best response to a population of self-play\nagents and their past checkpoints taken throughout training, a method we call\nFictitious Co-Play (FCP). Our experiments focus on a two-player collaborative\ncooking simulator that has recently been proposed as a challenge problem for\ncoordination with humans. We find that FCP agents score significantly higher\nthan SP, PP, and BCP when paired with novel agent and human partners.\nFurthermore, humans also report a strong subjective preference to partnering\nwith FCP agents over all baselines.", "author_comment": "Accepted at NeurIPS 2021 (spotlight)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.HC", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1801.08757": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1801.08757v1", "post_title": "Safe Exploration in Continuous Action Spaces", "authors": ["Gal Dalal", "Krishnamurthy Dvijotham", "Matej Vecerik", "Todd Hester", "Cosmin Paduraru", "Yuval Tassa"], "date_published": "2018-01-26 11:11:18+00:00", "data_last_modified": "2018-01-26 11:11:18+00:00", "url": "http://arxiv.org/abs/1801.08757v1", "abstract": "We address the problem of deploying a reinforcement learning (RL) agent on a\nphysical system such as a datacenter cooling unit or robot, where critical\nconstraints must never be violated. We show how to exploit the typically smooth\ndynamics of these systems and enable RL algorithms to never violate constraints\nduring learning. Our technique is to directly add to the policy a safety layer\nthat analytically solves an action correction formulation per each state. The\nnovelty of obtaining an elegant closed-form solution is attained due to a\nlinearized model, learned on past trajectories consisting of arbitrary actions.\nThis is to mimic the real-world circumstances where data logs were generated\nwith a behavior policy that is implausible to describe mathematically; such\ncases render the known safety-aware off-policy methods inapplicable. We\ndemonstrate the efficacy of our approach on new representative physics-based\nenvironments, and prevail where reward shaping fails by maintaining zero\nconstraint violations.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1703.03717": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1703.03717v2", "post_title": "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations", "authors": ["Andrew Slavin Ross", "Michael C. Hughes", "Finale Doshi-Velez"], "date_published": "2017-03-10 15:35:32+00:00", "data_last_modified": "2017-05-25 05:38:45+00:00", "url": "http://arxiv.org/abs/1703.03717v2", "abstract": "Neural networks are among the most accurate supervised learning methods in\nuse today, but their opacity makes them difficult to trust in critical\napplications, especially when conditions in training differ from those in test.\nRecent work on explanations for black-box models has produced tools (e.g. LIME)\nto show the implicit rules behind predictions, which can help us identify when\nmodels are right for the wrong reasons. However, these methods do not scale to\nexplaining entire datasets and cannot correct the problems they reveal. We\nintroduce a method for efficiently explaining and regularizing differentiable\nmodels by examining and selectively penalizing their input gradients, which\nprovide a normal to the decision boundary. We apply these penalties both based\non expert annotation and in an unsupervised fashion that encourages diverse\nmodels with qualitatively different decision boundaries for the same\nclassification problem. On multiple datasets, we show our approach generates\nfaithful explanations and models that generalize much better when conditions\ndiffer between training and test.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1207.0580": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1207.0580v1", "post_title": "Improving neural networks by preventing co-adaptation of feature detectors", "authors": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "date_published": "2012-07-03 06:35:15+00:00", "data_last_modified": "2012-07-03 06:35:15+00:00", "url": "http://arxiv.org/abs/1207.0580v1", "abstract": "When a large feedforward neural network is trained on a small training set,\nit typically performs poorly on held-out test data. This \"overfitting\" is\ngreatly reduced by randomly omitting half of the feature detectors on each\ntraining case. This prevents complex co-adaptations in which a feature detector\nis only helpful in the context of several other specific feature detectors.\nInstead, each neuron learns to detect a feature that is generally helpful for\nproducing the correct answer given the combinatorially large variety of\ninternal contexts in which it must operate. Random \"dropout\" gives big\nimprovements on many benchmark tasks and sets new records for speech and object\nrecognition.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.NE", "categories": ["cs.NE", "cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.14165": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.14165v4", "post_title": "Language Models are Few-Shot Learners", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "Tom Henighan", "Rewon Child", "Aditya Ramesh", "Daniel M. Ziegler", "Jeffrey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "date_published": "2020-05-28 17:29:03+00:00", "data_last_modified": "2020-07-22 19:47:17+00:00", "url": "http://arxiv.org/abs/2005.14165v4", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.", "author_comment": "40+32 pages", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.03026": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.03026v1", "post_title": "B-Pref: Benchmarking Preference-Based Reinforcement Learning", "authors": ["Kimin Lee", "Laura Smith", "Anca Dragan", "Pieter Abbeel"], "date_published": "2021-11-04 17:32:06+00:00", "data_last_modified": "2021-11-04 17:32:06+00:00", "url": "http://arxiv.org/abs/2111.03026v1", "abstract": "Reinforcement learning (RL) requires access to a reward function that\nincentivizes the right behavior, but these are notoriously hard to specify for\ncomplex tasks. Preference-based RL provides an alternative: learning policies\nusing a teacher's preferences without pre-defined rewards, thus overcoming\nconcerns associated with reward engineering. However, it is difficult to\nquantify the progress in preference-based RL due to the lack of a commonly\nadopted benchmark. In this paper, we introduce B-Pref: a benchmark specially\ndesigned for preference-based RL. A key challenge with such a benchmark is\nproviding the ability to evaluate candidate algorithms quickly, which makes\nrelying on real human input for evaluation prohibitive. At the same time,\nsimulating human input as giving perfect preferences for the ground truth\nreward function is unrealistic. B-Pref alleviates this by simulating teachers\nwith a wide array of irrationalities, and proposes metrics not solely for\nperformance but also for robustness to these potential irrationalities. We\nshowcase the utility of B-Pref by using it to analyze algorithmic design\nchoices, such as selecting informative queries, for state-of-the-art\npreference-based RL algorithms. We hope that B-Pref can serve as a common\nstarting point to study preference-based RL more systematically. Source code is\navailable at https://github.com/rll-research/B-Pref.", "author_comment": "NeurIPS Datasets and Benchmarks Track 2021. Code is available at\n  https://github.com/rll-research/B-Pref", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.HC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.03653": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.03653v2", "post_title": "Stovepiping and Malicious Software: A Critical Review of AGI Containment", "authors": ["Jason M. Pittman", "Jesus P. Espinoza", "Courtney Crosby"], "date_published": "2018-11-08 19:19:53+00:00", "data_last_modified": "2021-08-01 09:46:27+00:00", "url": "http://arxiv.org/abs/1811.03653v2", "abstract": "Awareness of the possible impacts associated with artificial intelligence has\nrisen in proportion to progress in the field. While there are tremendous\nbenefits to society, many argue that there are just as many, if not more,\nconcerns related to advanced forms of artificial intelligence. Accordingly,\nresearch into methods to develop artificial intelligence safely is increasingly\nimportant. In this paper, we provide an overview of one such safety paradigm:\ncontainment with a critical lens aimed toward generative adversarial networks\nand potentially malicious artificial intelligence. Additionally, we illuminate\nthe potential for a developmental blindspot in the stovepiping of containment\nmechanisms.", "author_comment": "Updated author name", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.07834": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.07834v3", "post_title": "Safely Probabilistically Complete Real-Time Planning and Exploration in Unknown Environments", "authors": ["David Fridovich-Keil", "Jaime F. Fisac", "Claire J. Tomlin"], "date_published": "2018-11-19 17:47:01+00:00", "data_last_modified": "2019-03-07 04:51:30+00:00", "url": "http://arxiv.org/abs/1811.07834v3", "abstract": "We present a new framework for motion planning that wraps around existing\nkinodynamic planners and guarantees recursive feasibility when operating in a\npriori unknown, static environments. Our approach makes strong guarantees about\noverall safety and collision avoidance by utilizing a robust controller derived\nfrom reachability analysis. We ensure that motion plans never exit the safe\nbackward reachable set of the initial state, while safely exploring the space.\nThis preserves the safety of the initial state, and guarantees that that we\nwill eventually find the goal if it is possible to do so while exploring\nsafely. We implement our framework in the Robot Operating System (ROS) software\nenvironment and demonstrate it in a real-time simulation.", "author_comment": "7 pages, accepted to ICRA 2019", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.SY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.01439": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.01439v1", "post_title": "Explaining Explanations in AI", "authors": ["Brent Mittelstadt", "Chris Russell", "Sandra Wachter"], "date_published": "2018-11-04 21:35:16+00:00", "data_last_modified": "2018-11-04 21:35:16+00:00", "url": "http://arxiv.org/abs/1811.01439v1", "abstract": "Recent work on interpretability in machine learning and AI has focused on the\nbuilding of simplified models that approximate the true criteria used to make\ndecisions. These models are a useful pedagogical device for teaching trained\nprofessionals how to predict what decisions will be made by the complex system,\nand most importantly how the system might break. However, when considering any\nsuch model it's important to remember Box's maxim that \"All models are wrong\nbut some are useful.\" We focus on the distinction between these models and\nexplanations in philosophy and sociology. These models can be understood as a\n\"do it yourself kit\" for explanations, allowing a practitioner to directly\nanswer \"what if questions\" or generate contrastive explanations without\nexternal assistance. Although a valuable ability, giving these models as\nexplanations appears more difficult than necessary, and other forms of\nexplanation may not have the same trade-offs. We contrast the different schools\nof thought on what makes an explanation, and suggest that machine learning\nmight benefit from viewing the problem more broadly.", "author_comment": "FAT* 2019 Proceedings", "journal_ref": null, "doi": "10.1145/3287560.3287574", "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2203.01441": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2203.01441v1", "post_title": "3D Common Corruptions and Data Augmentation", "authors": ["O\u011fuzhan Fatih Kar", "Teresa Yeo", "Andrei Atanov", "Amir Zamir"], "date_published": "2022-03-02 22:31:16+00:00", "data_last_modified": "2022-03-02 22:31:16+00:00", "url": "http://arxiv.org/abs/2203.01441v1", "abstract": "We introduce a set of image transformations that can be used as `corruptions'\nto evaluate the robustness of models as well as `data augmentation' mechanisms\nfor training neural networks. The primary distinction of the proposed\ntransformations is that, unlike existing approaches such as Common Corruptions,\nthe geometry of the scene is incorporated in the transformations -- thus\nleading to corruptions that are more likely to occur in the real world. We show\nthese transformations are `efficient' (can be computed on-the-fly),\n`extendable' (can be applied on most datasets of real images), expose\nvulnerability of existing models, and can effectively make models more robust\nwhen employed as `3D data augmentation' mechanisms. Our evaluations performed\non several tasks and datasets suggest incorporating 3D information into\nrobustness benchmarking and training opens up a promising direction for\nrobustness research.", "author_comment": "CVPR 2022. Project website at https://3dcommoncorruptions.epfl.ch/", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1712.06365": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1712.06365v4", "post_title": "'Indifference' methods for managing agent rewards", "authors": ["Stuart Armstrong", "Xavier O'Rourke"], "date_published": "2017-12-18 12:28:45+00:00", "data_last_modified": "2018-06-05 11:10:23+00:00", "url": "http://arxiv.org/abs/1712.06365v4", "abstract": "`Indifference' refers to a class of methods used to control reward based\nagents. Indifference techniques aim to achieve one or more of three distinct\ngoals: rewards dependent on certain events (without the agent being motivated\nto manipulate the probability of those events), effective disbelief (where\nagents behave as if particular events could never happen), and seamless\ntransition from one reward function to another (with the agent acting as if\nthis change is unanticipated). This paper presents several methods for\nachieving these goals in the POMDP setting, establishing their uses, strengths,\nand requirements. These methods of control work even when the implications of\nthe agent's reward are otherwise not fully understood.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.06826": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.06826v3", "post_title": "The Blessings of Multiple Causes", "authors": ["Yixin Wang", "David M. Blei"], "date_published": "2018-05-17 15:39:17+00:00", "data_last_modified": "2019-04-15 03:37:55+00:00", "url": "http://arxiv.org/abs/1805.06826v3", "abstract": "Causal inference from observational data often assumes \"ignorability,\" that\nall confounders are observed. This assumption is standard yet untestable.\nHowever, many scientific studies involve multiple causes, different variables\nwhose effects are simultaneously of interest. We propose the deconfounder, an\nalgorithm that combines unsupervised machine learning and predictive model\nchecking to perform causal inference in multiple-cause settings. The\ndeconfounder infers a latent variable as a substitute for unobserved\nconfounders and then uses that substitute to perform causal inference. We\ndevelop theory for the deconfounder, and show that it requires weaker\nassumptions than classical causal inference. We analyze its performance in\nthree types of studies: semi-simulated data around smoking and lung cancer,\nsemi-simulated data around genome-wide association studies, and a real dataset\nabout actors and movie revenue. The deconfounder provides a checkable approach\nto estimating closer-to-truth causal effects.", "author_comment": "72 pages", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG", "stat.ME"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.01134": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.01134v1", "post_title": "A Marauder's Map of Security and Privacy in Machine Learning", "authors": ["Nicolas Papernot"], "date_published": "2018-11-03 00:25:50+00:00", "data_last_modified": "2018-11-03 00:25:50+00:00", "url": "http://arxiv.org/abs/1811.01134v1", "abstract": "There is growing recognition that machine learning (ML) exposes new security\nand privacy vulnerabilities in software systems, yet the technical community's\nunderstanding of the nature and extent of these vulnerabilities remains limited\nbut expanding. In this talk, we explore the threat model space of ML algorithms\nthrough the lens of Saltzer and Schroeder's principles for the design of secure\ncomputer systems. This characterization of the threat space prompts an\ninvestigation of current and future research directions. We structure our\ndiscussion around three of these directions, which we believe are likely to\nlead to significant progress. The first encompasses a spectrum of approaches to\nverification and admission control, which is a prerequisite to enable fail-safe\ndefaults in machine learning systems. The second seeks to design mechanisms for\nassembling reliable records of compromise that would help understand the degree\nto which vulnerabilities are exploited by adversaries, as well as favor\npsychological acceptability of machine learning applications. The third pursues\nformal frameworks for security and privacy in machine learning, which we argue\nshould strive to align machine learning goals such as generalization with\nsecurity and privacy desiderata like robustness or privacy. Key insights\nresulting from these three directions pursued both in the ML and security\ncommunities are identified and the effectiveness of approaches are related to\nstructural elements of ML algorithms and the data used to train them. We\nconclude by systematizing best practices in our community.", "author_comment": "This report summarizes the keynote presented by the author in October\n  2018 at AISec (colocated with ACM CCS) on security and privacy in machine\n  learning", "journal_ref": null, "doi": null, "primary_category": "cs.CR", "categories": ["cs.CR"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.06544": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.06544v4", "post_title": "Deep Imitative Models for Flexible Inference, Planning, and Control", "authors": ["Nicholas Rhinehart", "Rowan McAllister", "Sergey Levine"], "date_published": "2018-10-15 17:51:03+00:00", "data_last_modified": "2019-10-01 00:13:58+00:00", "url": "http://arxiv.org/abs/1810.06544v4", "abstract": "Imitation Learning (IL) is an appealing approach to learn desirable\nautonomous behavior. However, directing IL to achieve arbitrary goals is\ndifficult. In contrast, planning-based algorithms use dynamics models and\nreward functions to achieve goals. Yet, reward functions that evoke desirable\nbehavior are often difficult to specify. In this paper, we propose Imitative\nModels to combine the benefits of IL and goal-directed planning. Imitative\nModels are probabilistic predictive models of desirable behavior able to plan\ninterpretable expert-like trajectories to achieve specified goals. We derive\nfamilies of flexible goal objectives, including constrained goal regions,\nunconstrained goal sets, and energy-based goals. We show that our method can\nuse these objectives to successfully direct behavior. Our method substantially\noutperforms six IL approaches and a planning-based approach in a dynamic\nsimulated autonomous driving task, and is efficiently learned from expert\ndemonstrations without online data collection. We also show our approach is\nrobust to poorly specified goals, such as goals on the wrong side of the road.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1909.09314": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1909.09314v2", "post_title": "Meta-Inverse Reinforcement Learning with Probabilistic Context Variables", "authors": ["Lantao Yu", "Tianhe Yu", "Chelsea Finn", "Stefano Ermon"], "date_published": "2019-09-20 04:22:13+00:00", "data_last_modified": "2019-10-26 21:15:42+00:00", "url": "http://arxiv.org/abs/1909.09314v2", "abstract": "Providing a suitable reward function to reinforcement learning can be\ndifficult in many real world applications. While inverse reinforcement learning\n(IRL) holds promise for automatically learning reward functions from\ndemonstrations, several major challenges remain. First, existing IRL methods\nlearn reward functions from scratch, requiring large numbers of demonstrations\nto correctly infer the reward for each task the agent may need to perform.\nSecond, existing methods typically assume homogeneous demonstrations for a\nsingle behavior or task, while in practice, it might be easier to collect\ndatasets of heterogeneous but related behaviors. To this end, we propose a deep\nlatent variable model that is capable of learning rewards from demonstrations\nof distinct but related tasks in an unsupervised way. Critically, our model can\ninfer rewards for new, structurally-similar tasks from a single demonstration.\nOur experiments on multiple continuous control tasks demonstrate the\neffectiveness of our approach compared to state-of-the-art imitation and\ninverse reinforcement learning methods.", "author_comment": "NeurIPS 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.05188": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.05188v3", "post_title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning", "authors": ["Jiachen Yang", "Alireza Nakhaei", "David Isele", "Kikuo Fujimura", "Hongyuan Zha"], "date_published": "2018-09-13 21:46:54+00:00", "data_last_modified": "2020-01-24 21:24:17+00:00", "url": "http://arxiv.org/abs/1809.05188v3", "abstract": "A variety of cooperative multi-agent control problems require agents to\nachieve individual goals while contributing to collective success. This\nmulti-goal multi-agent setting poses difficulties for recent algorithms, which\nprimarily target settings with a single global reward, due to two new\nchallenges: efficient exploration for learning both individual goal attainment\nand cooperation for others' success, and credit-assignment for interactions\nbetween actions and goals of different agents. To address both challenges, we\nrestructure the problem into a novel two-stage curriculum, in which\nsingle-agent goal attainment is learned prior to learning multi-agent\ncooperation, and we derive a new multi-goal multi-agent policy gradient with a\ncredit function for localized credit assignment. We use a function augmentation\nscheme to bridge value and policy functions across the curriculum. The complete\narchitecture, called CM3, learns significantly faster than direct adaptations\nof existing algorithms on three challenging multi-goal multi-agent problems:\ncooperative navigation in difficult formations, negotiating multi-vehicle lane\nchanges in the SUMO traffic simulator, and strategic cooperation in a Checkers\nenvironment.", "author_comment": "Published at International Conference on Learning Representations\n  2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.MA", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2103.06312": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2103.06312v1", "post_title": "The AI Index 2021 Annual Report", "authors": ["Daniel Zhang", "Saurabh Mishra", "Erik Brynjolfsson", "John Etchemendy", "Deep Ganguli", "Barbara Grosz", "Terah Lyons", "James Manyika", "Juan Carlos Niebles", "Michael Sellitto", "Yoav Shoham", "Jack Clark", "Raymond Perrault"], "date_published": "2021-03-09 02:29:44+00:00", "data_last_modified": "2021-03-09 02:29:44+00:00", "url": "http://arxiv.org/abs/2103.06312v1", "abstract": "Welcome to the fourth edition of the AI Index Report. This year we\nsignificantly expanded the amount of data available in the report, worked with\na broader set of external organizations to calibrate our data, and deepened our\nconnections with the Stanford Institute for Human-Centered Artificial\nIntelligence (HAI). The AI Index Report tracks, collates, distills, and\nvisualizes data related to artificial intelligence. Its mission is to provide\nunbiased, rigorously vetted, and globally sourced data for policymakers,\nresearchers, executives, journalists, and the general public to develop\nintuitions about the complex field of AI. The report aims to be the most\ncredible and authoritative source for data and insights about AI in the world.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.GL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.12888": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.12888v2", "post_title": "Imitation Learning as $f$-Divergence Minimization", "authors": ["Liyiming Ke", "Sanjiban Choudhury", "Matt Barnes", "Wen Sun", "Gilwoo Lee", "Siddhartha Srinivasa"], "date_published": "2019-05-30 07:19:13+00:00", "data_last_modified": "2020-05-31 08:38:33+00:00", "url": "http://arxiv.org/abs/1905.12888v2", "abstract": "We address the problem of imitation learning with multi-modal demonstrations.\nInstead of attempting to learn all modes, we argue that in many tasks it is\nsufficient to imitate any one of them. We show that the state-of-the-art\nmethods such as GAIL and behavior cloning, due to their choice of loss\nfunction, often incorrectly interpolate between such modes. Our key insight is\nto minimize the right divergence between the learner and the expert\nstate-action distributions, namely the reverse KL divergence or I-projection.\nWe propose a general imitation learning framework for estimating and minimizing\nany f-Divergence. By plugging in different divergences, we are able to recover\nexisting algorithms such as Behavior Cloning (Kullback-Leibler), GAIL (Jensen\nShannon) and Dagger (Total Variation). Empirical results show that our\napproximate I-projection technique is able to imitate multi-modal behaviors\nmore reliably than GAIL and behavior cloning.", "author_comment": "International Workshop on the Algorithmic Foundations of Robotics\n  (WAFR) 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.IT", "cs.RO", "math.IT", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2110.08514": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2110.08514v1", "post_title": "Analyzing Dynamic Adversarial Training Data in the Limit", "authors": ["Eric Wallace", "Adina Williams", "Robin Jia", "Douwe Kiela"], "date_published": "2021-10-16 08:48:52+00:00", "data_last_modified": "2021-10-16 08:48:52+00:00", "url": "http://arxiv.org/abs/2110.08514v1", "abstract": "To create models that are robust across a wide range of test inputs, training\ndatasets should include diverse examples that span numerous phenomena. Dynamic\nadversarial data collection (DADC), where annotators craft examples that\nchallenge continually improving models, holds promise as an approach for\ngenerating such diverse training sets. Prior work has shown that running DADC\nover 1-3 rounds can help models fix some error types, but it does not\nnecessarily lead to better generalization beyond adversarial test data. We\nargue that running DADC over many rounds maximizes its training-time benefits,\nas the different rounds can together cover many of the task-relevant phenomena.\nWe present the first study of longer-term DADC, where we collect 20 rounds of\nNLI examples for a small set of premise paragraphs, with both adversarial and\nnon-adversarial approaches. Models trained on DADC examples make 26% fewer\nerrors on our expert-curated test set compared to models trained on\nnon-adversarial data. Our analysis shows that DADC yields examples that are\nmore difficult, more lexically and syntactically diverse, and contain fewer\nannotation artifacts compared to non-adversarial examples.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.08579": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.08579v2", "post_title": "Forecasting Transformative AI: An Expert Survey", "authors": ["Ross Gruetzemacher", "David Paradice", "Kang Bok Lee"], "date_published": "2019-01-24 18:53:07+00:00", "data_last_modified": "2019-07-16 16:23:00+00:00", "url": "http://arxiv.org/abs/1901.08579v2", "abstract": "Transformative AI technologies have the potential to reshape critical aspects\nof society in the near future. However, in order to properly prepare policy\ninitiatives for the arrival of such technologies accurate forecasts and\ntimelines are necessary. A survey was administered to attendees of three AI\nconferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).\nThe survey included questions for estimating AI capabilities over the next\ndecade, questions for forecasting five scenarios of transformative AI and\nquestions concerning the impact of computational resources in AI research.\nRespondents indicated a median of 21.5% of human tasks (i.e., all tasks that\nhumans are currently paid to do) can be feasibly automated now, and that this\nfigure would rise to 40% in 5 years and 60% in 10 years. Median forecasts\nindicated a 50% probability of AI systems being capable of automating 90% of\ncurrent human tasks in 25 years and 99% of current human tasks in 50 years. The\nconference of attendance was found to have a statistically significant impact\non all forecasts, with attendees of HLAI providing more optimistic timelines\nwith less uncertainty. These findings suggest that AI experts expect major\nadvances in AI technology to continue over the next decade to a degree that\nwill likely have profound transformative impacts on society.", "author_comment": "11 pages, 4 figures, 2 tables", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.09453": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.09453v2", "post_title": "Image Synthesis with a Single (Robust) Classifier", "authors": ["Shibani Santurkar", "Dimitris Tsipras", "Brandon Tran", "Andrew Ilyas", "Logan Engstrom", "Aleksander Madry"], "date_published": "2019-06-06 09:12:08+00:00", "data_last_modified": "2019-08-08 15:47:42+00:00", "url": "http://arxiv.org/abs/1906.09453v2", "abstract": "We show that the basic classification framework alone can be used to tackle\nsome of the most challenging tasks in image synthesis. In contrast to other\nstate-of-the-art approaches, the toolkit we develop is rather minimal: it uses\na single, off-the-shelf classifier for all these tasks. The crux of our\napproach is that we train this classifier to be adversarially robust. It turns\nout that adversarial robustness is precisely what we need to directly\nmanipulate salient features of the input. Overall, our findings demonstrate the\nutility of robustness in the broader machine learning context. Code and models\nfor our experiments can be found at https://git.io/robust-apps.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2103.03872": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2103.03872v1", "post_title": "Rissanen Data Analysis: Examining Dataset Characteristics via Description Length", "authors": ["Ethan Perez", "Douwe Kiela", "Kyunghyun Cho"], "date_published": "2021-03-05 18:58:32+00:00", "data_last_modified": "2021-03-05 18:58:32+00:00", "url": "http://arxiv.org/abs/2103.03872v1", "abstract": "We introduce a method to determine if a certain capability helps to achieve\nan accurate model of given data. We view labels as being generated from the\ninputs by a program composed of subroutines with different capabilities, and we\nposit that a subroutine is useful if and only if the minimal program that\ninvokes it is shorter than the one that does not. Since minimum program length\nis uncomputable, we instead estimate the labels' minimum description length\n(MDL) as a proxy, giving us a theoretically-grounded method for analyzing\ndataset characteristics. We call the method Rissanen Data Analysis (RDA) after\nthe father of MDL, and we showcase its applicability on a wide variety of\nsettings in NLP, ranging from evaluating the utility of generating subquestions\nbefore answering a question, to analyzing the value of rationales and\nexplanations, to investigating the importance of different parts of speech, and\nuncovering dataset gender bias.", "author_comment": "Code at https://github.com/ethanjperez/rda along with a script to run\n  RDA on your own dataset", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.06956": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.06956v1", "post_title": "Human irrationality: both bad and good for reward inference", "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "date_published": "2021-11-12 21:44:15+00:00", "data_last_modified": "2021-11-12 21:44:15+00:00", "url": "http://arxiv.org/abs/2111.06956v1", "abstract": "Assuming humans are (approximately) rational enables robots to infer reward\nfunctions by observing human behavior. But people exhibit a wide array of\nirrationalities, and our goal with this work is to better understand the effect\nthey can have on reward inference. The challenge with studying this effect is\nthat there are many types of irrationality, with varying degrees of\nmathematical formalization. We thus operationalize irrationality in the\nlanguage of MDPs, by altering the Bellman optimality equation, and use this\nframework to study how these alterations would affect inference.\n  We find that wrongly modeling a systematically irrational human as\nnoisy-rational performs a lot worse than correctly capturing these biases -- so\nmuch so that it can be better to skip inference altogether and stick to the\nprior! More importantly, we show that an irrational human, when correctly\nmodelled, can communicate more information about the reward than a perfectly\nrational human can. That is, if a robot has the correct model of a human's\nirrationality, it can make an even stronger inference than it ever could if the\nhuman were rational. Irrationality fundamentally helps rather than hinder\nreward inference, but it needs to be correctly accounted for.", "author_comment": "12 pages, 10 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.10272": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.10272v2", "post_title": "Evaluating and Understanding the Robustness of Adversarial Logit Pairing", "authors": ["Logan Engstrom", "Andrew Ilyas", "Anish Athalye"], "date_published": "2018-07-26 17:58:26+00:00", "data_last_modified": "2018-11-23 19:07:57+00:00", "url": "http://arxiv.org/abs/1807.10272v2", "abstract": "We evaluate the robustness of Adversarial Logit Pairing, a recently proposed\ndefense against adversarial examples. We find that a network trained with\nAdversarial Logit Pairing achieves 0.6% accuracy in the threat model in which\nthe defense is considered. We provide a brief overview of the defense and the\nthreat models/claims considered, as well as a discussion of the methodology and\nresults of our attack, which may offer insights into the reasons underlying the\nvulnerability of ALP to adversarial attack.", "author_comment": "NeurIPS SECML 2018. Source code at\n  https://github.com/labsix/adversarial-logit-pairing-analysis", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.CR", "cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2202.11233": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2202.11233v1", "post_title": "Retrieval Augmented Classification for Long-Tail Visual Recognition", "authors": ["Alexander Long", "Wei Yin", "Thalaiyasingam Ajanthan", "Vu Nguyen", "Pulak Purkait", "Ravi Garg", "Alan Blair", "Chunhua Shen", "Anton van den Hengel"], "date_published": "2022-02-22 23:40:51+00:00", "data_last_modified": "2022-02-22 23:40:51+00:00", "url": "http://arxiv.org/abs/2202.11233v1", "abstract": "We introduce Retrieval Augmented Classification (RAC), a generic approach to\naugmenting standard image classification pipelines with an explicit retrieval\nmodule. RAC consists of a standard base image encoder fused with a parallel\nretrieval branch that queries a non-parametric external memory of pre-encoded\nimages and associated text snippets. We apply RAC to the problem of long-tail\nclassification and demonstrate a significant improvement over previous\nstate-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7%\nrespectively), despite using only the training datasets themselves as the\nexternal information source. We demonstrate that RAC's retrieval module,\nwithout prompting, learns a high level of accuracy on tail classes. This, in\nturn, frees the base encoder to focus on common classes, and improve its\nperformance thereon. RAC represents an alternative approach to utilizing large,\npretrained models without requiring fine-tuning, as well as a first step\ntowards more effectively making use of external memory within common computer\nvision architectures.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.09882": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.09882v5", "post_title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments", "authors": ["Mathilde Caron", "Ishan Misra", "Julien Mairal", "Priya Goyal", "Piotr Bojanowski", "Armand Joulin"], "date_published": "2020-06-17 14:00:42+00:00", "data_last_modified": "2021-01-08 17:01:05+00:00", "url": "http://arxiv.org/abs/2006.09882v5", "abstract": "Unsupervised image representations have significantly reduced the gap with\nsupervised pretraining, notably with the recent achievements of contrastive\nlearning methods. These contrastive methods typically work online and rely on a\nlarge number of explicit pairwise feature comparisons, which is computationally\nchallenging. In this paper, we propose an online algorithm, SwAV, that takes\nadvantage of contrastive methods without requiring to compute pairwise\ncomparisons. Specifically, our method simultaneously clusters the data while\nenforcing consistency between cluster assignments produced for different\naugmentations (or views) of the same image, instead of comparing features\ndirectly as in contrastive learning. Simply put, we use a swapped prediction\nmechanism where we predict the cluster assignment of a view from the\nrepresentation of another view. Our method can be trained with large and small\nbatches and can scale to unlimited amounts of data. Compared to previous\ncontrastive methods, our method is more memory efficient since it does not\nrequire a large memory bank or a special momentum network. In addition, we also\npropose a new data augmentation strategy, multi-crop, that uses a mix of views\nwith different resolutions in place of two full-resolution views, without\nincreasing the memory or compute requirements much. We validate our findings by\nachieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as\nsurpassing supervised pretraining on all the considered transfer tasks.", "author_comment": "NeurIPS 2020", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2108.07258": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2108.07258v2", "post_title": "On the Opportunities and Risks of Foundation Models", "authors": ["Rishi Bommasani", "Drew A. Hudson", "Ehsan Adeli", "Russ Altman", "Simran Arora", "Sydney von Arx", "Michael S. Bernstein", "Jeannette Bohg", "Antoine Bosselut", "Emma Brunskill", "Erik Brynjolfsson", "Shyamal Buch", "Dallas Card", "Rodrigo Castellon", "Niladri Chatterji", "Annie Chen", "Kathleen Creel", "Jared Quincy Davis", "Dora Demszky", "Chris Donahue", "Moussa Doumbouya", "Esin Durmus", "Stefano Ermon", "John Etchemendy", "Kawin Ethayarajh", "Li Fei-Fei", "Chelsea Finn", "Trevor Gale", "Lauren Gillespie", "Karan Goel", "Noah Goodman", "Shelby Grossman", "Neel Guha", "Tatsunori Hashimoto", "Peter Henderson", "John Hewitt", "Daniel E. Ho", "Jenny Hong", "Kyle Hsu", "Jing Huang", "Thomas Icard", "Saahil Jain", "Dan Jurafsky", "Pratyusha Kalluri", "Siddharth Karamcheti", "Geoff Keeling", "Fereshte Khani", "Omar Khattab", "Pang Wei Koh", "Mark Krass", "Ranjay Krishna", "Rohith Kuditipudi", "Ananya Kumar", "Faisal Ladhak", "Mina Lee", "Tony Lee", "Jure Leskovec", "Isabelle Levent", "Xiang Lisa Li", "Xuechen Li", "Tengyu Ma", "Ali Malik", "Christopher D. Manning", "Suvir Mirchandani", "Eric Mitchell", "Zanele Munyikwa", "Suraj Nair", "Avanika Narayan", "Deepak Narayanan", "Ben Newman", "Allen Nie", "Juan Carlos Niebles", "Hamed Nilforoshan", "Julian Nyarko", "Giray Ogut", "Laurel Orr", "Isabel Papadimitriou", "Joon Sung Park", "Chris Piech", "Eva Portelance", "Christopher Potts", "Aditi Raghunathan", "Rob Reich", "Hongyu Ren", "Frieda Rong", "Yusuf Roohani", "Camilo Ruiz", "Jack Ryan", "Christopher R\u00e9", "Dorsa Sadigh", "Shiori Sagawa", "Keshav Santhanam", "Andy Shih", "Krishnan Srinivasan", "Alex Tamkin", "Rohan Taori", "Armin W. Thomas", "Florian Tram\u00e8r", "Rose E. Wang", "William Wang", "Bohan Wu", "Jiajun Wu", "Yuhuai Wu", "Sang Michael Xie", "Michihiro Yasunaga", "Jiaxuan You", "Matei Zaharia", "Michael Zhang", "Tianyi Zhang", "Xikun Zhang", "Yuhui Zhang", "Lucia Zheng", "Kaitlyn Zhou", "Percy Liang"], "date_published": "2021-08-16 17:50:08+00:00", "data_last_modified": "2021-08-18 17:07:22+00:00", "url": "http://arxiv.org/abs/2108.07258v2", "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT,\nDALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a\nwide range of downstream tasks. We call these models foundation models to\nunderscore their critically central yet incomplete character. This report\nprovides a thorough account of the opportunities and risks of foundation\nmodels, ranging from their capabilities (e.g., language, vision, robotics,\nreasoning, human interaction) and technical principles(e.g., model\narchitectures, training procedures, data, systems, security, evaluation,\ntheory) to their applications (e.g., law, healthcare, education) and societal\nimpact (e.g., inequity, misuse, economic and environmental impact, legal and\nethical considerations). Though foundation models are based on standard deep\nlearning and transfer learning, their scale results in new emergent\ncapabilities,and their effectiveness across so many tasks incentivizes\nhomogenization. Homogenization provides powerful leverage but demands caution,\nas the defects of the foundation model are inherited by all the adapted models\ndownstream. Despite the impending widespread deployment of foundation models,\nwe currently lack a clear understanding of how they work, when they fail, and\nwhat they are even capable of due to their emergent properties. To tackle these\nquestions, we believe much of the critical research on foundation models will\nrequire deep interdisciplinary collaboration commensurate with their\nfundamentally sociotechnical nature.", "author_comment": "Authored by the Center for Research on Foundation Models (CRFM) at\n  the Stanford Institute for Human-Centered Artificial Intelligence (HAI)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.07802": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.07802v2", "post_title": "Playing the Game of Universal Adversarial Perturbations", "authors": ["Julien Perolat", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "date_published": "2018-09-20 18:48:36+00:00", "data_last_modified": "2018-09-25 20:16:45+00:00", "url": "http://arxiv.org/abs/1809.07802v2", "abstract": "We study the problem of learning classifiers robust to universal adversarial\nperturbations. While prior work approaches this problem via robust\noptimization, adversarial training, or input transformation, we instead phrase\nit as a two-player zero-sum game. In this new formulation, both players\nsimultaneously play the same game, where one player chooses a classifier that\nminimizes a classification loss whilst the other player creates an adversarial\nperturbation that increases the same loss when applied to every sample in the\ntraining set. By observing that performing a classification (respectively\ncreating adversarial samples) is the best response to the other player, we\npropose a novel extension of a game-theoretic algorithm, namely fictitious\nplay, to the domain of training robust classifiers. Finally, we empirically\nshow the robustness and versatility of our approach in two defence scenarios\nwhere universal attacks are performed on several image classification datasets\n-- CIFAR10, CIFAR100 and ImageNet.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.00069": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.00069v3", "post_title": "Explaining Explanations: An Overview of Interpretability of Machine Learning", "authors": ["Leilani H. Gilpin", "David Bau", "Ben Z. Yuan", "Ayesha Bajwa", "Michael Specter", "Lalana Kagal"], "date_published": "2018-05-31 19:48:00+00:00", "data_last_modified": "2019-02-03 21:06:50+00:00", "url": "http://arxiv.org/abs/1806.00069v3", "abstract": "There has recently been a surge of work in explanatory artificial\nintelligence (XAI). This research area tackles the important problem that\ncomplex machines and algorithms often cannot provide insights into their\nbehavior and thought processes. XAI allows users and parts of the internal\nsystem to be more transparent, providing explanations of their decisions in\nsome level of detail. These explanations are important to ensure algorithmic\nfairness, identify potential bias/problems in the training data, and to ensure\nthat the algorithms perform as expected. However, explanations produced by\nthese systems is neither standardized nor systematically assessed. In an effort\nto create best practices and identify open challenges, we provide our\ndefinition of explainability and show how it can be used to classify existing\nliterature. We discuss why current approaches to explanatory methods especially\nfor deep neural networks are insufficient. Finally, based on our survey, we\nconclude with suggested future research directions for explanatory artificial\nintelligence.", "author_comment": "The 5th IEEE International Conference on Data Science and Advanced\n  Analytics (DSAA 2018). [Research Track]", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2107.10939": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2107.10939v1", "post_title": "What are you optimizing for? Aligning Recommender Systems with Human Values", "authors": ["Jonathan Stray", "Ivan Vendrov", "Jeremy Nixon", "Steven Adler", "Dylan Hadfield-Menell"], "date_published": "2021-07-22 21:52:43+00:00", "data_last_modified": "2021-07-22 21:52:43+00:00", "url": "http://arxiv.org/abs/2107.10939v1", "abstract": "We describe cases where real recommender systems were modified in the service\nof various human values such as diversity, fairness, well-being, time well\nspent, and factual accuracy. From this we identify the current practice of\nvalues engineering: the creation of classifiers from human-created data with\nvalue-based labels. This has worked in practice for a variety of issues, but\nproblems are addressed one at a time, and users and other stakeholders have\nseldom been involved. Instead, we look to AI alignment work for approaches that\ncould learn complex values directly from stakeholders, and identify four major\ndirections: useful measures of alignment, participatory design and operation,\ninteractive value learning, and informed deliberative judgments.", "author_comment": "Originally presented at the ICML 2020 Participatory Approaches to\n  Machine Learning workshop", "journal_ref": null, "doi": null, "primary_category": "cs.IR", "categories": ["cs.IR", "cs.CY", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2107.06882": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2107.06882v1", "post_title": "Conservative Objective Models for Effective Offline Model-Based Optimization", "authors": ["Brandon Trabucco", "Aviral Kumar", "Xinyang Geng", "Sergey Levine"], "date_published": "2021-07-14 17:55:28+00:00", "data_last_modified": "2021-07-14 17:55:28+00:00", "url": "http://arxiv.org/abs/2107.06882v1", "abstract": "Computational design problems arise in a number of settings, from synthetic\nbiology to computer architectures. In this paper, we aim to solve data-driven\nmodel-based optimization (MBO) problems, where the goal is to find a design\ninput that maximizes an unknown objective function provided access to only a\nstatic dataset of prior experiments. Such data-driven optimization procedures\nare the only practical methods in many real-world domains where active data\ncollection is expensive (e.g., when optimizing over proteins) or dangerous\n(e.g., when optimizing over aircraft designs). Typical methods for MBO that\noptimize the design against a learned model suffer from distributional shift:\nit is easy to find a design that \"fools\" the model into predicting a high\nvalue. To overcome this, we propose conservative objective models (COMs), a\nmethod that learns a model of the objective function that lower bounds the\nactual value of the ground-truth objective on out-of-distribution inputs, and\nuses it for optimization. Structurally, COMs resemble adversarial training\nmethods used to overcome adversarial examples. COMs are simple to implement and\noutperform a number of existing methods on a wide range of MBO problems,\nincluding optimizing protein sequences, robot morphologies, neural network\nweights, and superconducting materials.", "author_comment": "ICML 2021. First two authors contributed equally. Code at:\n  https://github.com/brandontrabucco/design-baselines/blob/c65a53fe1e6567b740f0adf60c5db9921c1f2330/design_baselines/coms_cleaned/__init__.py", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.02503": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.02503v1", "post_title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models", "authors": ["Alex Tamkin", "Miles Brundage", "Jack Clark", "Deep Ganguli"], "date_published": "2021-02-04 09:27:04+00:00", "data_last_modified": "2021-02-04 09:27:04+00:00", "url": "http://arxiv.org/abs/2102.02503v1", "abstract": "On October 14th, 2020, researchers from OpenAI, the Stanford Institute for\nHuman-Centered Artificial Intelligence, and other universities convened to\ndiscuss open research questions surrounding GPT-3, the largest\npublicly-disclosed dense language model at the time. The meeting took place\nunder Chatham House Rules. Discussants came from a variety of research\nbackgrounds including computer science, linguistics, philosophy, political\nscience, communications, cyber policy, and more. Broadly, the discussion\ncentered around two main questions: 1) What are the technical capabilities and\nlimitations of large language models? 2) What are the societal effects of\nwidespread use of large language models? Here, we provide a detailed summary of\nthe discussion organized by the two themes above.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.07379": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.07379v6", "post_title": "Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting", "authors": ["Jun Shu", "Qi Xie", "Lixuan Yi", "Qian Zhao", "Sanping Zhou", "Zongben Xu", "Deyu Meng"], "date_published": "2019-02-20 02:29:55+00:00", "data_last_modified": "2019-09-27 02:48:20+00:00", "url": "http://arxiv.org/abs/1902.07379v6", "abstract": "Current deep neural networks (DNNs) can easily overfit to biased training\ndata with corrupted labels or class imbalance. Sample re-weighting strategy is\ncommonly used to alleviate this issue by designing a weighting function mapping\nfrom training loss to sample weight, and then iterating between weight\nrecalculating and classifier updating. Current approaches, however, need\nmanually pre-specify the weighting function as well as its additional\nhyper-parameters. It makes them fairly hard to be generally applied in practice\ndue to the significant variation of proper weighting schemes relying on the\ninvestigated problem and training data. To address this issue, we propose a\nmethod capable of adaptively learning an explicit weighting function directly\nfrom data. The weighting function is an MLP with one hidden layer, constituting\na universal approximator to almost any continuous functions, making the method\nable to fit a wide range of weighting functions including those assumed in\nconventional research. Guided by a small amount of unbiased meta-data, the\nparameters of the weighting function can be finely updated simultaneously with\nthe learning process of the classifiers. Synthetic and real experiments\nsubstantiate the capability of our method for achieving proper weighting\nfunctions in class imbalance and noisy label cases, fully complying with the\ncommon settings in traditional methods, and more complicated scenarios beyond\nconventional cases. This naturally leads to its better accuracy than other\nstate-of-the-art methods.", "author_comment": "NeurIPS 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.06162": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.06162v1", "post_title": "Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey", "authors": ["Longlong Jing", "Yingli Tian"], "date_published": "2019-02-16 21:30:18+00:00", "data_last_modified": "2019-02-16 21:30:18+00:00", "url": "http://arxiv.org/abs/1902.06162v1", "abstract": "Large-scale labeled data are generally required to train deep neural networks\nin order to obtain better performance in visual feature learning from images or\nvideos for computer vision applications. To avoid extensive cost of collecting\nand annotating large-scale datasets, as a subset of unsupervised learning\nmethods, self-supervised learning methods are proposed to learn general image\nand video features from large-scale unlabeled data without using any\nhuman-annotated labels. This paper provides an extensive review of deep\nlearning-based self-supervised general visual feature learning methods from\nimages or videos. First, the motivation, general pipeline, and terminologies of\nthis field are described. Then the common deep neural network architectures\nthat used for self-supervised learning are summarized. Next, the main\ncomponents and evaluation metrics of self-supervised learning methods are\nreviewed followed by the commonly used image and video datasets and the\nexisting self-supervised visual feature learning methods. Finally, quantitative\nperformance comparisons of the reviewed methods on benchmark datasets are\nsummarized and discussed for both image and video feature learning. At last,\nthis paper is concluded and lists a set of promising future directions for\nself-supervised visual feature learning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.03235": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.03235v2", "post_title": "Large scale distributed neural network training through online distillation", "authors": ["Rohan Anil", "Gabriel Pereyra", "Alexandre Passos", "Robert Ormandi", "George E. Dahl", "Geoffrey E. Hinton"], "date_published": "2018-04-09 20:56:03+00:00", "data_last_modified": "2020-08-20 22:04:36+00:00", "url": "http://arxiv.org/abs/1804.03235v2", "abstract": "Techniques such as ensembling and distillation promise model quality\nimprovements when paired with almost any base model. However, due to increased\ntest-time cost (for ensembles) and increased complexity of the training\npipeline (for distillation), these techniques are challenging to use in\nindustrial settings. In this paper we explore a variant of distillation which\nis relatively straightforward to use as it does not require a complicated\nmulti-stage setup or many new hyperparameters. Our first claim is that online\ndistillation enables us to use extra parallelism to fit very large datasets\nabout twice as fast. Crucially, we can still speed up training even after we\nhave already reached the point at which additional parallelism provides no\nbenefit for synchronous or asynchronous stochastic gradient descent. Two neural\nnetworks trained on disjoint subsets of the data can share knowledge by\nencouraging each model to agree with the predictions the other model would have\nmade. These predictions can come from a stale version of the other model so\nthey can be safely computed using weights that only rarely get transmitted. Our\nsecond claim is that online distillation is a cost-effective way to make the\nexact predictions of a model dramatically more reproducible. We support our\nclaims using experiments on the Criteo Display Ad Challenge dataset, ImageNet,\nand the largest to-date dataset used for neural language modeling, containing\n$6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data.", "author_comment": "Clarify that implementations should use available parallelism in\n  pseudo-code", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.09725": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.09725v3", "post_title": "Conservative Agency via Attainable Utility Preservation", "authors": ["Alexander Matt Turner", "Dylan Hadfield-Menell", "Prasad Tadepalli"], "date_published": "2019-02-26 04:42:54+00:00", "data_last_modified": "2020-06-10 15:10:04+00:00", "url": "http://arxiv.org/abs/1902.09725v3", "abstract": "Reward functions are easy to misspecify; although designers can make\ncorrections after observing mistakes, an agent pursuing a misspecified reward\nfunction can irreversibly change the state of its environment. If that change\nprecludes optimization of the correctly specified reward function, then\ncorrection is futile. For example, a robotic factory assistant could break\nexpensive equipment due to a reward misspecification; even if the designers\nimmediately correct the reward function, the damage is done. To mitigate this\nrisk, we introduce an approach that balances optimization of the primary reward\nfunction with preservation of the ability to optimize auxiliary reward\nfunctions. Surprisingly, even when the auxiliary reward functions are randomly\ngenerated and therefore uninformative about the correctly specified reward\nfunction, this approach induces conservative, effective behavior.", "author_comment": "Published in AI, Ethics, and Society 2020", "journal_ref": null, "doi": "10.1145/3375627.3375851", "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.07805": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.07805v2", "post_title": "Extracting Training Data from Large Language Models", "authors": ["Nicholas Carlini", "Florian Tramer", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Tom Brown", "Dawn Song", "Ulfar Erlingsson", "Alina Oprea", "Colin Raffel"], "date_published": "2020-12-14 18:39:09+00:00", "data_last_modified": "2021-06-15 17:45:26+00:00", "url": "http://arxiv.org/abs/2012.07805v2", "abstract": "It has become common to publish large (billion parameter) language models\nthat have been trained on private datasets. This paper demonstrates that in\nsuch settings, an adversary can perform a training data extraction attack to\nrecover individual training examples by querying the language model.\n  We demonstrate our attack on GPT-2, a language model trained on scrapes of\nthe public Internet, and are able to extract hundreds of verbatim text\nsequences from the model's training data. These extracted examples include\n(public) personally identifiable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible\neven though each of the above sequences are included in just one document in\nthe training data.\n  We comprehensively evaluate our extraction attack to understand the factors\nthat contribute to its success. Worryingly, we find that larger models are more\nvulnerable than smaller models. We conclude by drawing lessons and discussing\npossible safeguards for training large language models.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CR", "categories": ["cs.CR", "cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.11174": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.11174v1", "post_title": "TanksWorld: A Multi-Agent Environment for AI Safety Research", "authors": ["Corban G. Rivera", "Olivia Lyons", "Arielle Summitt", "Ayman Fatima", "Ji Pak", "William Shao", "Robert Chalmers", "Aryeh Englander", "Edward W. Staley", "I-Jeng Wang", "Ashley J. Llorens"], "date_published": "2020-02-25 21:00:52+00:00", "data_last_modified": "2020-02-25 21:00:52+00:00", "url": "http://arxiv.org/abs/2002.11174v1", "abstract": "The ability to create artificial intelligence (AI) capable of performing\ncomplex tasks is rapidly outpacing our ability to ensure the safe and assured\noperation of AI-enabled systems. Fortunately, a landscape of AI safety research\nis emerging in response to this asymmetry and yet there is a long way to go. In\nparticular, recent simulation environments created to illustrate AI safety\nrisks are relatively simple or narrowly-focused on a particular issue. Hence,\nwe see a critical need for AI safety research environments that abstract\nessential aspects of complex real-world applications. In this work, we\nintroduce the AI safety TanksWorld as an environment for AI safety research\nwith three essential aspects: competing performance objectives, human-machine\nteaming, and multi-agent competition. The AI safety TanksWorld aims to\naccelerate the advancement of safe multi-agent decision-making algorithms by\nproviding a software framework to support competitions with both system\nperformance and safety objectives. As a work in progress, this paper introduces\nour research objectives and learning environment with reference code and\nbaseline performance metrics to follow in a future work.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.MA"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2201.08102": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2201.08102v2", "post_title": "Safe Deep RL in 3D Environments using Human Feedback", "authors": ["Matthew Rahtz", "Vikrant Varma", "Ramana Kumar", "Zachary Kenton", "Shane Legg", "Jan Leike"], "date_published": "2022-01-20 10:26:34+00:00", "data_last_modified": "2022-01-21 16:10:14+00:00", "url": "http://arxiv.org/abs/2201.08102v2", "abstract": "Agents should avoid unsafe behaviour during both training and deployment.\nThis typically requires a simulator and a procedural specification of unsafe\nbehaviour. Unfortunately, a simulator is not always available, and procedurally\nspecifying constraints can be difficult or impossible for many real-world\ntasks. A recently introduced technique, ReQueST, aims to solve this problem by\nlearning a neural simulator of the environment from safe human trajectories,\nthen using the learned simulator to efficiently learn a reward model from human\nfeedback. However, it is yet unknown whether this approach is feasible in\ncomplex 3D environments with feedback obtained from real humans - whether\nsufficient pixel-based neural simulator quality can be achieved, and whether\nthe human data requirements are viable in terms of both quantity and quality.\nIn this paper we answer this question in the affirmative, using ReQueST to\ntrain an agent to perform a 3D first-person object collection task using data\nentirely from human contractors. We show that the resulting agent exhibits an\norder of magnitude reduction in unsafe behaviour compared to standard\nreinforcement learning.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.03453": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.03453v4", "post_title": "The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities", "authors": ["Joel Lehman", "Jeff Clune", "Dusan Misevic", "Christoph Adami", "Lee Altenberg", "Julie Beaulieu", "Peter J. Bentley", "Samuel Bernard", "Guillaume Beslon", "David M. Bryson", "Patryk Chrabaszcz", "Nick Cheney", "Antoine Cully", "Stephane Doncieux", "Fred C. Dyer", "Kai Olav Ellefsen", "Robert Feldt", "Stephan Fischer", "Stephanie Forrest", "Antoine Fr\u00e9noy", "Christian Gagn\u00e9", "Leni Le Goff", "Laura M. Grabowski", "Babak Hodjat", "Frank Hutter", "Laurent Keller", "Carole Knibbe", "Peter Krcah", "Richard E. Lenski", "Hod Lipson", "Robert MacCurdy", "Carlos Maestre", "Risto Miikkulainen", "Sara Mitri", "David E. Moriarty", "Jean-Baptiste Mouret", "Anh Nguyen", "Charles Ofria", "Marc Parizeau", "David Parsons", "Robert T. Pennock", "William F. Punch", "Thomas S. Ray", "Marc Schoenauer", "Eric Shulte", "Karl Sims", "Kenneth O. Stanley", "Fran\u00e7ois Taddei", "Danesh Tarapore", "Simon Thibault", "Westley Weimer", "Richard Watson", "Jason Yosinski"], "date_published": "2018-03-09 10:17:18+00:00", "data_last_modified": "2019-11-21 23:58:46+00:00", "url": "http://arxiv.org/abs/1803.03453v4", "abstract": "Biological evolution provides a creative fount of complex and subtle\nadaptations, often surprising the scientists who discover them. However,\nbecause evolution is an algorithmic process that transcends the substrate in\nwhich it occurs, evolution's creativity is not limited to nature. Indeed, many\nresearchers in the field of digital evolution have observed their evolving\nalgorithms and organisms subverting their intentions, exposing unrecognized\nbugs in their code, producing unexpected adaptations, or exhibiting outcomes\nuncannily convergent with ones in nature. Such stories routinely reveal\ncreativity by evolution in these digital worlds, but they rarely fit into the\nstandard scientific narrative. Instead they are often treated as mere obstacles\nto be overcome, rather than results that warrant study in their own right. The\nstories themselves are traded among researchers through oral tradition, but\nthat mode of information transmission is inefficient and prone to error and\noutright loss. Moreover, the fact that these stories tend to be shared only\namong practitioners means that many natural scientists do not realize how\ninteresting and lifelike digital organisms are and how natural their evolution\ncan be. To our knowledge, no collection of such anecdotes has been published\nbefore. This paper is the crowd-sourced product of researchers in the fields of\nartificial life and evolutionary computation who have provided first-hand\naccounts of such cases. It thus serves as a written, fact-checked collection of\nscientifically important and even entertaining stories. In doing so we also\npresent here substantial evidence that the existence and importance of\nevolutionary surprises extends beyond the natural world, and may indeed be a\nuniversal property of all complex evolving systems.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.NE", "categories": ["cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2110.07574": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2110.07574v1", "post_title": "Delphi: Towards Machine Ethics and Norms", "authors": ["Liwei Jiang", "Jena D. Hwang", "Chandra Bhagavatula", "Ronan Le Bras", "Maxwell Forbes", "Jon Borchardt", "Jenny Liang", "Oren Etzioni", "Maarten Sap", "Yejin Choi"], "date_published": "2021-10-14 17:38:12+00:00", "data_last_modified": "2021-10-14 17:38:12+00:00", "url": "http://arxiv.org/abs/2110.07574v1", "abstract": "What would it take to teach a machine to behave ethically? While broad\nethical rules may seem straightforward to state (\"thou shalt not kill\"),\napplying such rules to real-world situations is far more complex. For example,\nwhile \"helping a friend\" is generally a good thing to do, \"helping a friend\nspread fake news\" is not. We identify four underlying challenges towards\nmachine ethics and norms: (1) an understanding of moral precepts and social\nnorms; (2) the ability to perceive real-world situations visually or by reading\nnatural language descriptions; (3) commonsense reasoning to anticipate the\noutcome of alternative actions in different contexts; (4) most importantly, the\nability to make ethical judgments given the interplay between competing values\nand their grounding in different contexts (e.g., the right to freedom of\nexpression vs. preventing the spread of fake news).\n  Our paper begins to address these questions within the deep learning\nparadigm. Our prototype model, Delphi, demonstrates strong promise of\nlanguage-based commonsense moral reasoning, with up to 92.1% accuracy vetted by\nhumans. This is in stark contrast to the zero-shot performance of GPT-3 of\n52.3%, which suggests that massive scale alone does not endow pre-trained\nneural language models with human values. Thus, we present Commonsense Norm\nBank, a moral textbook customized for machines, which compiles 1.7M examples of\npeople's ethical judgments on a broad spectrum of everyday situations. In\naddition to the new resources and baseline performances for future research,\nour study provides new insights that lead to several important open research\nquestions: differentiating between universal human values and personal values,\nmodeling different moral frameworks, and explainable, consistent approaches to\nmachine ethics.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1910.13369": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1910.13369v2", "post_title": "A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning", "authors": ["Somil Bansal", "Andrea Bajcsy", "Ellis Ratner", "Anca D. Dragan", "Claire J. Tomlin"], "date_published": "2019-10-29 16:34:00+00:00", "data_last_modified": "2020-04-05 21:10:25+00:00", "url": "http://arxiv.org/abs/1910.13369v2", "abstract": "Real-world autonomous systems often employ probabilistic predictive models of\nhuman behavior during planning to reason about their future motion. Since\naccurately modeling human behavior a priori is challenging, such models are\noften parameterized, enabling the robot to adapt predictions based on\nobservations by maintaining a distribution over the model parameters. Although\nthis enables data and priors to improve the human model, observation models are\ndifficult to specify and priors may be incorrect, leading to erroneous state\npredictions that can degrade the safety of the robot motion plan. In this work,\nwe seek to design a predictor which is more robust to misspecified models and\npriors, but can still leverage human behavioral data online to reduce\nconservatism in a safe way. To do this, we cast human motion prediction as a\nHamilton-Jacobi reachability problem in the joint state space of the human and\nthe belief over the model parameters. We construct a new continuous-time\ndynamical system, where the inputs are the observations of human behavior, and\nthe dynamics include how the belief over the model parameters change. The\nresults of this reachability computation enable us to both analyze the effect\nof incorrect priors on future predictions in continuous state and time, as well\nas to make predictions of the human state in the future. We compare our\napproach to the worst-case forward reachable set and a stochastic predictor\nwhich uses Bayesian inference and produces full future state distributions. Our\ncomparisons in simulation and in hardware demonstrate how our framework can\nenable robust planning while not being overly conservative, even when the human\nmodel is inaccurate.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.02334": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.02334v6", "post_title": "Unsupervised Learning via Meta-Learning", "authors": ["Kyle Hsu", "Sergey Levine", "Chelsea Finn"], "date_published": "2018-10-04 17:29:17+00:00", "data_last_modified": "2019-03-21 23:43:47+00:00", "url": "http://arxiv.org/abs/1810.02334v6", "abstract": "A central goal of unsupervised learning is to acquire representations from\nunlabeled data or experience that can be used for more effective learning of\ndownstream tasks from modest amounts of labeled data. Many prior unsupervised\nlearning works aim to do so by developing proxy objectives based on\nreconstruction, disentanglement, prediction, and other metrics. Instead, we\ndevelop an unsupervised meta-learning method that explicitly optimizes for the\nability to learn a variety of tasks from small amounts of data. To do so, we\nconstruct tasks from unlabeled data in an automatic way and run meta-learning\nover the constructed tasks. Surprisingly, we find that, when integrated with\nmeta-learning, relatively simple task construction mechanisms, such as\nclustering embeddings, lead to good performance on a variety of downstream,\nhuman-specified tasks. Our experiments across four image datasets indicate that\nour unsupervised meta-learning approach acquires a learning algorithm without\nany labeled data that is applicable to a wide range of downstream\nclassification tasks, improving upon the embedding learned by four prior\nunsupervised learning methods.", "author_comment": "ICLR 2019 camera-ready. 24 pages, 2 figures, links to code available\n  at https://sites.google.com/view/unsupervised-via-meta", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.00222": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.00222v3", "post_title": "Meta-Learning Update Rules for Unsupervised Representation Learning", "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "date_published": "2018-03-31 22:44:28+00:00", "data_last_modified": "2019-02-26 05:26:00+00:00", "url": "http://arxiv.org/abs/1804.00222v3", "abstract": "A major goal of unsupervised learning is to discover data representations\nthat are useful for subsequent tasks, without access to supervised labels\nduring training. Typically, this involves minimizing a surrogate objective,\nsuch as the negative log likelihood of a generative model, with the hope that\nrepresentations useful for subsequent tasks will arise as a side effect. In\nthis work, we propose instead to directly target later desired tasks by\nmeta-learning an unsupervised learning rule which leads to representations\nuseful for those tasks. Specifically, we target semi-supervised classification\nperformance, and we meta-learn an algorithm -- an unsupervised weight update\nrule -- that produces representations useful for this task. Additionally, we\nconstrain our unsupervised update rule to a be a biologically-motivated,\nneuron-local function, which enables it to generalize to different neural\nnetwork architectures, datasets, and data modalities. We show that the\nmeta-learned update rule produces useful features and sometimes outperforms\nexisting unsupervised learning techniques. We further show that the\nmeta-learned unsupervised update rule generalizes to train networks with\ndifferent widths, depths, and nonlinearities. It also generalizes to train on\ndata with randomly permuted input dimensions and even generalizes from image\ndatasets to a text task.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.01293": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.01293v1", "post_title": "Scaling Laws for Transfer", "authors": ["Danny Hernandez", "Jared Kaplan", "Tom Henighan", "Sam McCandlish"], "date_published": "2021-02-02 04:07:38+00:00", "data_last_modified": "2021-02-02 04:07:38+00:00", "url": "http://arxiv.org/abs/2102.01293v1", "abstract": "We study empirical scaling laws for transfer learning between distributions\nin an unsupervised, fine-tuning setting. When we train increasingly large\nneural networks from-scratch on a fixed-size dataset, they eventually become\ndata-limited and stop improving in performance (cross-entropy loss). When we do\nthe same for models pre-trained on a large language dataset, the slope in\nperformance gains is merely reduced rather than going to zero. We calculate the\neffective data \"transferred\" from pre-training by determining how much data a\ntransformer of the same size would have required to achieve the same loss when\ntraining from scratch. In other words, we focus on units of data while holding\neverything else fixed. We find that the effective data transferred is described\nwell in the low data regime by a power-law of parameter count and fine-tuning\ndataset size. We believe the exponents in these power-laws correspond to\nmeasures of the generality of a model and proximity of distributions (in a\ndirected rather than symmetric sense). We find that pre-training effectively\nmultiplies the fine-tuning dataset size. Transfer, like overall performance,\nscales predictably in terms of parameters, data, and compute.", "author_comment": "19 pages, 15 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2202.09931": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2202.09931v1", "post_title": "Deconstructing Distributions: A Pointwise Framework of Learning", "authors": ["Gal Kaplun", "Nikhil Ghosh", "Saurabh Garg", "Boaz Barak", "Preetum Nakkiran"], "date_published": "2022-02-20 23:25:28+00:00", "data_last_modified": "2022-02-20 23:25:28+00:00", "url": "http://arxiv.org/abs/2202.09931v1", "abstract": "In machine learning, we traditionally evaluate the performance of a single\nmodel, averaged over a collection of test inputs. In this work, we propose a\nnew approach: we measure the performance of a collection of models when\nevaluated on a $\\textit{single input point}$. Specifically, we study a point's\n$\\textit{profile}$: the relationship between models' average performance on the\ntest distribution and their pointwise performance on this individual point. We\nfind that profiles can yield new insights into the structure of both models and\ndata -- in and out-of-distribution. For example, we empirically show that real\ndata distributions consist of points with qualitatively different profiles. On\none hand, there are \"compatible\" points with strong correlation between the\npointwise and average performance. On the other hand, there are points with\nweak and even $\\textit{negative}$ correlation: cases where improving overall\nmodel accuracy actually $\\textit{hurts}$ performance on these inputs. We prove\nthat these experimental observations are inconsistent with the predictions of\nseveral simplified models of learning proposed in prior work. As an\napplication, we use profiles to construct a dataset we call CIFAR-10-NEG: a\nsubset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is\n$\\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This\nillustrates, for the first time, an OOD dataset that completely inverts\n\"accuracy-on-the-line\" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,\nLiang, Carmon, and Schmidt 2021)", "author_comment": "GK and NG contributed equally", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.11932": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.11932v6", "post_title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment", "authors": ["Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits"], "date_published": "2019-07-27 15:07:04+00:00", "data_last_modified": "2020-04-08 23:10:10+00:00", "url": "http://arxiv.org/abs/1907.11932v6", "abstract": "Machine learning algorithms are often vulnerable to adversarial examples that\nhave imperceptible alterations from the original counterparts but can fool the\nstate-of-the-art models. It is helpful to evaluate or even improve the\nrobustness of these models by exposing the maliciously crafted adversarial\nexamples. In this paper, we present TextFooler, a simple but strong baseline to\ngenerate natural adversarial text. By applying it to two fundamental natural\nlanguage tasks, text classification and textual entailment, we successfully\nattacked three target models, including the powerful pre-trained BERT, and the\nwidely used convolutional and recurrent neural networks. We demonstrate the\nadvantages of this framework in three ways: (1) effective---it outperforms\nstate-of-the-art attacks in terms of success rate and perturbation rate, (2)\nutility-preserving---it preserves semantic content and grammaticality, and\nremains correctly classified by humans, and (3) efficient---it generates\nadversarial text with computational complexity linear to the text length. *The\ncode, pre-trained target models, and test examples are available at\nhttps://github.com/jind11/TextFooler.", "author_comment": "AAAI 2020 (Oral)", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1604.05280": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1604.05280v4", "post_title": "Asymptotic Convergence in Online Learning with Unbounded Delays", "authors": ["Scott Garrabrant", "Nate Soares", "Jessica Taylor"], "date_published": "2016-04-18 19:04:59+00:00", "data_last_modified": "2016-09-07 18:43:24+00:00", "url": "http://arxiv.org/abs/1604.05280v4", "abstract": "We study the problem of predicting the results of computations that are too\nexpensive to run, via the observation of the results of smaller computations.\nWe model this as an online learning problem with delayed feedback, where the\nlength of the delay is unbounded, which we study mainly in a stochastic\nsetting. We show that in this setting, consistency is not possible in general,\nand that optimal forecasters might not have average regret going to zero.\nHowever, it is still possible to give algorithms that converge asymptotically\nto Bayes-optimal predictions, by evaluating forecasters on specific sparse\nindependent subsequences of their predictions. We give an algorithm that does\nthis, which converges asymptotically on good behavior, and give very weak\nbounds on how long it takes to converge. We then relate our results back to the\nproblem of predicting large computations in a deterministic setting.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "math.PR"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2002.08484": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2002.08484v3", "post_title": "Estimating Training Data Influence by Tracing Gradient Descent", "authors": ["Garima Pruthi", "Frederick Liu", "Mukund Sundararajan", "Satyen Kale"], "date_published": "2020-02-19 22:40:32+00:00", "data_last_modified": "2020-11-14 18:47:35+00:00", "url": "http://arxiv.org/abs/2002.08484v3", "abstract": "We introduce a method called TracIn that computes the influence of a training\nexample on a prediction made by the model. The idea is to trace how the loss on\nthe test point changes during the training process whenever the training\nexample of interest was utilized. We provide a scalable implementation of\nTracIn via: (a) a first-order gradient approximation to the exact computation,\n(b) saved checkpoints of standard training procedures, and (c) cherry-picking\nlayers of a deep neural network. In contrast with previously proposed methods,\nTracIn is simple to implement; all it needs is the ability to work with\ngradients, checkpoints, and loss functions. The method is general. It applies\nto any machine learning model trained using stochastic gradient descent or a\nvariant of it, agnostic of architecture, domain and task. We expect the method\nto be widely useful within processes that study and improve training data.", "author_comment": "NeurIPS 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2107.12544": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2107.12544v1", "post_title": "Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration, and Planning", "authors": ["Pedro A. Tsividis", "Joao Loula", "Jake Burga", "Nathan Foss", "Andres Campero", "Thomas Pouncy", "Samuel J. Gershman", "Joshua B. Tenenbaum"], "date_published": "2021-07-27 01:38:13+00:00", "data_last_modified": "2021-07-27 01:38:13+00:00", "url": "http://arxiv.org/abs/2107.12544v1", "abstract": "Reinforcement learning (RL) studies how an agent comes to achieve reward in\nan environment through interactions over time. Recent advances in machine RL\nhave surpassed human expertise at the world's oldest board games and many\nclassic video games, but they require vast quantities of experience to learn\nsuccessfully -- none of today's algorithms account for the human ability to\nlearn so many different tasks, so quickly. Here we propose a new approach to\nthis challenge based on a particularly strong form of model-based RL which we\ncall Theory-Based Reinforcement Learning, because it uses human-like intuitive\ntheories -- rich, abstract, causal models of physical objects, intentional\nagents, and their interactions -- to explore and model an environment, and plan\neffectively to achieve task goals. We instantiate the approach in a video game\nplaying agent called EMPA (the Exploring, Modeling, and Planning Agent), which\nperforms Bayesian inference to learn probabilistic generative models expressed\nas programs for a game-engine simulator, and runs internal simulations over\nthese models to support efficient object-based, relational exploration and\nheuristic planning. EMPA closely matches human learning efficiency on a suite\nof 90 challenging Atari-style video games, learning new games in just minutes\nof game play and generalizing robustly to new game situations and new levels.\nThe model also captures fine-grained structure in people's exploration\ntrajectories and learning dynamics. Its design and behavior suggest a way\nforward for building more general human-like AI systems.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.04303": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.04303v1", "post_title": "Batch Active Preference-Based Learning of Reward Functions", "authors": ["Erdem B\u0131y\u0131k", "Dorsa Sadigh"], "date_published": "2018-10-10 00:02:55+00:00", "data_last_modified": "2018-10-10 00:02:55+00:00", "url": "http://arxiv.org/abs/1810.04303v1", "abstract": "Data generation and labeling are usually an expensive part of learning for\nrobotics. While active learning methods are commonly used to tackle the former\nproblem, preference-based learning is a concept that attempts to solve the\nlatter by querying users with preference questions. In this paper, we will\ndevelop a new algorithm, batch active preference-based learning, that enables\nefficient learning of reward functions using as few data samples as possible\nwhile still having short query generation times. We introduce several\napproximations to the batch active learning problem, and provide theoretical\nguarantees for the convergence of our algorithms. Finally, we present our\nexperimental results for a variety of robotics tasks in simulation. Our results\nsuggest that our batch active learning algorithm requires only a few queries\nthat are computed in a short amount of time. We then showcase our algorithm in\na study to learn human users' preferences.", "author_comment": "Proceedings of the 2nd Conference on Robot Learning (CoRL), October\n  2018", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.10186": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.10186v3", "post_title": "Attention is not Explanation", "authors": ["Sarthak Jain", "Byron C. Wallace"], "date_published": "2019-02-26 19:59:15+00:00", "data_last_modified": "2019-05-08 18:05:56+00:00", "url": "http://arxiv.org/abs/1902.10186v3", "abstract": "Attention mechanisms have seen wide adoption in neural NLP models. In\naddition to improving predictive performance, these are often touted as\naffording transparency: models equipped with attention provide a distribution\nover attended-to input units, and this is often presented (at least implicitly)\nas communicating the relative importance of inputs. However, it is unclear what\nrelationship exists between attention weights and model outputs. In this work,\nwe perform extensive experiments across a variety of NLP tasks that aim to\nassess the degree to which attention weights provide meaningful `explanations'\nfor predictions. We find that they largely do not. For example, learned\nattention weights are frequently uncorrelated with gradient-based measures of\nfeature importance, and one can identify very different attention distributions\nthat nonetheless yield equivalent predictions. Our findings show that standard\nattention modules do not provide meaningful explanations and should not be\ntreated as though they do. Code for all experiments is available at\nhttps://github.com/successar/AttentionExplanation.", "author_comment": "Accepted as NAACL 2019 Long Paper", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.07483": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.07483v3", "post_title": "O2A: One-shot Observational learning with Action vectors", "authors": ["Leo Pauly", "Wisdom C. Agboh", "David C. Hogg", "Raul Fuentes"], "date_published": "2018-10-17 11:33:11+00:00", "data_last_modified": "2020-12-21 13:43:15+00:00", "url": "http://arxiv.org/abs/1810.07483v3", "abstract": "We present O2A, a novel method for learning to perform robotic manipulation\ntasks from a single (one-shot) third-person demonstration video. To our\nknowledge, it is the first time this has been done for a single demonstration.\nThe key novelty lies in pre-training a feature extractor for creating a\nperceptual representation for actions that we call 'action vectors'. The action\nvectors are extracted using a 3D-CNN model pre-trained as an action classifier\non a generic action dataset. The distance between the action vectors from the\nobserved third-person demonstration and trial robot executions is used as a\nreward for reinforcement learning of the demonstrated task. We report on\nexperiments in simulation and on a real robot, with changes in viewpoint of\nobservation, properties of the objects involved, scene background and\nmorphology of the manipulator between the demonstration and the learning\ndomains. O2A outperforms baseline approaches under different domain shifts and\nhas comparable performance with an oracle (that uses an ideal reward function).", "author_comment": null, "journal_ref": "Front. Robot. AI 8:686368 (2021)", "doi": "10.3389/frobt.2021.686368", "primary_category": "cs.RO", "categories": ["cs.RO", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2004.04136": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2004.04136v4", "post_title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning", "authors": ["Aravind Srinivas", "Michael Laskin", "Pieter Abbeel"], "date_published": "2020-04-08 17:40:43+00:00", "data_last_modified": "2020-09-21 15:34:30+00:00", "url": "http://arxiv.org/abs/2004.04136v4", "abstract": "We present CURL: Contrastive Unsupervised Representations for Reinforcement\nLearning. CURL extracts high-level features from raw pixels using contrastive\nlearning and performs off-policy control on top of the extracted features. CURL\noutperforms prior pixel-based methods, both model-based and model-free, on\ncomplex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and\n1.2x performance gains at the 100K environment and interaction steps benchmarks\nrespectively. On the DeepMind Control Suite, CURL is the first image-based\nalgorithm to nearly match the sample-efficiency of methods that use state-based\nfeatures. Our code is open-sourced and available at\nhttps://github.com/MishaLaskin/curl.", "author_comment": "First two authors contributed equally, website:\n  https://mishalaskin.github.io/curl code: https://github.com/MishaLaskin/curl", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2008.03525": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2008.03525v1", "post_title": "Non-Adversarial Imitation Learning and its Connections to Adversarial Methods", "authors": ["Oleg Arenz", "Gerhard Neumann"], "date_published": "2020-08-08 13:43:06+00:00", "data_last_modified": "2020-08-08 13:43:06+00:00", "url": "http://arxiv.org/abs/2008.03525v1", "abstract": "Many modern methods for imitation learning and inverse reinforcement\nlearning, such as GAIL or AIRL, are based on an adversarial formulation. These\nmethods apply GANs to match the expert's distribution over states and actions\nwith the implicit state-action distribution induced by the agent's policy.\nHowever, by framing imitation learning as a saddle point problem, adversarial\nmethods can suffer from unstable optimization, and convergence can only be\nshown for small policy updates. We address these problems by proposing a\nframework for non-adversarial imitation learning. The resulting algorithms are\nsimilar to their adversarial counterparts and, thus, provide insights for\nadversarial imitation learning methods. Most notably, we show that AIRL is an\ninstance of our non-adversarial formulation, which enables us to greatly\nsimplify its derivations and obtain stronger convergence guarantees. We also\nshow that our non-adversarial formulation can be used to derive novel\nalgorithms by presenting a method for offline imitation learning that is\ninspired by the recent ValueDice algorithm, but does not rely on small policy\nupdates for convergence. In our simulated robot experiments, our offline method\nfor non-adversarial imitation learning seems to perform best when using many\nupdates for policy and discriminator at each iteration and outperforms\nbehavioral cloning and ValueDice.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.IT", "cs.RO", "math.IT", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.07768": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.07768v1", "post_title": "Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data", "authors": ["Felipe Petroski Such", "Aditya Rawal", "Joel Lehman", "Kenneth O. Stanley", "Jeff Clune"], "date_published": "2019-12-17 00:57:50+00:00", "data_last_modified": "2019-12-17 00:57:50+00:00", "url": "http://arxiv.org/abs/1912.07768v1", "abstract": "This paper investigates the intriguing question of whether we can create\nlearning algorithms that automatically generate training data, learning\nenvironments, and curricula in order to help AI agents rapidly learn. We show\nthat such algorithms are possible via Generative Teaching Networks (GTNs), a\ngeneral approach that is, in theory, applicable to supervised, unsupervised,\nand reinforcement learning, although our experiments only focus on the\nsupervised case. GTNs are deep neural networks that generate data and/or\ntraining environments that a learner (e.g. a freshly initialized neural\nnetwork) trains on for a few SGD steps before being tested on a target task. We\nthen differentiate through the entire learning process via meta-gradients to\nupdate the GTN parameters to improve performance on the target task. GTNs have\nthe beneficial property that they can theoretically generate any type of data\nor training environment, making their potential impact large. This paper\nintroduces GTNs, discusses their potential, and showcases that they can\nsubstantially accelerate learning. We also demonstrate a practical and exciting\napplication of GTNs: accelerating the evaluation of candidate architectures for\nneural architecture search (NAS), which is rate-limited by such evaluations,\nenabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art,\nfinding higher performing architectures when controlling for the search\nproposal mechanism. GTN-NAS also is competitive with the overall state of the\nart approaches, which achieve top performance while using orders of magnitude\nless computation than typical NAS methods. Speculating forward, GTNs may\nrepresent a first step toward the ambitious goal of algorithms that generate\ntheir own training data and, in doing so, open a variety of interesting new\nresearch questions and directions.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.00184": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.00184v1", "post_title": "Stakeholders in Explainable AI", "authors": ["Alun Preece", "Dan Harborne", "Dave Braines", "Richard Tomsett", "Supriyo Chakraborty"], "date_published": "2018-09-29 10:15:18+00:00", "data_last_modified": "2018-09-29 10:15:18+00:00", "url": "http://arxiv.org/abs/1810.00184v1", "abstract": "There is general consensus that it is important for artificial intelligence\n(AI) and machine learning systems to be explainable and/or interpretable.\nHowever, there is no general consensus over what is meant by 'explainable' and\n'interpretable'. In this paper, we argue that this lack of consensus is due to\nthere being several distinct stakeholder communities. We note that, while the\nconcerns of the individual communities are broadly compatible, they are not\nidentical, which gives rise to different intents and requirements for\nexplainability/interpretability. We use the software engineering distinction\nbetween validation and verification, and the epistemological distinctions\nbetween knowns/unknowns, to tease apart the concerns of the stakeholder\ncommunities and highlight the areas where their foci overlap or diverge. It is\nnot the purpose of the authors of this paper to 'take sides' - we count\nourselves as members, to varying degrees, of multiple communities - but rather\nto help disambiguate what stakeholders mean when they ask 'Why?' of an AI.", "author_comment": "Presented at AAAI FSS-18: Artificial Intelligence in Government and\n  Public Sector, Arlington, Virginia, USA", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.08267": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.08267v1", "post_title": "Solving Probability and Statistics Problems by Program Synthesis", "authors": ["Leonard Tang", "Elizabeth Ke", "Nikhil Singh", "Nakul Verma", "Iddo Drori"], "date_published": "2021-11-16 07:34:25+00:00", "data_last_modified": "2021-11-16 07:34:25+00:00", "url": "http://arxiv.org/abs/2111.08267v1", "abstract": "We solve university level probability and statistics questions by program\nsynthesis using OpenAI's Codex, a Transformer trained on text and fine-tuned on\ncode. We transform course problems from MIT's 18.05 Introduction to Probability\nand Statistics and Harvard's STAT110 Probability into programming tasks. We\nthen execute the generated code to get a solution. Since these course questions\nare grounded in probability, we often aim to have Codex generate probabilistic\nprograms that simulate a large number of probabilistic dependencies to compute\nits solution. Our approach requires prompt engineering to transform the\nquestion from its original form to an explicit, tractable form that results in\na correct program and solution. To estimate the amount of work needed to\ntranslate an original question into its tractable form, we measure the\nsimilarity between original and transformed questions. Our work is the first to\nintroduce a new dataset of university-level probability and statistics problems\nand solve these problems in a scalable fashion using the program synthesis\ncapabilities of large language models.", "author_comment": "33 pages, 4 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.00596": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.00596v4", "post_title": "A Comprehensive Survey on Graph Neural Networks", "authors": ["Zonghan Wu", "Shirui Pan", "Fengwen Chen", "Guodong Long", "Chengqi Zhang", "Philip S. Yu"], "date_published": "2019-01-03 03:20:55+00:00", "data_last_modified": "2019-12-04 01:43:00+00:00", "url": "http://arxiv.org/abs/1901.00596v4", "abstract": "Deep learning has revolutionized many machine learning tasks in recent years,\nranging from image classification and video processing to speech recognition\nand natural language understanding. The data in these tasks are typically\nrepresented in the Euclidean space. However, there is an increasing number of\napplications where data are generated from non-Euclidean domains and are\nrepresented as graphs with complex relationships and interdependency between\nobjects. The complexity of graph data has imposed significant challenges on\nexisting machine learning algorithms. Recently, many studies on extending deep\nlearning approaches for graph data have emerged. In this survey, we provide a\ncomprehensive overview of graph neural networks (GNNs) in data mining and\nmachine learning fields. We propose a new taxonomy to divide the\nstate-of-the-art graph neural networks into four categories, namely recurrent\ngraph neural networks, convolutional graph neural networks, graph autoencoders,\nand spatial-temporal graph neural networks. We further discuss the applications\nof graph neural networks across various domains and summarize the open source\ncodes, benchmark data sets, and model evaluation of graph neural networks.\nFinally, we propose potential research directions in this rapidly growing\nfield.", "author_comment": "Minor revision (updated tables and references)", "journal_ref": null, "doi": "10.1109/TNNLS.2020.2978386", "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1802.01421": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1802.01421v4", "post_title": "First-order Adversarial Vulnerability of Neural Networks and Input Dimension", "authors": ["Carl-Johann Simon-Gabriel", "Yann Ollivier", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "David Lopez-Paz"], "date_published": "2018-02-05 14:36:44+00:00", "data_last_modified": "2019-06-16 20:55:06+00:00", "url": "http://arxiv.org/abs/1802.01421v4", "abstract": "Over the past few years, neural networks were proven vulnerable to\nadversarial images: targeted but imperceptible image perturbations lead to\ndrastically different predictions. We show that adversarial vulnerability\nincreases with the gradients of the training objective when viewed as a\nfunction of the inputs. Surprisingly, vulnerability does not depend on network\ntopology: for many standard network architectures, we prove that at\ninitialization, the $\\ell_1$-norm of these gradients grows as the square root\nof the input dimension, leaving the networks increasingly vulnerable with\ngrowing image size. We empirically show that this dimension dependence persists\nafter either usual or robust training, but gets attenuated with higher\nregularization.", "author_comment": "Paper previously called: \"Adversarial Vulnerability of Neural\n  Networks Increases with Input Dimension\". 9 pages main text and references,\n  11 pages appendix, 14 figures", "journal_ref": "Proceedings of ICML 2019", "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.CV", "cs.LG", "68T45", "I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1809.05630": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1809.05630v2", "post_title": "Towards Better Interpretability in Deep Q-Networks", "authors": ["Raghuram Mandyam Annasamy", "Katia Sycara"], "date_published": "2018-09-15 01:34:27+00:00", "data_last_modified": "2018-11-15 03:06:56+00:00", "url": "http://arxiv.org/abs/1809.05630v2", "abstract": "Deep reinforcement learning techniques have demonstrated superior performance\nin a wide variety of environments. As improvements in training algorithms\ncontinue at a brisk pace, theoretical or empirical studies on understanding\nwhat these networks seem to learn, are far behind. In this paper we propose an\ninterpretable neural network architecture for Q-learning which provides a\nglobal explanation of the model's behavior using key-value memories, attention\nand reconstructible embeddings. With a directed exploration strategy, our model\ncan reach training rewards comparable to the state-of-the-art deep Q-learning\nmodels. However, results suggest that the features extracted by the neural\nnetwork are extremely shallow and subsequent testing using out-of-sample\nexamples shows that the agent can easily overfit to trajectories seen during\ntraining.", "author_comment": "Accepted at AAAI-19; (16 pages, 18 figures)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2202.01679": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2202.01679v1", "post_title": "Certifying Out-of-Domain Generalization for Blackbox Functions", "authors": ["Maurice Weber", "Linyi Li", "Boxin Wang", "Zhikuan Zhao", "Bo Li", "Ce Zhang"], "date_published": "2022-02-03 16:47:50+00:00", "data_last_modified": "2022-02-03 16:47:50+00:00", "url": "http://arxiv.org/abs/2202.01679v1", "abstract": "Certifying the robustness of model performance under bounded data\ndistribution shifts has recently attracted intensive interests under the\numbrella of distributional robustness. However, existing techniques either make\nstrong assumptions on the model class and loss functions that can be certified,\nsuch as smoothness expressed via Lipschitz continuity of gradients, or require\nto solve complex optimization problems. As a result, the wider application of\nthese techniques is currently limited by its scalability and flexibility --\nthese techniques often do not scale to large-scale datasets with modern deep\nneural networks or cannot handle loss functions which may be non-smooth, such\nas the 0-1 loss. In this paper, we focus on the problem of certifying\ndistributional robustness for black box models and bounded losses, without\nother assumptions. We propose a novel certification framework given bounded\ndistance of mean and variance of two distributions. Our certification technique\nscales to ImageNet-scale datasets, complex models, and a diverse range of loss\nfunctions. We then focus on one specific application enabled by such\nscalability and flexibility, i.e., certifying out-of-domain generalization for\nlarge neural networks and loss functions such as accuracy and AUC. We\nexperimentally validate our certification method on a number of datasets,\nranging from ImageNet, where we provide the first non-vacuous certified\nout-of-domain generalization, to smaller classification tasks where we are able\nto compare with the state-of-the-art and show that our method performs\nconsiderably better.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1711.06782": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1711.06782v1", "post_title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning", "authors": ["Benjamin Eysenbach", "Shixiang Gu", "Julian Ibarz", "Sergey Levine"], "date_published": "2017-11-18 00:53:20+00:00", "data_last_modified": "2017-11-18 00:53:20+00:00", "url": "http://arxiv.org/abs/1711.06782v1", "abstract": "Deep reinforcement learning algorithms can learn complex behavioral skills,\nbut real-world application of these methods requires a large amount of\nexperience to be collected by the agent. In practical settings, such as\nrobotics, this involves repeatedly attempting a task, resetting the environment\nbetween each attempt. However, not all tasks are easily or automatically\nreversible. In practice, this learning process requires extensive human\nintervention. In this work, we propose an autonomous method for safe and\nefficient reinforcement learning that simultaneously learns a forward and reset\npolicy, with the reset policy resetting the environment for a subsequent\nattempt. By learning a value function for the reset policy, we can\nautomatically determine when the forward policy is about to enter a\nnon-reversible state, providing for uncertainty-aware safety aborts. Our\nexperiments illustrate that proper use of the reset policy can greatly reduce\nthe number of manual resets required to learn a task, can reduce the number of\nunsafe actions that lead to non-reversible states, and can automatically induce\na curriculum.", "author_comment": "Videos of our experiments are available at:\n  https://sites.google.com/site/mlleavenotrace/", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.04635": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.04635v4", "post_title": "Learning to Play No-Press Diplomacy with Best Response Policy Iteration", "authors": ["Thomas Anthony", "Tom Eccles", "Andrea Tacchetti", "J\u00e1nos Kram\u00e1r", "Ian Gemp", "Thomas C. Hudson", "Nicolas Porcel", "Marc Lanctot", "Julien P\u00e9rolat", "Richard Everett", "Roman Werpachowski", "Satinder Singh", "Thore Graepel", "Yoram Bachrach"], "date_published": "2020-06-08 14:33:31+00:00", "data_last_modified": "2022-01-04 15:11:59+00:00", "url": "http://arxiv.org/abs/2006.04635v4", "abstract": "Recent advances in deep reinforcement learning (RL) have led to considerable\nprogress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The\npurely adversarial nature of such games allows for conceptually simple and\nprincipled application of RL methods. However real-world settings are\nmany-agent, and agent interactions are complex mixtures of common-interest and\ncompetitive aspects. We consider Diplomacy, a 7-player board game designed to\naccentuate dilemmas resulting from many-agent interactions. It also features a\nlarge combinatorial action space and simultaneous moves, which are challenging\nfor RL algorithms. We propose a simple yet effective approximate best response\noperator, designed to handle large combinatorial action spaces and simultaneous\nmoves. We also introduce a family of policy iteration methods that approximate\nfictitious play. With these methods, we successfully apply RL to Diplomacy: we\nshow that our agents convincingly outperform the previous state-of-the-art, and\ngame theoretic equilibrium analysis shows that the new process yields\nconsistent improvements.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.GT", "cs.MA", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.09246": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.09246v1", "post_title": "Oversight of Unsafe Systems via Dynamic Safety Envelopes", "authors": ["David Manheim"], "date_published": "2018-11-22 17:31:41+00:00", "data_last_modified": "2018-11-22 17:31:41+00:00", "url": "http://arxiv.org/abs/1811.09246v1", "abstract": "This paper reviews the reasons that Human-in-the-Loop is both critical for\npreventing widely-understood failure modes for machine learning, and not a\npractical solution. Following this, we review two current heuristic methods for\naddressing this. The first is provable safety envelopes, which are possible\nonly when the dynamics of the system are fully known, but can be useful safety\nguarantees when optimal behavior is based on machine learning with\npoorly-understood safety characteristics. The second is the simpler circuit\nbreaker model, which can forestall or prevent catastrophic outcomes by stopping\nthe system, without any specific model of the system. This paper proposes using\nheuristic, dynamic safety envelopes, which are a plausible halfway point\nbetween these approaches that allows human oversight without some of the more\ndifficult problems faced by Human-in-the-Loop systems. Finally, the paper\nconcludes with how this approach can be used for governance of systems where\notherwise unsafe systems are deployed.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "K.3.5; K.4.1"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.10493": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.10493v1", "post_title": "Discrete Representations Strengthen Vision Transformer Robustness", "authors": ["Chengzhi Mao", "Lu Jiang", "Mostafa Dehghani", "Carl Vondrick", "Rahul Sukthankar", "Irfan Essa"], "date_published": "2021-11-20 01:49:56+00:00", "data_last_modified": "2021-11-20 01:49:56+00:00", "url": "http://arxiv.org/abs/2111.10493v1", "abstract": "Vision Transformer (ViT) is emerging as the state-of-the-art architecture for\nimage recognition. While recent studies suggest that ViTs are more robust than\ntheir convolutional counterparts, our experiments find that ViTs are overly\nreliant on local features (e.g., nuisances and texture) and fail to make\nadequate use of global context (e.g., shape and structure). As a result, ViTs\nfail to generalize to out-of-distribution, real-world data. To address this\ndeficiency, we present a simple and effective architecture modification to\nViT's input layer by adding discrete tokens produced by a vector-quantized\nencoder. Different from the standard continuous pixel tokens, discrete tokens\nare invariant under small perturbations and contain less information\nindividually, which promote ViTs to learn global information that is invariant.\nExperimental results demonstrate that adding discrete representation on four\narchitecture variants strengthens ViT robustness by up to 12% across seven\nImageNet robustness benchmarks while maintaining the performance on ImageNet.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1802.01744": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1802.01744v2", "post_title": "Shared Autonomy via Deep Reinforcement Learning", "authors": ["Siddharth Reddy", "Anca D. Dragan", "Sergey Levine"], "date_published": "2018-02-06 00:45:12+00:00", "data_last_modified": "2018-05-23 03:12:34+00:00", "url": "http://arxiv.org/abs/1802.01744v2", "abstract": "In shared autonomy, user input is combined with semi-autonomous control to\nachieve a common goal. The goal is often unknown ex-ante, so prior work enables\nagents to infer the goal from user input and assist with the task. Such methods\ntend to assume some combination of knowledge of the dynamics of the\nenvironment, the user's policy given their goal, and the set of possible goals\nthe user might target, which limits their application to real-world scenarios.\nWe propose a deep reinforcement learning framework for model-free shared\nautonomy that lifts these assumptions. We use human-in-the-loop reinforcement\nlearning with neural network function approximation to learn an end-to-end\nmapping from environmental observation and user input to agent action values,\nwith task reward as the only form of supervision. This approach poses the\nchallenge of following user commands closely enough to provide the user with\nreal-time action feedback and thereby ensure high-quality user input, but also\ndeviating from the user's actions when they are suboptimal. We balance these\ntwo needs by discarding actions whose values fall below some threshold, then\nselecting the remaining action closest to the user's input. Controlled studies\nwith users (n = 12) and synthetic pilots playing a video game, and a pilot\nstudy with users (n = 4) flying a real quadrotor, demonstrate the ability of\nour algorithm to assist users with real-time control tasks in which the agent\ncannot directly access the user's private information through observations, but\nreceives a reward signal and user input that both depend on the user's intent.\nThe agent learns to assist the user without access to this private information,\nimplicitly inferring it from the user's input. This paper is a proof of concept\nthat illustrates the potential for deep reinforcement learning to enable\nflexible and practical assistive systems.", "author_comment": "Accepted to the Robotics: Science and Systems (RSS) 2018 conference", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.HC", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.03357": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.03357v2", "post_title": "Curiosity Killed or Incapacitated the Cat and the Asymptotically Optimal Agent", "authors": ["Michael K. Cohen", "Elliot Catt", "Marcus Hutter"], "date_published": "2020-06-05 10:42:29+00:00", "data_last_modified": "2021-05-26 15:55:28+00:00", "url": "http://arxiv.org/abs/2006.03357v2", "abstract": "Reinforcement learners are agents that learn to pick actions that lead to\nhigh reward. Ideally, the value of a reinforcement learner's policy approaches\noptimality--where the optimal informed policy is the one which maximizes\nreward. Unfortunately, we show that if an agent is guaranteed to be\n\"asymptotically optimal\" in any (stochastically computable) environment, then\nsubject to an assumption about the true environment, this agent will be either\n\"destroyed\" or \"incapacitated\" with probability 1. Much work in reinforcement\nlearning uses an ergodicity assumption to avoid this problem. Often, doing\ntheoretical research under simplifying assumptions prepares us to provide\npractical solutions even in the absence of those assumptions, but the\nergodicity assumption in reinforcement learning may have led us entirely astray\nin preparing safe and effective exploration strategies for agents in dangerous\nenvironments. Rather than assuming away the problem, we present an agent,\nMentee, with the modest guarantee of approaching the performance of a mentor,\ndoing safe exploration instead of reckless exploration. Critically, Mentee's\nexploration probability depends on the expected information gain from\nexploring. In a simple non-ergodic environment with a weak mentor, we find\nMentee outperforms existing asymptotically optimal agents and its mentor.", "author_comment": "13 pages, with 5 page appendix; 3 figures", "journal_ref": "Journal of Selected Areas in Information Theory 2 (2021)", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "I.2.0; I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1908.01007": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1908.01007v1", "post_title": "Improving Deep Reinforcement Learning in Minecraft with Action Advice", "authors": ["Spencer Frazier", "Mark Riedl"], "date_published": "2019-08-02 18:36:44+00:00", "data_last_modified": "2019-08-02 18:36:44+00:00", "url": "http://arxiv.org/abs/1908.01007v1", "abstract": "Training deep reinforcement learning agents complex behaviors in 3D virtual\nenvironments requires significant computational resources. This is especially\ntrue in environments with high degrees of aliasing, where many states share\nnearly identical visual features. Minecraft is an exemplar of such an\nenvironment. We hypothesize that interactive machine learning IML, wherein\nhuman teachers play a direct role in training through demonstrations, critique,\nor action advice, may alleviate agent susceptibility to aliasing. However,\ninteractive machine learning is only practical when the number of human\ninteractions is limited, requiring a balance between human teacher effort and\nagent performance. We conduct experiments with two reinforcement learning\nalgorithms which enable human teachers to give action advice, Feedback\nArbitration and Newtonian Action Advice, under visual aliasing conditions. To\nassess potential cognitive load per advice type, we vary the accuracy and\nfrequency of various human action advice techniques. Training efficiency,\nrobustness against infrequent and inaccurate advisor input, and sensitivity to\naliasing are examined.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1701.01302": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1701.01302v3", "post_title": "Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making", "authors": ["Andrew Critch"], "date_published": "2017-01-05 13:00:05+00:00", "data_last_modified": "2017-05-13 08:33:46+00:00", "url": "http://arxiv.org/abs/1701.01302v3", "abstract": "Existing multi-objective reinforcement learning (MORL) algorithms do not\naccount for objectives that arise from players with differing beliefs.\nConcretely, consider two players with different beliefs and utility functions\nwho may cooperate to build a machine that takes actions on their behalf. A\nrepresentation is needed for how much the machine's policy will prioritize each\nplayer's interests over time. Assuming the players have reached common\nknowledge of their situation, this paper derives a recursion that any Pareto\noptimal policy must satisfy. Two qualitative observations can be made from the\nrecursion: the machine must (1) use each player's own beliefs in evaluating how\nwell an action will serve that player's utility function, and (2) shift the\nrelative priority it assigns to each player's expected utilities over time, by\na factor proportional to how well that player's beliefs predict the machine's\ninputs. Observation (2) represents a substantial divergence from na\\\"{i}ve\nlinear utility aggregation (as in Harsanyi's utilitarian theorem, and existing\nMORL algorithms), which is shown here to be inadequate for Pareto optimal\nsequential decision-making on behalf of players with different beliefs.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.GT", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.09136": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.09136v1", "post_title": "Categorizing Wireheading in Partially Embedded Agents", "authors": ["Arushi Majha", "Sayan Sarkar", "Davide Zagami"], "date_published": "2019-06-21 13:38:35+00:00", "data_last_modified": "2019-06-21 13:38:35+00:00", "url": "http://arxiv.org/abs/1906.09136v1", "abstract": "$\\textit{Embedded agents}$ are not explicitly separated from their\nenvironment, lacking clear I/O channels. Such agents can reason about and\nmodify their internal parts, which they are incentivized to shortcut or\n$\\textit{wirehead}$ in order to achieve the maximal reward. In this paper, we\nprovide a taxonomy of ways by which wireheading can occur, followed by a\ndefinition of wirehead-vulnerable agents. Starting from the fully dualistic\nuniversal agent AIXI, we introduce a spectrum of partially embedded agents and\nidentify wireheading opportunities that such agents can exploit, experimentally\ndemonstrating the results with the GRL simulation platform AIXIjs. We\ncontextualize wireheading in the broader class of all misalignment problems -\nwhere the goals of the agent conflict with the goals of the human designer -\nand conjecture that the only other possible type of misalignment is\nspecification gaming. Motivated by this taxonomy, we define wirehead-vulnerable\nagents as embedded agents that choose to behave differently from fully\ndualistic agents lacking access to their internal parts.", "author_comment": "Accepted at the AI Safety Workshop in IJCAI 2019", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.15920": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.15920v2", "post_title": "Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones", "authors": ["Brijen Thananjeyan", "Ashwin Balakrishna", "Suraj Nair", "Michael Luo", "Krishnan Srinivasan", "Minho Hwang", "Joseph E. Gonzalez", "Julian Ibarz", "Chelsea Finn", "Ken Goldberg"], "date_published": "2020-10-29 20:10:02+00:00", "data_last_modified": "2021-05-17 21:20:48+00:00", "url": "http://arxiv.org/abs/2010.15920v2", "abstract": "Safety remains a central obstacle preventing widespread use of RL in the real\nworld: learning new tasks in uncertain environments requires extensive\nexploration, but safety requires limiting exploration. We propose Recovery RL,\nan algorithm which navigates this tradeoff by (1) leveraging offline data to\nlearn about constraint violating zones before policy learning and (2)\nseparating the goals of improving task performance and constraint satisfaction\nacross two policies: a task policy that only optimizes the task reward and a\nrecovery policy that guides the agent to safety when constraint violation is\nlikely. We evaluate Recovery RL on 6 simulation domains, including two\ncontact-rich manipulation tasks and an image-based navigation task, and an\nimage-based obstacle avoidance task on a physical robot. We compare Recovery RL\nto 5 prior safe RL methods which jointly optimize for task performance and\nsafety via constrained optimization or reward shaping and find that Recovery RL\noutperforms the next best prior method across all domains. Results suggest that\nRecovery RL trades off constraint violations and task successes 2 - 20 times\nmore efficiently in simulation domains and 3 times more efficiently in physical\nexperiments. See https://tinyurl.com/rl-recovery for videos and supplementary\nmaterial.", "author_comment": "RA-L and ICRA 2021. First two authors contributed equally", "journal_ref": "Robotics and Automation Letters (RA-L) and International\n  Conference on Robotics and Automation (ICRA) 2021", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2005.04305": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2005.04305v1", "post_title": "Measuring the Algorithmic Efficiency of Neural Networks", "authors": ["Danny Hernandez", "Tom B. Brown"], "date_published": "2020-05-08 22:26:37+00:00", "data_last_modified": "2020-05-08 22:26:37+00:00", "url": "http://arxiv.org/abs/2005.04305v1", "abstract": "Three factors drive the advance of AI: algorithmic innovation, data, and the\namount of compute available for training. Algorithmic progress has\ntraditionally been more difficult to quantify than compute and data. In this\nwork, we argue that algorithmic progress has an aspect that is both\nstraightforward to measure and interesting: reductions over time in the compute\nneeded to reach past capabilities. We show that the number of floating-point\noperations required to train a classifier to AlexNet-level performance on\nImageNet has decreased by a factor of 44x between 2012 and 2019. This\ncorresponds to algorithmic efficiency doubling every 16 months over a period of\n7 years. By contrast, Moore's Law would only have yielded an 11x cost\nimprovement. We observe that hardware and algorithmic efficiency gains multiply\nand can be on a similar scale over meaningful horizons, which suggests that a\ngood model of AI progress should integrate measures from both.", "author_comment": "20 pages, 5 figures", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1610.07997": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1610.07997v1", "post_title": "Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures", "authors": ["Roman V. Yampolskiy", "M. S. Spellchecker"], "date_published": "2016-10-25 18:14:24+00:00", "data_last_modified": "2016-10-25 18:14:24+00:00", "url": "http://arxiv.org/abs/1610.07997v1", "abstract": "In this work, we present and analyze reported failures of artificially\nintelligent systems and extrapolate our analysis to future AIs. We suggest that\nboth the frequency and the seriousness of future AI failures will steadily\nincrease. AI Safety can be improved based on ideas developed by cybersecurity\nexperts. For narrow AIs safety failures are at the same, moderate, level of\ncriticality as in cybersecurity, however for general AI, failures have a\nfundamentally different impact. A single failure of a superintelligent system\nmay cause a catastrophic event without a chance for recovery. The goal of\ncybersecurity is to reduce the number of successful attacks on the system; the\ngoal of AI Safety is to make sure zero attacks succeed in bypassing the safety\nmechanisms. Unfortunately, such a level of performance is unachievable. Every\nsecurity system will eventually fail; there is no such thing as a 100% secure\nsystem.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CY"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.12715": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.12715v4", "post_title": "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models", "authors": ["Sven Gowal", "Krishnamurthy Dvijotham", "Robert Stanforth", "Rudy Bunel", "Chongli Qin", "Jonathan Uesato", "Relja Arandjelovic", "Timothy Mann", "Pushmeet Kohli"], "date_published": "2018-10-30 13:12:47+00:00", "data_last_modified": "2019-08-29 12:23:52+00:00", "url": "http://arxiv.org/abs/1810.12715v4", "abstract": "Recent work has shown that it is possible to train deep neural networks that\nare provably robust to norm-bounded adversarial perturbations. Most of these\nmethods are based on minimizing an upper bound on the worst-case loss over all\npossible adversarial perturbations. While these techniques show promise, they\noften result in difficult optimization procedures that remain hard to scale to\nlarger networks. Through a comprehensive analysis, we show how a simple\nbounding technique, interval bound propagation (IBP), can be exploited to train\nlarge provably robust neural networks that beat the state-of-the-art in\nverified accuracy. While the upper bound computed by IBP can be quite weak for\ngeneral networks, we demonstrate that an appropriate loss and clever\nhyper-parameter schedule allow the network to adapt such that the IBP bound is\ntight. This results in a fast and stable learning algorithm that outperforms\nmore sophisticated methods and achieves state-of-the-art results on MNIST,\nCIFAR-10 and SVHN. It also allows us to train the largest model to be verified\nbeyond vacuous bounds on a downscaled version of ImageNet.", "author_comment": "[v2] Best paper at NeurIPS SECML 2018 Workshop [v4] Accepted at ICCV\n  2019 under the title \"Scalable Verified Training for Provably Robust Image\n  Classification\"", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.02845": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.02845v2", "post_title": "Likelihood Ratios for Out-of-Distribution Detection", "authors": ["Jie Ren", "Peter J. Liu", "Emily Fertig", "Jasper Snoek", "Ryan Poplin", "Mark A. DePristo", "Joshua V. Dillon", "Balaji Lakshminarayanan"], "date_published": "2019-06-07 00:01:42+00:00", "data_last_modified": "2019-12-05 19:59:35+00:00", "url": "http://arxiv.org/abs/1906.02845v2", "abstract": "Discriminative neural networks offer little or no performance guarantees when\ndeployed on data not generated by the same process as the training\ndistribution. On such out-of-distribution (OOD) inputs, the prediction may not\nonly be erroneous, but confidently so, limiting the safe deployment of\nclassifiers in real-world applications. One such challenging application is\nbacteria identification based on genomic sequences, which holds the promise of\nearly detection of diseases, but requires a model that can output low\nconfidence predictions on OOD genomic sequences from new bacteria that were not\npresent in the training data. We introduce a genomics dataset for OOD detection\nthat allows other researchers to benchmark progress on this important problem.\nWe investigate deep generative model based approaches for OOD detection and\nobserve that the likelihood score is heavily affected by population level\nbackground statistics. We propose a likelihood ratio method for deep generative\nmodels which effectively corrects for these confounding background statistics.\nWe benchmark the OOD detection performance of the proposed method against\nexisting approaches on the genomics dataset and show that our method achieves\nstate-of-the-art performance. We demonstrate the generality of the proposed\nmethod by showing that it significantly improves OOD detection when applied to\ndeep generative models of images.", "author_comment": "Accepted to NeurIPS 2019", "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.01291": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.01291v2", "post_title": "On the Utility of Model Learning in HRI", "authors": ["Gokul Swamy", "Jens Schulz", "Rohan Choudhury", "Dylan Hadfield-Menell", "Anca Dragan"], "date_published": "2019-01-04 19:55:49+00:00", "data_last_modified": "2020-05-22 02:34:37+00:00", "url": "http://arxiv.org/abs/1901.01291v2", "abstract": "Fundamental to robotics is the debate between model-based and model-free\nlearning: should the robot build an explicit model of the world, or learn a\npolicy directly? In the context of HRI, part of the world to be modeled is the\nhuman. One option is for the robot to treat the human as a black box and learn\na policy for how they act directly. But it can also model the human as an\nagent, and rely on a \"theory of mind\" to guide or bias the learning (grey box).\nWe contribute a characterization of the performance of these methods for an\nautonomous driving task under the optimistic case of having an ideal theory of\nmind, as well as under different scenarios in which the assumptions behind the\nrobot's theory of mind for the human are wrong, as they inevitably will be in\npractice.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.04480": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.04480v3", "post_title": "There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning", "authors": ["Nathan Grinsztajn", "Johan Ferret", "Olivier Pietquin", "Philippe Preux", "Matthieu Geist"], "date_published": "2021-06-08 16:07:10+00:00", "data_last_modified": "2021-10-29 11:40:07+00:00", "url": "http://arxiv.org/abs/2106.04480v3", "abstract": "We propose to learn to distinguish reversible from irreversible actions for\nbetter informed decision-making in Reinforcement Learning (RL). From\ntheoretical considerations, we show that approximate reversibility can be\nlearned through a simple surrogate task: ranking randomly sampled trajectory\nevents in chronological order. Intuitively, pairs of events that are always\nobserved in the same order are likely to be separated by an irreversible\nsequence of actions. Conveniently, learning the temporal order of events can be\ndone in a fully self-supervised way, which we use to estimate the reversibility\nof actions from experience, without any priors. We propose two different\nstrategies that incorporate reversibility in RL agents, one strategy for\nexploration (RAE) and one strategy for control (RAC). We demonstrate the\npotential of reversibility-aware agents in several environments, including the\nchallenging Sokoban game. In synthetic tasks, we show that we can learn control\npolicies that never fail and reduce to zero the side-effects of interactions,\neven without access to the reward function.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1606.03137": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1606.03137v3", "post_title": "Cooperative Inverse Reinforcement Learning", "authors": ["Dylan Hadfield-Menell", "Anca Dragan", "Pieter Abbeel", "Stuart Russell"], "date_published": "2016-06-09 22:39:54+00:00", "data_last_modified": "2016-11-12 20:33:43+00:00", "url": "http://arxiv.org/abs/1606.03137v3", "abstract": "For an autonomous system to be helpful to humans and to pose no unwarranted\nrisks, it needs to align its values with those of the humans in its environment\nin such a way that its actions contribute to the maximization of value for the\nhumans. We propose a formal definition of the value alignment problem as\ncooperative inverse reinforcement learning (CIRL). A CIRL problem is a\ncooperative, partial-information game with two agents, human and robot; both\nare rewarded according to the human's reward function, but the robot does not\ninitially know what this is. In contrast to classical IRL, where the human is\nassumed to act optimally in isolation, optimal CIRL solutions produce behaviors\nsuch as active teaching, active learning, and communicative actions that are\nmore effective in achieving value alignment. We show that computing optimal\njoint policies in CIRL games can be reduced to solving a POMDP, prove that\noptimality in isolation is suboptimal in CIRL, and derive an approximate CIRL\nalgorithm.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1606.07092": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1606.07092v1", "post_title": "Artificial Fun: Mapping Minds to the Space of Fun", "authors": ["Soenke Ziesche", "Roman V. Yampolskiy"], "date_published": "2016-06-22 20:28:53+00:00", "data_last_modified": "2016-06-22 20:28:53+00:00", "url": "http://arxiv.org/abs/1606.07092v1", "abstract": "Yampolskiy and others have shown that the space of possible minds is vast,\nactually infinite (Yampolskiy, 2015). A question of interest is 'Which\nactivities can minds perform during their lifetime?' This question is very\nbroad, thus in this article restricted to 'Which non-boring activities can\nminds perform?' The space of potential non-boring activities has been called by\nYudkowsky 'fun space' (Yudkowsky, 2009). This paper aims to discuss the\nrelation between various types of minds and the part of the fun space, which is\naccessible for them.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.OH", "categories": ["cs.OH"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1604.00289": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1604.00289v3", "post_title": "Building Machines That Learn and Think Like People", "authors": ["Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman"], "date_published": "2016-04-01 15:37:57+00:00", "data_last_modified": "2016-11-02 17:26:50+00:00", "url": "http://arxiv.org/abs/1604.00289v3", "abstract": "Recent progress in artificial intelligence (AI) has renewed interest in\nbuilding systems that learn and think like people. Many advances have come from\nusing deep neural networks trained end-to-end in tasks such as object\nrecognition, video games, and board games, achieving performance that equals or\neven beats humans in some respects. Despite their biological inspiration and\nperformance achievements, these systems differ from human intelligence in\ncrucial ways. We review progress in cognitive science suggesting that truly\nhuman-like learning and thinking machines will have to reach beyond current\nengineering trends in both what they learn, and how they learn it.\nSpecifically, we argue that these machines should (a) build causal models of\nthe world that support explanation and understanding, rather than merely\nsolving pattern recognition problems; (b) ground learning in intuitive theories\nof physics and psychology, to support and enrich the knowledge that is learned;\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\ngeneralize knowledge to new tasks and situations. We suggest concrete\nchallenges and promising routes towards these goals that can combine the\nstrengths of recent neural network advances with more structured cognitive\nmodels.", "author_comment": "In press at Behavioral and Brain Sciences. Open call for commentary\n  proposals (until Nov. 22, 2016).\n  https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/information/calls-for-commentary/open-calls-for-commentary", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.06782": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.06782v4", "post_title": "Gradient Surgery for Multi-Task Learning", "authors": ["Tianhe Yu", "Saurabh Kumar", "Abhishek Gupta", "Sergey Levine", "Karol Hausman", "Chelsea Finn"], "date_published": "2020-01-19 06:33:47+00:00", "data_last_modified": "2020-12-22 00:35:46+00:00", "url": "http://arxiv.org/abs/2001.06782v4", "abstract": "While deep learning and deep reinforcement learning (RL) systems have\ndemonstrated impressive results in domains such as image classification, game\nplaying, and robotic control, data efficiency remains a major challenge.\nMulti-task learning has emerged as a promising approach for sharing structure\nacross multiple tasks to enable more efficient learning. However, the\nmulti-task setting presents a number of optimization challenges, making it\ndifficult to realize large efficiency gains compared to learning tasks\nindependently. The reasons why multi-task learning is so challenging compared\nto single-task learning are not fully understood. In this work, we identify a\nset of three conditions of the multi-task optimization landscape that cause\ndetrimental gradient interference, and develop a simple yet general approach\nfor avoiding such interference between task gradients. We propose a form of\ngradient surgery that projects a task's gradient onto the normal plane of the\ngradient of any other task that has a conflicting gradient. On a series of\nchallenging multi-task supervised and multi-task RL problems, this approach\nleads to substantial gains in efficiency and performance. Further, it is\nmodel-agnostic and can be combined with previously-proposed multi-task\narchitectures for enhanced performance.", "author_comment": "NeurIPS 2020. Code is available at\n  https://github.com/tianheyu927/PCGrad", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "cs.RO", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1803.05049": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1803.05049v5", "post_title": "Fractal AI: A fragile theory of intelligence", "authors": ["Sergio Hernandez Cerezo", "Guillem Duran Ballester"], "date_published": "2018-03-13 21:17:26+00:00", "data_last_modified": "2020-07-30 09:52:44+00:00", "url": "http://arxiv.org/abs/1803.05049v5", "abstract": "Fractal AI is a theory for general artificial intelligence. It allows\nderiving new mathematical tools that constitute the foundations for a new kind\nof stochastic calculus, by modelling information using cellular automaton-like\nstructures instead of smooth functions. In the repository included we are\npresenting a new Agent, derived from the first principles of the theory, which\nis capable of solving Atari games several orders of magnitude more efficiently\nthan other similar techniques, like Monte Carlo Tree Search. The code provided\nshows how it is now possible to beat some of the current State of The Art\nbenchmarks on Atari games, without previous learning and using less than 1000\nsamples to calculate each one of the actions when standard MCTS uses 3 Million\nsamples. Among other things, Fractal AI makes it possible to generate a huge\ndatabase of top performing examples with a very little amount of computation\nrequired, transforming Reinforcement Learning into a supervised problem. The\nalgorithm presented is capable of solving the exploration vs exploitation\ndilemma on both the discrete and continuous cases, while maintaining control\nover any aspect of the behaviour of the Agent. From a general approach, new\ntechniques presented here have direct applications to other areas such as\nNon-equilibrium thermodynamics, chemistry, quantum physics, economics,\ninformation theory, and non-linear control theory.", "author_comment": "57 pages, python code on https://github.com/FragileTheory/FractalAI,\n  V4: typo in formula at 2.2.3, V4.1 typo in pseudocode at 4.3", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1907.04534": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1907.04534v1", "post_title": "The Role of Cooperation in Responsible AI Development", "authors": ["Amanda Askell", "Miles Brundage", "Gillian Hadfield"], "date_published": "2019-07-10 06:51:04+00:00", "data_last_modified": "2019-07-10 06:51:04+00:00", "url": "http://arxiv.org/abs/1907.04534v1", "abstract": "In this paper, we argue that competitive pressures could incentivize AI\ncompanies to underinvest in ensuring their systems are safe, secure, and have a\npositive social impact. Ensuring that AI systems are developed responsibly may\ntherefore require preventing and solving collective action problems between\ncompanies. We note that there are several key factors that improve the\nprospects for cooperation in collective action problems. We use this to\nidentify strategies to improve the prospects for industry cooperation on the\nresponsible development of AI.", "author_comment": "23 pages, 1 table", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI", "K.4.1; K.1"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1409.0473": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1409.0473v7", "post_title": "Neural Machine Translation by Jointly Learning to Align and Translate", "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "date_published": "2014-09-01 16:33:02+00:00", "data_last_modified": "2016-05-19 21:53:22+00:00", "url": "http://arxiv.org/abs/1409.0473v7", "abstract": "Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.", "author_comment": "Accepted at ICLR 2015 as oral presentation", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1310.4546": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1310.4546v1", "post_title": "Distributed Representations of Words and Phrases and their Compositionality", "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "date_published": "2013-10-16 23:28:53+00:00", "data_last_modified": "2013-10-16 23:28:53+00:00", "url": "http://arxiv.org/abs/1310.4546v1", "abstract": "The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large\nnumber of precise syntactic and semantic word relationships. In this paper we\npresent several extensions that improve both the quality of the vectors and the\ntraining speed. By subsampling of the frequent words we obtain significant\nspeedup and also learn more regular word representations. We also describe a\nsimple alternative to the hierarchical softmax called negative sampling. An\ninherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\nMotivated by this example, we present a simple method for finding phrases in\ntext, and show that learning good vector representations for millions of\nphrases is possible.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.11146": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.11146v2", "post_title": "Adversarial Reprogramming of Neural Networks", "authors": ["Gamaleldin F. Elsayed", "Ian Goodfellow", "Jascha Sohl-Dickstein"], "date_published": "2018-06-28 19:06:26+00:00", "data_last_modified": "2018-11-29 22:50:01+00:00", "url": "http://arxiv.org/abs/1806.11146v2", "abstract": "Deep neural networks are susceptible to \\emph{adversarial} attacks. In\ncomputer vision, well-crafted perturbations to images can cause neural networks\nto make mistakes such as confusing a cat with a computer. Previous adversarial\nattacks have been designed to degrade performance of models or cause machine\nlearning models to produce specific outputs chosen ahead of time by the\nattacker. We introduce attacks that instead {\\em reprogram} the target model to\nperform a task chosen by the attacker---without the attacker needing to specify\nor compute the desired output for each test-time input. This attack finds a\nsingle adversarial perturbation, that can be added to all test-time inputs to a\nmachine learning model in order to cause the model to perform a task chosen by\nthe adversary---even if the model was not trained to do this task. These\nperturbations can thus be considered a program for the new task. We demonstrate\nadversarial reprogramming on six ImageNet classification models, repurposing\nthese models to perform a counting task, as well as classification tasks:\nclassification of MNIST and CIFAR-10 examples presented as inputs to the\nImageNet model.", "author_comment": null, "journal_ref": "International Conference on Learning Representations 2019", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2105.11447": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2105.11447v1", "post_title": "True Few-Shot Learning with Language Models", "authors": ["Ethan Perez", "Douwe Kiela", "Kyunghyun Cho"], "date_published": "2021-05-24 17:55:51+00:00", "data_last_modified": "2021-05-24 17:55:51+00:00", "url": "http://arxiv.org/abs/2105.11447v1", "abstract": "Pretrained language models (LMs) perform well on many tasks even when\nlearning from a few examples, but prior work uses many held-out examples to\ntune various aspects of learning, such as hyperparameters, training objectives,\nand natural language templates (\"prompts\"). Here, we evaluate the few-shot\nability of LMs when such held-out examples are unavailable, a setting we call\ntrue few-shot learning. We test two model selection criteria, cross-validation\nand minimum description length, for choosing LM prompts and hyperparameters in\nthe true few-shot setting. On average, both marginally outperform random\nselection and greatly underperform selection based on held-out examples.\nMoreover, selection criteria often prefer models that perform significantly\nworse than randomly-selected ones. We find similar results even when taking\ninto account our uncertainty in a model's true performance during selection, as\nwell as when varying the amount of computation and number of examples used for\nselection. Overall, our findings suggest that prior work significantly\noverestimated the true few-shot ability of LMs given the difficulty of few-shot\nmodel selection.", "author_comment": "Code at https://github.com/ethanjperez/true_few_shot", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.10918": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.10918v1", "post_title": "Towards Empathic Deep Q-Learning", "authors": ["Bart Bussmann", "Jacqueline Heinerman", "Joel Lehman"], "date_published": "2019-06-26 08:59:02+00:00", "data_last_modified": "2019-06-26 08:59:02+00:00", "url": "http://arxiv.org/abs/1906.10918v1", "abstract": "As reinforcement learning (RL) scales to solve increasingly complex tasks,\ninterest continues to grow in the fields of AI safety and machine ethics. As a\ncontribution to these fields, this paper introduces an extension to Deep\nQ-Networks (DQNs), called Empathic DQN, that is loosely inspired both by\nempathy and the golden rule (\"Do unto others as you would have them do unto\nyou\"). Empathic DQN aims to help mitigate negative side effects to other agents\nresulting from myopic goal-directed behavior. We assume a setting where a\nlearning agent coexists with other independent agents (who receive unknown\nrewards), where some types of reward (e.g. negative rewards from physical harm)\nmay generalize across agents. Empathic DQN combines the typical (self-centered)\nvalue with the estimated value of other agents, by imagining (by its own\nstandards) the value of it being in the other's situation (by considering\nconstructed states where both agents are swapped). Proof-of-concept results in\ntwo gridworld environments highlight the approach's potential to decrease\ncollateral harms. While extending Empathic DQN to complex environments is\nnon-trivial, we believe that this first step highlights the potential of\nbridge-work between machine ethics and RL to contribute useful priors for\nnorm-abiding RL agents.", "author_comment": "To be presented as a poster at the IJCAI-19 AI Safety Workshop", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.05990": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.05990v1", "post_title": "What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study", "authors": ["Marcin Andrychowicz", "Anton Raichuk", "Piotr Sta\u0144czyk", "Manu Orsini", "Sertan Girgin", "Raphael Marinier", "L\u00e9onard Hussenot", "Matthieu Geist", "Olivier Pietquin", "Marcin Michalski", "Sylvain Gelly", "Olivier Bachem"], "date_published": "2020-06-10 17:59:03+00:00", "data_last_modified": "2020-06-10 17:59:03+00:00", "url": "http://arxiv.org/abs/2006.05990v1", "abstract": "In recent years, on-policy reinforcement learning (RL) has been successfully\napplied to many different continuous control tasks. While RL algorithms are\noften conceptually simple, their state-of-the-art implementations take numerous\nlow- and high-level design decisions that strongly affect the performance of\nthe resulting agents. Those choices are usually not extensively discussed in\nthe literature, leading to discrepancy between published descriptions of\nalgorithms and their implementations. This makes it hard to attribute progress\nin RL and slows down overall progress [Engstrom'20]. As a step towards filling\nthat gap, we implement >50 such ``choices'' in a unified on-policy RL\nframework, allowing us to investigate their impact in a large-scale empirical\nstudy. We train over 250'000 agents in five continuous control environments of\ndifferent complexity and provide insights and practical recommendations for\non-policy training of RL agents.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1802.05666": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1802.05666v2", "post_title": "Adversarial Risk and the Dangers of Evaluating Against Weak Attacks", "authors": ["Jonathan Uesato", "Brendan O'Donoghue", "Aaron van den Oord", "Pushmeet Kohli"], "date_published": "2018-02-15 17:13:18+00:00", "data_last_modified": "2018-06-12 14:20:27+00:00", "url": "http://arxiv.org/abs/1802.05666v2", "abstract": "This paper investigates recently proposed approaches for defending against\nadversarial examples and evaluating adversarial robustness. We motivate\n'adversarial risk' as an objective for achieving models robust to worst-case\ninputs. We then frame commonly used attacks and evaluation metrics as defining\na tractable surrogate objective to the true adversarial risk. This suggests\nthat models may optimize this surrogate rather than the true adversarial risk.\nWe formalize this notion as 'obscurity to an adversary,' and develop tools and\nheuristics for identifying obscured models and designing transparent models. We\ndemonstrate that this is a significant problem in practice by repurposing\ngradient-free optimization techniques into adversarial attacks, which we use to\ndecrease the accuracy of several recently proposed defenses to near zero. Our\nhope is that our formulations and results will help researchers to develop more\npowerful defenses.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.05542": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.05542v1", "post_title": "Unsupervised Visuomotor Control through Distributional Planning Networks", "authors": ["Tianhe Yu", "Gleb Shevchuk", "Dorsa Sadigh", "Chelsea Finn"], "date_published": "2019-02-14 18:54:54+00:00", "data_last_modified": "2019-02-14 18:54:54+00:00", "url": "http://arxiv.org/abs/1902.05542v1", "abstract": "While reinforcement learning (RL) has the potential to enable robots to\nautonomously acquire a wide range of skills, in practice, RL usually requires\nmanual, per-task engineering of reward functions, especially in real world\nsettings where aspects of the environment needed to compute progress are not\ndirectly accessible. To enable robots to autonomously learn skills, we instead\nconsider the problem of reinforcement learning without access to rewards. We\naim to learn an unsupervised embedding space under which the robot can measure\nprogress towards a goal for itself. Our approach explicitly optimizes for a\nmetric space under which action sequences that reach a particular state are\noptimal when the goal is the final state reached. This enables learning\neffective and control-centric representations that lead to more autonomous\nreinforcement learning algorithms. Our experiments on three simulated\nenvironments and two real-world manipulation problems show that our method can\nlearn effective goal metrics from unlabeled interaction, and use the learned\ngoal metrics for autonomous reinforcement learning.", "author_comment": "Videos available at https://sites.google.com/view/dpn-public/", "journal_ref": null, "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.CV", "cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.05464": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.05464v3", "post_title": "On Gradient-Based Learning in Continuous Games", "authors": ["Eric Mazumdar", "Lillian J. Ratliff", "S. Shankar Sastry"], "date_published": "2018-04-16 01:14:17+00:00", "data_last_modified": "2020-02-20 18:26:35+00:00", "url": "http://arxiv.org/abs/1804.05464v3", "abstract": "We formulate a general framework for competitive gradient-based learning that\nencompasses a wide breadth of multi-agent learning algorithms, and analyze the\nlimiting behavior of competitive gradient-based learning algorithms using\ndynamical systems theory. For both general-sum and potential games, we\ncharacterize a non-negligible subset of the local Nash equilibria that will be\navoided if each agent employs a gradient-based learning algorithm. We also shed\nlight on the issue of convergence to non-Nash strategies in general- and\nzero-sum games, which may have no relevance to the underlying game, and arise\nsolely due to the choice of algorithm. The existence and frequency of such\nstrategies may explain some of the difficulties encountered when using gradient\ndescent in zero-sum games as, e.g., in the training of generative adversarial\nnetworks. To reinforce the theoretical contributions, we provide empirical\nresults that highlight the frequency of linear quadratic dynamic games (a\nbenchmark for multi-agent reinforcement learning) that admit global Nash\nequilibria that are almost surely avoided by policy gradient.", "author_comment": null, "journal_ref": "SIAM Journal on Mathematics of Data Science 2020 2:1, 103-131", "doi": "10.1137/18M1231298", "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.09624": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.09624v1", "post_title": "On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference", "authors": ["Rohin Shah", "Noah Gundotra", "Pieter Abbeel", "Anca D. Dragan"], "date_published": "2019-06-23 18:41:31+00:00", "data_last_modified": "2019-06-23 18:41:31+00:00", "url": "http://arxiv.org/abs/1906.09624v1", "abstract": "Our goal is for agents to optimize the right reward function, despite how\ndifficult it is for us to specify what that is. Inverse Reinforcement Learning\n(IRL) enables us to infer reward functions from demonstrations, but it usually\nassumes that the expert is noisily optimal. Real people, on the other hand,\noften have systematic biases: risk-aversion, myopia, etc. One option is to try\nto characterize these biases and account for them explicitly during learning.\nBut in the era of deep learning, a natural suggestion researchers make is to\navoid mathematical models of human behavior that are fraught with specific\nassumptions, and instead use a purely data-driven approach. We decided to put\nthis to the test -- rather than relying on assumptions about which specific\nbias the demonstrator has when planning, we instead learn the demonstrator's\nplanning algorithm that they use to generate demonstrations, as a\ndifferentiable planner. Our exploration yielded mixed findings: on the one\nhand, learning the planner can lead to better reward inference than relying on\nthe wrong assumption; on the other hand, this benefit is dwarfed by the loss we\nincur by going from an exact to a differentiable planner. This suggest that at\nleast for the foreseeable future, agents need a middle ground between the\nflexibility of data-driven methods and the useful bias of known human biases.\nCode is available at https://tinyurl.com/learningbiases.", "author_comment": "Published at ICML 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.04260": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.04260v1", "post_title": "Provably Robust Detection of Out-of-distribution Data (almost) for free", "authors": ["Alexander Meinke", "Julian Bitterwolf", "Matthias Hein"], "date_published": "2021-06-08 11:40:49+00:00", "data_last_modified": "2021-06-08 11:40:49+00:00", "url": "http://arxiv.org/abs/2106.04260v1", "abstract": "When applying machine learning in safety-critical systems, a reliable\nassessment of the uncertainy of a classifier is required. However, deep neural\nnetworks are known to produce highly overconfident predictions on\nout-of-distribution (OOD) data and even if trained to be non-confident on OOD\ndata one can still adversarially manipulate OOD data so that the classifer\nagain assigns high confidence to the manipulated samples. In this paper we\npropose a novel method where from first principles we combine a certifiable OOD\ndetector with a standard classifier into an OOD aware classifier. In this way\nwe achieve the best of two worlds: certifiably adversarially robust OOD\ndetection, even for OOD samples close to the in-distribution, without loss in\nprediction accuracy and close to state-of-the-art OOD detection performance for\nnon-manipulated OOD data. Moreover, due to the particular construction our\nclassifier provably avoids the asymptotic overconfidence problem of standard\nneural networks.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.09716": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.09716v1", "post_title": "Robustness via curvature regularization, and vice versa", "authors": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Jonathan Uesato", "Pascal Frossard"], "date_published": "2018-11-23 22:03:40+00:00", "data_last_modified": "2018-11-23 22:03:40+00:00", "url": "http://arxiv.org/abs/1811.09716v1", "abstract": "State-of-the-art classifiers have been shown to be largely vulnerable to\nadversarial perturbations. One of the most effective strategies to improve\nrobustness is adversarial training. In this paper, we investigate the effect of\nadversarial training on the geometry of the classification landscape and\ndecision boundaries. We show in particular that adversarial training leads to a\nsignificant decrease in the curvature of the loss surface with respect to\ninputs, leading to a drastically more \"linear\" behaviour of the network. Using\na locally quadratic approximation, we provide theoretical evidence on the\nexistence of a strong relation between large robustness and small curvature. To\nfurther show the importance of reduced curvature for improving the robustness,\nwe propose a new regularizer that directly minimizes curvature of the loss\nsurface, and leads to adversarial robustness that is on par with adversarial\ntraining. Besides being a more efficient and principled alternative to\nadversarial training, the proposed regularizer confirms our claims on the\nimportance of exhibiting quasi-linear behavior in the vicinity of data points\nin order to achieve robustness.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2007.09540": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2007.09540v1", "post_title": "Multi-Principal Assistance Games", "authors": ["Arnaud Fickinger", "Simon Zhuang", "Dylan Hadfield-Menell", "Stuart Russell"], "date_published": "2020-07-19 00:23:25+00:00", "data_last_modified": "2020-07-19 00:23:25+00:00", "url": "http://arxiv.org/abs/2007.09540v1", "abstract": "Assistance games (also known as cooperative inverse reinforcement learning\ngames) have been proposed as a model for beneficial AI, wherein a robotic agent\nmust act on behalf of a human principal but is initially uncertain about the\nhumans payoff function. This paper studies multi-principal assistance games,\nwhich cover the more general case in which the robot acts on behalf of N humans\nwho may have widely differing payoffs. Impossibility theorems in social choice\ntheory and voting theory can be applied to such games, suggesting that\nstrategic behavior by the human principals may complicate the robots task in\nlearning their payoffs. We analyze in particular a bandit apprentice game in\nwhich the humans act first to demonstrate their individual preferences for the\narms and then the robot acts to maximize the sum of human payoffs. We explore\nthe extent to which the cost of choosing suboptimal arms reduces the incentive\nto mislead, a form of natural mechanism design. In this context we propose a\nsocial choice method that uses shared control of a system to combine preference\ninference with social welfare optimization.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.09030": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.09030v1", "post_title": "On Adversarial Examples for Character-Level Neural Machine Translation", "authors": ["Javid Ebrahimi", "Daniel Lowd", "Dejing Dou"], "date_published": "2018-06-23 20:08:56+00:00", "data_last_modified": "2018-06-23 20:08:56+00:00", "url": "http://arxiv.org/abs/1806.09030v1", "abstract": "Evaluating on adversarial examples has become a standard procedure to measure\nrobustness of deep learning models. Due to the difficulty of creating white-box\nadversarial examples for discrete text input, most analyses of the robustness\nof NLP models have been done through black-box adversarial examples. We\ninvestigate adversarial examples for character-level neural machine translation\n(NMT), and contrast black-box adversaries with a novel white-box adversary,\nwhich employs differentiable string-edit operations to rank adversarial\nchanges. We propose two novel types of attacks which aim to remove or change a\nword in a translation, rather than simply break the NMT. We demonstrate that\nwhite-box adversarial examples are significantly stronger than their black-box\ncounterparts in different attack scenarios, which show more serious\nvulnerabilities than previously known. In addition, after performing\nadversarial training, which takes only 3 times longer than regular training, we\ncan improve the model's robustness significantly.", "author_comment": null, "journal_ref": "COLING 2018", "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1812.05979": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1812.05979v1", "post_title": "Scaling shared model governance via model splitting", "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "date_published": "2018-12-14 15:29:21+00:00", "data_last_modified": "2018-12-14 15:29:21+00:00", "url": "http://arxiv.org/abs/1812.05979v1", "abstract": "Currently the only techniques for sharing governance of a deep learning model\nare homomorphic encryption and secure multiparty computation. Unfortunately,\nneither of these techniques is applicable to the training of large neural\nnetworks due to their large computational and communication overheads. As a\nscalable technique for shared model governance, we propose splitting deep\nlearning model between multiple parties. This paper empirically investigates\nthe security guarantee of this technique, which is introduced as the problem of\nmodel completion: Given the entire training data set or an environment\nsimulator, and a subset of the parameters of a trained deep learning model, how\nmuch training is required to recover the model's original performance? We\ndefine a metric for evaluating the hardness of the model completion problem and\nstudy it empirically in both supervised learning on ImageNet and reinforcement\nlearning on Atari and DeepMind~Lab. Our experiments show that (1) the model\ncompletion problem is harder in reinforcement learning than in supervised\nlearning because of the unavailability of the trained agent's trajectories, and\n(2) its hardness depends not primarily on the number of parameters of the\nmissing part, but more so on their type and location. Our results suggest that\nmodel splitting might be a feasible technique for shared model governance in\nsome settings where training is very expensive.", "author_comment": "9 pages", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CR", "cs.NE"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1312.6114v": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1312.6114v10", "post_title": "Auto-Encoding Variational Bayes", "authors": ["Diederik P Kingma", "Max Welling"], "date_published": "2013-12-20 20:58:10+00:00", "data_last_modified": "2014-05-01 15:43:28+00:00", "url": "http://arxiv.org/abs/1312.6114v10", "abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.01672": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.01672v3", "post_title": "Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization", "authors": ["Alexandre Laterre", "Yunguan Fu", "Mohamed Khalil Jabri", "Alain-Sam Cohen", "David Kas", "Karl Hajjar", "Torbjorn S. Dahl", "Amine Kerkeni", "Karim Beguir"], "date_published": "2018-07-04 16:40:53+00:00", "data_last_modified": "2018-12-06 23:32:05+00:00", "url": "http://arxiv.org/abs/1807.01672v3", "abstract": "Adversarial self-play in two-player games has delivered impressive results\nwhen used with reinforcement learning algorithms that combine deep neural\nnetworks and tree search. Algorithms like AlphaZero and Expert Iteration learn\ntabula-rasa, producing highly informative training data on the fly. However,\nthe self-play training strategy is not directly applicable to single-player\ngames. Recently, several practically important combinatorial optimisation\nproblems, such as the travelling salesman problem and the bin packing problem,\nhave been reformulated as reinforcement learning problems, increasing the\nimportance of enabling the benefits of self-play beyond two-player games. We\npresent the Ranked Reward (R2) algorithm which accomplishes this by ranking the\nrewards obtained by a single agent over multiple games to create a relative\nperformance metric. Results from applying the R2 algorithm to instances of a\ntwo-dimensional and three-dimensional bin packing problems show that it\noutperforms generic Monte Carlo tree search, heuristic algorithms and integer\nprogramming solvers. We also present an analysis of the ranked reward\nmechanism, in particular, the effects of problem instances with varying\ndifficulty and different ranking thresholds.", "author_comment": null, "journal_ref": "Presented at the Thirty-second Conference on Neural Information\n  Processing Systems (NeurIPS 2018), Deep Reinforcement Learning Workshop,\n  Montreal, Canada, December 3-8, 2018", "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2007.02382": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2007.02382v2", "post_title": "Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions", "authors": ["Michael Chang", "Sidhant Kaushik", "S. Matthew Weinberg", "Thomas L. Griffiths", "Sergey Levine"], "date_published": "2020-07-05 16:41:09+00:00", "data_last_modified": "2020-08-14 05:20:29+00:00", "url": "http://arxiv.org/abs/2007.02382v2", "abstract": "This paper seeks to establish a framework for directing a society of simple,\nspecialized, self-interested agents to solve what traditionally are posed as\nmonolithic single-agent sequential decision problems. What makes it challenging\nto use a decentralized approach to collectively optimize a central objective is\nthe difficulty in characterizing the equilibrium strategy profile of\nnon-cooperative games. To overcome this challenge, we design a mechanism for\ndefining the learning environment of each agent for which we know that the\noptimal solution for the global objective coincides with a Nash equilibrium\nstrategy profile of the agents optimizing their own local objectives. The\nsociety functions as an economy of agents that learn the credit assignment\nprocess itself by buying and selling to each other the right to operate on the\nenvironment state. We derive a class of decentralized reinforcement learning\nalgorithms that are broadly applicable not only to standard reinforcement\nlearning but also for selecting options in semi-MDPs and dynamically composing\ncomputation graphs. Lastly, we demonstrate the potential advantages of a\nsociety's inherent modular structure for more efficient transfer learning.", "author_comment": "18 pages, 13 figures, accepted to the International Conference on\n  Machine Learning (ICML) 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.GT", "cs.MA", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.04338": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.04338v1", "post_title": "Engines of Power: Electricity, AI, and General-Purpose Military Transformations", "authors": ["Jeffrey Ding", "Allan Dafoe"], "date_published": "2021-06-08 13:55:19+00:00", "data_last_modified": "2021-06-08 13:55:19+00:00", "url": "http://arxiv.org/abs/2106.04338v1", "abstract": "Major theories of military innovation focus on relatively narrow\ntechnological developments, such as nuclear weapons or aircraft carriers.\nArguably the most profound military implications of technological change,\nhowever, come from more fundamental advances arising from general purpose\ntechnologies, such as the steam engine, electricity, and the computer. With few\nexceptions, political scientists have not theorized about GPTs. Drawing from\nthe economics literature on GPTs, we distill several propositions on how and\nwhen GPTs affect military affairs. We call these effects general-purpose\nmilitary transformations. In particular, we argue that the impacts of GMTs on\nmilitary effectiveness are broad, delayed, and shaped by indirect productivity\nspillovers. Additionally, GMTs differentially advantage those militaries that\ncan draw from a robust industrial base in the GPT. To illustrate the\nexplanatory value of our theory, we conduct a case study of the military\nconsequences of electricity, the prototypical GPT. Finally, we apply our\nfindings to artificial intelligence, which will plausibly cause a profound\ngeneral-purpose military transformation.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "econ.GN", "categories": ["econ.GN", "q-fin.EC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1804.09160": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1804.09160v2", "post_title": "No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling", "authors": ["Xin Wang", "Wenhu Chen", "Yuan-Fang Wang", "William Yang Wang"], "date_published": "2018-04-24 17:41:24+00:00", "data_last_modified": "2018-07-09 00:15:14+00:00", "url": "http://arxiv.org/abs/1804.09160v2", "abstract": "Though impressive results have been achieved in visual captioning, the task\nof generating abstract stories from photo streams is still a little-tapped\nproblem. Different from captions, stories have more expressive language styles\nand contain many imaginary concepts that do not appear in the images. Thus it\nposes challenges to behavioral cloning algorithms. Furthermore, due to the\nlimitations of automatic metrics on evaluating story quality, reinforcement\nlearning methods with hand-crafted rewards also face difficulties in gaining an\noverall performance boost. Therefore, we propose an Adversarial REward Learning\n(AREL) framework to learn an implicit reward function from human\ndemonstrations, and then optimize policy search with the learned reward\nfunction. Though automatic eval- uation indicates slight performance boost over\nstate-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation\nshows that our approach achieves significant improvement in generating more\nhuman-like stories than SOTA systems.", "author_comment": "ACL 2018. 15 pages, 10 figures, 4 tables, with supplementary material", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.00078": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.00078v1", "post_title": "Regulatory Markets for AI Safety", "authors": ["Jack Clark", "Gillian K. Hadfield"], "date_published": "2019-12-11 19:21:54+00:00", "data_last_modified": "2019-12-11 19:21:54+00:00", "url": "http://arxiv.org/abs/2001.00078v1", "abstract": "We propose a new model for regulation to achieve AI safety: global regulatory\nmarkets. We first sketch the model in general terms and provide an overview of\nthe costs and benefits of this approach. We then demonstrate how the model\nmight work in practice: responding to the risk of adversarial attacks on AI\nmodels employed in commercial drones.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "econ.GN", "q-fin.EC"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.04784": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.04784v1", "post_title": "Improving Generalization for Abstract Reasoning Tasks Using Disentangled Feature Representations", "authors": ["Xander Steenbrugge", "Sam Leroux", "Tim Verbelen", "Bart Dhoedt"], "date_published": "2018-11-12 15:23:26+00:00", "data_last_modified": "2018-11-12 15:23:26+00:00", "url": "http://arxiv.org/abs/1811.04784v1", "abstract": "In this work we explore the generalization characteristics of unsupervised\nrepresentation learning by leveraging disentangled VAE's to learn a useful\nlatent space on a set of relational reasoning problems derived from Raven\nProgressive Matrices. We show that the latent representations, learned by\nunsupervised training using the right objective function, significantly\noutperform the same architectures trained with purely supervised learning,\nespecially when it comes to generalization.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.07722": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.07722v1", "post_title": "Task-Agnostic Meta-Learning for Few-shot Learning", "authors": ["Muhammad Abdullah Jamal", "Guo-Jun Qi", "Mubarak Shah"], "date_published": "2018-05-20 07:50:42+00:00", "data_last_modified": "2018-05-20 07:50:42+00:00", "url": "http://arxiv.org/abs/1805.07722v1", "abstract": "Meta-learning approaches have been proposed to tackle the few-shot learning\nproblem.Typically, a meta-learner is trained on a variety of tasks in the hopes\nof being generalizable to new tasks. However, the generalizability on new tasks\nof a meta-learner could be fragile when it is over-trained on existing tasks\nduring meta-training phase. In other words, the initial model of a meta-learner\ncould be too biased towards existing tasks to adapt to new tasks, especially\nwhen only very few examples are available to update the model. To avoid a\nbiased meta-learner and improve its generalizability, we propose a novel\nparadigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we\npresent an entropy-based approach that meta-learns an unbiased initial model\nwith the largest uncertainty over the output labels by preventing it from\nover-performing in classification tasks. Alternatively, a more general\ninequality-minimization TAML is presented for more ubiquitous scenarios by\ndirectly minimizing the inequality of initial losses beyond the classification\ntasks wherever a suitable loss can be defined.Experiments on benchmarked\ndatasets demonstrate that the proposed approaches outperform compared\nmeta-learning algorithms in both few-shot classification and reinforcement\nlearning tasks.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.12186": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.12186v4", "post_title": "Asymptotically Unambitious Artificial General Intelligence", "authors": ["Michael K Cohen", "Badri Vellambi", "Marcus Hutter"], "date_published": "2019-05-29 02:48:15+00:00", "data_last_modified": "2020-07-21 13:27:38+00:00", "url": "http://arxiv.org/abs/1905.12186v4", "abstract": "General intelligence, the ability to solve arbitrary solvable problems, is\nsupposed by many to be artificially constructible. Narrow intelligence, the\nability to solve a given particularly difficult problem, has seen impressive\nrecent development. Notable examples include self-driving cars, Go engines,\nimage classifiers, and translators. Artificial General Intelligence (AGI)\npresents dangers that narrow intelligence does not: if something smarter than\nus across every domain were indifferent to our concerns, it would be an\nexistential threat to humanity, just as we threaten many species despite no ill\nwill. Even the theory of how to maintain the alignment of an AGI's goals with\nour own has proven highly elusive. We present the first algorithm we are aware\nof for asymptotically unambitious AGI, where \"unambitiousness\" includes not\nseeking arbitrary power. Thus, we identify an exception to the Instrumental\nConvergence Thesis, which is roughly that by default, an AGI would seek power,\nincluding over us.", "author_comment": "9 pages with 5 figures; 10 page Appendix with 2 figures", "journal_ref": "Proc.AAAI. 34 (2020) 2467-2476", "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "I.2.0, I.2.6", "I.2.0; I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.05743": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.05743v2", "post_title": "Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning", "authors": ["Akanksha Atrey", "Kaleigh Clary", "David Jensen"], "date_published": "2019-12-09 12:42:07+00:00", "data_last_modified": "2020-02-20 21:40:15+00:00", "url": "http://arxiv.org/abs/1912.05743v2", "abstract": "Saliency maps are frequently used to support explanations of the behavior of\ndeep reinforcement learning (RL) agents. However, a review of how saliency maps\nare used in practice indicates that the derived explanations are often\nunfalsifiable and can be highly subjective. We introduce an empirical approach\ngrounded in counterfactual reasoning to test the hypotheses generated from\nsaliency maps and assess the degree to which they correspond to the semantics\nof RL environments. We use Atari games, a common benchmark for deep RL, to\nevaluate three types of saliency maps. Our results show the extent to which\nexisting claims about Atari games can be evaluated and suggest that saliency\nmaps are best viewed as an exploratory tool rather than an explanatory tool.", "author_comment": "Published at ICLR 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2006.06547": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2006.06547v2", "post_title": "Avoiding Side Effects in Complex Environments", "authors": ["Alexander Matt Turner", "Neale Ratzlaff", "Prasad Tadepalli"], "date_published": "2020-06-11 16:02:30+00:00", "data_last_modified": "2020-10-22 15:15:46+00:00", "url": "http://arxiv.org/abs/2006.06547v2", "abstract": "Reward function specification can be difficult. Rewarding the agent for\nmaking a widget may be easy, but penalizing the multitude of possible negative\nside effects is hard. In toy environments, Attainable Utility Preservation\n(AUP) avoided side effects by penalizing shifts in the ability to achieve\nrandomly generated goals. We scale this approach to large, randomly generated\nenvironments based on Conway's Game of Life. By preserving optimal value for a\nsingle randomly generated reward function, AUP incurs modest overhead while\nleading the agent to complete the specified task and avoid many side effects.\nVideos and code are available at https://avoiding-side-effects.github.io/.", "author_comment": "Accepted as spotlight paper at NeurIPS 2020. 10 pages main paper; 19\n  pages with appendices", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.14603": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.14603v1", "post_title": "Learning to be Safe: Deep RL with a Safety Critic", "authors": ["Krishnan Srinivasan", "Benjamin Eysenbach", "Sehoon Ha", "Jie Tan", "Chelsea Finn"], "date_published": "2020-10-27 20:53:20+00:00", "data_last_modified": "2020-10-27 20:53:20+00:00", "url": "http://arxiv.org/abs/2010.14603v1", "abstract": "Safety is an essential component for deploying reinforcement learning (RL)\nalgorithms in real-world scenarios, and is critical during the learning process\nitself. A natural first approach toward safe RL is to manually specify\nconstraints on the policy's behavior. However, just as learning has enabled\nprogress in large-scale development of AI systems, learning safety\nspecifications may also be necessary to ensure safety in messy open-world\nenvironments where manual safety specifications cannot scale. Akin to how\nhumans learn incrementally starting in child-safe environments, we propose to\nlearn how to be safe in one set of tasks and environments, and then use that\nlearned intuition to constrain future behaviors when learning new, modified\ntasks. We empirically study this form of safety-constrained transfer learning\nin three challenging domains: simulated navigation, quadruped locomotion, and\ndexterous in-hand manipulation. In comparison to standard deep RL techniques\nand prior approaches to safe RL, we find that our method enables the learning\nof new tasks and in new environments with both substantially fewer safety\nincidents, such as falling or dropping an object, and faster, more stable\nlearning. This suggests a path forward not only for safer RL systems, but also\nfor more effective RL systems.", "author_comment": "In submission, 16 pages (including appendix)", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1909.06769": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1909.06769v1", "post_title": "VILD: Variational Imitation Learning with Diverse-quality Demonstrations", "authors": ["Voot Tangkaratt", "Bo Han", "Mohammad Emtiyaz Khan", "Masashi Sugiyama"], "date_published": "2019-09-15 09:42:33+00:00", "data_last_modified": "2019-09-15 09:42:33+00:00", "url": "http://arxiv.org/abs/1909.06769v1", "abstract": "The goal of imitation learning (IL) is to learn a good policy from\nhigh-quality demonstrations. However, the quality of demonstrations in reality\ncan be diverse, since it is easier and cheaper to collect demonstrations from a\nmix of experts and amateurs. IL in such situations can be challenging,\nespecially when the level of demonstrators' expertise is unknown. We propose a\nnew IL method called \\underline{v}ariational \\underline{i}mitation\n\\underline{l}earning with \\underline{d}iverse-quality demonstrations (VILD),\nwhere we explicitly model the level of demonstrators' expertise with a\nprobabilistic graphical model and estimate it along with a reward function. We\nshow that a naive approach to estimation is not suitable to large state and\naction spaces, and fix its issues by using a variational approach which can be\neasily implemented using existing reinforcement learning methods. Experiments\non continuous-control benchmarks demonstrate that VILD outperforms\nstate-of-the-art methods. Our work enables scalable and data-efficient IL under\nmore realistic settings than before.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.08167": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.08167v2", "post_title": "Expressing Robot Incapability", "authors": ["Minae Kwon", "Sandy H. Huang", "Anca D. Dragan"], "date_published": "2018-10-18 17:14:04+00:00", "data_last_modified": "2020-06-12 14:11:28+00:00", "url": "http://arxiv.org/abs/1810.08167v2", "abstract": "Our goal is to enable robots to express their incapability, and to do so in a\nway that communicates both what they are trying to accomplish and why they are\nunable to accomplish it. We frame this as a trajectory optimization problem:\nmaximize the similarity between the motion expressing incapability and what\nwould amount to successful task execution, while obeying the physical limits of\nthe robot. We introduce and evaluate candidate similarity measures, and show\nthat one in particular generalizes to a range of tasks, while producing\nexpressive motions that are tailored to each task. Our user study supports that\nour approach automatically generates motions expressing incapability that\ncommunicate both what and why to end-users, and improve their overall\nperception of the robot and willingness to collaborate with it in the future.", "author_comment": "HRI 2018", "journal_ref": null, "doi": "10.1145/3171221.3171276", "primary_category": "cs.RO", "categories": ["cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.12797": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.12797v1", "post_title": "ReAct: Out-of-distribution Detection With Rectified Activations", "authors": ["Yiyou Sun", "Chuan Guo", "Yixuan Li"], "date_published": "2021-11-24 21:02:07+00:00", "data_last_modified": "2021-11-24 21:02:07+00:00", "url": "http://arxiv.org/abs/2111.12797v1", "abstract": "Out-of-distribution (OOD) detection has received much attention lately due to\nits practical importance in enhancing the safe deployment of neural networks.\nOne of the primary challenges is that models often produce highly confident\npredictions on OOD data, which undermines the driving principle in OOD\ndetection that the model should only be confident about in-distribution\nsamples. In this work, we propose ReAct--a simple and effective technique for\nreducing model overconfidence on OOD data. Our method is motivated by novel\nanalysis on internal activations of neural networks, which displays highly\ndistinctive signature patterns for OOD distributions. Our method can generalize\neffectively to different network architectures and different OOD detection\nscores. We empirically demonstrate that ReAct achieves competitive detection\nperformance on a comprehensive suite of benchmark datasets, and give\ntheoretical explication for our method's efficacy. On the ImageNet benchmark,\nReAct reduces the false positive rate (FPR95) by 25.05% compared to the\nprevious best method.", "author_comment": "NeurIPS 2021", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.03088": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.03088v1", "post_title": "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions", "authors": ["Matthew MacKay", "Paul Vicol", "Jon Lorraine", "David Duvenaud", "Roger Grosse"], "date_published": "2019-03-07 18:26:46+00:00", "data_last_modified": "2019-03-07 18:26:46+00:00", "url": "http://arxiv.org/abs/1903.03088v1", "abstract": "Hyperparameter optimization can be formulated as a bilevel optimization\nproblem, where the optimal parameters on the training set depend on the\nhyperparameters. We aim to adapt regularization hyperparameters for neural\nnetworks by fitting compact approximations to the best-response function, which\nmaps hyperparameters to optimal weights and biases. We show how to construct\nscalable best-response approximations for neural networks by modeling the\nbest-response as a single network whose hidden units are gated conditionally on\nthe regularizer. We justify this approximation by showing the exact\nbest-response for a shallow linear network with L2-regularized Jacobian can be\nrepresented by a similar gating mechanism. We fit this model using a\ngradient-based hyperparameter optimization algorithm which alternates between\napproximating the best-response around the current hyperparameters and\noptimizing the hyperparameters using the approximate best-response function.\nUnlike other gradient-based approaches, we do not require differentiating the\ntraining loss with respect to the hyperparameters, allowing us to tune discrete\nhyperparameters, data augmentation hyperparameters, and dropout probabilities.\nBecause the hyperparameters are adapted online, our approach discovers\nhyperparameter schedules that can outperform fixed hyperparameter values.\nEmpirically, our approach outperforms competing hyperparameter optimization\nmethods on large-scale deep learning problems. We call our networks, which\nupdate their own hyperparameters online during training, Self-Tuning Networks\n(STNs).", "author_comment": "Published as a conference paper at ICLR 2019", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1512.02595": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1512.02595v1", "post_title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "authors": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos", "Erich Elsen", "Jesse Engel", "Linxi Fan", "Christopher Fougner", "Tony Han", "Awni Hannun", "Billy Jun", "Patrick LeGresley", "Libby Lin", "Sharan Narang", "Andrew Ng", "Sherjil Ozair", "Ryan Prenger", "Jonathan Raiman", "Sanjeev Satheesh", "David Seetapun", "Shubho Sengupta", "Yi Wang", "Zhiqian Wang", "Chong Wang", "Bo Xiao", "Dani Yogatama", "Jun Zhan", "Zhenyao Zhu"], "date_published": "2015-12-08 19:13:50+00:00", "data_last_modified": "2015-12-08 19:13:50+00:00", "url": "http://arxiv.org/abs/1512.02595v1", "abstract": "We show that an end-to-end deep learning approach can be used to recognize\neither English or Mandarin Chinese speech--two vastly different languages.\nBecause it replaces entire pipelines of hand-engineered components with neural\nnetworks, end-to-end learning allows us to handle a diverse variety of speech\nincluding noisy environments, accents and different languages. Key to our\napproach is our application of HPC techniques, resulting in a 7x speedup over\nour previous system. Because of this efficiency, experiments that previously\ntook weeks now run in days. This enables us to iterate more quickly to identify\nsuperior architectures and algorithms. As a result, in several cases, our\nsystem is competitive with the transcription of human workers when benchmarked\non standard datasets. Finally, using a technique called Batch Dispatch with\nGPUs in the data center, we show that our system can be inexpensively deployed\nin an online setting, delivering low latency when serving users at scale.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1202.6177": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1202.6177v1", "post_title": "Can Intelligence Explode?", "authors": ["Marcus Hutter"], "date_published": "2012-02-28 10:46:29+00:00", "data_last_modified": "2012-02-28 10:46:29+00:00", "url": "http://arxiv.org/abs/1202.6177v1", "abstract": "The technological singularity refers to a hypothetical scenario in which\ntechnological advances virtually explode. The most popular scenario is the\ncreation of super-intelligent algorithms that recursively create ever higher\nintelligences. It took many decades for these ideas to spread from science\nfiction to popular science magazines and finally to attract the attention of\nserious philosophers. David Chalmers' (JCS 2010) article is the first\ncomprehensive philosophical analysis of the singularity in a respected\nphilosophy journal. The motivation of my article is to augment Chalmers' and to\ndiscuss some issues not addressed by him, in particular what it could mean for\nintelligence to explode. In this course, I will (have to) provide a more\ncareful treatment of what intelligence actually is, separate speed from\nintelligence explosion, compare what super-intelligent participants and\nclassical human observers might experience and do, discuss immediate\nimplications for the diversity and value of life, consider possible bounds on\nintelligence, and contemplate intelligences right at the singularity.", "author_comment": "20 LaTeX pages", "journal_ref": "Journal of Consciousness Studies, 19:1-2 (2012) 143-166", "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "physics.soc-ph"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1602.04938": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1602.04938v3", "post_title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "date_published": "2016-02-16 08:20:14+00:00", "data_last_modified": "2016-08-09 17:54:52+00:00", "url": "http://arxiv.org/abs/1602.04938v3", "abstract": "Despite widespread adoption, machine learning models remain mostly black\nboxes. Understanding the reasons behind predictions is, however, quite\nimportant in assessing trust, which is fundamental if one plans to take action\nbased on a prediction, or when choosing whether to deploy a new model. Such\nunderstanding also provides insights into the model, which can be used to\ntransform an untrustworthy model or prediction into a trustworthy one. In this\nwork, we propose LIME, a novel explanation technique that explains the\npredictions of any classifier in an interpretable and faithful manner, by\nlearning an interpretable model locally around the prediction. We also propose\na method to explain models by presenting representative individual predictions\nand their explanations in a non-redundant way, framing the task as a submodular\noptimization problem. We demonstrate the flexibility of these methods by\nexplaining different models for text (e.g. random forests) and image\nclassification (e.g. neural networks). We show the utility of explanations via\nnovel experiments, both simulated and with human subjects, on various scenarios\nthat require trust: deciding if one should trust a prediction, choosing between\nmodels, improving an untrustworthy classifier, and identifying why a classifier\nshould not be trusted.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.15121": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.15121v1", "post_title": "Pyramid Adversarial Training Improves ViT Performance", "authors": ["Charles Herrmann", "Kyle Sargent", "Lu Jiang", "Ramin Zabih", "Huiwen Chang", "Ce Liu", "Dilip Krishnan", "Deqing Sun"], "date_published": "2021-11-30 04:38:14+00:00", "data_last_modified": "2021-11-30 04:38:14+00:00", "url": "http://arxiv.org/abs/2111.15121v1", "abstract": "Aggressive data augmentation is a key component of the strong generalization\ncapabilities of Vision Transformer (ViT). One such data augmentation technique\nis adversarial training; however, many prior works have shown that this often\nresults in poor clean accuracy. In this work, we present Pyramid Adversarial\nTraining, a simple and effective technique to improve ViT's overall\nperformance. We pair it with a \"matched\" Dropout and stochastic depth\nregularization, which adopts the same Dropout and stochastic depth\nconfiguration for the clean and adversarial samples. Similar to the\nimprovements on CNNs by AdvProp (not directly applicable to ViT), our Pyramid\nAdversarial Training breaks the trade-off between in-distribution accuracy and\nout-of-distribution robustness for ViT and related architectures. It leads to\n$1.82\\%$ absolute improvement on ImageNet clean accuracy for the ViT-B model\nwhen trained only on ImageNet-1K data, while simultaneously boosting\nperformance on $7$ ImageNet robustness metrics, by absolute numbers ranging\nfrom $1.76\\%$ to $11.45\\%$. We set a new state-of-the-art for ImageNet-C (41.4\nmCE), ImageNet-R ($53.92\\%$), and ImageNet-Sketch ($41.04\\%$) without extra\ndata, using only the ViT-B/16 backbone and our Pyramid Adversarial Training.\nOur code will be publicly available upon acceptance.", "author_comment": "32 pages, including references & supplementary material", "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1901.00064": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1901.00064v3", "post_title": "Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)", "authors": ["Peter Eckersley"], "date_published": "2018-12-31 23:51:27+00:00", "data_last_modified": "2019-03-05 03:12:49+00:00", "url": "http://arxiv.org/abs/1901.00064v3", "abstract": "Utility functions or their equivalents (value functions, objective functions,\nloss functions, reward functions, preference orderings) are a central tool in\nmost current machine learning systems. These mechanisms for defining goals and\nguiding optimization run into practical and conceptual difficulty when there\nare independent, multi-dimensional objectives that need to be pursued\nsimultaneously and cannot be reduced to each other. Ethicists have proved\nseveral impossibility theorems that stem from this origin; those results appear\nto show that there is no way of formally specifying what it means for an\noutcome to be good for a population without violating strong human ethical\nintuitions (in such cases, the objective function is a social welfare\nfunction). We argue that this is a practical problem for any machine learning\nsystem (such as medical decision support systems or autonomous weapons) or\nrigidly rule-based bureaucracy that will make high stakes decisions about human\nlives: such systems should not use objective functions in the strict\nmathematical sense.\n  We explore the alternative of using uncertain objectives, represented for\ninstance as partially ordered preferences, or as probability distributions over\ntotal orders. We show that previously known impossibility theorems can be\ntransformed into uncertainty theorems in both of those settings, and prove\nlower bounds on how much uncertainty is implied by the impossibility results.\nWe close by proposing two conjectures about the relationship between\nuncertainty in objectives and severe unintended consequences from AI systems.", "author_comment": "Published in SafeAI 2019: Proceedings of the AAAI Workshop on\n  Artificial Intelligence Safety 2019", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.05876": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.05876v2", "post_title": "Neurosymbolic AI: The 3rd Wave", "authors": ["Artur d'Avila Garcez", "Luis C. Lamb"], "date_published": "2020-12-10 18:31:38+00:00", "data_last_modified": "2020-12-16 23:21:05+00:00", "url": "http://arxiv.org/abs/2012.05876v2", "abstract": "Current advances in Artificial Intelligence (AI) and Machine Learning (ML)\nhave achieved unprecedented impact across research communities and industry.\nNevertheless, concerns about trust, safety, interpretability and accountability\nof AI were raised by influential thinkers. Many have identified the need for\nwell-founded knowledge representation and reasoning to be integrated with deep\nlearning and for sound explainability. Neural-symbolic computing has been an\nactive area of research for many years seeking to bring together robust\nlearning in neural networks with reasoning and explainability via symbolic\nrepresentations for network models. In this paper, we relate recent and early\nresearch results in neurosymbolic AI with the objective of identifying the key\ningredients of the next wave of AI systems. We focus on research that\nintegrates in a principled way neural network-based learning with symbolic\nknowledge representation and logical reasoning. The insights provided by 20\nyears of neural-symbolic computing are shown to shed new light onto the\nincreasingly prominent role of trust, safety, interpretability and\naccountability of AI. We also identify promising directions and challenges for\nthe next decade of AI research from the perspective of neural-symbolic systems.", "author_comment": "37 pages", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG", "I.2.4; I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1807.00196": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1807.00196v1", "post_title": "Modeling Friends and Foes", "authors": ["Pedro A. Ortega", "Shane Legg"], "date_published": "2018-06-30 16:07:43+00:00", "data_last_modified": "2018-06-30 16:07:43+00:00", "url": "http://arxiv.org/abs/1807.00196v1", "abstract": "How can one detect friendly and adversarial behavior from raw data? Detecting\nwhether an environment is a friend, a foe, or anything in between, remains a\npoorly understood yet desirable ability for safe and robust agents. This paper\nproposes a definition of these environmental \"attitudes\" based on an\ncharacterization of the environment's ability to react to the agent's private\nstrategy. We define an objective function for a one-shot game that allows\nderiving the environment's probability distribution under friendly and\nadversarial assumptions alongside the agent's optimal strategy. Furthermore, we\npresent an algorithm to compute these equilibrium strategies, and show\nexperimentally that both friendly and adversarial environments possess\nnon-trivial optimal strategies.", "author_comment": "13 pages, 9 figures", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1806.09795": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1806.09795v3", "post_title": "Multi-agent Inverse Reinforcement Learning for Certain General-sum Stochastic Games", "authors": ["Xiaomin Lin", "Stephen C. Adams", "Peter A. Beling"], "date_published": "2018-06-26 05:14:13+00:00", "data_last_modified": "2019-10-11 01:32:22+00:00", "url": "http://arxiv.org/abs/1806.09795v3", "abstract": "This paper addresses the problem of multi-agent inverse reinforcement\nlearning (MIRL) in a two-player general-sum stochastic game framework. Five\nvariants of MIRL are considered: uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and\nuNE-MIRL, each distinguished by its solution concept. Problem uCS-MIRL is a\ncooperative game in which the agents employ cooperative strategies that aim to\nmaximize the total game value. In problem uCE-MIRL, agents are assumed to\nfollow strategies that constitute a correlated equilibrium while maximizing\ntotal game value. Problem uNE-MIRL is similar to uCE-MIRL in total game value\nmaximization, but it is assumed that the agents are playing a Nash equilibrium.\nProblems advE-MIRL and cooE-MIRL assume agents are playing an adversarial\nequilibrium and a coordination equilibrium, respectively. We propose novel\napproaches to address these five problems under the assumption that the game\nobserver either knows or is able to accurate estimate the policies and solution\nconcepts for players. For uCS-MIRL, we first develop a characteristic set of\nsolutions ensuring that the observed bi-policy is a uCS and then apply a\nBayesian inverse learning method. For uCE-MIRL, we develop a linear programming\nproblem subject to constraints that define necessary and sufficient conditions\nfor the observed policies to be correlated equilibria. The objective is to\nchoose a solution that not only minimizes the total game value difference\nbetween the observed bi-policy and a local uCS, but also maximizes the scale of\nthe solution. We apply a similar treatment to the problem of uNE-MIRL. The\nremaining two problems can be solved efficiently by taking advantage of\nsolution uniqueness and setting up a convex optimization problem. Results are\nvalidated on various benchmark grid-world games.", "author_comment": "30 pages", "journal_ref": "Journal of Artificial Intelligence Research 66 (2019), pp 473-502", "doi": "10.1613/jair.1.11541", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.GT", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.01412": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.01412v1", "post_title": "Deep Learning for Symbolic Mathematics", "authors": ["Guillaume Lample", "Fran\u00e7ois Charton"], "date_published": "2019-12-02 15:05:24+00:00", "data_last_modified": "2019-12-02 15:05:24+00:00", "url": "http://arxiv.org/abs/1912.01412v1", "abstract": "Neural networks have a reputation for being better at solving statistical or\napproximate problems than at performing calculations or working with symbolic\ndata. In this paper, we show that they can be surprisingly good at more\nelaborated tasks in mathematics, such as symbolic integration and solving\ndifferential equations. We propose a syntax for representing mathematical\nproblems, and methods for generating large datasets that can be used to train\nsequence-to-sequence models. We achieve results that outperform commercial\nComputer Algebra Systems such as Matlab or Mathematica.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.SC", "categories": ["cs.SC", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.05671": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.05671v4", "post_title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "date_published": "2019-12-11 22:22:21+00:00", "data_last_modified": "2020-07-18 20:31:17+00:00", "url": "http://arxiv.org/abs/1912.05671v4", "abstract": "We study whether a neural network optimizes to the same, linearly connected\nminimum under different samples of SGD noise (e.g., random data order and\naugmentation). We find that standard vision models become stable to SGD noise\nin this way early in training. From then on, the outcome of optimization is\ndetermined to a linearly connected region. We use this technique to study\niterative magnitude pruning (IMP), the procedure used by work on the lottery\nticket hypothesis to identify subnetworks that could have trained in isolation\nto full accuracy. We find that these subnetworks only reach full accuracy when\nthey are stable to SGD noise, which either occurs at initialization for\nsmall-scale settings (MNIST) or early in training for large-scale settings\n(ResNet-50 and Inception-v3 on ImageNet).", "author_comment": "Published in ICML 2020. This submission subsumes arXiv:1903.01611\n  (\"Stabilizing the Lottery Ticket Hypothesis\" and \"The Lottery Ticket\n  Hypothesis at Scale\")", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1711.02827": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1711.02827v2", "post_title": "Inverse Reward Design", "authors": ["Dylan Hadfield-Menell", "Smitha Milli", "Pieter Abbeel", "Stuart Russell", "Anca Dragan"], "date_published": "2017-11-08 04:44:32+00:00", "data_last_modified": "2020-10-07 15:41:58+00:00", "url": "http://arxiv.org/abs/1711.02827v2", "abstract": "Autonomous agents optimize the reward function we give them. What they don't\nknow is how hard it is for us to design a reward function that actually\ncaptures what we want. When designing the reward, we might think of some\nspecific training scenarios, and make sure that the reward will lead to the\nright behavior in those scenarios. Inevitably, agents encounter new scenarios\n(e.g., new types of terrain) where optimizing that same reward may lead to\nundesired behavior. Our insight is that reward functions are merely\nobservations about what the designer actually wants, and that they should be\ninterpreted in the context in which they were designed. We introduce inverse\nreward design (IRD) as the problem of inferring the true objective based on the\ndesigned reward and the training MDP. We introduce approximate methods for\nsolving IRD problems, and use their solution to plan risk-averse behavior in\ntest MDPs. Empirical results suggest that this approach can help alleviate\nnegative side effects of misspecified reward functions and mitigate reward\nhacking.", "author_comment": "Advances in Neural Information Processing Systems 30 (NIPS 2017)\n  Revised Oct 2020 to fix a typo in Eq. 3", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1902.06787": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1902.06787v6", "post_title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Angel Alexander Cabrera", "Adam Perer", "Eric Xing", "Ameet Talwalkar"], "date_published": "2019-02-18 20:23:12+00:00", "data_last_modified": "2020-11-08 15:49:08+00:00", "url": "http://arxiv.org/abs/1902.06787v6", "abstract": "Most of the work on interpretable machine learning has focused on designing\neither inherently interpretable models, which typically trade-off accuracy for\ninterpretability, or post-hoc explanation systems, whose explanation quality\ncan be unpredictable. Our method, ExpO, is a hybridization of these approaches\nthat regularizes a model for explanation quality at training time. Importantly,\nthese regularizers are differentiable, model agnostic, and require no domain\nknowledge to define. We demonstrate that post-hoc explanations for\nExpO-regularized models have better explanation quality, as measured by the\ncommon fidelity and stability metrics. We verify that improving these metrics\nleads to significantly more useful explanations with a user study on a\nrealistic task.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.10615": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.10615v3", "post_title": "Adversarial Policies: Attacking Deep Reinforcement Learning", "authors": ["Adam Gleave", "Michael Dennis", "Cody Wild", "Neel Kant", "Sergey Levine", "Stuart Russell"], "date_published": "2019-05-25 15:23:19+00:00", "data_last_modified": "2021-01-17 19:25:56+00:00", "url": "http://arxiv.org/abs/1905.10615v3", "abstract": "Deep reinforcement learning (RL) policies are known to be vulnerable to\nadversarial perturbations to their observations, similar to adversarial\nexamples for classifiers. However, an attacker is not usually able to directly\nmodify another agent's observations. This might lead one to wonder: is it\npossible to attack an RL agent simply by choosing an adversarial policy acting\nin a multi-agent environment so as to create natural observations that are\nadversarial? We demonstrate the existence of adversarial policies in zero-sum\ngames between simulated humanoid robots with proprioceptive observations,\nagainst state-of-the-art victims trained via self-play to be robust to\nopponents. The adversarial policies reliably win against the victims but\ngenerate seemingly random and uncoordinated behavior. We find that these\npolicies are more successful in high-dimensional environments, and induce\nsubstantially different activations in the victim policy network than when the\nvictim plays against a normal opponent. Videos are available at\nhttps://adversarialpolicies.github.io/.", "author_comment": "Presented at ICLR 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.CR", "stat.ML", "I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1810.11545": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1810.11545v2", "post_title": "Efficiently Combining Human Demonstrations and Interventions for Safe Training of Autonomous Systems in Real-Time", "authors": ["Vinicius G. Goecks", "Gregory M. Gremillion", "Vernon J. Lawhern", "John Valasek", "Nicholas R. Waytowich"], "date_published": "2018-10-26 22:23:27+00:00", "data_last_modified": "2018-11-28 21:31:07+00:00", "url": "http://arxiv.org/abs/1810.11545v2", "abstract": "This paper investigates how to utilize different forms of human interaction\nto safely train autonomous systems in real-time by learning from both human\ndemonstrations and interventions. We implement two components of the\nCycle-of-Learning for Autonomous Systems, which is our framework for combining\nmultiple modalities of human interaction. The current effort employs human\ndemonstrations to teach a desired behavior via imitation learning, then\nleverages intervention data to correct for undesired behaviors produced by the\nimitation learner to teach novel tasks to an autonomous agent safely, after\nonly minutes of training. We demonstrate this method in an autonomous perching\ntask using a quadrotor with continuous roll, pitch, yaw, and throttle commands\nand imagery captured from a downward-facing camera in a high-fidelity simulated\nenvironment. Our method improves task completion performance for the same\namount of human interaction when compared to learning from demonstrations\nalone, while also requiring on average 32% less data to achieve that\nperformance. This provides evidence that combining multiple modes of human\ninteraction can increase both the training speed and overall performance of\npolicies for autonomous systems.", "author_comment": "9 pages, 6 figures", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.HC", "cs.RO"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1805.07871": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1805.07871v1", "post_title": "A Framework and Method for Online Inverse Reinforcement Learning", "authors": ["Saurabh Arora", "Prashant Doshi", "Bikramjit Banerjee"], "date_published": "2018-05-21 02:27:58+00:00", "data_last_modified": "2018-05-21 02:27:58+00:00", "url": "http://arxiv.org/abs/1805.07871v1", "abstract": "Inverse reinforcement learning (IRL) is the problem of learning the\npreferences of an agent from the observations of its behavior on a task. While\nthis problem has been well investigated, the related problem of {\\em online}\nIRL---where the observations are incrementally accrued, yet the demands of the\napplication often prohibit a full rerun of an IRL method---has received\nrelatively less attention. We introduce the first formal framework for online\nIRL, called incremental IRL (I2RL), and a new method that advances maximum\nentropy IRL with hidden variables, to this setting. Our formal analysis shows\nthat the new method has a monotonically improving performance with more\ndemonstration data, as well as probabilistically bounded error, both under full\nand partial observability. Experiments in a simulated robotic application of\npenetrating a continuous patrol under occlusion shows the relatively improved\nperformance and speed up of the new method and validates the utility of online\nIRL.", "author_comment": null, "journal_ref": "Journal of Autonomous Agents and Multi-Agent Systems, Volume 35,\n  Article number: 4 (2021)", "doi": "10.1007/s10458-020-09485-4", "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2102.06701": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2102.06701v1", "post_title": "Explaining Neural Scaling Laws", "authors": ["Yasaman Bahri", "Ethan Dyer", "Jared Kaplan", "Jaehoon Lee", "Utkarsh Sharma"], "date_published": "2021-02-12 18:57:46+00:00", "data_last_modified": "2021-02-12 18:57:46+00:00", "url": "http://arxiv.org/abs/2102.06701v1", "abstract": "The test loss of well-trained neural networks often follows precise power-law\nscaling relations with either the size of the training dataset or the number of\nparameters in the network. We propose a theory that explains and connects these\nscaling laws. We identify variance-limited and resolution-limited scaling\nbehavior for both dataset and model size, for a total of four scaling regimes.\nThe variance-limited scaling follows simply from the existence of a\nwell-behaved infinite data or infinite width limit, while the\nresolution-limited regime can be explained by positing that models are\neffectively resolving a smooth data manifold. In the large width limit, this\ncan be equivalently obtained from the spectrum of certain kernels, and we\npresent evidence that large width and large dataset resolution-limited scaling\nexponents are related by a duality. We exhibit all four scaling regimes in the\ncontrolled setting of large random feature and pretrained models and test the\npredictions empirically on a range of standard architectures and datasets. We\nalso observe several empirical relationships between datasets and scaling\nexponents: super-classing image tasks does not change exponents, while changing\ninput distribution (via changing datasets or adding noise) has a strong effect.\nWe further explore the effect of architecture aspect ratio on scaling\nexponents.", "author_comment": "11 pages, 5 figures + Supplement", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cond-mat.dis-nn", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1808.01174": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1808.01174v3", "post_title": "Generalization Error in Deep Learning", "authors": ["Daniel Jakubovitz", "Raja Giryes", "Miguel R. D. Rodrigues"], "date_published": "2018-08-03 12:57:12+00:00", "data_last_modified": "2019-04-06 15:25:50+00:00", "url": "http://arxiv.org/abs/1808.01174v3", "abstract": "Deep learning models have lately shown great performance in various fields\nsuch as computer vision, speech recognition, speech translation, and natural\nlanguage processing. However, alongside their state-of-the-art performance, it\nis still generally unclear what is the source of their generalization ability.\nThus, an important question is what makes deep neural networks able to\ngeneralize well from the training set to new data. In this article, we provide\nan overview of the existing theory and bounds for the characterization of the\ngeneralization error of deep neural networks, combining both classical and more\nrecent theoretical and empirical results.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1802.01604": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1802.01604v1", "post_title": "Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries", "authors": ["Chandrayee Basu", "Mukesh Singhal", "Anca D. Dragan"], "date_published": "2018-02-05 19:03:26+00:00", "data_last_modified": "2018-02-05 19:03:26+00:00", "url": "http://arxiv.org/abs/1802.01604v1", "abstract": "We focus on learning the desired objective function for a robot. Although\ntrajectory demonstrations can be very informative of the desired objective,\nthey can also be difficult for users to provide. Answers to comparison queries,\nasking which of two trajectories is preferable, are much easier for users, and\nhave emerged as an effective alternative. Unfortunately, comparisons are far\nless informative. We propose that there is much richer information that users\ncan easily provide and that robots ought to leverage. We focus on augmenting\ncomparisons with feature queries, and introduce a unified formalism for\ntreating all answers as observations about the true desired reward. We derive\nan active query selection algorithm, and test these queries in simulation and\non real users. We find that richer, feature-augmented queries can extract more\ninformation faster, leading to robots that better match user preferences in\ntheir behavior.", "author_comment": "8 pages, 8 figures, HRI 2018", "journal_ref": null, "doi": "10.1145/3171221.3171284", "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2108.00106": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2108.00106v2", "post_title": "Soft Calibration Objectives for Neural Networks", "authors": ["Archit Karandikar", "Nicholas Cain", "Dustin Tran", "Balaji Lakshminarayanan", "Jonathon Shlens", "Michael C. Mozer", "Becca Roelofs"], "date_published": "2021-07-30 23:30:20+00:00", "data_last_modified": "2021-12-07 18:31:29+00:00", "url": "http://arxiv.org/abs/2108.00106v2", "abstract": "Optimal decision making requires that classifiers produce uncertainty\nestimates consistent with their empirical accuracy. However, deep neural\nnetworks are often under- or over-confident in their predictions. Consequently,\nmethods have been developed to improve the calibration of their predictive\nuncertainty both during training and post-hoc. In this work, we propose\ndifferentiable losses to improve calibration based on a soft (continuous)\nversion of the binning operation underlying popular calibration-error\nestimators. When incorporated into training, these soft calibration losses\nachieve state-of-the-art single-model ECE across multiple datasets with less\nthan 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE\n(70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative\ndecrease in accuracy relative to the cross entropy baseline on CIFAR-100. When\nincorporated post-training, the soft-binning-based calibration error objective\nimproves upon temperature scaling, a popular recalibration method. Overall,\nexperiments across losses and datasets demonstrate that using\ncalibration-sensitive procedures yield better uncertainty estimates under\ndataset shift than the standard practice of using a cross entropy loss and\npost-hoc recalibration methods.", "author_comment": "17 pages total, 10 page main paper, 5 page appendix, 10 figures\n  total, 8 figures in main paper, 2 figures in appendix", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1911.00459": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1911.00459v1", "post_title": "Positive-Unlabeled Reward Learning", "authors": ["Danfei Xu", "Misha Denil"], "date_published": "2019-11-01 16:47:44+00:00", "data_last_modified": "2019-11-01 16:47:44+00:00", "url": "http://arxiv.org/abs/1911.00459v1", "abstract": "Learning reward functions from data is a promising path towards achieving\nscalable Reinforcement Learning (RL) for robotics. However, a major challenge\nin training agents from learned reward models is that the agent can learn to\nexploit errors in the reward model to achieve high reward behaviors that do not\ncorrespond to the intended task. These reward delusions can lead to unintended\nand even dangerous behaviors. On the other hand, adversarial imitation learning\nframeworks tend to suffer the opposite problem, where the discriminator learns\nto trivially distinguish agent and expert behavior, resulting in reward models\nthat produce low reward signal regardless of the input state. In this paper, we\nconnect these two classes of reward learning methods to positive-unlabeled (PU)\nlearning, and we show that by applying a large-scale PU learning algorithm to\nthe reward learning problem, we can address both the reward under- and\nover-estimation problems simultaneously. Our approach drastically improves both\nGAIL and supervised reward learning, without any additional assumptions.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1903.08894": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1903.08894v1", "post_title": "Towards Characterizing Divergence in Deep Q-Learning", "authors": ["Joshua Achiam", "Ethan Knight", "Pieter Abbeel"], "date_published": "2019-03-21 09:42:41+00:00", "data_last_modified": "2019-03-21 09:42:41+00:00", "url": "http://arxiv.org/abs/1903.08894v1", "abstract": "Deep Q-Learning (DQL), a family of temporal difference algorithms for\ncontrol, employs three techniques collectively known as the `deadly triad' in\nreinforcement learning: bootstrapping, off-policy learning, and function\napproximation. Prior work has demonstrated that together these can lead to\ndivergence in Q-learning algorithms, but the conditions under which divergence\noccurs are not well-understood. In this note, we give a simple analysis based\non a linear approximation to the Q-value updates, which we believe provides\ninsight into divergence under the deadly triad. The central point in our\nanalysis is to consider when the leading order approximation to the deep-Q\nupdate is or is not a contraction in the sup norm. Based on this analysis, we\ndevelop an algorithm which permits stable deep Q-learning for continuous\ncontrol without any of the tricks conventionally used (such as target networks,\nadaptive gradient optimizers, or using multiple Q functions). We demonstrate\nthat our algorithm performs above or near state-of-the-art on standard MuJoCo\nbenchmarks from the OpenAI Gym.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2106.05091": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2106.05091v1", "post_title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training", "authors": ["Kimin Lee", "Laura Smith", "Pieter Abbeel"], "date_published": "2021-06-09 14:10:50+00:00", "data_last_modified": "2021-06-09 14:10:50+00:00", "url": "http://arxiv.org/abs/2106.05091v1", "abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often\nbe difficult, involving meticulous design of reward functions that are\nsufficiently informative yet easy enough to provide. Human-in-the-loop RL\nmethods allow practitioners to instead interactively teach agents through\ntailored feedback; however, such approaches have been challenging to scale\nsince human feedback is very expensive. In this work, we aim to make this\nprocess more sample- and feedback-efficient. We present an off-policy,\ninteractive RL algorithm that capitalizes on the strengths of both feedback and\noff-policy learning. Specifically, we learn a reward model by actively querying\na teacher's preferences between two clips of behavior and use it to train an\nagent. To enable off-policy learning, we relabel all the agent's past\nexperience when its reward model changes. We additionally show that\npre-training our agents with unsupervised exploration substantially increases\nthe mileage of its queries. We demonstrate that our approach is capable of\nlearning tasks of higher complexity than previously considered by\nhuman-in-the-loop methods, including a variety of locomotion and robotic\nmanipulation skills. We also show that our method is able to utilize real-time\nhuman feedback to effectively prevent reward exploitation and learn new\nbehaviors that are difficult to specify with standard reward functions.", "author_comment": "ICML 2021. First two authors contributed equally. Website:\n  https://sites.google.com/view/icml21pebble Code:\n  https://github.com/pokaxpoka/B_Pref", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1904.01318": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1904.01318v1", "post_title": "Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents", "authors": ["Christian Rupprecht", "Cyril Ibrahim", "Christopher J. Pal"], "date_published": "2019-04-02 10:21:23+00:00", "data_last_modified": "2019-04-02 10:21:23+00:00", "url": "http://arxiv.org/abs/1904.01318v1", "abstract": "As deep reinforcement learning driven by visual perception becomes more\nwidely used there is a growing need to better understand and probe the learned\nagents. Understanding the decision making process and its relationship to\nvisual inputs can be very valuable to identify problems in learned behavior.\nHowever, this topic has been relatively under-explored in the research\ncommunity. In this work we present a method for synthesizing visual inputs of\ninterest for a trained agent. Such inputs or states could be situations in\nwhich specific actions are necessary. Further, critical states in which a very\nhigh or a very low reward can be achieved are often interesting to understand\nthe situational awareness of the system as they can correspond to risky states.\nTo this end, we learn a generative model over the state space of the\nenvironment and use its latent space to optimize a target function for the\nstate of interest. In our experiments we show that this method can generate\ninsights for a variety of environments and reinforcement learning methods. We\nexplore results in the standard Atari benchmark games as well as in an\nautonomous driving simulator. Based on the efficiency with which we have been\nable to identify behavioural weaknesses with this technique, we believe this\ngeneral approach could serve as an important tool for AI safety applications.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CV", "categories": ["cs.CV"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2108.09293": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2108.09293v3", "post_title": "Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions", "authors": ["Hammond Pearce", "Baleegh Ahmad", "Benjamin Tan", "Brendan Dolan-Gavitt", "Ramesh Karri"], "date_published": "2021-08-20 17:30:33+00:00", "data_last_modified": "2021-12-16 15:54:11+00:00", "url": "http://arxiv.org/abs/2108.09293v3", "abstract": "There is burgeoning interest in designing AI-based systems to assist humans\nin designing computing systems, including tools that automatically generate\ncomputer code. The most notable of these comes in the form of the first\nself-described `AI pair programmer', GitHub Copilot, a language model trained\nover open-source GitHub code. However, code often contains bugs - and so, given\nthe vast quantity of unvetted code that Copilot has processed, it is certain\nthat the language model will have learned from exploitable, buggy code. This\nraises concerns on the security of Copilot's code contributions. In this work,\nwe systematically investigate the prevalence and conditions that can cause\nGitHub Copilot to recommend insecure code. To perform this analysis we prompt\nCopilot to generate code in scenarios relevant to high-risk CWEs (e.g. those\nfrom MITRE's \"Top 25\" list). We explore Copilot's performance on three distinct\ncode generation axes -- examining how it performs given diversity of\nweaknesses, diversity of prompts, and diversity of domains. In total, we\nproduce 89 different scenarios for Copilot to complete, producing 1,689\nprograms. Of these, we found approximately 40% to be vulnerable.", "author_comment": "Accepted for publication in IEEE Symposium on Security and Privacy\n  2022", "journal_ref": null, "doi": null, "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.03030": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.03030v2", "post_title": "Meta-learning of Sequential Strategies", "authors": ["Pedro A. Ortega", "Jane X. Wang", "Mark Rowland", "Tim Genewein", "Zeb Kurth-Nelson", "Razvan Pascanu", "Nicolas Heess", "Joel Veness", "Alex Pritzel", "Pablo Sprechmann", "Siddhant M. Jayakumar", "Tom McGrath", "Kevin Miller", "Mohammad Azar", "Ian Osband", "Neil Rabinowitz", "Andr\u00e1s Gy\u00f6rgy", "Silvia Chiappa", "Simon Osindero", "Yee Whye Teh", "Hado van Hasselt", "Nando de Freitas", "Matthew Botvinick", "Shane Legg"], "date_published": "2019-05-08 12:27:20+00:00", "data_last_modified": "2019-07-18 18:09:19+00:00", "url": "http://arxiv.org/abs/1905.03030v2", "abstract": "In this report we review memory-based meta-learning as a tool for building\nsample-efficient strategies that learn from past experience to adapt to any\ntask within a target class. Our goal is to equip the reader with the conceptual\nfoundations of this tool for building new, scalable agents that operate on\nbroad domains. To do so, we present basic algorithmic templates for building\nnear-optimal predictors and reinforcement learners which behave as if they had\na probabilistic model that allowed them to efficiently exploit task structure.\nFurthermore, we recast memory-based meta-learning within a Bayesian framework,\nshowing that the meta-learned strategies are near-optimal because they amortize\nBayes-filtered data, where the adaptation is implemented in the memory dynamics\nas a state-machine of sufficient statistics. Essentially, memory-based\nmeta-learning translates the hard problem of probabilistic sequential inference\ninto a regression problem.", "author_comment": "DeepMind Technical Report (15 pages, 6 figures). Version V1.1", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2010.07877": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2010.07877v1", "post_title": "Avoiding Side Effects By Considering Future Tasks", "authors": ["Victoria Krakovna", "Laurent Orseau", "Richard Ngo", "Miljan Martic", "Shane Legg"], "date_published": "2020-10-15 16:55:26+00:00", "data_last_modified": "2020-10-15 16:55:26+00:00", "url": "http://arxiv.org/abs/2010.07877v1", "abstract": "Designing reward functions is difficult: the designer has to specify what to\ndo (what it means to complete the task) as well as what not to do (side effects\nthat should be avoided while completing the task). To alleviate the burden on\nthe reward designer, we propose an algorithm to automatically generate an\nauxiliary reward function that penalizes side effects. This auxiliary objective\nrewards the ability to complete possible future tasks, which decreases if the\nagent causes side effects during the current task. The future task reward can\nalso give the agent an incentive to interfere with events in the environment\nthat make future tasks less achievable, such as irreversible actions by other\nagents. To avoid this interference incentive, we introduce a baseline policy\nthat represents a default course of action (such as doing nothing), and use it\nto filter out future tasks that are not achievable by default. We formally\ndefine interference incentives and show that the future task approach with a\nbaseline policy avoids these incentives in the deterministic case. Using\ngridworld environments that test for side effects and interference, we show\nthat our method avoids interference and is more effective for avoiding side\neffects than the common approach of penalizing irreversible actions.", "author_comment": "Published in NeurIPS 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1905.12686": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1905.12686v4", "post_title": "Learning Representations by Humans, for Humans", "authors": ["Sophie Hilgard", "Nir Rosenfeld", "Mahzarin R. Banaji", "Jack Cao", "David C. Parkes"], "date_published": "2019-05-29 19:19:09+00:00", "data_last_modified": "2021-09-15 22:03:35+00:00", "url": "http://arxiv.org/abs/1905.12686v4", "abstract": "When machine predictors can achieve higher performance than the human\ndecision-makers they support, improving the performance of human\ndecision-makers is often conflated with improving machine accuracy. Here we\npropose a framework to directly support human decision-making, in which the\nrole of machines is to reframe problems rather than to prescribe actions\nthrough prediction. Inspired by the success of representation learning in\nimproving performance of machine predictors, our framework learns human-facing\nrepresentations optimized for human performance. This \"Mind Composed with\nMachine\" framework incorporates a human decision-making model directly into the\nrepresentation learning paradigm and is trained with a novel human-in-the-loop\ntraining procedure. We empirically demonstrate the successful application of\nthe framework to various tasks and representational forms.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.HC", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1802.01780": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1802.01780v1", "post_title": "Goal Inference Improves Objective and Perceived Performance in Human-Robot Collaboration", "authors": ["Chang Liu", "Jessica B. Hamrick", "Jaime F. Fisac", "Anca D. Dragan", "J. Karl Hedrick", "S. Shankar Sastry", "Thomas L. Griffiths"], "date_published": "2018-02-06 03:31:23+00:00", "data_last_modified": "2018-02-06 03:31:23+00:00", "url": "http://arxiv.org/abs/1802.01780v1", "abstract": "The study of human-robot interaction is fundamental to the design and use of\nrobotics in real-world applications. Robots will need to predict and adapt to\nthe actions of human collaborators in order to achieve good performance and\nimprove safety and end-user adoption. This paper evaluates a human-robot\ncollaboration scheme that combines the task allocation and motion levels of\nreasoning: the robotic agent uses Bayesian inference to predict the next goal\nof its human partner from his or her ongoing motion, and re-plans its own\nactions in real time. This anticipative adaptation is desirable in many\npractical scenarios, where humans are unable or unwilling to take on the\ncognitive overhead required to explicitly communicate their intent to the\nrobot. A behavioral experiment indicates that the combination of goal inference\nand dynamic task planning significantly improves both objective and perceived\nperformance of the human-robot team. Participants were highly sensitive to the\ndifferences between robot behaviors, preferring to work with a robot that\nadapted to their actions over one that did not.", "author_comment": "Published at the International Conference on Autonomous Agents and\n  Multiagent Systems (AAMAS 2016)", "journal_ref": "C. Liu, J. Hamrick, J. Fisac, A. Dragan, J. K. Hedrick, S. Sastry,\n  T. Griffiths. \"Goal Inference Improves Objective and Perceived Performance in\n  Human-Robot Collaboration\". Autonomous Agents and Multiagent Systems (AAMAS),\n  2016", "doi": null, "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.HC", "68T05", "I.2.0; I.2.6; I.2.8; I.2.9"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.02671": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.02671v2", "post_title": "Learning in two-player games between transparent opponents", "authors": ["Adrian Hutter"], "date_published": "2020-12-04 15:41:07+00:00", "data_last_modified": "2021-08-19 21:30:22+00:00", "url": "http://arxiv.org/abs/2012.02671v2", "abstract": "We consider a scenario in which two reinforcement learning agents repeatedly\nplay a matrix game against each other and update their parameters after each\nround. The agents' decision-making is transparent to each other, which allows\neach agent to predict how their opponent will play against them. To prevent an\ninfinite regress of both agents recursively predicting each other indefinitely,\neach agent is required to give an opponent-independent response with some\nprobability at least epsilon. Transparency also allows each agent to anticipate\nand shape the other agent's gradient step, i.e. to move to regions of parameter\nspace in which the opponent's gradient points in a direction favourable to\nthem. We study the resulting dynamics experimentally, using two algorithms from\nprevious literature (LOLA and SOS) for opponent-aware learning. We find that\nthe combination of mutually transparent decision-making and opponent-aware\nlearning robustly leads to mutual cooperation in a single-shot prisoner's\ndilemma. In a game of chicken, in which both agents try to manoeuvre their\nopponent towards their preferred equilibrium, converging to a mutually\nbeneficial outcome turns out to be much harder, and opponent-aware learning can\neven lead to worst-case outcomes for both agents. This highlights the need to\ndevelop opponent-aware learning algorithms that achieve acceptable outcomes in\nsocial dilemmas involving an equilibrium selection problem.", "author_comment": "30 pages, 14 figures; v2: includes changes based on peer feedback", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.GT", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.03820": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.03820v3", "post_title": "Meta-Learning without Memorization", "authors": ["Mingzhang Yin", "George Tucker", "Mingyuan Zhou", "Sergey Levine", "Chelsea Finn"], "date_published": "2019-12-09 02:30:46+00:00", "data_last_modified": "2020-04-27 22:33:53+00:00", "url": "http://arxiv.org/abs/1912.03820v3", "abstract": "The ability to learn new concepts with small amounts of data is a critical\naspect of intelligence that has proven challenging for deep learning methods.\nMeta-learning has emerged as a promising technique for leveraging data from\nprevious tasks to enable efficient learning of new tasks. However, most\nmeta-learning algorithms implicitly require that the meta-training tasks be\nmutually-exclusive, such that no single model can solve all of the tasks at\nonce. For example, when creating tasks for few-shot image classification, prior\nwork uses a per-task random assignment of image classes to N-way classification\nlabels. If this is not done, the meta-learner can ignore the task training data\nand learn a single model that performs all of the meta-training tasks\nzero-shot, but does not adapt effectively to new image classes. This\nrequirement means that the user must take great care in designing the tasks,\nfor example by shuffling labels or removing task identifying information from\nthe inputs. In some domains, this makes meta-learning entirely inapplicable. In\nthis paper, we address this challenge by designing a meta-regularization\nobjective using information theory that places precedence on data-driven\nadaptation. This causes the meta-learner to decide what must be learned from\nthe task training data and what should be inferred from the task testing input.\nBy doing so, our algorithm can successfully use data from\nnon-mutually-exclusive tasks to efficiently adapt to novel tasks. We\ndemonstrate its applicability to both contextual and gradient-based\nmeta-learning algorithms, and apply it in practical settings where applying\nstandard meta-learning has been difficult. Our approach substantially\noutperforms standard meta-learning algorithms in these settings.", "author_comment": "ICLR 2020", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1906.08237": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1906.08237v2", "post_title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "date_published": "2019-06-19 17:35:48+00:00", "data_last_modified": "2020-01-02 12:48:08+00:00", "url": "http://arxiv.org/abs/1906.08237v2", "abstract": "With the capability of modeling bidirectional contexts, denoising\nautoencoding based pretraining like BERT achieves better performance than\npretraining approaches based on autoregressive language modeling. However,\nrelying on corrupting the input with masks, BERT neglects dependency between\nthe masked positions and suffers from a pretrain-finetune discrepancy. In light\nof these pros and cons, we propose XLNet, a generalized autoregressive\npretraining method that (1) enables learning bidirectional contexts by\nmaximizing the expected likelihood over all permutations of the factorization\norder and (2) overcomes the limitations of BERT thanks to its autoregressive\nformulation. Furthermore, XLNet integrates ideas from Transformer-XL, the\nstate-of-the-art autoregressive model, into pretraining. Empirically, under\ncomparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a\nlarge margin, including question answering, natural language inference,\nsentiment analysis, and document ranking.", "author_comment": "Pretrained models and code are available at\n  https://github.com/zihangdai/xlnet", "journal_ref": null, "doi": null, "primary_category": "cs.CL", "categories": ["cs.CL", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1506.07359": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1506.07359v1", "post_title": "Sequential Extensions of Causal and Evidential Decision Theory", "authors": ["Tom Everitt", "Jan Leike", "Marcus Hutter"], "date_published": "2015-06-24 13:16:16+00:00", "data_last_modified": "2015-06-24 13:16:16+00:00", "url": "http://arxiv.org/abs/1506.07359v1", "abstract": "Moving beyond the dualistic view in AI where agent and environment are\nseparated incurs new challenges for decision making, as calculation of expected\nutility is no longer straightforward. The non-dualistic decision theory\nliterature is split between causal decision theory and evidential decision\ntheory. We extend these decision algorithms to the sequential setting where the\nagent alternates between taking actions and observing their consequences. We\nfind that evidential decision theory has two natural extensions while causal\ndecision theory only has one.", "author_comment": "ADT 2015", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1409.0813": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1409.0813v2", "post_title": "Friendly Artificial Intelligence: the Physics Challenge", "authors": ["Max Tegmark"], "date_published": "2014-09-02 18:20:28+00:00", "data_last_modified": "2014-09-03 15:05:07+00:00", "url": "http://arxiv.org/abs/1409.0813v2", "abstract": "Relentless progress in artificial intelligence (AI) is increasingly raising\nconcerns that machines will replace humans on the job market, and perhaps\naltogether. Eliezer Yudkowski and others have explored the possibility that a\npromising future for humankind could be guaranteed by a superintelligent\n\"Friendly AI\", designed to safeguard humanity and its values. I argue that,\nfrom a physics perspective where everything is simply an arrangement of\nelementary particles, this might be even harder than it appears. Indeed, it may\nrequire thinking rigorously about the meaning of life: What is \"meaning\" in a\nparticle arrangement? What is \"life\"? What is the ultimate ethical imperative,\ni.e., how should we strive to rearrange the particles of our Universe and shape\nits future? If we fail to answer the last question rigorously, this future is\nunlikely to contain humans.", "author_comment": "3 pages", "journal_ref": "In proceedings of the AAAI 2015 Workshop On AI and Ethics, p87,\n  Toby Walsh, Ed. (2015)", "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1811.07871": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1811.07871v1", "post_title": "Scalable agent alignment via reward modeling: a research direction", "authors": ["Jan Leike", "David Krueger", "Tom Everitt", "Miljan Martic", "Vishal Maini", "Shane Legg"], "date_published": "2018-11-19 18:48:04+00:00", "data_last_modified": "2018-11-19 18:48:04+00:00", "url": "http://arxiv.org/abs/1811.07871v1", "abstract": "One obstacle to applying reinforcement learning algorithms to real-world\nproblems is the lack of suitable reward functions. Designing such reward\nfunctions is difficult in part because the user only has an implicit\nunderstanding of the task objective. This gives rise to the agent alignment\nproblem: how do we create agents that behave in accordance with the user's\nintentions? We outline a high-level research direction to solve the agent\nalignment problem centered around reward modeling: learning a reward function\nfrom interaction with the user and optimizing the learned reward function with\nreinforcement learning. We discuss the key challenges we expect to face when\nscaling reward modeling to complex and general domains, concrete approaches to\nmitigate these challenges, and ways to establish trust in the resulting agents.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2012.07532": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2012.07532v1", "post_title": "An overview of 11 proposals for building safe advanced AI", "authors": ["Evan Hubinger"], "date_published": "2020-12-04 22:53:18+00:00", "data_last_modified": "2020-12-04 22:53:18+00:00", "url": "http://arxiv.org/abs/2012.07532v1", "abstract": "This paper analyzes and compares 11 different proposals for building safe\nadvanced AI under the current machine learning paradigm, including major\ncontenders such as iterated amplification, AI safety via debate, and recursive\nreward modeling. Each proposal is evaluated on the four components of outer\nalignment, inner alignment, training competitiveness, and performance\ncompetitiveness, of which the distinction between the latter two is introduced\nin this paper. While prior literature has primarily focused on analyzing\nindividual proposals, or primarily focused on outer alignment at the expense of\ninner alignment, this analysis seeks to take a comparative look at a wide range\nof proposals including a comparative analysis across all four previously\nmentioned components.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2001.09318": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2001.09318v1", "post_title": "Silly rules improve the capacity of agents to learn stable enforcement and compliance behaviors", "authors": ["Raphael K\u00f6ster", "Dylan Hadfield-Menell", "Gillian K. Hadfield", "Joel Z. Leibo"], "date_published": "2020-01-25 14:00:33+00:00", "data_last_modified": "2020-01-25 14:00:33+00:00", "url": "http://arxiv.org/abs/2001.09318v1", "abstract": "How can societies learn to enforce and comply with social norms? Here we\ninvestigate the learning dynamics and emergence of compliance and enforcement\nof social norms in a foraging game, implemented in a multi-agent reinforcement\nlearning setting. In this spatiotemporally extended game, individuals are\nincentivized to implement complex berry-foraging policies and punish\ntransgressions against social taboos covering specific berry types. We show\nthat agents benefit when eating poisonous berries is taboo, meaning the\nbehavior is punished by other agents, as this helps overcome a\ncredit-assignment problem in discovering delayed health effects. Critically,\nhowever, we also show that introducing an additional taboo, which results in\npunishment for eating a harmless berry, improves the rate and stability with\nwhich agents learn to punish taboo violations and comply with taboos.\nCounterintuitively, our results show that an arbitrary taboo (a \"silly rule\")\ncan enhance social learning dynamics and achieve better outcomes in the middle\nstages of learning. We discuss the results in the context of studying\nnormativity as a group-level emergent phenomenon.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2003.03384": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2003.03384v2", "post_title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch", "authors": ["Esteban Real", "Chen Liang", "David R. So", "Quoc V. Le"], "date_published": "2020-03-06 19:00:04+00:00", "data_last_modified": "2020-06-30 04:32:44+00:00", "url": "http://arxiv.org/abs/2003.03384v2", "abstract": "Machine learning research has advanced in multiple aspects, including model\nstructures and learning methods. The effort to automate such research, known as\nAutoML, has also made significant progress. However, this progress has largely\nfocused on the architecture of neural networks, where it has relied on\nsophisticated expert-designed layers as building blocks---or similarly\nrestrictive search spaces. Our goal is to show that AutoML can go further: it\nis possible today to automatically discover complete machine learning\nalgorithms just using basic mathematical operations as building blocks. We\ndemonstrate this by introducing a novel framework that significantly reduces\nhuman bias through a generic search space. Despite the vastness of this space,\nevolutionary search can still discover two-layer neural networks trained by\nbackpropagation. These simple neural networks can then be surpassed by evolving\ndirectly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques\nemerge in the top algorithms, such as bilinear interactions, normalized\ngradients, and weight averaging. Moreover, evolution adapts algorithms to\ndifferent task types: e.g., dropout-like techniques appear when little data is\navailable. We believe these preliminary successes in discovering machine\nlearning algorithms from scratch indicate a promising new direction for the\nfield.", "author_comment": "Accepted for publication at the 37th International Conference on\n  Machine Learning (ICML 2020). Near camera-ready version", "journal_ref": null, "doi": null, "primary_category": "cs.LG", "categories": ["cs.LG", "cs.NE", "stat.ML", "I.2.2; I.2.6"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "1912.02757": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "1912.02757v2", "post_title": "Deep Ensembles: A Loss Landscape Perspective", "authors": ["Stanislav Fort", "Huiyi Hu", "Balaji Lakshminarayanan"], "date_published": "2019-12-05 17:48:18+00:00", "data_last_modified": "2020-06-25 03:57:04+00:00", "url": "http://arxiv.org/abs/1912.02757v2", "abstract": "Deep ensembles have been empirically shown to be a promising approach for\nimproving accuracy, uncertainty and out-of-distribution robustness of deep\nlearning models. While deep ensembles were theoretically motivated by the\nbootstrap, non-bootstrap ensembles trained with just random initialization also\nperform well in practice, which suggests that there could be other explanations\nfor why deep ensembles work well. Bayesian neural networks, which learn\ndistributions over the parameters of the network, are theoretically\nwell-motivated by Bayesian principles, but do not perform as well as deep\nensembles in practice, particularly under dataset shift. One possible\nexplanation for this gap between theory and practice is that popular scalable\nvariational Bayesian methods tend to focus on a single mode, whereas deep\nensembles tend to explore diverse modes in function space. We investigate this\nhypothesis by building on recent work on understanding the loss landscape of\nneural networks and adding our own exploration to measure the similarity of\nfunctions in the space of predictions. Our results show that random\ninitializations explore entirely different modes, while functions along an\noptimization trajectory or sampled from the subspace thereof cluster within a\nsingle mode predictions-wise, while often deviating significantly in the weight\nspace. Developing the concept of the diversity--accuracy plane, we show that\nthe decorrelation power of random initializations is unmatched by popular\nsubspace sampling methods. Finally, we evaluate the relative effects of\nensembling, subspace based methods and ensembles of subspace based methods, and\nthe experimental results validate our hypothesis.", "author_comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}, "2111.13872": {"source": "arxiv", "source_filetype": "latex", "converted_with": "pandoc", "paper_version": "2111.13872v1", "post_title": "Normative Disagreement as a Challenge for Cooperative AI", "authors": ["Julian Stastny", "Maxime Rich\u00e9", "Alexander Lyzhov", "Johannes Treutlein", "Allan Dafoe", "Jesse Clifton"], "date_published": "2021-11-27 11:37:42+00:00", "data_last_modified": "2021-11-27 11:37:42+00:00", "url": "http://arxiv.org/abs/2111.13872v1", "abstract": "Cooperation in settings where agents have both common and conflicting\ninterests (mixed-motive environments) has recently received considerable\nattention in multi-agent learning. However, the mixed-motive environments\ntypically studied have a single cooperative outcome on which all agents can\nagree. Many real-world multi-agent environments are instead bargaining problems\n(BPs): they have several Pareto-optimal payoff profiles over which agents have\nconflicting preferences. We argue that typical cooperation-inducing learning\nalgorithms fail to cooperate in BPs when there is room for normative\ndisagreement resulting in the existence of multiple competing cooperative\nequilibria, and illustrate this problem empirically. To remedy the issue, we\nintroduce the notion of norm-adaptive policies. Norm-adaptive policies are\ncapable of behaving according to different norms in different circumstances,\ncreating opportunities for resolving normative disagreement. We develop a class\nof norm-adaptive policies and show in experiments that these significantly\nincrease cooperation. However, norm-adaptiveness cannot address residual\nbargaining failure arising from a fundamental tradeoff between exploitability\nand cooperative robustness.", "author_comment": "Accepted at the Cooperative AI workshop and the Strategic ML workshop\n  at NeurIPS 2021", "journal_ref": null, "doi": null, "primary_category": "cs.MA", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG"], "citation_level": 0, "main_tex_filename": "", "text": "", "bibliography_bbl": "", "bibliography_bib": ""}}